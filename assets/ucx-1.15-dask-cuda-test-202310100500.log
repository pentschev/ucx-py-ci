============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-10 05:37:22,711 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:22,716 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43115 instead
  warnings.warn(
2023-10-10 05:37:22,720 - distributed.scheduler - INFO - State start
2023-10-10 05:37:22,742 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:22,742 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-10 05:37:22,743 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43115/status
2023-10-10 05:37:22,743 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:37:22,804 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45903'
2023-10-10 05:37:22,821 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40355'
2023-10-10 05:37:22,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35361'
2023-10-10 05:37:22,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40273'
2023-10-10 05:37:24,150 - distributed.scheduler - INFO - Receive client connection: Client-151ae144-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:24,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38838
2023-10-10 05:37:24,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:24,455 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:24,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:24,511 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:24,511 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:24,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:24,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:24,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:24,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:24,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:24,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:24,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-10 05:37:24,846 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34279
2023-10-10 05:37:24,847 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34279
2023-10-10 05:37:24,847 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42035
2023-10-10 05:37:24,847 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-10 05:37:24,847 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:24,847 - distributed.worker - INFO -               Threads:                          4
2023-10-10 05:37:24,847 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-10 05:37:24,847 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-2oa4y4me
2023-10-10 05:37:24,848 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bee656d8-d0a1-4983-8802-9a8cac473cb0
2023-10-10 05:37:24,849 - distributed.worker - INFO - Starting Worker plugin PreImport-964d5c31-d8a2-4376-8d56-3731a1ff3747
2023-10-10 05:37:24,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-815693dd-3ae9-435b-8ed6-e649280c01c8
2023-10-10 05:37:24,849 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:25,509 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34279', status: init, memory: 0, processing: 0>
2023-10-10 05:37:25,511 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34279
2023-10-10 05:37:25,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38854
2023-10-10 05:37:25,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:25,513 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-10 05:37:25,513 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:25,514 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-10 05:37:26,305 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38921
2023-10-10 05:37:26,306 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38921
2023-10-10 05:37:26,306 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38243
2023-10-10 05:37:26,306 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-10 05:37:26,306 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,306 - distributed.worker - INFO -               Threads:                          4
2023-10-10 05:37:26,306 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-10 05:37:26,307 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-pj_4odps
2023-10-10 05:37:26,307 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b30842a-b38d-4c3c-85bb-ae1095a1f548
2023-10-10 05:37:26,311 - distributed.worker - INFO - Starting Worker plugin PreImport-9bfde993-e5d3-491a-b8d7-9c9a496d746b
2023-10-10 05:37:26,312 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2de59f9-4f22-4d00-8465-96285d864aae
2023-10-10 05:37:26,313 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,348 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38921', status: init, memory: 0, processing: 0>
2023-10-10 05:37:26,349 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38921
2023-10-10 05:37:26,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38864
2023-10-10 05:37:26,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:26,352 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-10 05:37:26,352 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-10 05:37:26,462 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37887
2023-10-10 05:37:26,463 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37887
2023-10-10 05:37:26,463 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40217
2023-10-10 05:37:26,463 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-10 05:37:26,463 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,462 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35193
2023-10-10 05:37:26,463 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35193
2023-10-10 05:37:26,463 - distributed.worker - INFO -               Threads:                          4
2023-10-10 05:37:26,463 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33403
2023-10-10 05:37:26,463 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-10 05:37:26,463 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-10 05:37:26,463 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-1lqh2xf8
2023-10-10 05:37:26,463 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,463 - distributed.worker - INFO -               Threads:                          4
2023-10-10 05:37:26,463 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-10 05:37:26,463 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-edmg4s8l
2023-10-10 05:37:26,464 - distributed.worker - INFO - Starting Worker plugin PreImport-59f400be-5352-4a66-8638-68cd888f8c45
2023-10-10 05:37:26,464 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dbcd78da-20ca-445a-b23b-a81e27a6e579
2023-10-10 05:37:26,464 - distributed.worker - INFO - Starting Worker plugin PreImport-e59e3e46-1ee3-4933-b42a-d34461f304c7
2023-10-10 05:37:26,464 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38e1716a-23bb-45b6-93b9-a710c4b7c2a1
2023-10-10 05:37:26,464 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b635f619-e815-4263-a794-f833854ba11c
2023-10-10 05:37:26,464 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-047c39a6-5840-4270-b89c-863378a5bbc1
2023-10-10 05:37:26,464 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,467 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,494 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35193', status: init, memory: 0, processing: 0>
2023-10-10 05:37:26,495 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35193
2023-10-10 05:37:26,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38884
2023-10-10 05:37:26,496 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37887', status: init, memory: 0, processing: 0>
2023-10-10 05:37:26,497 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37887
2023-10-10 05:37:26,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38876
2023-10-10 05:37:26,497 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:26,498 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-10 05:37:26,498 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,498 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:26,499 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-10 05:37:26,499 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:26,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-10 05:37:26,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-10 05:37:26,543 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-10 05:37:26,543 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-10 05:37:26,543 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-10 05:37:26,552 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-10 05:37:26,556 - distributed.scheduler - INFO - Remove client Client-151ae144-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:26,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38838; closing.
2023-10-10 05:37:26,556 - distributed.scheduler - INFO - Remove client Client-151ae144-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:26,557 - distributed.scheduler - INFO - Close client connection: Client-151ae144-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:26,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45903'. Reason: nanny-close
2023-10-10 05:37:26,558 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:26,559 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40355'. Reason: nanny-close
2023-10-10 05:37:26,559 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:26,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37887. Reason: nanny-close
2023-10-10 05:37:26,559 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35361'. Reason: nanny-close
2023-10-10 05:37:26,560 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:26,560 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38921. Reason: nanny-close
2023-10-10 05:37:26,560 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40273'. Reason: nanny-close
2023-10-10 05:37:26,560 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:26,561 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35193. Reason: nanny-close
2023-10-10 05:37:26,561 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34279. Reason: nanny-close
2023-10-10 05:37:26,562 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-10 05:37:26,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38876; closing.
2023-10-10 05:37:26,562 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37887', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916246.562827')
2023-10-10 05:37:26,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-10 05:37:26,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-10 05:37:26,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-10 05:37:26,563 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38884; closing.
2023-10-10 05:37:26,564 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:26,564 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35193', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916246.564571')
2023-10-10 05:37:26,564 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:26,564 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:26,564 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38864; closing.
2023-10-10 05:37:26,565 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38854; closing.
2023-10-10 05:37:26,565 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:26,565 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:38884>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:38884>: Stream is closed
2023-10-10 05:37:26,567 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38921', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916246.5672433')
2023-10-10 05:37:26,567 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34279', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916246.5677757')
2023-10-10 05:37:26,567 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:37:27,725 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:37:27,725 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:37:27,726 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:37:27,727 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-10 05:37:27,728 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-10 05:37:29,811 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:29,815 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-10 05:37:29,820 - distributed.scheduler - INFO - State start
2023-10-10 05:37:29,841 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:29,842 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:37:29,842 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-10 05:37:29,843 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:37:29,919 - distributed.scheduler - INFO - Receive client connection: Client-19568da6-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:29,930 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53220
2023-10-10 05:37:29,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38735'
2023-10-10 05:37:29,977 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42927'
2023-10-10 05:37:29,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42307'
2023-10-10 05:37:29,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45737'
2023-10-10 05:37:30,001 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35391'
2023-10-10 05:37:30,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41507'
2023-10-10 05:37:30,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33949'
2023-10-10 05:37:30,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41887'
2023-10-10 05:37:31,941 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:31,941 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:31,941 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:31,941 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:31,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:31,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:31,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:31,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:31,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:31,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:31,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:31,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:31,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:31,963 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:31,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:31,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:31,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:31,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:31,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:31,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:31,983 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:31,983 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:31,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:31,987 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:34,779 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38023
2023-10-10 05:37:34,780 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38023
2023-10-10 05:37:34,780 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46775
2023-10-10 05:37:34,780 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:34,780 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,780 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:34,780 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:34,780 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lvtpbq9n
2023-10-10 05:37:34,781 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d121ff9e-1c60-4dd3-8012-0b9cbfbb4e0b
2023-10-10 05:37:34,785 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34851
2023-10-10 05:37:34,786 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34851
2023-10-10 05:37:34,786 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43439
2023-10-10 05:37:34,786 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:34,786 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,786 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:34,786 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:34,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-25yd1vs6
2023-10-10 05:37:34,787 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e966c97-d1d5-4668-9b22-676908c9e221
2023-10-10 05:37:34,786 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43157
2023-10-10 05:37:34,787 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43157
2023-10-10 05:37:34,787 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42901
2023-10-10 05:37:34,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:34,787 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,787 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:34,787 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:34,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h74pe5ii
2023-10-10 05:37:34,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8cdfd468-2842-44d0-96c9-ae547d1e4b75
2023-10-10 05:37:34,849 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42671
2023-10-10 05:37:34,850 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42671
2023-10-10 05:37:34,850 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42783
2023-10-10 05:37:34,850 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:34,850 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,850 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:34,850 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:34,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7x0_k0ht
2023-10-10 05:37:34,851 - distributed.worker - INFO - Starting Worker plugin PreImport-9a40c106-dbf9-45bd-b09f-f4c1451cc25e
2023-10-10 05:37:34,851 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-888b18af-55a4-4e08-86b0-7d49713c631a
2023-10-10 05:37:34,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2720ea23-c614-4a67-a9cb-fcfe0bf1015b
2023-10-10 05:37:34,854 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46291
2023-10-10 05:37:34,855 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46291
2023-10-10 05:37:34,854 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37633
2023-10-10 05:37:34,855 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37633
2023-10-10 05:37:34,855 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46307
2023-10-10 05:37:34,855 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:34,855 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36195
2023-10-10 05:37:34,855 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,854 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36537
2023-10-10 05:37:34,855 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:34,855 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36537
2023-10-10 05:37:34,855 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,855 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35779
2023-10-10 05:37:34,855 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:34,855 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46851
2023-10-10 05:37:34,855 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35779
2023-10-10 05:37:34,855 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:34,855 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:34,855 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:34,855 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40363
2023-10-10 05:37:34,856 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,856 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ohimnd4
2023-10-10 05:37:34,856 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:34,856 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:34,856 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jt5akl_l
2023-10-10 05:37:34,856 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,856 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:34,856 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:34,856 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:34,856 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_osprlo8
2023-10-10 05:37:34,856 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:34,856 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xjjykwre
2023-10-10 05:37:34,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99afd54f-f256-44f4-8f48-4f1b2b116710
2023-10-10 05:37:34,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-82218551-3c45-4f2b-bc81-b7e585d17e51
2023-10-10 05:37:34,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b49ff3f2-20d6-46e3-b0b3-cde9d285f226
2023-10-10 05:37:34,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33475684-7c0a-41c5-b5be-0396850a0284
2023-10-10 05:37:34,981 - distributed.worker - INFO - Starting Worker plugin PreImport-c1e4098b-f267-4a9d-9122-164cd133c8f3
2023-10-10 05:37:34,981 - distributed.worker - INFO - Starting Worker plugin PreImport-99cc1e3e-968c-40e1-b149-12e7734e458c
2023-10-10 05:37:34,981 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,981 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6553186f-42f1-47ef-a568-2a5071ee1966
2023-10-10 05:37:34,981 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13d24694-4c2a-4e9d-b35f-d16597652b43
2023-10-10 05:37:34,981 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55e01d62-680f-4c6a-98db-997a75f6193a
2023-10-10 05:37:34,981 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,981 - distributed.worker - INFO - Starting Worker plugin PreImport-7a7be1cf-cc40-4cf4-950f-b09736e566e1
2023-10-10 05:37:34,982 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,982 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:34,987 - distributed.worker - INFO - Starting Worker plugin PreImport-cc8e7d94-1206-4071-8797-475651f37270
2023-10-10 05:37:34,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6f93884-04c9-4da5-bd1d-536aa86ce508
2023-10-10 05:37:34,990 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cc3536c-7d4b-4a96-80e3-4de1916198c8
2023-10-10 05:37:35,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58c3a9b2-afcd-47fa-a868-2245b3cd115b
2023-10-10 05:37:35,013 - distributed.worker - INFO - Starting Worker plugin PreImport-8c2e0ed5-77f3-4cb5-a1d2-0e20eac02c7a
2023-10-10 05:37:35,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3861ae74-ce41-428c-82b1-83be52153819
2023-10-10 05:37:35,013 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,013 - distributed.worker - INFO - Starting Worker plugin PreImport-b3faecf6-d688-4326-afaa-12adb653d5ff
2023-10-10 05:37:35,014 - distributed.worker - INFO - Starting Worker plugin PreImport-841749d3-5159-4850-b1de-6fccb5884803
2023-10-10 05:37:35,014 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,014 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,015 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38023', status: init, memory: 0, processing: 0>
2023-10-10 05:37:35,016 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38023
2023-10-10 05:37:35,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53348
2023-10-10 05:37:35,016 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42671', status: init, memory: 0, processing: 0>
2023-10-10 05:37:35,017 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:35,017 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42671
2023-10-10 05:37:35,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53320
2023-10-10 05:37:35,018 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:35,018 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:35,019 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34851', status: init, memory: 0, processing: 0>
2023-10-10 05:37:35,019 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34851
2023-10-10 05:37:35,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53336
2023-10-10 05:37:35,019 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:35,019 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:35,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:35,021 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:35,021 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:35,022 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,021 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43157', status: init, memory: 0, processing: 0>
2023-10-10 05:37:35,022 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43157
2023-10-10 05:37:35,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53354
2023-10-10 05:37:35,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:35,024 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:35,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46291', status: init, memory: 0, processing: 0>
2023-10-10 05:37:35,024 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46291
2023-10-10 05:37:35,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53362
2023-10-10 05:37:35,025 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:35,025 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,026 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:35,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:35,027 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:35,028 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:35,041 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35779', status: init, memory: 0, processing: 0>
2023-10-10 05:37:35,042 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35779
2023-10-10 05:37:35,042 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53370
2023-10-10 05:37:35,043 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:35,044 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:35,044 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,046 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:35,048 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37633', status: init, memory: 0, processing: 0>
2023-10-10 05:37:35,048 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37633
2023-10-10 05:37:35,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53396
2023-10-10 05:37:35,049 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36537', status: init, memory: 0, processing: 0>
2023-10-10 05:37:35,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36537
2023-10-10 05:37:35,050 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53386
2023-10-10 05:37:35,050 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:35,051 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:35,051 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,051 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:35,053 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:35,053 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:35,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:35,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:35,137 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:35,137 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:35,137 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:35,137 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:35,137 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:35,137 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:35,138 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:35,138 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:35,142 - distributed.scheduler - INFO - Remove client Client-19568da6-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:35,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53220; closing.
2023-10-10 05:37:35,142 - distributed.scheduler - INFO - Remove client Client-19568da6-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:35,143 - distributed.scheduler - INFO - Close client connection: Client-19568da6-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:35,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38735'. Reason: nanny-close
2023-10-10 05:37:35,145 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:35,146 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42927'. Reason: nanny-close
2023-10-10 05:37:35,146 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:35,146 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46291. Reason: nanny-close
2023-10-10 05:37:35,147 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42307'. Reason: nanny-close
2023-10-10 05:37:35,147 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:35,147 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43157. Reason: nanny-close
2023-10-10 05:37:35,148 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45737'. Reason: nanny-close
2023-10-10 05:37:35,148 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38023. Reason: nanny-close
2023-10-10 05:37:35,148 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:35,149 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53362; closing.
2023-10-10 05:37:35,149 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:35,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35391'. Reason: nanny-close
2023-10-10 05:37:35,149 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42671. Reason: nanny-close
2023-10-10 05:37:35,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46291', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916255.1494815')
2023-10-10 05:37:35,149 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:35,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41507'. Reason: nanny-close
2023-10-10 05:37:35,150 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:35,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:35,150 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37633. Reason: nanny-close
2023-10-10 05:37:35,150 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53354; closing.
2023-10-10 05:37:35,150 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:35,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33949'. Reason: nanny-close
2023-10-10 05:37:35,151 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:35,151 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:35,151 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:35,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36537. Reason: nanny-close
2023-10-10 05:37:35,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43157', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916255.1513565')
2023-10-10 05:37:35,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41887'. Reason: nanny-close
2023-10-10 05:37:35,151 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:35,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35779. Reason: nanny-close
2023-10-10 05:37:35,152 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:35,152 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:35,152 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34851. Reason: nanny-close
2023-10-10 05:37:35,152 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:35,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:35,152 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53354>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53354>: Stream is closed
2023-10-10 05:37:35,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:35,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53348; closing.
2023-10-10 05:37:35,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:35,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916255.1540685')
2023-10-10 05:37:35,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:35,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53320; closing.
2023-10-10 05:37:35,154 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:35,155 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:35,155 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42671', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916255.1549778')
2023-10-10 05:37:35,155 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53396; closing.
2023-10-10 05:37:35,155 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:35,155 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:35,156 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916255.1560538')
2023-10-10 05:37:35,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53370; closing.
2023-10-10 05:37:35,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53386; closing.
2023-10-10 05:37:35,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53336; closing.
2023-10-10 05:37:35,157 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35779', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916255.1571279')
2023-10-10 05:37:35,157 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36537', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916255.1575596')
2023-10-10 05:37:35,157 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916255.1579244')
2023-10-10 05:37:35,158 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:37:36,611 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:37:36,611 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:37:36,612 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:37:36,613 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:37:36,613 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-10 05:37:38,631 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:38,636 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-10 05:37:38,639 - distributed.scheduler - INFO - State start
2023-10-10 05:37:38,659 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:38,660 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:37:38,661 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-10 05:37:38,661 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:37:38,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36405'
2023-10-10 05:37:38,871 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46801'
2023-10-10 05:37:38,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43109'
2023-10-10 05:37:38,886 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43863'
2023-10-10 05:37:38,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39329'
2023-10-10 05:37:38,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42097'
2023-10-10 05:37:38,913 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41503'
2023-10-10 05:37:38,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37705'
2023-10-10 05:37:40,103 - distributed.scheduler - INFO - Receive client connection: Client-1ea0282f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:40,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43020
2023-10-10 05:37:40,738 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:40,738 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:40,738 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:40,738 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:40,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:40,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:40,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:40,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:40,742 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:40,743 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:40,743 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:40,743 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:40,794 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:40,794 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:40,798 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:40,798 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:40,799 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:40,802 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:40,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:40,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:40,819 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:41,510 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:41,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:41,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:45,619 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41071
2023-10-10 05:37:45,620 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41071
2023-10-10 05:37:45,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40053
2023-10-10 05:37:45,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,620 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,620 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:45,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:45,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oq0k05hz
2023-10-10 05:37:45,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-069b5df1-bf3e-4c61-9a89-71f5bcc72550
2023-10-10 05:37:45,621 - distributed.worker - INFO - Starting Worker plugin PreImport-4aa42f78-6681-46b9-b167-6a4ac28cc9bc
2023-10-10 05:37:45,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a21458a-0a18-4fac-9ab2-3da804a71592
2023-10-10 05:37:45,628 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,661 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41071', status: init, memory: 0, processing: 0>
2023-10-10 05:37:45,663 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41071
2023-10-10 05:37:45,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43028
2023-10-10 05:37:45,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:45,665 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,665 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:45,783 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38451
2023-10-10 05:37:45,784 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38451
2023-10-10 05:37:45,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36477
2023-10-10 05:37:45,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,784 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,784 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:45,784 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:45,784 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ilf1scv4
2023-10-10 05:37:45,785 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a42fab0-cdf2-4e24-9554-c84ecf2307f4
2023-10-10 05:37:45,785 - distributed.worker - INFO - Starting Worker plugin PreImport-cfcae0ab-115d-4dd2-9066-ada06db37e76
2023-10-10 05:37:45,785 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50935f31-b847-4b4a-82b8-7cd386e7ddad
2023-10-10 05:37:45,850 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41833
2023-10-10 05:37:45,852 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41833
2023-10-10 05:37:45,852 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33899
2023-10-10 05:37:45,852 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,852 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,852 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:45,852 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:45,852 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-foccknd9
2023-10-10 05:37:45,853 - distributed.worker - INFO - Starting Worker plugin PreImport-bb637799-5400-4ab6-a831-56599fc1cfa7
2023-10-10 05:37:45,853 - distributed.worker - INFO - Starting Worker plugin RMMSetup-848f3bf1-c596-46d1-81c3-6375ff612e60
2023-10-10 05:37:45,856 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46125
2023-10-10 05:37:45,857 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46125
2023-10-10 05:37:45,857 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37993
2023-10-10 05:37:45,857 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,857 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,857 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:45,857 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:45,857 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rlw9_1fb
2023-10-10 05:37:45,857 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45829
2023-10-10 05:37:45,858 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45829
2023-10-10 05:37:45,858 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39497
2023-10-10 05:37:45,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c49fb95-0163-4e38-9954-230eb3e93a60
2023-10-10 05:37:45,858 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,858 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,858 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:45,858 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:45,858 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ovb19n04
2023-10-10 05:37:45,858 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c6cb326-06ce-42aa-8fa0-dbaf14ef7717
2023-10-10 05:37:45,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d412da18-f4f8-45bf-b03e-ab524fa45131
2023-10-10 05:37:45,859 - distributed.worker - INFO - Starting Worker plugin PreImport-fb9a4e56-3a4d-414d-852f-c84b0704199a
2023-10-10 05:37:45,859 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35c309d3-9989-4cb4-8ecf-e700387d96f7
2023-10-10 05:37:45,858 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46515
2023-10-10 05:37:45,859 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46515
2023-10-10 05:37:45,859 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35907
2023-10-10 05:37:45,859 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,859 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,859 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:45,859 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:45,859 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kqopbt06
2023-10-10 05:37:45,860 - distributed.worker - INFO - Starting Worker plugin PreImport-b941ce8d-6a41-45e0-b11e-63a67fbd61a0
2023-10-10 05:37:45,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32416eea-5260-456e-a7cd-64ef7bfe5b57
2023-10-10 05:37:45,870 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2573a71b-2ad7-47f0-a91d-88bc04b7da66
2023-10-10 05:37:45,869 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37195
2023-10-10 05:37:45,870 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37195
2023-10-10 05:37:45,870 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37039
2023-10-10 05:37:45,870 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37039
2023-10-10 05:37:45,871 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32925
2023-10-10 05:37:45,871 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36399
2023-10-10 05:37:45,871 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,871 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,871 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,871 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,871 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:45,871 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:45,871 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:45,871 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:45,871 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ksm4a7u7
2023-10-10 05:37:45,871 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-06yrwdmz
2023-10-10 05:37:45,871 - distributed.worker - INFO - Starting Worker plugin PreImport-a0bc427d-71e8-43bd-aac6-0fab36f06995
2023-10-10 05:37:45,871 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-66627c61-147d-4996-aa7b-c3c9cf611ffd
2023-10-10 05:37:45,872 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-483d24e5-4a11-4929-af0a-23d49191fc28
2023-10-10 05:37:45,872 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3ef9af0-151c-424c-961a-655beebab769
2023-10-10 05:37:45,873 - distributed.worker - INFO - Starting Worker plugin PreImport-b7da8129-cc21-48f3-9f1f-337209b0405b
2023-10-10 05:37:45,873 - distributed.worker - INFO - Starting Worker plugin RMMSetup-022cfb6e-0cfc-4f23-84fe-eaa3e7a5cf09
2023-10-10 05:37:45,878 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,891 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,891 - distributed.worker - INFO - Starting Worker plugin PreImport-52db5220-ce9c-4e39-8075-ac6119abf168
2023-10-10 05:37:45,892 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,898 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38451', status: init, memory: 0, processing: 0>
2023-10-10 05:37:45,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38451
2023-10-10 05:37:45,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43042
2023-10-10 05:37:45,899 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-555ebaee-673f-4cf4-8e49-ec98a437566c
2023-10-10 05:37:45,899 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:45,900 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,901 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:45,903 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,903 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,903 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,911 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45829', status: init, memory: 0, processing: 0>
2023-10-10 05:37:45,912 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45829
2023-10-10 05:37:45,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43048
2023-10-10 05:37:45,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:45,914 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,914 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:45,922 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37195', status: init, memory: 0, processing: 0>
2023-10-10 05:37:45,923 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37195
2023-10-10 05:37:45,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43076
2023-10-10 05:37:45,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:45,924 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,924 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,925 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41833', status: init, memory: 0, processing: 0>
2023-10-10 05:37:45,926 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41833
2023-10-10 05:37:45,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43064
2023-10-10 05:37:45,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:45,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:45,928 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,928 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:45,930 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46125', status: init, memory: 0, processing: 0>
2023-10-10 05:37:45,931 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46125
2023-10-10 05:37:45,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43062
2023-10-10 05:37:45,932 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37039', status: init, memory: 0, processing: 0>
2023-10-10 05:37:45,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:45,933 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37039
2023-10-10 05:37:45,933 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43100
2023-10-10 05:37:45,933 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,933 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:45,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:45,936 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,936 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,938 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46515', status: init, memory: 0, processing: 0>
2023-10-10 05:37:45,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:45,939 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46515
2023-10-10 05:37:45,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43084
2023-10-10 05:37:45,940 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:45,941 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:45,942 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:45,944 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:45,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:45,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:45,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:45,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:45,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:45,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:45,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:45,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:45,975 - distributed.scheduler - INFO - Remove client Client-1ea0282f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:45,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43020; closing.
2023-10-10 05:37:45,976 - distributed.scheduler - INFO - Remove client Client-1ea0282f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:45,976 - distributed.scheduler - INFO - Close client connection: Client-1ea0282f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:45,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36405'. Reason: nanny-close
2023-10-10 05:37:45,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46801'. Reason: nanny-close
2023-10-10 05:37:45,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43109'. Reason: nanny-close
2023-10-10 05:37:45,978 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:45,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43863'. Reason: nanny-close
2023-10-10 05:37:45,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:45,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38451. Reason: nanny-close
2023-10-10 05:37:45,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39329'. Reason: nanny-close
2023-10-10 05:37:45,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:45,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45829. Reason: nanny-close
2023-10-10 05:37:45,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42097'. Reason: nanny-close
2023-10-10 05:37:45,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:45,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41071. Reason: nanny-close
2023-10-10 05:37:45,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41503'. Reason: nanny-close
2023-10-10 05:37:45,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:45,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:45,981 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43042; closing.
2023-10-10 05:37:45,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37705'. Reason: nanny-close
2023-10-10 05:37:45,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916265.9817264')
2023-10-10 05:37:45,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46515. Reason: nanny-close
2023-10-10 05:37:45,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:45,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:45,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37195. Reason: nanny-close
2023-10-10 05:37:45,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41833. Reason: nanny-close
2023-10-10 05:37:45,982 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:45,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43048; closing.
2023-10-10 05:37:45,983 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:45,983 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45829', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916265.9839096')
2023-10-10 05:37:45,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:45,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:45,984 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:45,984 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43028; closing.
2023-10-10 05:37:45,984 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:45,985 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:45,985 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:45,985 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46125. Reason: nanny-close
2023-10-10 05:37:45,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916265.985382')
2023-10-10 05:37:45,985 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:45,985 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43076; closing.
2023-10-10 05:37:45,985 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37039. Reason: nanny-close
2023-10-10 05:37:45,985 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43084; closing.
2023-10-10 05:37:45,986 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:45,986 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:45,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37195', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916265.98709')
2023-10-10 05:37:45,987 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:45,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46515', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916265.9874213')
2023-10-10 05:37:45,987 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:45,987 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43064; closing.
2023-10-10 05:37:45,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916265.9883282')
2023-10-10 05:37:45,988 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:45,989 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43062; closing.
2023-10-10 05:37:45,989 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43100; closing.
2023-10-10 05:37:45,989 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:45,989 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46125', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916265.9898734')
2023-10-10 05:37:45,990 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916265.9901803')
2023-10-10 05:37:45,990 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:37:45,991 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:45,990 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43100>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-10 05:37:45,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43062>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-10 05:37:47,494 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:37:47,495 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:37:47,495 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:37:47,496 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:37:47,497 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-10 05:37:49,564 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:49,569 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43257 instead
  warnings.warn(
2023-10-10 05:37:49,573 - distributed.scheduler - INFO - State start
2023-10-10 05:37:49,594 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:49,595 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:37:49,596 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43257/status
2023-10-10 05:37:49,596 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:37:49,793 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42025'
2023-10-10 05:37:49,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44693'
2023-10-10 05:37:49,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41529'
2023-10-10 05:37:49,836 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43825'
2023-10-10 05:37:49,837 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33177'
2023-10-10 05:37:49,847 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42189'
2023-10-10 05:37:49,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41933'
2023-10-10 05:37:49,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40867'
2023-10-10 05:37:51,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:51,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:51,562 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:51,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:51,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:51,850 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:51,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:51,858 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:51,858 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:51,858 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:51,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:51,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:51,861 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:51,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:51,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:51,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:51,862 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:51,863 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:51,863 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:37:51,863 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:37:51,865 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:51,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:51,867 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:51,868 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:37:53,095 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45791
2023-10-10 05:37:53,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45791
2023-10-10 05:37:53,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43759
2023-10-10 05:37:53,096 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:53,096 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:53,096 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:53,096 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:53,096 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pe3pe4v8
2023-10-10 05:37:53,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04c56e80-f114-4a5c-94fa-4f82bd90e6b5
2023-10-10 05:37:53,716 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1968a133-fb70-4264-82b4-20eacd2809ba
2023-10-10 05:37:53,717 - distributed.worker - INFO - Starting Worker plugin PreImport-206dbc36-2161-4dc4-b611-9c7f505a2375
2023-10-10 05:37:53,718 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:53,761 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45791', status: init, memory: 0, processing: 0>
2023-10-10 05:37:53,772 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45791
2023-10-10 05:37:53,772 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60738
2023-10-10 05:37:53,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:53,776 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:53,776 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:53,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:54,257 - distributed.scheduler - INFO - Receive client connection: Client-2518d427-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:54,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60750
2023-10-10 05:37:54,405 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45825
2023-10-10 05:37:54,405 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45825
2023-10-10 05:37:54,405 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36215
2023-10-10 05:37:54,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,406 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,406 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:54,406 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:54,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v55lxk35
2023-10-10 05:37:54,405 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40345
2023-10-10 05:37:54,406 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40345
2023-10-10 05:37:54,406 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41507
2023-10-10 05:37:54,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,406 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea53b985-f677-4181-8ef5-2d7aa962ff2b
2023-10-10 05:37:54,406 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:54,407 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:54,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gcjg9azf
2023-10-10 05:37:54,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c135245-5518-45a0-aed0-f8f1aedd0478
2023-10-10 05:37:54,407 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36665
2023-10-10 05:37:54,408 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36665
2023-10-10 05:37:54,408 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40143
2023-10-10 05:37:54,408 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,408 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,408 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:54,408 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:54,408 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zo2d2nez
2023-10-10 05:37:54,409 - distributed.worker - INFO - Starting Worker plugin PreImport-cf3ca4c0-bf1a-4650-a2b3-ce2b7db45881
2023-10-10 05:37:54,409 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fed7ed99-5a2c-44f3-8883-30bb6d4a78d4
2023-10-10 05:37:54,409 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6949357-ddb0-454d-9250-3c467bdf9f25
2023-10-10 05:37:54,438 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39581
2023-10-10 05:37:54,439 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39581
2023-10-10 05:37:54,439 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39625
2023-10-10 05:37:54,439 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,439 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,439 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:54,439 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:54,439 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-49_gp02l
2023-10-10 05:37:54,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21e33a79-8924-4fc7-acaa-3b365bf514c3
2023-10-10 05:37:54,439 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33721
2023-10-10 05:37:54,440 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33721
2023-10-10 05:37:54,440 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33447
2023-10-10 05:37:54,440 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,440 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,440 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:54,440 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:54,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4doxa2ic
2023-10-10 05:37:54,441 - distributed.worker - INFO - Starting Worker plugin PreImport-5b1cf013-618b-46d6-be95-1b8a677fc907
2023-10-10 05:37:54,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f012a0d9-1e5d-4a2f-9a83-caf511238508
2023-10-10 05:37:54,441 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d40223f-d7ba-402b-b96f-f914b576ab64
2023-10-10 05:37:54,508 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34953
2023-10-10 05:37:54,509 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34953
2023-10-10 05:37:54,509 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45133
2023-10-10 05:37:54,509 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,509 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,509 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:54,509 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:54,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y39fpt8y
2023-10-10 05:37:54,510 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea267b14-204a-48ad-96c1-f6bb3dcdf930
2023-10-10 05:37:54,517 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42991
2023-10-10 05:37:54,518 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42991
2023-10-10 05:37:54,518 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38411
2023-10-10 05:37:54,518 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,518 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,518 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:37:54,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:37:54,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-49ffl33r
2023-10-10 05:37:54,519 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a03430ec-8ef2-4d87-9f9f-687b10cedba0
2023-10-10 05:37:54,783 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d5bd75c-4562-4dbf-9af1-bd67c323808e
2023-10-10 05:37:54,783 - distributed.worker - INFO - Starting Worker plugin PreImport-5c114473-d207-47bb-bff4-98a679b4a6a1
2023-10-10 05:37:54,784 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,792 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,810 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45825', status: init, memory: 0, processing: 0>
2023-10-10 05:37:54,814 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6961ab5-988b-4f02-9e95-8a78e321ecd7
2023-10-10 05:37:54,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45825
2023-10-10 05:37:54,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60772
2023-10-10 05:37:54,815 - distributed.worker - INFO - Starting Worker plugin PreImport-8b75c385-d691-4452-aef2-d7e1d3d64723
2023-10-10 05:37:54,815 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:54,816 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,816 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,816 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,818 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36665', status: init, memory: 0, processing: 0>
2023-10-10 05:37:54,818 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36665
2023-10-10 05:37:54,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60788
2023-10-10 05:37:54,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:54,819 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:54,820 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,820 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:54,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7eba29a5-3035-4bdd-9dc0-710f33e576ab
2023-10-10 05:37:54,829 - distributed.worker - INFO - Starting Worker plugin PreImport-1f6c29c8-bee2-4c09-a1e7-8c0a6d0a6619
2023-10-10 05:37:54,829 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,840 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33721', status: init, memory: 0, processing: 0>
2023-10-10 05:37:54,841 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33721
2023-10-10 05:37:54,841 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60796
2023-10-10 05:37:54,842 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:54,843 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,843 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:54,851 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40345', status: init, memory: 0, processing: 0>
2023-10-10 05:37:54,852 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40345
2023-10-10 05:37:54,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60810
2023-10-10 05:37:54,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:54,854 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,854 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:54,862 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39581', status: init, memory: 0, processing: 0>
2023-10-10 05:37:54,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39581
2023-10-10 05:37:54,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60812
2023-10-10 05:37:54,864 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:54,865 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,865 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,867 - distributed.worker - INFO - Starting Worker plugin PreImport-9d98d587-1178-4991-ac61-9898aadd112b
2023-10-10 05:37:54,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a74b5cb-cb72-479a-b63e-9a41ee3b9e16
2023-10-10 05:37:54,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:54,867 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-abc87f97-e257-44ba-a6f2-50b220098c1e
2023-10-10 05:37:54,874 - distributed.worker - INFO - Starting Worker plugin PreImport-b54b45fe-d205-4a3a-849f-4223e26cd065
2023-10-10 05:37:54,875 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,892 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34953', status: init, memory: 0, processing: 0>
2023-10-10 05:37:54,893 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34953
2023-10-10 05:37:54,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60816
2023-10-10 05:37:54,894 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:54,895 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,895 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:54,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42991', status: init, memory: 0, processing: 0>
2023-10-10 05:37:54,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42991
2023-10-10 05:37:54,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60822
2023-10-10 05:37:54,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:37:54,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:37:54,901 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:37:54,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:37:54,941 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:54,941 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:54,941 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:54,941 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:54,941 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:54,942 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:54,942 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:54,942 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:37:54,951 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:37:54,952 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:37:54,952 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:37:54,952 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:37:54,952 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:37:54,952 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:37:54,952 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:37:54,952 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:37:54,958 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:37:54,960 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:37:54,962 - distributed.scheduler - INFO - Remove client Client-2518d427-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:54,962 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60750; closing.
2023-10-10 05:37:54,962 - distributed.scheduler - INFO - Remove client Client-2518d427-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:54,963 - distributed.scheduler - INFO - Close client connection: Client-2518d427-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:54,964 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42025'. Reason: nanny-close
2023-10-10 05:37:54,964 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:54,965 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44693'. Reason: nanny-close
2023-10-10 05:37:54,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:54,966 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34953. Reason: nanny-close
2023-10-10 05:37:54,966 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41529'. Reason: nanny-close
2023-10-10 05:37:54,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:54,966 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42991. Reason: nanny-close
2023-10-10 05:37:54,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43825'. Reason: nanny-close
2023-10-10 05:37:54,967 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:54,967 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45825. Reason: nanny-close
2023-10-10 05:37:54,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33177'. Reason: nanny-close
2023-10-10 05:37:54,968 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:54,968 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60816; closing.
2023-10-10 05:37:54,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39581. Reason: nanny-close
2023-10-10 05:37:54,968 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:54,968 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42189'. Reason: nanny-close
2023-10-10 05:37:54,968 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34953', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916274.968526')
2023-10-10 05:37:54,968 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:54,968 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:54,969 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45791. Reason: nanny-close
2023-10-10 05:37:54,969 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41933'. Reason: nanny-close
2023-10-10 05:37:54,969 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:54,969 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36665. Reason: nanny-close
2023-10-10 05:37:54,969 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40867'. Reason: nanny-close
2023-10-10 05:37:54,969 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:37:54,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:54,969 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:54,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60822; closing.
2023-10-10 05:37:54,970 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:54,970 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33721. Reason: nanny-close
2023-10-10 05:37:54,970 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:54,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916274.9708953')
2023-10-10 05:37:54,971 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40345. Reason: nanny-close
2023-10-10 05:37:54,971 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60772; closing.
2023-10-10 05:37:54,971 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:54,971 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:54,971 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:54,972 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45825', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916274.9719417')
2023-10-10 05:37:54,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60812; closing.
2023-10-10 05:37:54,972 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:54,972 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:54,973 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:37:54,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916274.9731426')
2023-10-10 05:37:54,973 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:54,973 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:54,973 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60738; closing.
2023-10-10 05:37:54,973 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60788; closing.
2023-10-10 05:37:54,974 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45791', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916274.9742706')
2023-10-10 05:37:54,974 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:54,974 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36665', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916274.9747505')
2023-10-10 05:37:54,974 - distributed.nanny - INFO - Worker closed
2023-10-10 05:37:54,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60796; closing.
2023-10-10 05:37:54,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60810; closing.
2023-10-10 05:37:54,975 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916274.975693')
2023-10-10 05:37:54,976 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40345', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916274.9760857')
2023-10-10 05:37:54,976 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:37:56,681 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:37:56,681 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:37:56,682 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:37:56,683 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:37:56,683 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-10 05:37:58,700 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:58,704 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39527 instead
  warnings.warn(
2023-10-10 05:37:58,708 - distributed.scheduler - INFO - State start
2023-10-10 05:37:58,727 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:37:58,728 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:37:58,729 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39527/status
2023-10-10 05:37:58,729 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:37:58,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46847'
2023-10-10 05:37:58,957 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44199'
2023-10-10 05:37:58,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33765'
2023-10-10 05:37:58,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46739'
2023-10-10 05:37:58,983 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36941'
2023-10-10 05:37:58,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44685'
2023-10-10 05:37:58,998 - distributed.scheduler - INFO - Receive client connection: Client-2a95a0c6-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:37:58,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44689'
2023-10-10 05:37:59,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43671'
2023-10-10 05:37:59,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32784
2023-10-10 05:38:00,742 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:00,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:00,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:00,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:00,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:00,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:00,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:00,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:00,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:00,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:00,838 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:00,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:00,838 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:00,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:00,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:00,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:00,842 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:00,842 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:00,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:00,898 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:00,902 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:00,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:00,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:00,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:02,697 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32787
2023-10-10 05:38:02,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32787
2023-10-10 05:38:02,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42423
2023-10-10 05:38:02,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:02,698 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:02,698 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:02,698 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:02,698 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p7um4a9t
2023-10-10 05:38:02,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90e5b404-fe1e-4c4e-ac13-4d9629b2d1d1
2023-10-10 05:38:03,123 - distributed.worker - INFO - Starting Worker plugin PreImport-269d15d1-e8f4-460d-8ecf-726127ecc452
2023-10-10 05:38:03,124 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52e261e5-0e2c-4774-9a68-a3033f31df21
2023-10-10 05:38:03,124 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,160 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32787', status: init, memory: 0, processing: 0>
2023-10-10 05:38:03,161 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32787
2023-10-10 05:38:03,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51944
2023-10-10 05:38:03,163 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:03,163 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,163 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,166 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:03,659 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43973
2023-10-10 05:38:03,660 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43973
2023-10-10 05:38:03,660 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33449
2023-10-10 05:38:03,660 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,660 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,660 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:03,660 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:03,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pzx3ruyw
2023-10-10 05:38:03,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-66a248ec-219e-4dc1-bbc5-d8a5e0e95700
2023-10-10 05:38:03,664 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34567
2023-10-10 05:38:03,664 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34567
2023-10-10 05:38:03,665 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34789
2023-10-10 05:38:03,665 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,665 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,665 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:03,665 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:03,665 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0k1uhu1h
2023-10-10 05:38:03,664 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46195
2023-10-10 05:38:03,665 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46195
2023-10-10 05:38:03,665 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37043
2023-10-10 05:38:03,665 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,666 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d34aed1-88bd-4908-b680-bd44f830e04c
2023-10-10 05:38:03,666 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,666 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:03,666 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:03,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_l02yha_
2023-10-10 05:38:03,666 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d54d1aee-49de-4fd5-a89c-e9bea0eccc38
2023-10-10 05:38:03,678 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37821
2023-10-10 05:38:03,679 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37821
2023-10-10 05:38:03,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34593
2023-10-10 05:38:03,678 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45781
2023-10-10 05:38:03,679 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,679 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45781
2023-10-10 05:38:03,679 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37553
2023-10-10 05:38:03,679 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,679 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,679 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:03,679 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:03,679 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m8a0lndv
2023-10-10 05:38:03,679 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:03,679 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:03,679 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qa37w0b7
2023-10-10 05:38:03,680 - distributed.worker - INFO - Starting Worker plugin RMMSetup-031b41a8-90e1-4ddb-8f7b-235ec336d32a
2023-10-10 05:38:03,680 - distributed.worker - INFO - Starting Worker plugin PreImport-2738189e-6d9c-4e20-a950-1ac5d7bfd4a4
2023-10-10 05:38:03,680 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f15b7e9f-deca-4f3b-b531-5c3de83319f8
2023-10-10 05:38:03,680 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c739e8e-7ef3-4838-889a-0c202dbd8ff4
2023-10-10 05:38:03,821 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44461
2023-10-10 05:38:03,822 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44461
2023-10-10 05:38:03,822 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37013
2023-10-10 05:38:03,822 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,822 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,822 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:03,822 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:03,822 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v3cv7fcw
2023-10-10 05:38:03,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c631aa2-459e-4e24-9140-c3f30928c6e3
2023-10-10 05:38:03,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32777
2023-10-10 05:38:03,830 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32777
2023-10-10 05:38:03,830 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42593
2023-10-10 05:38:03,830 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,830 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,830 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:03,830 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:03,830 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nzj4hj6t
2023-10-10 05:38:03,831 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb7c3203-4bf5-419d-a883-763a5ce450ff
2023-10-10 05:38:03,893 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04ded0cc-9917-46db-9b1f-60154c348d10
2023-10-10 05:38:03,893 - distributed.worker - INFO - Starting Worker plugin PreImport-9072b41a-3fff-4334-af9b-da5bdb2c9674
2023-10-10 05:38:03,894 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,895 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebb58c02-4fb8-432f-9016-ca56154eb012
2023-10-10 05:38:03,895 - distributed.worker - INFO - Starting Worker plugin PreImport-16c44eb0-6cdf-4ed5-a9da-6528b1a0a808
2023-10-10 05:38:03,895 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c9bf936-f0f5-44b6-9220-152f29029296
2023-10-10 05:38:03,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-168988bf-1185-48e7-86a9-e5c824b99d44
2023-10-10 05:38:03,903 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,904 - distributed.worker - INFO - Starting Worker plugin PreImport-331699be-81c4-444f-afe0-30b7578cee71
2023-10-10 05:38:03,904 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,904 - distributed.worker - INFO - Starting Worker plugin PreImport-f3494ad5-0b94-4a45-a057-17e3b2266c32
2023-10-10 05:38:03,904 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,926 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34567', status: init, memory: 0, processing: 0>
2023-10-10 05:38:03,926 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34567
2023-10-10 05:38:03,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51968
2023-10-10 05:38:03,928 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:03,929 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,929 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,930 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37821', status: init, memory: 0, processing: 0>
2023-10-10 05:38:03,930 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:03,931 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37821
2023-10-10 05:38:03,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51952
2023-10-10 05:38:03,933 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:03,934 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,934 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,935 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43973', status: init, memory: 0, processing: 0>
2023-10-10 05:38:03,935 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43973
2023-10-10 05:38:03,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51970
2023-10-10 05:38:03,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:03,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:03,938 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,938 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:03,942 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46195', status: init, memory: 0, processing: 0>
2023-10-10 05:38:03,942 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46195
2023-10-10 05:38:03,942 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51990
2023-10-10 05:38:03,944 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:03,944 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45781', status: init, memory: 0, processing: 0>
2023-10-10 05:38:03,945 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45781
2023-10-10 05:38:03,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51978
2023-10-10 05:38:03,946 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,946 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,947 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:03,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:03,948 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:03,948 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:03,983 - distributed.worker - INFO - Starting Worker plugin PreImport-b21e7f1a-088b-4924-83d7-468c9fddb93b
2023-10-10 05:38:03,983 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c8788f5-0d60-45ca-98cb-ac5de05b6f45
2023-10-10 05:38:03,984 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:03,990 - distributed.worker - INFO - Starting Worker plugin PreImport-ac960819-6720-4e30-97c0-3f01fffd7388
2023-10-10 05:38:03,991 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46a2a306-a722-4943-810a-1d2aaada21d9
2023-10-10 05:38:03,991 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:04,015 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32777', status: init, memory: 0, processing: 0>
2023-10-10 05:38:04,016 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32777
2023-10-10 05:38:04,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52006
2023-10-10 05:38:04,017 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:04,017 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44461', status: init, memory: 0, processing: 0>
2023-10-10 05:38:04,018 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44461
2023-10-10 05:38:04,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52002
2023-10-10 05:38:04,018 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:04,018 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:04,020 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:04,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:04,021 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:04,021 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:04,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:04,104 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,105 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,105 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,105 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,105 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,106 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,106 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,106 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,116 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:38:04,116 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:38:04,116 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:38:04,116 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:38:04,116 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:38:04,116 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:38:04,116 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:38:04,116 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:38:04,122 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,124 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:04,126 - distributed.scheduler - INFO - Remove client Client-2a95a0c6-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:04,126 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32784; closing.
2023-10-10 05:38:04,127 - distributed.scheduler - INFO - Remove client Client-2a95a0c6-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:04,127 - distributed.scheduler - INFO - Close client connection: Client-2a95a0c6-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:04,128 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46847'. Reason: nanny-close
2023-10-10 05:38:04,128 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:04,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44199'. Reason: nanny-close
2023-10-10 05:38:04,130 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:04,130 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32777. Reason: nanny-close
2023-10-10 05:38:04,130 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33765'. Reason: nanny-close
2023-10-10 05:38:04,130 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:04,131 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32787. Reason: nanny-close
2023-10-10 05:38:04,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46739'. Reason: nanny-close
2023-10-10 05:38:04,131 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:04,131 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46195. Reason: nanny-close
2023-10-10 05:38:04,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36941'. Reason: nanny-close
2023-10-10 05:38:04,132 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:04,132 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45781. Reason: nanny-close
2023-10-10 05:38:04,132 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:04,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44685'. Reason: nanny-close
2023-10-10 05:38:04,132 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52006; closing.
2023-10-10 05:38:04,132 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:04,132 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916284.1327484')
2023-10-10 05:38:04,132 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43973. Reason: nanny-close
2023-10-10 05:38:04,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44689'. Reason: nanny-close
2023-10-10 05:38:04,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:04,133 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:04,133 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34567. Reason: nanny-close
2023-10-10 05:38:04,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43671'. Reason: nanny-close
2023-10-10 05:38:04,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:04,133 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:04,134 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37821. Reason: nanny-close
2023-10-10 05:38:04,134 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:04,134 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51944; closing.
2023-10-10 05:38:04,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:04,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:04,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51990; closing.
2023-10-10 05:38:04,135 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:04,135 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32787', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916284.1353447')
2023-10-10 05:38:04,135 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44461. Reason: nanny-close
2023-10-10 05:38:04,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:04,136 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46195', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916284.136213')
2023-10-10 05:38:04,136 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:04,136 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51978; closing.
2023-10-10 05:38:04,136 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:04,136 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51970; closing.
2023-10-10 05:38:04,137 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:04,137 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:04,137 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:04,137 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916284.1373901')
2023-10-10 05:38:04,137 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:04,137 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43973', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916284.1377008')
2023-10-10 05:38:04,138 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51968; closing.
2023-10-10 05:38:04,138 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916284.1385617')
2023-10-10 05:38:04,138 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51952; closing.
2023-10-10 05:38:04,139 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:04,139 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:04,139 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37821', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916284.139473')
2023-10-10 05:38:04,139 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52002; closing.
2023-10-10 05:38:04,140 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916284.1402175')
2023-10-10 05:38:04,140 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:38:05,696 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:38:05,696 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:38:05,697 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:38:05,698 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:38:05,698 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-10 05:38:07,720 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:07,725 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33759 instead
  warnings.warn(
2023-10-10 05:38:07,730 - distributed.scheduler - INFO - State start
2023-10-10 05:38:07,754 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:07,755 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:38:07,756 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33759/status
2023-10-10 05:38:07,756 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:38:07,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38263'
2023-10-10 05:38:07,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45023'
2023-10-10 05:38:07,857 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33301'
2023-10-10 05:38:07,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40267'
2023-10-10 05:38:07,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33445'
2023-10-10 05:38:07,881 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39475'
2023-10-10 05:38:07,889 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39041'
2023-10-10 05:38:07,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38209'
2023-10-10 05:38:09,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:09,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:09,706 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:09,706 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:09,706 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:09,706 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:09,709 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:09,710 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:09,710 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:09,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:09,748 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:09,751 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:09,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:09,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:09,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:09,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:09,756 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:09,759 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:09,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:09,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:09,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:09,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:09,883 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:09,887 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:12,978 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38507
2023-10-10 05:38:12,979 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38507
2023-10-10 05:38:12,979 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33243
2023-10-10 05:38:12,979 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:12,979 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:12,979 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:12,979 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:12,979 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-odjqhjr2
2023-10-10 05:38:12,980 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e85f9f75-0fbc-4f75-97d6-4d5e01eb5e94
2023-10-10 05:38:12,991 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34929
2023-10-10 05:38:12,991 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34929
2023-10-10 05:38:12,991 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36155
2023-10-10 05:38:12,991 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:12,992 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:12,992 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:12,992 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:12,992 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cur3mtdh
2023-10-10 05:38:12,992 - distributed.worker - INFO - Starting Worker plugin PreImport-73c41838-2a1a-447b-8642-fdd25d562087
2023-10-10 05:38:12,992 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5b68437-0a6b-4dce-b9d5-8160ae6815ec
2023-10-10 05:38:12,993 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7dc1d6d6-4734-4cbb-bd6e-35b58eebda3f
2023-10-10 05:38:12,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39761
2023-10-10 05:38:12,994 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39761
2023-10-10 05:38:12,994 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37119
2023-10-10 05:38:12,994 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:12,994 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:12,994 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:12,994 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:12,994 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s0qxf7kl
2023-10-10 05:38:12,995 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30f283b8-4a0e-4653-bdd4-f82611ccb576
2023-10-10 05:38:13,013 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45329
2023-10-10 05:38:13,014 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45329
2023-10-10 05:38:13,014 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41341
2023-10-10 05:38:13,014 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,014 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,014 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:13,014 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:13,014 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5q1fy075
2023-10-10 05:38:13,015 - distributed.worker - INFO - Starting Worker plugin RMMSetup-420f1635-6748-4423-af50-a03dcd12a72f
2023-10-10 05:38:13,016 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43689
2023-10-10 05:38:13,016 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43689
2023-10-10 05:38:13,016 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38999
2023-10-10 05:38:13,017 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,017 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,017 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:13,017 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:13,017 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-61xmvdu4
2023-10-10 05:38:13,017 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e45cdc3d-ed39-4fcc-9d19-579081ffd089
2023-10-10 05:38:13,020 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36993
2023-10-10 05:38:13,020 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36993
2023-10-10 05:38:13,020 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34905
2023-10-10 05:38:13,021 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,021 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,021 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:13,021 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:13,021 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a6nhecuq
2023-10-10 05:38:13,021 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca26d59d-127a-4d06-b1ba-998bfb2ce00d
2023-10-10 05:38:13,030 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34281
2023-10-10 05:38:13,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34281
2023-10-10 05:38:13,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40237
2023-10-10 05:38:13,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,031 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,031 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:13,031 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:13,031 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mgsyw9y3
2023-10-10 05:38:13,032 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e6fea61-ee32-406e-889e-4987e2f2bb3c
2023-10-10 05:38:13,031 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33369
2023-10-10 05:38:13,032 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33369
2023-10-10 05:38:13,032 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34553
2023-10-10 05:38:13,032 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,032 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,033 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:13,033 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:13,033 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3cbskuj3
2023-10-10 05:38:13,033 - distributed.worker - INFO - Starting Worker plugin RMMSetup-755d1f15-2b92-4616-a609-5856945f3219
2023-10-10 05:38:13,178 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,178 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5bcdc9c8-f572-4dc7-b17f-4d0347079e3b
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin PreImport-a73db01f-3a28-40a5-863c-7da1f70e79aa
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab7f52bb-1032-48bf-b6f5-a6a6e3f2cacc
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin PreImport-ac189fa9-0ee6-4005-86e0-1a1d1fe7316f
2023-10-10 05:38:13,179 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin PreImport-e6dd465c-fedd-42bc-898e-7d01c91e010f
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73fd9a4b-a32f-41b2-b0f6-70aeae422fd7
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84a81551-f8a9-4da4-be16-445fa047cdf5
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-71e9a12f-584a-46fb-99d3-5789ae80bb46
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin PreImport-c3e529af-2805-404c-93ac-7449c2684770
2023-10-10 05:38:13,179 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin PreImport-2ef2cdc1-8a6f-45a8-a109-22439357b9c1
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin PreImport-a89dfce0-ed3d-4ef7-abf0-56e74e741b7d
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3db5611b-cb85-4841-8735-abec5cf2de0b
2023-10-10 05:38:13,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-afb784ad-574e-4072-9ec1-1aac9424e57d
2023-10-10 05:38:13,179 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,180 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,181 - distributed.worker - INFO - Starting Worker plugin PreImport-147cdc58-d83e-4a2e-a6c9-968f0bc048f1
2023-10-10 05:38:13,181 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,181 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,184 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,203 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34281', status: init, memory: 0, processing: 0>
2023-10-10 05:38:13,219 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34281
2023-10-10 05:38:13,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46190
2023-10-10 05:38:13,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:13,221 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,221 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,221 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34929', status: init, memory: 0, processing: 0>
2023-10-10 05:38:13,222 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34929
2023-10-10 05:38:13,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46148
2023-10-10 05:38:13,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:13,222 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38507', status: init, memory: 0, processing: 0>
2023-10-10 05:38:13,223 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38507
2023-10-10 05:38:13,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46162
2023-10-10 05:38:13,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:13,224 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,224 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36993', status: init, memory: 0, processing: 0>
2023-10-10 05:38:13,224 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,224 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36993
2023-10-10 05:38:13,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46176
2023-10-10 05:38:13,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:13,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:13,225 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,225 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43689', status: init, memory: 0, processing: 0>
2023-10-10 05:38:13,225 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:13,226 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43689
2023-10-10 05:38:13,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46222
2023-10-10 05:38:13,226 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,226 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,227 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39761', status: init, memory: 0, processing: 0>
2023-10-10 05:38:13,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:13,227 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39761
2023-10-10 05:38:13,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46192
2023-10-10 05:38:13,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:13,228 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45329', status: init, memory: 0, processing: 0>
2023-10-10 05:38:13,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:13,228 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45329
2023-10-10 05:38:13,229 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46202
2023-10-10 05:38:13,229 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,229 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:13,229 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33369', status: init, memory: 0, processing: 0>
2023-10-10 05:38:13,230 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33369
2023-10-10 05:38:13,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46218
2023-10-10 05:38:13,230 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,230 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:13,231 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:13,231 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,231 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:13,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:13,233 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:13,233 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:13,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:13,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:16,521 - distributed.scheduler - INFO - Receive client connection: Client-2fefac93-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:16,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46314
2023-10-10 05:38:16,532 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:16,532 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:16,532 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:16,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:16,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:16,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:16,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:16,534 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:16,537 - distributed.scheduler - INFO - Remove client Client-2fefac93-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:16,538 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46314; closing.
2023-10-10 05:38:16,538 - distributed.scheduler - INFO - Remove client Client-2fefac93-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:16,538 - distributed.scheduler - INFO - Close client connection: Client-2fefac93-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:16,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38263'. Reason: nanny-close
2023-10-10 05:38:16,540 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:16,541 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45023'. Reason: nanny-close
2023-10-10 05:38:16,541 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:16,541 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43689. Reason: nanny-close
2023-10-10 05:38:16,541 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33301'. Reason: nanny-close
2023-10-10 05:38:16,541 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:16,542 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40267'. Reason: nanny-close
2023-10-10 05:38:16,542 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45329. Reason: nanny-close
2023-10-10 05:38:16,542 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:16,542 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36993. Reason: nanny-close
2023-10-10 05:38:16,542 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33445'. Reason: nanny-close
2023-10-10 05:38:16,543 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:16,543 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34929. Reason: nanny-close
2023-10-10 05:38:16,543 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39475'. Reason: nanny-close
2023-10-10 05:38:16,543 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:16,543 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:16,543 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33369. Reason: nanny-close
2023-10-10 05:38:16,544 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46222; closing.
2023-10-10 05:38:16,544 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39041'. Reason: nanny-close
2023-10-10 05:38:16,544 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:16,544 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43689', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916296.5444925')
2023-10-10 05:38:16,544 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:16,544 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39761. Reason: nanny-close
2023-10-10 05:38:16,544 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38209'. Reason: nanny-close
2023-10-10 05:38:16,544 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:16,544 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34281. Reason: nanny-close
2023-10-10 05:38:16,545 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:16,545 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:16,545 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38507. Reason: nanny-close
2023-10-10 05:38:16,545 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:16,545 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46148; closing.
2023-10-10 05:38:16,546 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:16,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:16,546 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:16,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:16,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:16,547 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916296.547087')
2023-10-10 05:38:16,547 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46176; closing.
2023-10-10 05:38:16,547 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:16,547 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46202; closing.
2023-10-10 05:38:16,547 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:16,548 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:16,548 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:16,548 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:16,549 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:16,548 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46148>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46148>: Stream is closed
2023-10-10 05:38:16,551 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916296.55106')
2023-10-10 05:38:16,551 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46218; closing.
2023-10-10 05:38:16,551 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45329', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916296.551836')
2023-10-10 05:38:16,552 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46192; closing.
2023-10-10 05:38:16,553 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33369', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916296.553227')
2023-10-10 05:38:16,553 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39761', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916296.55372')
2023-10-10 05:38:16,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46190; closing.
2023-10-10 05:38:16,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46162; closing.
2023-10-10 05:38:16,554 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34281', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916296.5548177')
2023-10-10 05:38:16,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38507', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916296.555282')
2023-10-10 05:38:16,555 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:38:17,906 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:38:17,906 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:38:17,907 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:38:17,908 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:38:17,909 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-10 05:38:19,885 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:19,890 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42209 instead
  warnings.warn(
2023-10-10 05:38:19,893 - distributed.scheduler - INFO - State start
2023-10-10 05:38:19,915 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:19,916 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:38:19,916 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42209/status
2023-10-10 05:38:19,917 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:38:20,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39021'
2023-10-10 05:38:21,766 - distributed.scheduler - INFO - Receive client connection: Client-373ac47f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:21,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54018
2023-10-10 05:38:21,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:21,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:22,344 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:23,168 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33029
2023-10-10 05:38:23,169 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33029
2023-10-10 05:38:23,169 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-10 05:38:23,169 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:23,169 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:23,169 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:23,169 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-10 05:38:23,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-00qribum
2023-10-10 05:38:23,169 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea7686a3-7150-432e-8d9b-c3ec77ef9d41
2023-10-10 05:38:23,170 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3856d72e-e479-424b-8225-4f8b8fe4227f
2023-10-10 05:38:23,170 - distributed.worker - INFO - Starting Worker plugin PreImport-ef19d5b9-1139-41cf-ab94-f4b446832c74
2023-10-10 05:38:23,171 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:23,212 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33029', status: init, memory: 0, processing: 0>
2023-10-10 05:38:23,213 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33029
2023-10-10 05:38:23,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54032
2023-10-10 05:38:23,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:23,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:23,217 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:23,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:23,312 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:23,315 - distributed.scheduler - INFO - Remove client Client-373ac47f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:23,315 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54018; closing.
2023-10-10 05:38:23,315 - distributed.scheduler - INFO - Remove client Client-373ac47f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:23,316 - distributed.scheduler - INFO - Close client connection: Client-373ac47f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:23,316 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39021'. Reason: nanny-close
2023-10-10 05:38:23,317 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:23,318 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33029. Reason: nanny-close
2023-10-10 05:38:23,322 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54032; closing.
2023-10-10 05:38:23,322 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:23,322 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33029', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916303.3227487')
2023-10-10 05:38:23,323 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:38:23,324 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:24,483 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:38:24,483 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:38:24,484 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:38:24,485 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:38:24,485 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-10 05:38:28,624 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:28,629 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37301 instead
  warnings.warn(
2023-10-10 05:38:28,633 - distributed.scheduler - INFO - State start
2023-10-10 05:38:29,346 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:29,347 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:38:29,348 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37301/status
2023-10-10 05:38:29,348 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:38:29,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40969'
2023-10-10 05:38:31,208 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:31,208 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:31,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:32,343 - distributed.scheduler - INFO - Receive client connection: Client-3c4f549f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:32,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50200
2023-10-10 05:38:32,617 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45027
2023-10-10 05:38:32,618 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45027
2023-10-10 05:38:32,618 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45331
2023-10-10 05:38:32,618 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:32,618 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:32,618 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:32,618 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-10 05:38:32,618 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ztisk3cw
2023-10-10 05:38:32,619 - distributed.worker - INFO - Starting Worker plugin PreImport-7feab78b-5444-4a3c-8cea-6beba832ce96
2023-10-10 05:38:32,620 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b9f73bd-11cf-42f1-93d3-e7ef33e83dea
2023-10-10 05:38:32,620 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cb33540f-c971-4aba-8b63-80543bf6f9fb
2023-10-10 05:38:32,621 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:32,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45027', status: init, memory: 0, processing: 0>
2023-10-10 05:38:32,656 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45027
2023-10-10 05:38:32,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50226
2023-10-10 05:38:32,657 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:32,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:32,658 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:32,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:32,671 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:32,673 - distributed.scheduler - INFO - Remove client Client-3c4f549f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:32,674 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50200; closing.
2023-10-10 05:38:32,674 - distributed.scheduler - INFO - Remove client Client-3c4f549f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:32,674 - distributed.scheduler - INFO - Close client connection: Client-3c4f549f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:32,675 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40969'. Reason: nanny-close
2023-10-10 05:38:32,691 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:32,692 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45027. Reason: nanny-close
2023-10-10 05:38:32,694 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:32,694 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50226; closing.
2023-10-10 05:38:32,695 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45027', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916312.6948984')
2023-10-10 05:38:32,695 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:38:32,696 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:33,842 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:38:33,843 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:38:33,843 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:38:33,844 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:38:33,845 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-10 05:38:35,969 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:35,974 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45707 instead
  warnings.warn(
2023-10-10 05:38:35,978 - distributed.scheduler - INFO - State start
2023-10-10 05:38:36,002 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:36,003 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:38:36,003 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45707/status
2023-10-10 05:38:36,004 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:38:40,895 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50236'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50236>: Stream is closed
2023-10-10 05:38:41,163 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:38:41,163 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:38:41,164 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:38:41,164 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:38:41,165 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-10 05:38:43,488 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:43,493 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39241 instead
  warnings.warn(
2023-10-10 05:38:43,497 - distributed.scheduler - INFO - State start
2023-10-10 05:38:43,525 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:43,526 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-10 05:38:43,527 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39241/status
2023-10-10 05:38:43,527 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:38:43,582 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43845'
2023-10-10 05:38:45,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:45,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:45,154 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:45,164 - distributed.scheduler - INFO - Receive client connection: Client-4523c53f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:45,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55218
2023-10-10 05:38:46,785 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39363
2023-10-10 05:38:46,786 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39363
2023-10-10 05:38:46,786 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37697
2023-10-10 05:38:46,786 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-10 05:38:46,786 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:46,786 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:46,786 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-10 05:38:46,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-kaj24enh
2023-10-10 05:38:46,786 - distributed.worker - INFO - Starting Worker plugin RMMSetup-abba4308-c348-4437-a277-40132efc0ead
2023-10-10 05:38:46,786 - distributed.worker - INFO - Starting Worker plugin PreImport-a4f81fd8-c494-49ba-8853-644d6d4dbe71
2023-10-10 05:38:46,786 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b128a2c0-2c00-4d7c-b42b-78564fdda43d
2023-10-10 05:38:46,787 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:46,819 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39363', status: init, memory: 0, processing: 0>
2023-10-10 05:38:46,820 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39363
2023-10-10 05:38:46,820 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55224
2023-10-10 05:38:46,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:46,822 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-10 05:38:46,822 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:46,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-10 05:38:46,858 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:46,862 - distributed.scheduler - INFO - Remove client Client-4523c53f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:46,862 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55218; closing.
2023-10-10 05:38:46,862 - distributed.scheduler - INFO - Remove client Client-4523c53f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:46,863 - distributed.scheduler - INFO - Close client connection: Client-4523c53f-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:46,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43845'. Reason: nanny-close
2023-10-10 05:38:46,866 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:46,867 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39363. Reason: nanny-close
2023-10-10 05:38:46,869 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-10 05:38:46,869 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55224; closing.
2023-10-10 05:38:46,870 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916326.8701897')
2023-10-10 05:38:46,870 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:38:46,871 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:47,881 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:38:47,881 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:38:47,882 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:38:47,883 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-10 05:38:47,883 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-10 05:38:50,163 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:50,168 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-10 05:38:50,171 - distributed.scheduler - INFO - State start
2023-10-10 05:38:50,196 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:50,197 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:38:50,198 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-10 05:38:50,198 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:38:50,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45323'
2023-10-10 05:38:50,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41591'
2023-10-10 05:38:50,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39715'
2023-10-10 05:38:50,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38565'
2023-10-10 05:38:50,412 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39739'
2023-10-10 05:38:50,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35935'
2023-10-10 05:38:50,430 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46187'
2023-10-10 05:38:50,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36603'
2023-10-10 05:38:50,583 - distributed.scheduler - INFO - Receive client connection: Client-492f3ae3-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:50,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51966
2023-10-10 05:38:52,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:52,144 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:52,148 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:52,221 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:52,221 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:52,225 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:52,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:52,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:52,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:52,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:52,241 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:52,241 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:52,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:52,242 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:52,246 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:52,264 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:52,264 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:52,268 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:52,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:52,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:52,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:38:52,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:38:52,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:52,307 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:38:54,718 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34125
2023-10-10 05:38:54,719 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34125
2023-10-10 05:38:54,719 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40037
2023-10-10 05:38:54,720 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:54,720 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:54,720 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:54,720 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:54,720 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-62r3tkv4
2023-10-10 05:38:54,721 - distributed.worker - INFO - Starting Worker plugin PreImport-0b610d44-5c90-4b1a-b8eb-25a09bfd54d8
2023-10-10 05:38:54,721 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4338317e-3582-4bc3-9c6b-33959b1931d0
2023-10-10 05:38:54,721 - distributed.worker - INFO - Starting Worker plugin RMMSetup-36051756-a155-4b9b-af8d-a991947e197c
2023-10-10 05:38:54,851 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:54,884 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34125', status: init, memory: 0, processing: 0>
2023-10-10 05:38:54,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34125
2023-10-10 05:38:54,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51980
2023-10-10 05:38:54,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:54,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:54,889 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:54,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:55,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44113
2023-10-10 05:38:55,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44113
2023-10-10 05:38:55,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43847
2023-10-10 05:38:55,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,219 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,220 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:55,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:55,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34379
2023-10-10 05:38:55,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hp9kr0c1
2023-10-10 05:38:55,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34379
2023-10-10 05:38:55,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46739
2023-10-10 05:38:55,220 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,220 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:55,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:55,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lx2904d9
2023-10-10 05:38:55,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7802dd2-579c-4711-a51e-295878afb75a
2023-10-10 05:38:55,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca5defc3-8b2e-448f-bb99-992a2f9d7614
2023-10-10 05:38:55,241 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45941
2023-10-10 05:38:55,242 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45941
2023-10-10 05:38:55,242 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39843
2023-10-10 05:38:55,242 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,242 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,242 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:55,242 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:55,243 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_qefrq0v
2023-10-10 05:38:55,243 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ced868c3-37cf-48a7-a100-154430dffc26
2023-10-10 05:38:55,252 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33683
2023-10-10 05:38:55,254 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33683
2023-10-10 05:38:55,255 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36415
2023-10-10 05:38:55,255 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,255 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,255 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:55,255 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:55,255 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v7md07ij
2023-10-10 05:38:55,256 - distributed.worker - INFO - Starting Worker plugin PreImport-de30368e-f731-4e35-b6bb-8f2afa5c3458
2023-10-10 05:38:55,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-955d69f0-8a38-42f4-9af4-81eae8079112
2023-10-10 05:38:55,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40811
2023-10-10 05:38:55,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40811
2023-10-10 05:38:55,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33217
2023-10-10 05:38:55,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33779
2023-10-10 05:38:55,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e52d318a-b196-4306-b91e-04a2ef28aded
2023-10-10 05:38:55,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33779
2023-10-10 05:38:55,257 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45055
2023-10-10 05:38:55,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,257 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,257 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:55,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:55,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y3u3huk2
2023-10-10 05:38:55,257 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:55,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:55,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xoyyb8g1
2023-10-10 05:38:55,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43dae683-c839-440d-8fa1-836ed55191fb
2023-10-10 05:38:55,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b34332a-c78d-4a77-b99f-a81ef7d591f9
2023-10-10 05:38:55,404 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44055
2023-10-10 05:38:55,405 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44055
2023-10-10 05:38:55,405 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39043
2023-10-10 05:38:55,405 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,405 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,405 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:38:55,405 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-10 05:38:55,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0n4mjdb2
2023-10-10 05:38:55,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c9baa458-20d4-4c0c-9fe0-35dffbe10f1b
2023-10-10 05:38:55,566 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53cbc750-88f1-4ac0-983c-c5cd678986db
2023-10-10 05:38:55,568 - distributed.worker - INFO - Starting Worker plugin PreImport-dc15dcec-4c0e-4d2f-b988-cf5d71580fa5
2023-10-10 05:38:55,568 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,579 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-678b294e-2e20-4ad7-a450-ca00af0de805
2023-10-10 05:38:55,580 - distributed.worker - INFO - Starting Worker plugin PreImport-93a54a85-3b05-491b-9404-44ced0da4e4c
2023-10-10 05:38:55,580 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cfcc3c6d-0637-4aad-ba49-8c4ccea74a22
2023-10-10 05:38:55,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89c3f141-5a37-4f66-aaf2-ca243c73e601
2023-10-10 05:38:55,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d07bbfcd-f46c-4333-be07-808773670596
2023-10-10 05:38:55,594 - distributed.worker - INFO - Starting Worker plugin PreImport-54f43c25-81a2-41ab-b15b-80ff25dfdbd2
2023-10-10 05:38:55,594 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1dcd44d7-561b-4444-b4d3-94ab92a91ad0
2023-10-10 05:38:55,594 - distributed.worker - INFO - Starting Worker plugin PreImport-956fda29-d34e-4efe-97da-8bcb16f3a3f7
2023-10-10 05:38:55,594 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,594 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,594 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,595 - distributed.worker - INFO - Starting Worker plugin PreImport-0f0c7ec3-85a9-4410-8ce2-549ba2877c44
2023-10-10 05:38:55,595 - distributed.worker - INFO - Starting Worker plugin PreImport-82cdceb7-ee8b-4e7e-9ac8-aabad23a8c15
2023-10-10 05:38:55,596 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,596 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44113', status: init, memory: 0, processing: 0>
2023-10-10 05:38:55,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44113
2023-10-10 05:38:55,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51986
2023-10-10 05:38:55,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:55,607 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34379', status: init, memory: 0, processing: 0>
2023-10-10 05:38:55,608 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,608 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34379
2023-10-10 05:38:55,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51998
2023-10-10 05:38:55,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:55,609 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:55,610 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,610 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:55,621 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33779', status: init, memory: 0, processing: 0>
2023-10-10 05:38:55,622 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33779
2023-10-10 05:38:55,622 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52002
2023-10-10 05:38:55,623 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33683', status: init, memory: 0, processing: 0>
2023-10-10 05:38:55,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:55,624 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33683
2023-10-10 05:38:55,624 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52000
2023-10-10 05:38:55,624 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,624 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,625 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:55,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:55,626 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,626 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:55,631 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44055', status: init, memory: 0, processing: 0>
2023-10-10 05:38:55,631 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44055
2023-10-10 05:38:55,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52040
2023-10-10 05:38:55,633 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:55,634 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45941', status: init, memory: 0, processing: 0>
2023-10-10 05:38:55,634 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,634 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,634 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45941
2023-10-10 05:38:55,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52014
2023-10-10 05:38:55,635 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40811', status: init, memory: 0, processing: 0>
2023-10-10 05:38:55,636 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40811
2023-10-10 05:38:55,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52024
2023-10-10 05:38:55,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:55,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:55,637 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,637 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,637 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:38:55,638 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:38:55,638 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:38:55,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:55,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:38:55,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:55,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:55,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:55,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:55,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:55,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:55,727 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:55,727 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-10 05:38:55,739 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:55,739 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:55,739 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:55,739 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:55,739 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:55,739 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:55,739 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:55,740 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:38:55,743 - distributed.scheduler - INFO - Remove client Client-492f3ae3-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:55,744 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51966; closing.
2023-10-10 05:38:55,744 - distributed.scheduler - INFO - Remove client Client-492f3ae3-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:55,744 - distributed.scheduler - INFO - Close client connection: Client-492f3ae3-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:38:55,745 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45323'. Reason: nanny-close
2023-10-10 05:38:55,746 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:55,748 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41591'. Reason: nanny-close
2023-10-10 05:38:55,748 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:55,748 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44055. Reason: nanny-close
2023-10-10 05:38:55,748 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39715'. Reason: nanny-close
2023-10-10 05:38:55,748 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:55,749 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40811. Reason: nanny-close
2023-10-10 05:38:55,749 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38565'. Reason: nanny-close
2023-10-10 05:38:55,749 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:55,749 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34379. Reason: nanny-close
2023-10-10 05:38:55,749 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39739'. Reason: nanny-close
2023-10-10 05:38:55,750 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:55,750 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33779. Reason: nanny-close
2023-10-10 05:38:55,750 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35935'. Reason: nanny-close
2023-10-10 05:38:55,750 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:55,750 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:55,751 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52040; closing.
2023-10-10 05:38:55,751 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45941. Reason: nanny-close
2023-10-10 05:38:55,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46187'. Reason: nanny-close
2023-10-10 05:38:55,751 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44055', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916335.7512321')
2023-10-10 05:38:55,751 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:55,751 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:55,751 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:55,751 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34125. Reason: nanny-close
2023-10-10 05:38:55,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36603'. Reason: nanny-close
2023-10-10 05:38:55,752 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:38:55,752 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:55,752 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33683. Reason: nanny-close
2023-10-10 05:38:55,752 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44113. Reason: nanny-close
2023-10-10 05:38:55,752 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:55,753 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:55,753 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52002; closing.
2023-10-10 05:38:55,753 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52024; closing.
2023-10-10 05:38:55,753 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:55,753 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51998; closing.
2023-10-10 05:38:55,753 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:55,754 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33779', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916335.754111')
2023-10-10 05:38:55,754 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:55,754 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:55,754 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916335.7546182')
2023-10-10 05:38:55,754 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:55,755 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34379', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916335.7549934')
2023-10-10 05:38:55,755 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:38:55,755 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52014; closing.
2023-10-10 05:38:55,756 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:55,756 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:55,756 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45941', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916335.7564957')
2023-10-10 05:38:55,756 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:55,756 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51980; closing.
2023-10-10 05:38:55,756 - distributed.nanny - INFO - Worker closed
2023-10-10 05:38:55,757 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52000; closing.
2023-10-10 05:38:55,757 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51986; closing.
2023-10-10 05:38:55,757 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34125', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916335.7575736')
2023-10-10 05:38:55,758 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33683', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916335.7580636')
2023-10-10 05:38:55,758 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44113', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916335.75848')
2023-10-10 05:38:55,758 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:38:57,363 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:38:57,363 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:38:57,364 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:38:57,365 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:38:57,365 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-10 05:38:59,303 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:59,306 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35827 instead
  warnings.warn(
2023-10-10 05:38:59,310 - distributed.scheduler - INFO - State start
2023-10-10 05:38:59,376 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:38:59,377 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:38:59,377 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35827/status
2023-10-10 05:38:59,377 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:38:59,483 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40757'
2023-10-10 05:39:01,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:39:01,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:39:01,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:39:01,356 - distributed.scheduler - INFO - Receive client connection: Client-4ebcbc13-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:39:01,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49404
2023-10-10 05:39:01,969 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37493
2023-10-10 05:39:01,970 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37493
2023-10-10 05:39:01,970 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37013
2023-10-10 05:39:01,970 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:39:01,970 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:39:01,970 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:39:01,970 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-10 05:39:01,970 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-smk8qrsp
2023-10-10 05:39:01,971 - distributed.worker - INFO - Starting Worker plugin PreImport-8eab4cb2-b5b6-4a28-814d-de9cd2e050b2
2023-10-10 05:39:01,971 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53253d0b-ba31-4dca-a29f-f2a7d22c81e0
2023-10-10 05:39:01,971 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d47262c-b7a2-4033-9905-4bd0c031249e
2023-10-10 05:39:02,068 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:39:02,092 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37493', status: init, memory: 0, processing: 0>
2023-10-10 05:39:02,093 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37493
2023-10-10 05:39:02,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49410
2023-10-10 05:39:02,094 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:39:02,095 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:39:02,095 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:39:02,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:39:02,190 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:39:02,194 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:39:02,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:39:02,198 - distributed.scheduler - INFO - Remove client Client-4ebcbc13-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:39:02,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49404; closing.
2023-10-10 05:39:02,198 - distributed.scheduler - INFO - Remove client Client-4ebcbc13-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:39:02,198 - distributed.scheduler - INFO - Close client connection: Client-4ebcbc13-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:39:02,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40757'. Reason: nanny-close
2023-10-10 05:39:02,200 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:39:02,201 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37493. Reason: nanny-close
2023-10-10 05:39:02,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:39:02,203 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49410; closing.
2023-10-10 05:39:02,203 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37493', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916342.2035575')
2023-10-10 05:39:02,203 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:39:02,205 - distributed.nanny - INFO - Worker closed
2023-10-10 05:39:03,216 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:39:03,216 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:39:03,216 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:39:03,217 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:39:03,218 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-10 05:39:05,314 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:39:05,318 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40497 instead
  warnings.warn(
2023-10-10 05:39:05,322 - distributed.scheduler - INFO - State start
2023-10-10 05:39:05,377 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-10 05:39:05,379 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-10 05:39:05,379 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40497/status
2023-10-10 05:39:05,380 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-10 05:39:05,570 - distributed.scheduler - INFO - Receive client connection: Client-5242faf0-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:39:05,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49488
2023-10-10 05:39:05,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42995'
2023-10-10 05:39:07,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-10 05:39:07,553 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-10 05:39:07,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-10 05:39:08,436 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39867
2023-10-10 05:39:08,436 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39867
2023-10-10 05:39:08,437 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35297
2023-10-10 05:39:08,437 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-10 05:39:08,437 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:39:08,437 - distributed.worker - INFO -               Threads:                          1
2023-10-10 05:39:08,437 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-10 05:39:08,437 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-29x3uo2r
2023-10-10 05:39:08,438 - distributed.worker - INFO - Starting Worker plugin PreImport-dd9f3540-69a4-43dc-85bd-ba1c97fb7b80
2023-10-10 05:39:08,438 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d8b3410a-b277-4d1f-9ad7-4cd3a19a28d8
2023-10-10 05:39:08,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df784269-fcac-4219-b1c3-37c7409fe659
2023-10-10 05:39:08,548 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:39:08,580 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39867', status: init, memory: 0, processing: 0>
2023-10-10 05:39:08,581 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39867
2023-10-10 05:39:08,581 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49522
2023-10-10 05:39:08,583 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-10 05:39:08,584 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-10 05:39:08,584 - distributed.worker - INFO - -------------------------------------------------
2023-10-10 05:39:08,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-10 05:39:08,647 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-10-10 05:39:08,651 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-10 05:39:08,655 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:39:08,657 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-10 05:39:08,659 - distributed.scheduler - INFO - Remove client Client-5242faf0-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:39:08,659 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49488; closing.
2023-10-10 05:39:08,660 - distributed.scheduler - INFO - Remove client Client-5242faf0-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:39:08,660 - distributed.scheduler - INFO - Close client connection: Client-5242faf0-672f-11ee-b141-d8c49764f6bb
2023-10-10 05:39:08,661 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42995'. Reason: nanny-close
2023-10-10 05:39:08,661 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-10 05:39:08,662 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39867. Reason: nanny-close
2023-10-10 05:39:08,664 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49522; closing.
2023-10-10 05:39:08,664 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-10 05:39:08,665 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696916348.6649384')
2023-10-10 05:39:08,665 - distributed.scheduler - INFO - Lost all workers
2023-10-10 05:39:08,666 - distributed.nanny - INFO - Worker closed
2023-10-10 05:39:09,627 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-10 05:39:09,627 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-10 05:39:09,628 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-10 05:39:09,629 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-10 05:39:09,629 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37451 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46575 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39693 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36459 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41189 instead
  warnings.warn(
2023-10-10 05:40:14,100 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-10 05:40:14,103 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-10 05:40:14,104 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-10 05:40:14,109 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-10 05:40:14,108 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-10 05:40:14,110 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-10 05:40:14,111 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-10 05:40:14,115 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://10.33.225.163:47195', name: 3, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-10 05:40:14,120 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1237' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1238' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1236' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40853 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42223 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41345 instead
  warnings.warn(
2023-10-10 05:41:03,062 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-10 05:41:03,065 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://10.33.225.163:39641', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46023 instead
  warnings.warn(
2023-10-10 05:41:23,734 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-10 05:41:23,738 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #018] ep: 0x7f5e718a60c0, tag: 0x2ff84f5973de01c7, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #018] ep: 0x7f5e718a60c0, tag: 0x2ff84f5973de01c7, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-10 05:41:23,742 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:51911', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37661 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39837 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35805 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45251 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39213 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36965 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44865 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44159 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44349 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36997 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36485 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37027 instead
  warnings.warn(
[1696916685.222290] [dgx13:71306:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:52176) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41635 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37963 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43035 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44997 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39781 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35743 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43049 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40529 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35981 instead
  warnings.warn(
[1696916907.064804] [dgx13:74924:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:35614) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37597 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33841 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40333 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42901 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34443 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36129 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34259 instead
  warnings.warn(
2023-10-10 05:50:48,740 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-10 05:50:48,748 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-10 05:50:48,751 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-10 05:50:48,757 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46775'.
2023-10-10 05:50:48,757 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:54978 remote=tcp://127.0.0.1:44749>: Stream is closed
2023-10-10 05:50:48,757 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-10 05:50:48,760 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f8f8632e7f0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-10 05:50:48,762 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:42185'.
2023-10-10 05:50:48,762 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55010 remote=tcp://127.0.0.1:44749>: Stream is closed
2023-10-10 05:50:48,763 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:54996 remote=tcp://127.0.0.1:44749>: Stream is closed
2023-10-10 05:50:48,764 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:44553'.
2023-10-10 05:50:48,765 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f29db41f7f0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-10 05:50:48,768 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46073'.
2023-10-10 05:50:48,768 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:46073'. Shutting down.
2023-10-10 05:50:48,767 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f0cb4d3e7f0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-10 05:50:48,772 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f8cd98e47f0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-10 05:50:50,763 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-10 05:50:50,768 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-10 05:50:50,771 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-10-10 05:50:50,776 - distributed.nanny - ERROR - Worker process died unexpectedly
