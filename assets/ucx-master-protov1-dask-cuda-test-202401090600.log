============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-09 06:28:52,743 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:28:52,748 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:28:52,752 - distributed.scheduler - INFO - State start
2024-01-09 06:28:52,791 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:28:52,793 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-09 06:28:52,795 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:28:52,796 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:28:52,919 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38249'
2024-01-09 06:28:52,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44303'
2024-01-09 06:28:52,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37587'
2024-01-09 06:28:52,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33331'
2024-01-09 06:28:53,691 - distributed.scheduler - INFO - Receive client connection: Client-5ae1f4f2-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:28:53,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53180
2024-01-09 06:28:54,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:28:54,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:28:54,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:28:54,822 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:28:54,825 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:28:54,825 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:28:54,826 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33655
2024-01-09 06:28:54,826 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34101
2024-01-09 06:28:54,826 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33655
2024-01-09 06:28:54,826 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34101
2024-01-09 06:28:54,827 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37797
2024-01-09 06:28:54,827 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33819
2024-01-09 06:28:54,827 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:28:54,827 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:28:54,827 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,827 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,827 - distributed.worker - INFO -               Threads:                          4
2024-01-09 06:28:54,827 - distributed.worker - INFO -               Threads:                          4
2024-01-09 06:28:54,827 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-09 06:28:54,827 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-09 06:28:54,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-pe0dtozd
2024-01-09 06:28:54,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-mmygpcbp
2024-01-09 06:28:54,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5175ebd3-f53d-4844-9ca5-603762675953
2024-01-09 06:28:54,827 - distributed.worker - INFO - Starting Worker plugin PreImport-3809d9c6-e13e-49f6-bc56-263356d80311
2024-01-09 06:28:54,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-917b933c-065e-43d0-a445-3da373afa613
2024-01-09 06:28:54,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30ec4808-f00c-4748-903f-50070999837e
2024-01-09 06:28:54,827 - distributed.worker - INFO - Starting Worker plugin PreImport-6da08c8f-9b0b-4d28-a24f-8c3ce018a1d2
2024-01-09 06:28:54,827 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c12e589-ac26-4ee4-a0f2-a40f4b5c09f4
2024-01-09 06:28:54,828 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,872 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:28:54,872 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:28:54,874 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:28:54,874 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:28:54,876 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:28:54,877 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44659
2024-01-09 06:28:54,877 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44659
2024-01-09 06:28:54,877 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42831
2024-01-09 06:28:54,877 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:28:54,877 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,877 - distributed.worker - INFO -               Threads:                          4
2024-01-09 06:28:54,878 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-09 06:28:54,878 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-77_ycnok
2024-01-09 06:28:54,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6930d47-82f6-461f-8e8e-cc6be2b94833
2024-01-09 06:28:54,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6306ab3-7ed6-4eea-99ec-e4f424a3c755
2024-01-09 06:28:54,878 - distributed.worker - INFO - Starting Worker plugin PreImport-2317f68f-60f5-47aa-a8a2-ec28764fac3e
2024-01-09 06:28:54,878 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,878 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:28:54,879 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39579
2024-01-09 06:28:54,879 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39579
2024-01-09 06:28:54,879 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35829
2024-01-09 06:28:54,879 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:28:54,879 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,879 - distributed.worker - INFO -               Threads:                          4
2024-01-09 06:28:54,879 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-09 06:28:54,879 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-d22xrhe0
2024-01-09 06:28:54,880 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c14d3608-90d3-44ee-953b-e377151441cf
2024-01-09 06:28:54,880 - distributed.worker - INFO - Starting Worker plugin PreImport-5c6a8892-1897-4a2a-bbdc-8f1e200fef45
2024-01-09 06:28:54,880 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efd56759-de58-4aa2-a42a-f698220a3181
2024-01-09 06:28:54,880 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,922 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34101', status: init, memory: 0, processing: 0>
2024-01-09 06:28:54,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34101
2024-01-09 06:28:54,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53252
2024-01-09 06:28:54,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:28:54,925 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33655', status: init, memory: 0, processing: 0>
2024-01-09 06:28:54,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33655
2024-01-09 06:28:54,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53256
2024-01-09 06:28:54,926 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:28:54,926 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:28:54,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:28:54,928 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:28:54,928 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,930 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:28:54,973 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44659', status: init, memory: 0, processing: 0>
2024-01-09 06:28:54,974 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44659
2024-01-09 06:28:54,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53272
2024-01-09 06:28:54,975 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39579', status: init, memory: 0, processing: 0>
2024-01-09 06:28:54,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:28:54,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39579
2024-01-09 06:28:54,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53284
2024-01-09 06:28:54,976 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:28:54,976 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,977 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:28:54,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:28:54,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:28:54,978 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:28:54,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:28:55,051 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:28:55,051 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:28:55,051 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:28:55,051 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:28:55,056 - distributed.scheduler - INFO - Remove client Client-5ae1f4f2-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:28:55,056 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53180; closing.
2024-01-09 06:28:55,057 - distributed.scheduler - INFO - Remove client Client-5ae1f4f2-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:28:55,057 - distributed.scheduler - INFO - Close client connection: Client-5ae1f4f2-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:28:55,622 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35201', status: init, memory: 0, processing: 0>
2024-01-09 06:28:55,623 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35201
2024-01-09 06:28:55,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53298
2024-01-09 06:28:55,638 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53298; closing.
2024-01-09 06:28:55,639 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35201', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781735.6392205')
2024-01-09 06:28:55,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43161', status: init, memory: 0, processing: 0>
2024-01-09 06:28:55,697 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43161
2024-01-09 06:28:55,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53314
2024-01-09 06:28:55,721 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40487', status: init, memory: 0, processing: 0>
2024-01-09 06:28:55,722 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40487
2024-01-09 06:28:55,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53326
2024-01-09 06:28:55,740 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53314; closing.
2024-01-09 06:28:55,741 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43161', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781735.741077')
2024-01-09 06:28:55,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53326; closing.
2024-01-09 06:28:55,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40487', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781735.7431097')
2024-01-09 06:28:55,754 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35367', status: init, memory: 0, processing: 0>
2024-01-09 06:28:55,754 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35367
2024-01-09 06:28:55,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53338
2024-01-09 06:28:55,796 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53338; closing.
2024-01-09 06:28:55,797 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35367', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781735.7972844')
2024-01-09 06:28:56,824 - distributed.scheduler - INFO - Receive client connection: Client-5a625f7c-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:28:56,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53382
2024-01-09 06:28:56,834 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:28:56,834 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:28:56,834 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:28:56,834 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-09 06:28:56,839 - distributed.scheduler - INFO - Remove client Client-5a625f7c-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:28:56,839 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53382; closing.
2024-01-09 06:28:56,840 - distributed.scheduler - INFO - Remove client Client-5a625f7c-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:28:56,840 - distributed.scheduler - INFO - Close client connection: Client-5a625f7c-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:28:56,841 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37587'. Reason: nanny-close
2024-01-09 06:28:56,842 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:28:56,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38249'. Reason: nanny-close
2024-01-09 06:28:56,842 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:28:56,843 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44303'. Reason: nanny-close
2024-01-09 06:28:56,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44659. Reason: nanny-close
2024-01-09 06:28:56,843 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:28:56,843 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33331'. Reason: nanny-close
2024-01-09 06:28:56,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33655. Reason: nanny-close
2024-01-09 06:28:56,844 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:28:56,844 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34101. Reason: nanny-close
2024-01-09 06:28:56,844 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39579. Reason: nanny-close
2024-01-09 06:28:56,845 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:28:56,845 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53272; closing.
2024-01-09 06:28:56,846 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781736.845941')
2024-01-09 06:28:56,846 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:28:56,846 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:28:56,846 - distributed.nanny - INFO - Worker closed
2024-01-09 06:28:56,846 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:28:56,847 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53284; closing.
2024-01-09 06:28:56,847 - distributed.nanny - INFO - Worker closed
2024-01-09 06:28:56,847 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53256; closing.
2024-01-09 06:28:56,847 - distributed.nanny - INFO - Worker closed
2024-01-09 06:28:56,847 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53252; closing.
2024-01-09 06:28:56,848 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39579', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781736.8482313')
2024-01-09 06:28:56,848 - distributed.nanny - INFO - Worker closed
2024-01-09 06:28:56,848 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781736.8486245')
2024-01-09 06:28:56,849 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34101', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781736.849042')
2024-01-09 06:28:56,849 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:28:57,657 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:28:57,658 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:28:57,658 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:28:57,660 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-09 06:28:57,661 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-09 06:29:00,107 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:00,113 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39653 instead
  warnings.warn(
2024-01-09 06:29:00,118 - distributed.scheduler - INFO - State start
2024-01-09 06:29:00,142 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:00,143 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:29:00,144 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:29:00,145 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:29:00,276 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37845'
2024-01-09 06:29:00,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36151'
2024-01-09 06:29:00,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38593'
2024-01-09 06:29:00,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38749'
2024-01-09 06:29:00,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34037'
2024-01-09 06:29:00,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46531'
2024-01-09 06:29:00,344 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42079'
2024-01-09 06:29:00,354 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46451'
2024-01-09 06:29:02,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:02,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:02,388 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:02,389 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38031
2024-01-09 06:29:02,389 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38031
2024-01-09 06:29:02,389 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45687
2024-01-09 06:29:02,389 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:02,389 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:02,389 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:02,389 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:02,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fv2dcrz4
2024-01-09 06:29:02,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64b07290-312e-4be6-b490-f160e59375e1
2024-01-09 06:29:02,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:02,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:02,565 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:02,566 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33887
2024-01-09 06:29:02,566 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33887
2024-01-09 06:29:02,567 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41335
2024-01-09 06:29:02,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:02,567 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:02,567 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:02,567 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:02,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b3sja4ss
2024-01-09 06:29:02,567 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65c62a28-3381-4e92-b66e-88ea0e88bbba
2024-01-09 06:29:02,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:02,589 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:02,594 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:02,595 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38397
2024-01-09 06:29:02,595 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38397
2024-01-09 06:29:02,595 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33707
2024-01-09 06:29:02,595 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:02,595 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:02,595 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:02,596 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:02,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4rtekld3
2024-01-09 06:29:02,596 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61b7ce22-8726-4124-b522-501edd0c5dd7
2024-01-09 06:29:02,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:02,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:02,751 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:02,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:02,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:02,752 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35979
2024-01-09 06:29:02,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35979
2024-01-09 06:29:02,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38979
2024-01-09 06:29:02,753 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:02,753 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:02,753 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:02,753 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:02,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q4fw_ufd
2024-01-09 06:29:02,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e33d5f0-1db3-47ca-b304-d6d2d1a342d7
2024-01-09 06:29:02,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:02,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:02,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:02,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:02,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:02,756 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:02,757 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:02,759 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37265
2024-01-09 06:29:02,759 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37265
2024-01-09 06:29:02,759 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35397
2024-01-09 06:29:02,759 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:02,759 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:02,759 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:02,759 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:02,759 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-23zkw1xd
2024-01-09 06:29:02,759 - distributed.worker - INFO - Starting Worker plugin PreImport-3379b4c9-75b2-4ef9-b244-11cbfdfb08d5
2024-01-09 06:29:02,759 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bf03261-8aa5-4910-ba62-6fde97bc0be0
2024-01-09 06:29:02,760 - distributed.worker - INFO - Starting Worker plugin RMMSetup-501d1c0c-01d4-496f-8ac8-50ff8eed70a6
2024-01-09 06:29:02,761 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:02,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:02,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38097
2024-01-09 06:29:02,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38097
2024-01-09 06:29:02,763 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43017
2024-01-09 06:29:02,763 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:02,763 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:02,764 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:02,764 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:02,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4sj0xdmc
2024-01-09 06:29:02,764 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:02,764 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39299
2024-01-09 06:29:02,764 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39299
2024-01-09 06:29:02,764 - distributed.worker - INFO - Starting Worker plugin RMMSetup-296d9d64-e0b9-4527-979f-de0aeb77d66c
2024-01-09 06:29:02,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45915
2024-01-09 06:29:02,764 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:02,764 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:02,764 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:02,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:02,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2tonnrn8
2024-01-09 06:29:02,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81298e83-c6aa-480e-9d91-3b0c24bd2abc
2024-01-09 06:29:02,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37111
2024-01-09 06:29:02,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37111
2024-01-09 06:29:02,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35405
2024-01-09 06:29:02,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:02,766 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:02,766 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:02,766 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:02,766 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jixbkmew
2024-01-09 06:29:02,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ead867cc-1301-4824-9dc3-a528ebc3c8d6
2024-01-09 06:29:03,401 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37845'. Reason: nanny-close
2024-01-09 06:29:03,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36151'. Reason: nanny-close
2024-01-09 06:29:03,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38593'. Reason: nanny-close
2024-01-09 06:29:03,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38749'. Reason: nanny-close
2024-01-09 06:29:03,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34037'. Reason: nanny-close
2024-01-09 06:29:03,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46531'. Reason: nanny-close
2024-01-09 06:29:03,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42079'. Reason: nanny-close
2024-01-09 06:29:03,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46451'. Reason: nanny-close
2024-01-09 06:29:05,415 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7eeb799d-2d4a-4cf1-b7d2-06a146ca33d7
2024-01-09 06:29:05,416 - distributed.worker - INFO - Starting Worker plugin PreImport-d9fb644c-8cd3-458f-8a07-a4d175ed9f90
2024-01-09 06:29:05,417 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:05,456 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:05,457 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:05,457 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:05,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:05,505 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:05,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38031. Reason: nanny-close
2024-01-09 06:29:05,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9ab8348-aa68-4ecc-83f3-f52f436bc020
2024-01-09 06:29:05,509 - distributed.worker - INFO - Starting Worker plugin PreImport-7f5f6432-e5df-47cb-88d1-e4f056879ec6
2024-01-09 06:29:05,510 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:05,510 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:40292 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:40292 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-01-09 06:29:05,515 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:05,515 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:9369; closing.
2024-01-09 06:29:05,521 - distributed.worker - INFO - Starting Worker plugin PreImport-15c0acc6-0be4-4e65-baf5-15332607ea0b
2024-01-09 06:29:05,522 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-863d15b3-f00e-43f3-b882-cd65f8f3e5b8
2024-01-09 06:29:05,522 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:05,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-907918cc-9012-4e8b-ab2c-10ff318fb94b
2024-01-09 06:29:05,536 - distributed.worker - INFO - Starting Worker plugin PreImport-f38d8e26-97c4-47f2-b947-0679373b5d52
2024-01-09 06:29:05,536 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:05,541 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:05,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c6124ac-5bd1-4d01-9755-09cd6f00c009
2024-01-09 06:29:05,551 - distributed.worker - INFO - Starting Worker plugin PreImport-b01b5723-6381-42d4-a20d-3cecd9202f80
2024-01-09 06:29:05,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:05,563 - distributed.worker - INFO - Starting Worker plugin PreImport-d4bf2bdd-f972-42e4-b92a-0189c58270bd
2024-01-09 06:29:05,564 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7860412-d4fb-4770-bfcb-00b214bef06e
2024-01-09 06:29:05,565 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a2332e3-0bcd-4a58-9abf-dae73527d51b
2024-01-09 06:29:05,565 - distributed.worker - INFO - Starting Worker plugin PreImport-18aab6c6-a6c1-4a89-8998-f158599d5b9e
2024-01-09 06:29:05,565 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:05,566 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:08,376 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:08,377 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:08,377 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:08,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:08,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:08,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38397. Reason: nanny-close
2024-01-09 06:29:08,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:08,415 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:08,598 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:08,599 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:08,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:08,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:08,605 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:08,606 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37265. Reason: nanny-close
2024-01-09 06:29:08,609 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:08,610 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:40202 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-09 06:29:08,917 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64732 parent=64539 started daemon>
2024-01-09 06:29:08,917 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64724 parent=64539 started daemon>
2024-01-09 06:29:08,918 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64719 parent=64539 started daemon>
2024-01-09 06:29:08,918 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64714 parent=64539 started daemon>
2024-01-09 06:29:08,918 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64709 parent=64539 started daemon>
2024-01-09 06:29:08,918 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64705 parent=64539 started daemon>
2024-01-09 06:29:09,083 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 64705 exit status was already read will report exitcode 255
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-09 06:29:11,558 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:11,563 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38419 instead
  warnings.warn(
2024-01-09 06:29:11,567 - distributed.scheduler - INFO - State start
2024-01-09 06:29:11,569 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2tonnrn8', purging
2024-01-09 06:29:11,570 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-q4fw_ufd', purging
2024-01-09 06:29:11,570 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4sj0xdmc', purging
2024-01-09 06:29:11,571 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-b3sja4ss', purging
2024-01-09 06:29:11,571 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jixbkmew', purging
2024-01-09 06:29:11,950 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:11,952 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:29:11,952 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:29:11,953 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:29:12,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42791'
2024-01-09 06:29:12,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36835'
2024-01-09 06:29:12,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32809'
2024-01-09 06:29:12,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44731'
2024-01-09 06:29:12,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42467'
2024-01-09 06:29:12,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34673'
2024-01-09 06:29:12,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45717'
2024-01-09 06:29:12,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40251'
2024-01-09 06:29:12,364 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42791'. Reason: nanny-close
2024-01-09 06:29:12,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36835'. Reason: nanny-close
2024-01-09 06:29:12,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32809'. Reason: nanny-close
2024-01-09 06:29:12,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44731'. Reason: nanny-close
2024-01-09 06:29:12,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42467'. Reason: nanny-close
2024-01-09 06:29:12,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34673'. Reason: nanny-close
2024-01-09 06:29:12,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45717'. Reason: nanny-close
2024-01-09 06:29:12,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40251'. Reason: nanny-close
2024-01-09 06:29:14,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:14,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:14,316 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:14,316 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:14,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:14,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:14,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:14,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45571
2024-01-09 06:29:14,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45571
2024-01-09 06:29:14,322 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:14,322 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46459
2024-01-09 06:29:14,322 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:14,322 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:14,322 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:14,323 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:14,323 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tuh2tpqh
2024-01-09 06:29:14,323 - distributed.worker - INFO - Starting Worker plugin RMMSetup-343ead0c-bd79-46e5-81c6-7dcaa374caab
2024-01-09 06:29:14,326 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40433
2024-01-09 06:29:14,326 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40433
2024-01-09 06:29:14,326 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38095
2024-01-09 06:29:14,327 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:14,327 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:14,327 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:14,327 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:14,327 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ysr4ty8q
2024-01-09 06:29:14,327 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c4e17fa-eefd-4493-bf10-44bee22f2a77
2024-01-09 06:29:14,328 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:14,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46489
2024-01-09 06:29:14,329 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46489
2024-01-09 06:29:14,329 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44059
2024-01-09 06:29:14,329 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:14,329 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:14,329 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:14,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:14,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jhp5t33i
2024-01-09 06:29:14,330 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5741c55-ce16-4e9a-80eb-064d087d48ff
2024-01-09 06:29:14,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:14,401 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:14,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:14,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:14,408 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:14,410 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35129
2024-01-09 06:29:14,410 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35129
2024-01-09 06:29:14,410 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36715
2024-01-09 06:29:14,410 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:14,410 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:14,410 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:14,410 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:14,410 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:14,410 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:14,410 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k5v7wig_
2024-01-09 06:29:14,411 - distributed.worker - INFO - Starting Worker plugin PreImport-323a9129-f246-486e-8d5d-497efbe12d39
2024-01-09 06:29:14,411 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a2bf581a-718e-4949-965b-d6f999b0c135
2024-01-09 06:29:14,411 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a86443d8-6b95-4459-b5ef-0c559d98b04f
2024-01-09 06:29:14,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:14,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:14,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:14,412 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36895
2024-01-09 06:29:14,412 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36895
2024-01-09 06:29:14,412 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37391
2024-01-09 06:29:14,412 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:14,412 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:14,412 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:14,412 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:14,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d9punnq9
2024-01-09 06:29:14,413 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9928a715-42a8-413e-8681-ae6e8a1507df
2024-01-09 06:29:14,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:14,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:14,415 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42461
2024-01-09 06:29:14,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42461
2024-01-09 06:29:14,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35231
2024-01-09 06:29:14,416 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:14,416 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:14,416 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:14,416 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:14,416 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-09fdqkh2
2024-01-09 06:29:14,416 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28925cfa-9e0d-45f6-9abb-a74236b7c7ae
2024-01-09 06:29:14,416 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43647
2024-01-09 06:29:14,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43647
2024-01-09 06:29:14,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35961
2024-01-09 06:29:14,417 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:14,417 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:14,417 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:14,417 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:14,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zne4rhf0
2024-01-09 06:29:14,417 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56b1ac36-6208-4b0b-ad53-5c9599997f74
2024-01-09 06:29:14,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:14,622 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:14,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:14,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43927
2024-01-09 06:29:14,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43927
2024-01-09 06:29:14,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33731
2024-01-09 06:29:14,627 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:14,628 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:14,628 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:14,628 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:14,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-07zwqv7m
2024-01-09 06:29:14,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43e7bf2c-7194-423f-a4ad-023049ae6dac
2024-01-09 06:29:15,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41ceb73f-2d69-456f-a52c-5e4d30afc151
2024-01-09 06:29:15,167 - distributed.worker - INFO - Starting Worker plugin PreImport-51110232-4cfc-4e0d-9f90-215990df7446
2024-01-09 06:29:15,168 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81ffcbd9-f750-4d54-aa62-80b19cec3db3
2024-01-09 06:29:16,328 - distributed.worker - INFO - Starting Worker plugin PreImport-730fa0b0-e7a9-4c5c-9685-f8b2e1e9508d
2024-01-09 06:29:16,329 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:16,369 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:16,369 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:16,381 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:16,382 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40433. Reason: nanny-close
2024-01-09 06:29:16,384 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:16,385 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:16,444 - distributed.worker - INFO - Starting Worker plugin PreImport-4da20770-6e42-4ad2-91a2-3d31ad600403
2024-01-09 06:29:16,444 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea3d7199-1259-4b21-904b-16a643eb98a9
2024-01-09 06:29:16,445 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,484 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:16,485 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:16,485 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,488 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:16,494 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:16,495 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46489. Reason: nanny-close
2024-01-09 06:29:16,497 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:16,499 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:16,591 - distributed.worker - INFO - Starting Worker plugin PreImport-49e6a608-7c0b-480b-98cc-fcc1036c9679
2024-01-09 06:29:16,592 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b607051-9f59-47bf-8f23-5bb2cdcc392d
2024-01-09 06:29:16,593 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,606 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c144acc2-e93e-4812-947f-eec253e276a8
2024-01-09 06:29:16,606 - distributed.worker - INFO - Starting Worker plugin PreImport-9d619e66-c6bf-4c98-8c2d-891c41a8f3a9
2024-01-09 06:29:16,607 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,609 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af746052-1896-4355-825c-2bf33ce07667
2024-01-09 06:29:16,610 - distributed.worker - INFO - Starting Worker plugin PreImport-34209942-b216-4da7-b4c9-e6ec665c52b6
2024-01-09 06:29:16,611 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,618 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44ebf707-f9d7-404f-86eb-58ce9f047596
2024-01-09 06:29:16,618 - distributed.worker - INFO - Starting Worker plugin PreImport-d49eb2ac-fdbc-4443-a475-78e37adca986
2024-01-09 06:29:16,619 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,618 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:16,629 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:16,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:16,635 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:16,635 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36895. Reason: nanny-close
2024-01-09 06:29:16,637 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:16,639 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:16,640 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:16,641 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:16,641 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:16,646 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:16,647 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:16,648 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:16,651 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:16,652 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:16,653 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,654 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:16,654 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:16,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:16,655 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:16,685 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:16,686 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:16,687 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:16,687 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35129. Reason: nanny-close
2024-01-09 06:29:16,688 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43647. Reason: nanny-close
2024-01-09 06:29:16,688 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42461. Reason: nanny-close
2024-01-09 06:29:16,689 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:16,691 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:16,692 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:16,692 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:16,695 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:16,696 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:16,697 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:16,697 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43927. Reason: nanny-close
2024-01-09 06:29:16,701 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:16,704 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:16,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:16,734 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:16,735 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:16,737 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:57200 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-09 06:29:16,774 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65000 parent=64803 started daemon>
2024-01-09 06:29:16,787 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64996 parent=64803 started daemon>
2024-01-09 06:29:16,788 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64992 parent=64803 started daemon>
2024-01-09 06:29:16,789 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64988 parent=64803 started daemon>
2024-01-09 06:29:16,789 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64983 parent=64803 started daemon>
2024-01-09 06:29:16,789 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64974 parent=64803 started daemon>
2024-01-09 06:29:16,789 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64969 parent=64803 started daemon>
2024-01-09 06:29:17,020 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 64969 exit status was already read will report exitcode 255
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-09 06:29:19,472 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:19,477 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34985 instead
  warnings.warn(
2024-01-09 06:29:19,481 - distributed.scheduler - INFO - State start
2024-01-09 06:29:19,482 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-tuh2tpqh', purging
2024-01-09 06:29:19,534 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:19,535 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:29:19,535 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:29:19,536 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:29:20,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46263'
2024-01-09 06:29:20,454 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39203'
2024-01-09 06:29:20,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46575'
2024-01-09 06:29:20,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41051'
2024-01-09 06:29:20,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44229'
2024-01-09 06:29:20,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45433'
2024-01-09 06:29:20,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42999'
2024-01-09 06:29:20,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36335'
2024-01-09 06:29:20,950 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46263'. Reason: nanny-close
2024-01-09 06:29:20,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39203'. Reason: nanny-close
2024-01-09 06:29:20,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46575'. Reason: nanny-close
2024-01-09 06:29:20,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41051'. Reason: nanny-close
2024-01-09 06:29:20,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44229'. Reason: nanny-close
2024-01-09 06:29:20,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45433'. Reason: nanny-close
2024-01-09 06:29:20,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42999'. Reason: nanny-close
2024-01-09 06:29:20,952 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36335'. Reason: nanny-close
2024-01-09 06:29:22,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:22,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:22,392 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:22,393 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34141
2024-01-09 06:29:22,393 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34141
2024-01-09 06:29:22,394 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35309
2024-01-09 06:29:22,394 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:22,394 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:22,394 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:22,394 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:22,394 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ax_ys56
2024-01-09 06:29:22,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:22,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:22,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5ea915d-120d-4032-959d-180cbfa03729
2024-01-09 06:29:22,398 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:22,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44769
2024-01-09 06:29:22,399 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44769
2024-01-09 06:29:22,399 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45377
2024-01-09 06:29:22,399 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:22,399 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:22,399 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:22,399 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:22,399 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vywv9pb4
2024-01-09 06:29:22,400 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7dc82539-4ac1-4909-86ac-d3bbfb4ad587
2024-01-09 06:29:22,401 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:22,401 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:22,406 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:22,407 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37387
2024-01-09 06:29:22,407 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37387
2024-01-09 06:29:22,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38253
2024-01-09 06:29:22,407 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:22,407 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:22,407 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:22,407 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:22,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kzn23gm6
2024-01-09 06:29:22,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-131c90ab-131d-4449-b24a-c15d2f0c9d1c
2024-01-09 06:29:22,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:22,426 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:22,430 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:22,431 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38967
2024-01-09 06:29:22,431 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38967
2024-01-09 06:29:22,431 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36719
2024-01-09 06:29:22,431 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:22,431 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:22,431 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:22,431 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:22,432 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lz1tx2wh
2024-01-09 06:29:22,432 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8eea3624-12a6-4225-aa39-f318bb914cfa
2024-01-09 06:29:22,442 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:22,442 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:22,442 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:22,442 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:22,447 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:22,447 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:22,448 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34191
2024-01-09 06:29:22,448 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34191
2024-01-09 06:29:22,448 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41161
2024-01-09 06:29:22,448 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:22,448 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:22,448 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44137
2024-01-09 06:29:22,448 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:22,448 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44137
2024-01-09 06:29:22,448 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:22,448 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38801
2024-01-09 06:29:22,448 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ut2prxmm
2024-01-09 06:29:22,448 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:22,448 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:22,448 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:22,448 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:22,448 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80ae3d8b-daba-4842-8caa-47218bbd19ba
2024-01-09 06:29:22,448 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sb8grnx4
2024-01-09 06:29:22,448 - distributed.worker - INFO - Starting Worker plugin RMMSetup-794eb993-bd51-48c8-a476-03877d6c9d7a
2024-01-09 06:29:22,456 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:22,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:22,461 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:22,461 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40649
2024-01-09 06:29:22,461 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40649
2024-01-09 06:29:22,462 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44767
2024-01-09 06:29:22,462 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:22,462 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:22,462 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:22,462 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:22,462 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eiyrgc42
2024-01-09 06:29:22,462 - distributed.worker - INFO - Starting Worker plugin RMMSetup-366a42fa-52bb-4890-ad7a-8be16a6314d1
2024-01-09 06:29:22,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:22,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:22,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:22,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36505
2024-01-09 06:29:22,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36505
2024-01-09 06:29:22,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38575
2024-01-09 06:29:22,526 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:22,526 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:22,526 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:22,526 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:22,526 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f_webvvw
2024-01-09 06:29:22,526 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9463d53d-34fe-4b98-9dc2-46f06acac2cb
2024-01-09 06:29:24,662 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e43815e4-af5a-4b76-8911-24aa8251985e
2024-01-09 06:29:24,664 - distributed.worker - INFO - Starting Worker plugin PreImport-b342f8ca-239c-4b58-a9ca-aeb6995711d8
2024-01-09 06:29:24,666 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,667 - distributed.worker - INFO - Starting Worker plugin PreImport-63980470-c762-47c0-bf01-4e9eb7cd6556
2024-01-09 06:29:24,667 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e05e29e5-086c-4143-a640-18e9b74804d8
2024-01-09 06:29:24,668 - distributed.worker - INFO - Starting Worker plugin PreImport-de9b524e-dd70-48a3-9019-51203ee0f775
2024-01-09 06:29:24,669 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,670 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfab0212-3213-42ed-a049-6c106be9f172
2024-01-09 06:29:24,671 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,700 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b06283f-63a0-4a33-8740-30232a863792
2024-01-09 06:29:24,703 - distributed.worker - INFO - Starting Worker plugin PreImport-91a2893e-98bc-49c4-b415-be35c94ec873
2024-01-09 06:29:24,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,759 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-838668e3-62cf-4d44-8879-9304f8846710
2024-01-09 06:29:24,759 - distributed.worker - INFO - Starting Worker plugin PreImport-e5d77b1d-20f1-48b8-9095-b77e02df4c11
2024-01-09 06:29:24,759 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,760 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:24,761 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b2e778c-620b-4c8b-90bd-37b07ac05701
2024-01-09 06:29:24,761 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:24,761 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,761 - distributed.worker - INFO - Starting Worker plugin PreImport-4a2e3df4-8b5d-43cd-8b88-7f0816bd13da
2024-01-09 06:29:24,763 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:24,764 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:24,765 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:24,765 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:24,766 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:24,766 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:24,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:24,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b0da409-e06a-4e8a-be10-2ab15633ffe3
2024-01-09 06:29:24,780 - distributed.worker - INFO - Starting Worker plugin PreImport-07a4e69b-bd02-4d59-b3c1-e71d561d193e
2024-01-09 06:29:24,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,790 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:24,791 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:24,792 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,796 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:24,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:24,801 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:24,801 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:24,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:24,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:24,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:24,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:24,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38967. Reason: nanny-close
2024-01-09 06:29:24,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40649. Reason: nanny-close
2024-01-09 06:29:24,813 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:24,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44769. Reason: nanny-close
2024-01-09 06:29:24,813 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae9b17b7-59e9-4f8b-b8fa-8cf71c7c6497
2024-01-09 06:29:24,814 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34141. Reason: nanny-close
2024-01-09 06:29:24,815 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37387. Reason: nanny-close
2024-01-09 06:29:24,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:24,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:24,818 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:24,818 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:24,819 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:24,814 - distributed.worker - INFO - Starting Worker plugin PreImport-fab12c3f-7a36-4b16-89e0-b371aeb25eac
2024-01-09 06:29:24,820 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:24,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:24,821 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:24,822 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:24,823 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:24,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,827 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:24,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:24,828 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:24,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:24,832 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:24,832 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:24,861 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:24,862 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:24,863 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44137. Reason: nanny-close
2024-01-09 06:29:24,864 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34191. Reason: nanny-close
2024-01-09 06:29:24,866 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:24,868 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:24,868 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:24,872 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:24,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:24,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:24,889 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:24,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:24,914 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:24,915 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36505. Reason: nanny-close
2024-01-09 06:29:24,917 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:24,918 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:35398 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-09 06:29:25,289 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65287 parent=65090 started daemon>
2024-01-09 06:29:25,289 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65283 parent=65090 started daemon>
2024-01-09 06:29:25,289 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65275 parent=65090 started daemon>
2024-01-09 06:29:25,289 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65270 parent=65090 started daemon>
2024-01-09 06:29:25,290 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65265 parent=65090 started daemon>
2024-01-09 06:29:25,290 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65260 parent=65090 started daemon>
2024-01-09 06:29:25,290 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65256 parent=65090 started daemon>
2024-01-09 06:29:25,520 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65256 exit status was already read will report exitcode 255
2024-01-09 06:29:25,559 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65275 exit status was already read will report exitcode 255
2024-01-09 06:29:25,594 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65265 exit status was already read will report exitcode 255
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-09 06:29:28,042 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:28,046 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45975 instead
  warnings.warn(
2024-01-09 06:29:28,050 - distributed.scheduler - INFO - State start
2024-01-09 06:29:28,184 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:28,185 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:29:28,185 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:29:28,186 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:29:28,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41887'
2024-01-09 06:29:28,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38273'
2024-01-09 06:29:28,802 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39033'
2024-01-09 06:29:28,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37697'
2024-01-09 06:29:28,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36235'
2024-01-09 06:29:28,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43295'
2024-01-09 06:29:28,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44815'
2024-01-09 06:29:28,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37119'
2024-01-09 06:29:30,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:30,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:30,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:30,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:30,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:30,741 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41205
2024-01-09 06:29:30,741 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41205
2024-01-09 06:29:30,741 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34109
2024-01-09 06:29:30,741 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:30,741 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:30,741 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:30,741 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:30,741 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t232f9i5
2024-01-09 06:29:30,741 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-521772bd-7919-49a6-8158-b2442871ffeb
2024-01-09 06:29:30,743 - distributed.worker - INFO - Starting Worker plugin PreImport-9061938f-0767-4046-9df7-11c84a137bcb
2024-01-09 06:29:30,743 - distributed.worker - INFO - Starting Worker plugin RMMSetup-528a48a5-c421-4fda-8777-047ce3c3a715
2024-01-09 06:29:30,743 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:30,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37591
2024-01-09 06:29:30,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37591
2024-01-09 06:29:30,744 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34285
2024-01-09 06:29:30,744 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:30,744 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:30,744 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:30,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:30,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fzmw3dba
2024-01-09 06:29:30,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ae83acb-fa74-481e-b91c-13f3044b89d3
2024-01-09 06:29:30,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:30,795 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:30,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:30,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:30,800 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:30,800 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33657
2024-01-09 06:29:30,800 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33657
2024-01-09 06:29:30,800 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33825
2024-01-09 06:29:30,801 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:30,801 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:30,801 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:30,801 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:30,801 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:30,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7b1hxbgb
2024-01-09 06:29:30,801 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9a4f4cb-f2df-4861-9943-7348349f1c4c
2024-01-09 06:29:30,801 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35121
2024-01-09 06:29:30,801 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35121
2024-01-09 06:29:30,802 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43485
2024-01-09 06:29:30,802 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:30,802 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:30,802 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:30,802 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:30,802 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2lgl8rz7
2024-01-09 06:29:30,802 - distributed.worker - INFO - Starting Worker plugin RMMSetup-324091e6-172d-4940-8bf4-fcfbdf92de49
2024-01-09 06:29:30,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:30,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:30,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:30,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46697
2024-01-09 06:29:30,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46697
2024-01-09 06:29:30,810 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37193
2024-01-09 06:29:30,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:30,811 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:30,811 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:30,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:30,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uel28v71
2024-01-09 06:29:30,811 - distributed.worker - INFO - Starting Worker plugin PreImport-c85316b0-33a5-47d9-9a24-72c1fdfbfde5
2024-01-09 06:29:30,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aefe1b0f-c060-4a61-b045-715d89c049a7
2024-01-09 06:29:30,813 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3f08fbe-21e7-40ff-a95c-d4286e318b39
2024-01-09 06:29:30,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:30,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:30,829 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:30,830 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33621
2024-01-09 06:29:30,830 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33621
2024-01-09 06:29:30,830 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45023
2024-01-09 06:29:30,830 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:30,830 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:30,830 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:30,830 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:30,830 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-karzt0xx
2024-01-09 06:29:30,831 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a753c920-3d5e-485f-9f9a-88b8c4e6b377
2024-01-09 06:29:30,911 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41887'. Reason: nanny-close
2024-01-09 06:29:30,911 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38273'. Reason: nanny-close
2024-01-09 06:29:30,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39033'. Reason: nanny-close
2024-01-09 06:29:30,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37697'. Reason: nanny-close
2024-01-09 06:29:30,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36235'. Reason: nanny-close
2024-01-09 06:29:30,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43295'. Reason: nanny-close
2024-01-09 06:29:30,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44815'. Reason: nanny-close
2024-01-09 06:29:30,913 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37119'. Reason: nanny-close
2024-01-09 06:29:30,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:30,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:30,943 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:30,944 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37257
2024-01-09 06:29:30,944 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37257
2024-01-09 06:29:30,944 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46257
2024-01-09 06:29:30,944 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:30,944 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:30,944 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:30,944 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:30,944 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5mc1c3r4
2024-01-09 06:29:30,945 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79462d89-e10a-4174-9509-c574960e7202
2024-01-09 06:29:30,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:30,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:30,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:30,954 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34793
2024-01-09 06:29:30,955 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34793
2024-01-09 06:29:30,955 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42215
2024-01-09 06:29:30,955 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:30,955 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:30,955 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:30,955 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:30,955 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-625fpbf5
2024-01-09 06:29:30,956 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e217d702-cf0f-44dd-9c65-7a4161de44a5
2024-01-09 06:29:33,866 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-892ec3bb-75bf-4829-b1a9-8942cc348e6f
2024-01-09 06:29:33,867 - distributed.worker - INFO - Starting Worker plugin PreImport-5352eaa6-42be-4656-80c4-604a45b8daca
2024-01-09 06:29:33,869 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:33,922 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:33,931 - distributed.worker - INFO - Starting Worker plugin PreImport-6bc4c762-c17c-46f4-b189-dc217dfa56c1
2024-01-09 06:29:33,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4181d95-f90c-4f7f-831d-648bc37eb81b
2024-01-09 06:29:33,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:33,936 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32b368c2-1b6a-4663-a587-4cccd635aa31
2024-01-09 06:29:33,937 - distributed.worker - INFO - Starting Worker plugin PreImport-fa7511e0-f1b0-4a05-b0d6-318877edebb4
2024-01-09 06:29:33,938 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:33,942 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:33,950 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c538bb91-db20-4750-b5aa-7ebcc6996023
2024-01-09 06:29:33,952 - distributed.worker - INFO - Starting Worker plugin PreImport-c42909aa-2ada-4e44-b483-9c7b9982b9e8
2024-01-09 06:29:33,952 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:33,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7e3400e-f86d-4fe3-8e3d-6e09ccef1864
2024-01-09 06:29:33,967 - distributed.worker - INFO - Starting Worker plugin PreImport-6b0fe574-6a22-44d1-b3c2-19d0d405879f
2024-01-09 06:29:33,968 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:33,970 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32555d69-7383-4e4a-9f2b-d44f617674f0
2024-01-09 06:29:33,971 - distributed.worker - INFO - Starting Worker plugin PreImport-d8f1abf2-1001-41ab-9a6a-f9c1b9c13d55
2024-01-09 06:29:33,972 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:35,945 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:35,946 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:35,946 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:35,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:35,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:35,987 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37591. Reason: nanny-close
2024-01-09 06:29:35,990 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:35,992 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:36,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:36,066 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:36,067 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:36,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:36,088 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:36,089 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33657. Reason: nanny-close
2024-01-09 06:29:36,091 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:36,092 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:35792 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-09 06:29:36,374 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65576 parent=65379 started daemon>
2024-01-09 06:29:36,374 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65572 parent=65379 started daemon>
2024-01-09 06:29:36,375 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65568 parent=65379 started daemon>
2024-01-09 06:29:36,375 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65564 parent=65379 started daemon>
2024-01-09 06:29:36,375 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65559 parent=65379 started daemon>
2024-01-09 06:29:36,375 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65554 parent=65379 started daemon>
2024-01-09 06:29:36,375 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65545 parent=65379 started daemon>
2024-01-09 06:29:36,495 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65576 exit status was already read will report exitcode 255
2024-01-09 06:29:36,582 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65545 exit status was already read will report exitcode 255
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-09 06:29:39,051 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:39,055 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33033 instead
  warnings.warn(
2024-01-09 06:29:39,059 - distributed.scheduler - INFO - State start
2024-01-09 06:29:39,061 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-625fpbf5', purging
2024-01-09 06:29:39,061 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-t232f9i5', purging
2024-01-09 06:29:39,062 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5mc1c3r4', purging
2024-01-09 06:29:39,062 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-karzt0xx', purging
2024-01-09 06:29:39,063 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2lgl8rz7', purging
2024-01-09 06:29:39,063 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-uel28v71', purging
2024-01-09 06:29:39,466 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:39,468 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:29:39,468 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:29:39,469 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:29:40,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38953'
2024-01-09 06:29:40,281 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41945'
2024-01-09 06:29:40,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36823'
2024-01-09 06:29:40,304 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34697'
2024-01-09 06:29:40,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37777'
2024-01-09 06:29:40,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39361'
2024-01-09 06:29:40,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39241'
2024-01-09 06:29:40,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46375'
2024-01-09 06:29:40,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38953'. Reason: nanny-close
2024-01-09 06:29:40,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41945'. Reason: nanny-close
2024-01-09 06:29:40,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36823'. Reason: nanny-close
2024-01-09 06:29:40,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34697'. Reason: nanny-close
2024-01-09 06:29:40,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37777'. Reason: nanny-close
2024-01-09 06:29:40,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39361'. Reason: nanny-close
2024-01-09 06:29:40,705 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39241'. Reason: nanny-close
2024-01-09 06:29:40,705 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46375'. Reason: nanny-close
2024-01-09 06:29:42,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:42,210 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:42,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:42,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:42,214 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:42,215 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45199
2024-01-09 06:29:42,215 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45199
2024-01-09 06:29:42,215 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35191
2024-01-09 06:29:42,215 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:42,215 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:42,215 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:42,216 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:42,216 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cdw17_70
2024-01-09 06:29:42,216 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a0d3c5b-9bc3-4800-9982-6f96686b0fb8
2024-01-09 06:29:42,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:42,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41039
2024-01-09 06:29:42,218 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41039
2024-01-09 06:29:42,218 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32865
2024-01-09 06:29:42,218 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:42,218 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:42,218 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:42,218 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:42,218 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-253v9tai
2024-01-09 06:29:42,219 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13ab155a-2314-4fd6-9561-d7a41c2facb0
2024-01-09 06:29:42,231 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:42,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:42,236 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:42,237 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44385
2024-01-09 06:29:42,237 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44385
2024-01-09 06:29:42,237 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42389
2024-01-09 06:29:42,237 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:42,237 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:42,237 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:42,237 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:42,237 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-51kz52zi
2024-01-09 06:29:42,238 - distributed.worker - INFO - Starting Worker plugin RMMSetup-419803fa-97da-48c9-bc9a-fbc926b3012d
2024-01-09 06:29:42,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:42,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:42,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:42,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:42,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:42,274 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38005
2024-01-09 06:29:42,274 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38005
2024-01-09 06:29:42,275 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40353
2024-01-09 06:29:42,275 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:42,275 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:42,275 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:42,275 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:42,275 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qw_vk74m
2024-01-09 06:29:42,275 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:42,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eaf8398d-7d9c-40ce-ab5d-1ffb4e3d00ba
2024-01-09 06:29:42,276 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44359
2024-01-09 06:29:42,276 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44359
2024-01-09 06:29:42,276 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39399
2024-01-09 06:29:42,276 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:42,276 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:42,276 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:42,276 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:42,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lfmn23cz
2024-01-09 06:29:42,277 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb614955-067e-4001-b312-b2f1e3804d01
2024-01-09 06:29:42,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:42,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:42,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:42,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:42,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:42,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:42,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41165
2024-01-09 06:29:42,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39809
2024-01-09 06:29:42,286 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39809
2024-01-09 06:29:42,286 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41165
2024-01-09 06:29:42,286 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39823
2024-01-09 06:29:42,286 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41543
2024-01-09 06:29:42,286 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:42,286 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:42,286 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:42,286 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:42,286 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:42,286 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:42,286 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:42,286 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:42,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x_0hqbpt
2024-01-09 06:29:42,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gu7d7f7n
2024-01-09 06:29:42,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9e6082b-0d92-488a-9e17-25dc34862ed1
2024-01-09 06:29:42,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ed16d37-dc12-4384-ac4d-b198bd13c179
2024-01-09 06:29:42,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:42,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:42,329 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:42,330 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40553
2024-01-09 06:29:42,330 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40553
2024-01-09 06:29:42,330 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32873
2024-01-09 06:29:42,330 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:42,330 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:42,330 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:42,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:29:42,331 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t3kticd_
2024-01-09 06:29:42,331 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2dc0d16c-5e69-4078-b6d2-8ec21782fc3d
2024-01-09 06:29:44,272 - distributed.worker - INFO - Starting Worker plugin PreImport-fd12688a-c59d-4c81-a8d5-7a7e43e161ed
2024-01-09 06:29:44,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e265d652-8cc0-4ff3-a16d-c53c352f2b09
2024-01-09 06:29:44,273 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:44,363 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:44,363 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:44,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:44,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45199. Reason: nanny-close
2024-01-09 06:29:44,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:44,416 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:44,444 - distributed.worker - INFO - Starting Worker plugin PreImport-a53ea06e-41f4-49a9-9a12-7a89fd399e51
2024-01-09 06:29:44,445 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0df7459d-d524-4b31-aa5c-d80b421f58c0
2024-01-09 06:29:44,445 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,450 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-020d17ef-2808-467e-92c6-cb1122835094
2024-01-09 06:29:44,453 - distributed.worker - INFO - Starting Worker plugin PreImport-d21b6aa6-4606-41de-b26e-596c9f34e91c
2024-01-09 06:29:44,454 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ab953dc-7632-45d9-b355-58e5975b9316
2024-01-09 06:29:44,481 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:44,482 - distributed.worker - INFO - Starting Worker plugin PreImport-44a3912f-abc9-42e4-89e8-0e277c161a20
2024-01-09 06:29:44,482 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:44,482 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,483 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:44,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:44,490 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:44,490 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:44,513 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:44,514 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44385. Reason: nanny-close
2024-01-09 06:29:44,517 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:44,519 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:44,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:44,521 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:44,521 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,524 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:44,523 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9715a8f3-2fff-4ec6-a830-c7c0d1274727
2024-01-09 06:29:44,524 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:44,525 - distributed.worker - INFO - Starting Worker plugin PreImport-b9f9be16-2611-42dc-9a11-613a262d6fb0
2024-01-09 06:29:44,525 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41039. Reason: nanny-close
2024-01-09 06:29:44,526 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,526 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eee53f0d-45d4-4aee-af61-aeda06a8bc71
2024-01-09 06:29:44,527 - distributed.worker - INFO - Starting Worker plugin PreImport-5eed7109-4a88-40e4-8b9e-9ba094c8dcdb
2024-01-09 06:29:44,527 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:44,530 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:44,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81969188-675d-4615-af22-10b11e793ad8
2024-01-09 06:29:44,542 - distributed.worker - INFO - Starting Worker plugin PreImport-070346dc-4b1a-48b4-a44b-617ebcca0afe
2024-01-09 06:29:44,542 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e36f1367-4590-40c9-896a-63eabfe7b4e5
2024-01-09 06:29:44,548 - distributed.worker - INFO - Starting Worker plugin PreImport-5935659d-1952-4394-bd42-5d6fadf1cef3
2024-01-09 06:29:44,549 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:44,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:44,556 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:44,565 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:44,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:44,566 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:44,566 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38005. Reason: nanny-close
2024-01-09 06:29:44,566 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:44,569 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:44,571 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:44,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:44,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41165. Reason: nanny-close
2024-01-09 06:29:44,578 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:44,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:44,579 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:44,579 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,580 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:44,581 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:44,583 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:44,584 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:44,584 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:44,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:44,616 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:44,616 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:44,617 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:44,617 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39809. Reason: nanny-close
2024-01-09 06:29:44,618 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40553. Reason: nanny-close
2024-01-09 06:29:44,618 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44359. Reason: nanny-close
2024-01-09 06:29:44,620 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:44,621 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:44,621 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:44,621 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:44,623 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:44,623 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:47578 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-09 06:29:44,785 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65835 parent=65638 started daemon>
2024-01-09 06:29:44,785 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65831 parent=65638 started daemon>
2024-01-09 06:29:44,785 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65827 parent=65638 started daemon>
2024-01-09 06:29:44,786 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65823 parent=65638 started daemon>
2024-01-09 06:29:44,786 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65818 parent=65638 started daemon>
2024-01-09 06:29:44,786 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65808 parent=65638 started daemon>
2024-01-09 06:29:44,786 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65804 parent=65638 started daemon>
2024-01-09 06:29:45,065 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65804 exit status was already read will report exitcode 255
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 184, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 82, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 10, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/__init__.py", line 4, in <module>
    from dask.array import backends, fft, lib, linalg, ma, overlap, random
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/backends.py", line 8, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/core.py", line 37, in <module>
    from dask.array.chunk_types import is_valid_array_chunk, is_valid_chunk_type
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/chunk_types.py", line 110, in <module>
    import cupy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/__init__.py", line 44, in <module>
    from cupy import random  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/random/__init__.py", line 46, in <module>
    from cupy.random._distributions import beta  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/random/_distributions.py", line 1, in <module>
    from cupy.random import _generator
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/random/_generator.py", line 18, in <module>
    from cupy.random import _kernels
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/random/_kernels.py", line 878, in <module>
    beta_kernel = _core.ElementwiseKernel(
  File "cupy/_core/_kernel.pyx", line 793, in cupy._core._kernel.ElementwiseKernel.__init__
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/cuda/compiler.py", line 712, in is_valid_kernel_name
    def is_valid_kernel_name(name):
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 184, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 82, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 10, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/__init__.py", line 4, in <module>
    from dask.array import backends, fft, lib, linalg, ma, overlap, random
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/ma.py", line 10, in <module>
    from dask.array.routines import _average
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/routines.py", line 563, in <module>
    def ptp(a, axis=None):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 980, in wrapper
    method.__doc__ = _derived_from(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 941, in _derived_from
    doc = unsupported_arguments(doc, not_supported)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 873, in unsupported_arguments
    subset = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 876, in <listcomp>
    if re.match(r"^\s*" + arg + " ?:", line)
  File "/opt/conda/envs/gdf/lib/python3.9/re.py", line 191, in match
    return _compile(pattern, flags).match(string)
KeyboardInterrupt
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-09 06:29:51,446 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:51,450 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:29:51,454 - distributed.scheduler - INFO - State start
2024-01-09 06:29:51,478 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:51,479 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:29:51,480 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:29:51,480 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:29:51,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45245'
2024-01-09 06:29:52,274 - distributed.scheduler - INFO - Receive client connection: Client-7daf7789-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:29:52,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47160
2024-01-09 06:29:52,452 - distributed.scheduler - INFO - Receive client connection: Client-7d63cb78-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:29:52,452 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47180
2024-01-09 06:29:53,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:29:53,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:29:53,874 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:29:53,875 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41701
2024-01-09 06:29:53,876 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41701
2024-01-09 06:29:53,876 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32887
2024-01-09 06:29:53,876 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:29:53,876 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:53,876 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:29:53,876 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:29:53,876 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bbwvcqvl
2024-01-09 06:29:53,876 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b20b790-2c56-4367-b547-e5788cde6e4b
2024-01-09 06:29:53,876 - distributed.worker - INFO - Starting Worker plugin PreImport-45b8964c-8b9a-42bd-9038-f0a7e9458a60
2024-01-09 06:29:53,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0c77026-d4ea-4ecb-bbaf-db4ec22173cb
2024-01-09 06:29:53,878 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:53,946 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41701', status: init, memory: 0, processing: 0>
2024-01-09 06:29:53,949 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41701
2024-01-09 06:29:53,949 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47202
2024-01-09 06:29:53,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:29:53,951 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:29:53,951 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:29:53,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:29:53,990 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:29:53,993 - distributed.scheduler - INFO - Remove client Client-7d63cb78-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:29:53,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47180; closing.
2024-01-09 06:29:53,994 - distributed.scheduler - INFO - Remove client Client-7d63cb78-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:29:53,994 - distributed.scheduler - INFO - Close client connection: Client-7d63cb78-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:29:53,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45245'. Reason: nanny-close
2024-01-09 06:29:53,995 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:29:53,996 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41701. Reason: nanny-close
2024-01-09 06:29:53,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47202; closing.
2024-01-09 06:29:53,999 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:29:53,999 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41701', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781793.9997013')
2024-01-09 06:29:54,000 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:29:54,001 - distributed.nanny - INFO - Worker closed
2024-01-09 06:29:54,429 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40333', status: init, memory: 0, processing: 0>
2024-01-09 06:29:54,431 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40333
2024-01-09 06:29:54,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47212
2024-01-09 06:29:54,447 - distributed.scheduler - INFO - Remove client Client-7daf7789-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:29:54,448 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47160; closing.
2024-01-09 06:29:54,448 - distributed.scheduler - INFO - Remove client Client-7daf7789-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:29:54,449 - distributed.scheduler - INFO - Close client connection: Client-7daf7789-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:29:54,474 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47212; closing.
2024-01-09 06:29:54,475 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40333', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781794.4749765')
2024-01-09 06:29:54,475 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:29:54,861 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:29:54,861 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:29:54,862 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:29:54,863 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:29:54,863 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-09 06:29:57,313 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:57,318 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:29:57,321 - distributed.scheduler - INFO - State start
2024-01-09 06:29:57,503 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:29:57,504 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:29:57,505 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:29:57,506 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:30:00,104 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:47216'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47216>: Stream is closed
2024-01-09 06:30:00,369 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:30:00,370 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:30:00,370 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:30:00,371 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:30:00,372 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-09 06:30:02,977 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:30:02,985 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:30:02,992 - distributed.scheduler - INFO - State start
2024-01-09 06:30:03,028 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:30:03,030 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-09 06:30:03,031 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:30:03,031 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:30:03,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33509'
2024-01-09 06:30:03,984 - distributed.scheduler - INFO - Receive client connection: Client-84eed6b6-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:30:04,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33978
2024-01-09 06:30:05,113 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:05,113 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:05,117 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:05,118 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36911
2024-01-09 06:30:05,118 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36911
2024-01-09 06:30:05,118 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38805
2024-01-09 06:30:05,118 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-09 06:30:05,118 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:05,118 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:05,118 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:30:05,118 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-18u2alrg
2024-01-09 06:30:05,119 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7eae4608-b099-4a27-b102-37d5807245fd
2024-01-09 06:30:05,119 - distributed.worker - INFO - Starting Worker plugin PreImport-d3658f45-20fe-4711-9d3a-e1205e5108da
2024-01-09 06:30:05,119 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-afcd39b9-941a-44f6-929d-33e40b58c77e
2024-01-09 06:30:05,119 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:05,173 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36911', status: init, memory: 0, processing: 0>
2024-01-09 06:30:05,174 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36911
2024-01-09 06:30:05,174 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34020
2024-01-09 06:30:05,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:05,176 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-09 06:30:05,176 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:05,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-09 06:30:05,229 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:05,232 - distributed.scheduler - INFO - Remove client Client-84eed6b6-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:30:05,232 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33978; closing.
2024-01-09 06:30:05,232 - distributed.scheduler - INFO - Remove client Client-84eed6b6-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:30:05,233 - distributed.scheduler - INFO - Close client connection: Client-84eed6b6-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:30:05,916 - distributed.scheduler - INFO - Receive client connection: Client-84134e53-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:30:05,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34030
2024-01-09 06:30:05,923 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:05,929 - distributed.scheduler - INFO - Remove client Client-84134e53-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:30:05,930 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34030; closing.
2024-01-09 06:30:05,930 - distributed.scheduler - INFO - Remove client Client-84134e53-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:30:05,930 - distributed.scheduler - INFO - Close client connection: Client-84134e53-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:30:05,931 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33509'. Reason: nanny-close
2024-01-09 06:30:05,932 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:05,933 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36911. Reason: nanny-close
2024-01-09 06:30:05,935 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-09 06:30:05,935 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34020; closing.
2024-01-09 06:30:05,935 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36911', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781805.935317')
2024-01-09 06:30:05,935 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:30:05,936 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:06,332 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35135', status: init, memory: 0, processing: 0>
2024-01-09 06:30:06,333 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35135
2024-01-09 06:30:06,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34052
2024-01-09 06:30:06,368 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34052; closing.
2024-01-09 06:30:06,368 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35135', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781806.3686528')
2024-01-09 06:30:06,369 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:30:06,496 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:30:06,497 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:30:06,497 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:30:06,498 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-09 06:30:06,499 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-09 06:30:09,064 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:30:09,069 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44097 instead
  warnings.warn(
2024-01-09 06:30:09,073 - distributed.scheduler - INFO - State start
2024-01-09 06:30:09,097 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:30:09,098 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:30:09,098 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:30:09,099 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:30:09,242 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40073'
2024-01-09 06:30:09,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46137'
2024-01-09 06:30:09,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46593'
2024-01-09 06:30:09,285 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38089'
2024-01-09 06:30:09,296 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39765'
2024-01-09 06:30:09,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42575'
2024-01-09 06:30:09,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36415'
2024-01-09 06:30:09,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35579'
2024-01-09 06:30:11,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:11,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:11,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:11,105 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35961
2024-01-09 06:30:11,105 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35961
2024-01-09 06:30:11,105 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40041
2024-01-09 06:30:11,105 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:11,105 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:11,105 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:11,105 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:30:11,105 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mzv3j8lz
2024-01-09 06:30:11,105 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee00d6ba-c2cf-4a45-8b65-55880fc49912
2024-01-09 06:30:11,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:11,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:11,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:11,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39679
2024-01-09 06:30:11,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39679
2024-01-09 06:30:11,322 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35821
2024-01-09 06:30:11,322 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:11,322 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:11,322 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:11,322 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:30:11,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-js9wp412
2024-01-09 06:30:11,323 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-460d1671-d472-4751-81a2-99bf82774068
2024-01-09 06:30:11,323 - distributed.worker - INFO - Starting Worker plugin PreImport-d559d0d1-9de4-4a0c-af53-b9aaca3d8197
2024-01-09 06:30:11,323 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80f384b4-276c-46a1-b3e2-3808e348327c
2024-01-09 06:30:11,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:11,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:11,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:11,338 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:11,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:11,339 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:11,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:11,339 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38975
2024-01-09 06:30:11,339 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38975
2024-01-09 06:30:11,339 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43117
2024-01-09 06:30:11,339 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:11,339 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:11,339 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:11,339 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:30:11,339 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ukxzlpb5
2024-01-09 06:30:11,340 - distributed.worker - INFO - Starting Worker plugin PreImport-6c10faa2-764e-4e91-916d-a1aa42eb20f8
2024-01-09 06:30:11,340 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5a9e30b-500d-44e2-9950-dd548d875fd5
2024-01-09 06:30:11,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:11,340 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3961701b-0e8b-4371-9cea-c343accf0a8f
2024-01-09 06:30:11,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:11,342 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:11,343 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39307
2024-01-09 06:30:11,343 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39307
2024-01-09 06:30:11,343 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44357
2024-01-09 06:30:11,343 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:11,343 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:11,343 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:11,343 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:30:11,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:11,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-noiu8qg6
2024-01-09 06:30:11,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:11,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:11,344 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff12801c-f645-48dc-a9b6-1ee829ed1c69
2024-01-09 06:30:11,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38921
2024-01-09 06:30:11,344 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38921
2024-01-09 06:30:11,345 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33623
2024-01-09 06:30:11,345 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:11,345 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:11,345 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:11,345 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:30:11,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iaw8zntt
2024-01-09 06:30:11,345 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:11,345 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31d36213-64d8-4dac-98ac-47930b8a8741
2024-01-09 06:30:11,346 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36551
2024-01-09 06:30:11,346 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36551
2024-01-09 06:30:11,346 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43063
2024-01-09 06:30:11,346 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:11,346 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:11,346 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:11,346 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:30:11,346 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1bvgildk
2024-01-09 06:30:11,347 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76835fb0-8335-441c-9ebd-e11fc29bc30d
2024-01-09 06:30:11,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:11,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:11,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:11,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33131
2024-01-09 06:30:11,349 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33131
2024-01-09 06:30:11,349 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35003
2024-01-09 06:30:11,349 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:11,349 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:11,349 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:11,349 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:30:11,349 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z8vbr_4k
2024-01-09 06:30:11,349 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4f21dad-c725-4b76-9990-61ef04b92c35
2024-01-09 06:30:11,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:11,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39917
2024-01-09 06:30:11,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39917
2024-01-09 06:30:11,353 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39163
2024-01-09 06:30:11,354 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:11,354 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:11,354 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:11,354 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:30:11,354 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iee6mj88
2024-01-09 06:30:11,354 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c9b4653-c5a2-46ef-bee3-67ce6b12c707
2024-01-09 06:30:15,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47e873ed-a6ca-4f89-a37b-56c3e3e74eb1
2024-01-09 06:30:15,511 - distributed.worker - INFO - Starting Worker plugin PreImport-71ff5ec1-48a8-4f91-a7a7-39c7075843da
2024-01-09 06:30:15,512 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,517 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,534 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87b67fc0-0237-414e-84f5-f1534dfeb201
2024-01-09 06:30:15,535 - distributed.worker - INFO - Starting Worker plugin PreImport-338e27dd-1425-4c84-8867-ce81d57ed0e7
2024-01-09 06:30:15,535 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:15,545 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:15,546 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:15,553 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:15,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,554 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:15,554 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:15,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:15,560 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:15,561 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:15,587 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:15,588 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:15,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:15,650 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:30:15,651 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:30:15,651 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:30:15,651 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:30:15,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:30:15,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:30:15,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:30:15,655 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-09 06:30:15,674 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:15,674 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:15,674 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63d400c0-a48d-43e7-837a-2c46faa2b384
2024-01-09 06:30:15,675 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:15,675 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:15,675 - distributed.worker - INFO - Starting Worker plugin PreImport-1f3f2a39-adcb-4d82-ab0d-2e0c48b0a5bf
2024-01-09 06:30:15,676 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,683 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:15,684 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:15,684 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:15,684 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:15,693 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40073'. Reason: nanny-close
2024-01-09 06:30:15,693 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:15,694 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46137'. Reason: nanny-close
2024-01-09 06:30:15,694 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:15,694 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46593'. Reason: nanny-close
2024-01-09 06:30:15,694 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39679. Reason: nanny-close
2024-01-09 06:30:15,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38089'. Reason: nanny-close
2024-01-09 06:30:15,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39765'. Reason: nanny-close
2024-01-09 06:30:15,695 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:15,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42575'. Reason: nanny-close
2024-01-09 06:30:15,695 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35961. Reason: nanny-close
2024-01-09 06:30:15,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36415'. Reason: nanny-close
2024-01-09 06:30:15,696 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:15,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35579'. Reason: nanny-close
2024-01-09 06:30:15,696 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38975. Reason: nanny-close
2024-01-09 06:30:15,696 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33131. Reason: nanny-close
2024-01-09 06:30:15,697 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:15,697 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:15,698 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:15,698 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:15,699 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:15,699 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:15,700 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:15,700 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:15,717 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:15,718 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:15,718 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,718 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8bcf4ccc-dc5c-456b-9110-9bd9641b6c48
2024-01-09 06:30:15,720 - distributed.worker - INFO - Starting Worker plugin PreImport-c16ec9cc-c4dc-4c6d-8698-967f0bb3b48b
2024-01-09 06:30:15,720 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,721 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:15,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:15,753 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:15,753 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:15,763 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:15,764 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:15,764 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39307. Reason: nanny-close
2024-01-09 06:30:15,764 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39917. Reason: nanny-close
2024-01-09 06:30:15,766 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:15,767 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:15,768 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:15,768 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:15,959 - distributed.worker - INFO - Starting Worker plugin PreImport-acece8c0-a90f-4cd2-ad20-c0a0323cdc98
2024-01-09 06:30:15,961 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86d80bb8-43fe-4108-959f-4d8135e5dfa3
2024-01-09 06:30:15,962 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:15,972 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3db7703-38cc-448a-adb2-528908c07d77
2024-01-09 06:30:15,973 - distributed.worker - INFO - Starting Worker plugin PreImport-453be9d5-e785-448a-94a4-797be1f8cf6d
2024-01-09 06:30:15,974 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:16,003 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:16,003 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:16,004 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:16,004 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:16,004 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:16,004 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:16,006 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:16,006 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:16,016 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:16,017 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36551. Reason: nanny-close
2024-01-09 06:30:16,020 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:16,022 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:16,039 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:16,040 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38921. Reason: nanny-close
2024-01-09 06:30:16,042 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:16,044 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-09 06:30:20,290 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:30:20,295 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-09 06:30:20,299 - distributed.scheduler - INFO - State start
2024-01-09 06:30:20,322 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:30:20,323 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-09 06:30:20,324 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-09 06:30:20,325 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-09 06:30:20,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42719'
2024-01-09 06:30:22,286 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:22,286 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:22,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:22,290 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37213
2024-01-09 06:30:22,290 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37213
2024-01-09 06:30:22,290 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37115
2024-01-09 06:30:22,290 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:22,291 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:22,291 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:22,291 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:30:22,291 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fa5nj3jo
2024-01-09 06:30:22,291 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5c94d03-3740-458e-bb1c-1c9c7e33cedf
2024-01-09 06:30:22,750 - distributed.worker - INFO - Starting Worker plugin PreImport-29a08efb-fbab-40dd-908b-4cd255995fa3
2024-01-09 06:30:22,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d7a2984-f271-4314-8590-ed431b44e86b
2024-01-09 06:30:22,750 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:22,808 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37213', status: init, memory: 0, processing: 0>
2024-01-09 06:30:22,824 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37213
2024-01-09 06:30:22,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38480
2024-01-09 06:30:22,825 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:22,826 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:22,826 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:22,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:22,907 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33797', status: init, memory: 0, processing: 0>
2024-01-09 06:30:22,908 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33797
2024-01-09 06:30:22,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38488
2024-01-09 06:30:23,406 - distributed.scheduler - INFO - Receive client connection: Client-8e6cde9c-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:30:23,406 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38496
2024-01-09 06:30:30,221 - distributed.scheduler - INFO - Receive client connection: Client-8e9843b9-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:30:30,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37862
2024-01-09 06:30:33,419 - distributed.scheduler - INFO - Remove client Client-8e6cde9c-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:30:33,420 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38496; closing.
2024-01-09 06:30:33,420 - distributed.scheduler - INFO - Remove client Client-8e6cde9c-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:30:33,420 - distributed.scheduler - INFO - Close client connection: Client-8e6cde9c-aeb8-11ee-ba91-d8c49764f6bb
2024-01-09 06:30:33,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42719'. Reason: nanny-close
2024-01-09 06:30:33,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:33,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37213. Reason: nanny-close
2024-01-09 06:30:33,427 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38480; closing.
2024-01-09 06:30:33,427 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-09 06:30:33,427 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37213', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781833.4278882')
2024-01-09 06:30:33,430 - distributed.nanny - INFO - Worker closed
2024-01-09 06:30:33,531 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:33,532 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:30:33,534 - distributed.scheduler - INFO - Remove client Client-8e9843b9-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:30:33,535 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37862; closing.
2024-01-09 06:30:33,535 - distributed.scheduler - INFO - Remove client Client-8e9843b9-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:30:33,535 - distributed.scheduler - INFO - Close client connection: Client-8e9843b9-aeb8-11ee-bd35-d8c49764f6bb
2024-01-09 06:30:33,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38488; closing.
2024-01-09 06:30:33,541 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33797', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704781833.5413628')
2024-01-09 06:30:33,541 - distributed.scheduler - INFO - Lost all workers
2024-01-09 06:30:34,187 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-09 06:30:34,187 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-09 06:30:34,188 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:30:34,189 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-09 06:30:34,190 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-09 06:30:36,649 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:30:36,654 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34199 instead
  warnings.warn(
2024-01-09 06:30:36,658 - distributed.scheduler - INFO - State start
2024-01-09 06:30:36,680 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-09 06:30:36,680 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-09 06:30:36,681 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-09 06:30:36,682 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-09 06:30:36,914 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33583'
2024-01-09 06:30:38,522 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33583'. Reason: nanny-close
2024-01-09 06:30:38,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:30:38,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:30:38,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:30:38,769 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45931
2024-01-09 06:30:38,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45931
2024-01-09 06:30:38,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39929
2024-01-09 06:30:38,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-09 06:30:38,769 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:38,769 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:30:38,770 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:30:38,770 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oevoj97m
2024-01-09 06:30:38,770 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b7e39d92-d4c2-4c4f-84e0-6536fbe81acd
2024-01-09 06:30:39,192 - distributed.worker - INFO - Starting Worker plugin PreImport-9ede1b29-50e0-4f7a-8e7d-c4b3dd9b35b3
2024-01-09 06:30:39,193 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e2b3c26-66f7-40f7-8529-f87538e276ff
2024-01-09 06:30:39,193 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:39,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:30:39,249 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-09 06:30:39,249 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:30:39,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-09 06:30:39,285 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45931. Reason: scheduler-close
2024-01-09 06:30:39,287 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:38104 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:38104 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-01-09 06:30:39,298 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-09 06:30:42,502 - distributed.nanny - WARNING - Worker process still alive after 3.19999740600586 seconds, killing
2024-01-09 06:30:42,526 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67347 parent=67181 started daemon>
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41325 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37435 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39563 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] 2024-01-09 06:31:59,095 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1590, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-01-09 06:31:59,109 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucxx://10.33.225.163:56841', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1590, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42271 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43597 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46093 instead
  warnings.warn(
2024-01-09 06:33:19,706 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #018] ep: 0x7f5182f6f0c0, tag: 0x4ec66c3feaade605, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #018] ep: 0x7f5182f6f0c0, tag: 0x4ec66c3feaade605, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38909 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36015 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46843 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] [1704782069.429379] [dgx13:71080:0]            sock.c:481  UCX  ERROR bind(fd=124 addr=0.0.0.0:59820) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40327 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46493 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44163 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33827 instead
  warnings.warn(
[1704782259.084794] [dgx13:74299:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:33558) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44629 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34767 instead
  warnings.warn(
[1704782302.072685] [dgx13:74945:0]            sock.c:481  UCX  ERROR bind(fd=151 addr=0.0.0.0:35464) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41999 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42599 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42013 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44425 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39041 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45843 instead
  warnings.warn(
2024-01-09 06:43:03,387 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1590, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-01-09 06:43:03,391 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:39057', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1590, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46825 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] [1704782671.903349] [dgx13:80574:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:60312) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39733 instead
  warnings.warn(
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-3729' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34297 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46231 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43813 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44193 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] [1704782855.097805] [dgx13:83642:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:39315) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34205 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45799 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33201 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45243 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35241 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39417 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39769 instead
  warnings.warn(
2024-01-09 06:50:29,242 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:34850 remote=tcp://127.0.0.1:45599>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40573 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35213 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35127 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45459 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32845 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39567 instead
  warnings.warn(
[1704783161.690767] [dgx13:87939:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:38694) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37729 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35793 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45733 instead
  warnings.warn(
[1704783208.627343] [dgx13:88470:0]            sock.c:481  UCX  ERROR bind(fd=165 addr=0.0.0.0:48526) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35315 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] [1704783265.558384] [dgx13:89354:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:39697) failed: Address already in use
2024-01-09 06:54:27,519 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34635 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45033 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41235 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46753 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1704783343.457825] [dgx13:90018:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:37688) failed: Address already in use
[1704783344.277685] [dgx13:90022:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:56886) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] [1704783356.414372] [dgx13:64145:0]            sock.c:481  UCX  ERROR bind(fd=253 addr=0.0.0.0:37472) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster 2024-01-09 06:56:22,991 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56550 remote=tcp://127.0.0.1:36151>: Stream is closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-09 06:56:27,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:56:27,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:56:27,636 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:56:27,636 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:56:27,688 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:56:27,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:56:27,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:56:27,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:56:27,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:56:27,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:56:27,807 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:56:27,807 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:56:27,876 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:56:27,876 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:56:27,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:56:27,882 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:56:28,267 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:56:28,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33039
2024-01-09 06:56:28,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33039
2024-01-09 06:56:28,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39357
2024-01-09 06:56:28,269 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,269 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,270 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:56:28,270 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kkz5qpz1
2024-01-09 06:56:28,270 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46e8d45f-909a-4685-a556-da393ed362fa
2024-01-09 06:56:28,270 - distributed.worker - INFO - Starting Worker plugin PreImport-85399ba8-74c3-4717-bac4-853b4c74a519
2024-01-09 06:56:28,271 - distributed.worker - INFO - Starting Worker plugin RMMSetup-22a6dbe2-c744-412b-aa9f-5563251ce152
2024-01-09 06:56:28,271 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,292 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:56:28,293 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40843
2024-01-09 06:56:28,293 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40843
2024-01-09 06:56:28,293 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34147
2024-01-09 06:56:28,294 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,294 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,294 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:56:28,294 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7j1h0n0y
2024-01-09 06:56:28,294 - distributed.worker - INFO - Starting Worker plugin PreImport-4b29ede8-9f57-464a-a945-616561864e0c
2024-01-09 06:56:28,295 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7488393d-4df4-485d-933e-d42d08ee0a76
2024-01-09 06:56:28,295 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c41762f-d70e-4192-81e7-4c376bec2796
2024-01-09 06:56:28,295 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:56:28,366 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,366 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37387
2024-01-09 06:56:28,384 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:56:28,385 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,385 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,386 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37387
2024-01-09 06:56:28,406 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:56:28,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35807
2024-01-09 06:56:28,407 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35807
2024-01-09 06:56:28,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37711
2024-01-09 06:56:28,407 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,407 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,407 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:56:28,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-utfnv_36
2024-01-09 06:56:28,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-afbb5628-8094-4a79-a3f5-969aaedf3b3b
2024-01-09 06:56:28,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33524f56-5212-42c3-9a57-acbe094e68f2
2024-01-09 06:56:28,408 - distributed.worker - INFO - Starting Worker plugin PreImport-f4c02a1e-ee90-4424-93c0-8e7e312c138a
2024-01-09 06:56:28,408 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,469 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:56:28,470 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43231
2024-01-09 06:56:28,470 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43231
2024-01-09 06:56:28,470 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33747
2024-01-09 06:56:28,470 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,470 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,470 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:56:28,470 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-39ekjwyo
2024-01-09 06:56:28,470 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb631dd9-9c82-46bb-9d7d-4f2c8917b6e9
2024-01-09 06:56:28,471 - distributed.worker - INFO - Starting Worker plugin PreImport-431f2d27-ca5c-48d8-a881-d2e7aedc96d8
2024-01-09 06:56:28,471 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6eccd8b4-1844-45d1-a138-755bb1502998
2024-01-09 06:56:28,471 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:56:28,473 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:56:28,473 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,473 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,473 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42457
2024-01-09 06:56:28,474 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42457
2024-01-09 06:56:28,474 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44483
2024-01-09 06:56:28,474 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,474 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,474 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:56:28,474 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_ms0dp3r
2024-01-09 06:56:28,474 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4311c0c1-96fc-4e2a-96a1-d750b81e9f1a
2024-01-09 06:56:28,474 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37387
2024-01-09 06:56:28,474 - distributed.worker - INFO - Starting Worker plugin PreImport-a5fc9847-86ad-44be-bf20-929e3539bbd8
2024-01-09 06:56:28,474 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c212cafd-ef58-4234-a7cd-059f56ad9fad
2024-01-09 06:56:28,474 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,485 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:56:28,486 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45225
2024-01-09 06:56:28,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45225
2024-01-09 06:56:28,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40159
2024-01-09 06:56:28,486 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,486 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,486 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:56:28,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-43nz4rff
2024-01-09 06:56:28,486 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e237566d-dadd-41d1-adff-193dc8d03a46
2024-01-09 06:56:28,486 - distributed.worker - INFO - Starting Worker plugin PreImport-4e902160-cbb6-4e61-94b4-8292dd919737
2024-01-09 06:56:28,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7838ce90-d584-448e-b7f7-774754c67aca
2024-01-09 06:56:28,487 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,538 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:56:28,539 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38999
2024-01-09 06:56:28,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38999
2024-01-09 06:56:28,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36473
2024-01-09 06:56:28,539 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,539 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,539 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:56:28,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nkjq9w6q
2024-01-09 06:56:28,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e233c6d-8f63-4e6e-8720-a381fae932c4
2024-01-09 06:56:28,540 - distributed.worker - INFO - Starting Worker plugin PreImport-268616ac-ab03-4650-bab8-b7f006ac7c36
2024-01-09 06:56:28,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0bff4e14-8f31-4b37-aa89-3b04daa72150
2024-01-09 06:56:28,541 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,550 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:56:28,551 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46205
2024-01-09 06:56:28,551 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46205
2024-01-09 06:56:28,551 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35741
2024-01-09 06:56:28,551 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,551 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,551 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:56:28,551 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dphq5rw6
2024-01-09 06:56:28,551 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-756b3566-cb19-43b9-a2ff-02265da32917
2024-01-09 06:56:28,552 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a46117a6-5151-4591-966b-6486f0bffa22
2024-01-09 06:56:28,552 - distributed.worker - INFO - Starting Worker plugin PreImport-52a2062e-1d22-4e85-a10a-970c285e555b
2024-01-09 06:56:28,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:56:28,628 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,628 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37387
2024-01-09 06:56:28,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:56:28,635 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37387
2024-01-09 06:56:28,646 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:56:28,646 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,647 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37387
2024-01-09 06:56:28,692 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:56:28,693 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,693 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,694 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37387
2024-01-09 06:56:28,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:56:28,696 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37387
2024-01-09 06:56:28,696 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:56:28,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37387
2024-01-09 06:56:28,718 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:56:28,718 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:56:28,718 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:56:28,719 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:56:28,719 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:56:28,719 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:56:28,719 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:56:28,719 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-09 06:56:28,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33039. Reason: nanny-close
2024-01-09 06:56:28,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40843. Reason: nanny-close
2024-01-09 06:56:28,726 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35807. Reason: nanny-close
2024-01-09 06:56:28,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:37387 has been closed.
2024-01-09 06:56:28,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45225. Reason: nanny-close
2024-01-09 06:56:28,727 - distributed.core - INFO - Connection to tcp://127.0.0.1:37387 has been closed.
2024-01-09 06:56:28,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43231. Reason: nanny-close
2024-01-09 06:56:28,727 - distributed.nanny - INFO - Worker closed
2024-01-09 06:56:28,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42457. Reason: nanny-close
2024-01-09 06:56:28,728 - distributed.core - INFO - Connection to tcp://127.0.0.1:37387 has been closed.
2024-01-09 06:56:28,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46205. Reason: nanny-close
2024-01-09 06:56:28,728 - distributed.nanny - INFO - Worker closed
2024-01-09 06:56:28,728 - distributed.core - INFO - Connection to tcp://127.0.0.1:37387 has been closed.
2024-01-09 06:56:28,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38999. Reason: nanny-close
2024-01-09 06:56:28,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:37387 has been closed.
2024-01-09 06:56:28,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:37387 has been closed.
2024-01-09 06:56:28,729 - distributed.nanny - INFO - Worker closed
2024-01-09 06:56:28,730 - distributed.nanny - INFO - Worker closed
2024-01-09 06:56:28,730 - distributed.core - INFO - Connection to tcp://127.0.0.1:37387 has been closed.
2024-01-09 06:56:28,730 - distributed.nanny - INFO - Worker closed
2024-01-09 06:56:28,730 - distributed.nanny - INFO - Worker closed
2024-01-09 06:56:28,730 - distributed.core - INFO - Connection to tcp://127.0.0.1:37387 has been closed.
2024-01-09 06:56:28,731 - distributed.nanny - INFO - Worker closed
2024-01-09 06:56:28,732 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-09 06:57:07,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:07,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:07,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:07,421 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39257
2024-01-09 06:57:07,421 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39257
2024-01-09 06:57:07,421 - distributed.worker - INFO -           Worker name:                          0
2024-01-09 06:57:07,421 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43983
2024-01-09 06:57:07,421 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39113
2024-01-09 06:57:07,421 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:07,421 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:07,421 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-09 06:57:07,421 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qfasonc1
2024-01-09 06:57:07,422 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96eaaf3b-431b-49d0-a04b-c1a29c72700a
2024-01-09 06:57:07,422 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6c4d2d7-f60b-4088-bf4b-4edc2da5f9cd
2024-01-09 06:57:07,422 - distributed.worker - INFO - Starting Worker plugin PreImport-1ad21701-0985-4601-a2cd-b5715e844a5b
2024-01-09 06:57:07,425 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-09 06:57:07,425 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39257. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-09 06:57:07,425 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-09 06:57:07,427 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-09 06:57:11,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:11,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:11,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:11,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:11,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:11,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:11,971 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:11,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:11,971 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:11,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:12,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:12,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:12,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:12,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:12,145 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-09 06:57:12,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-09 06:57:12,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:12,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33271
2024-01-09 06:57:12,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33271
2024-01-09 06:57:12,506 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34473
2024-01-09 06:57:12,506 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,506 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,506 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:12,506 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:57:12,506 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2gyoav_u
2024-01-09 06:57:12,506 - distributed.worker - INFO - Starting Worker plugin PreImport-014c481e-0b29-407d-a5c3-88d462eb2a0d
2024-01-09 06:57:12,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49c915c1-b679-4e1a-8811-a45d3b78d182
2024-01-09 06:57:12,507 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4326313c-fd52-4b87-a27f-96e07b1abe61
2024-01-09 06:57:12,507 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,507 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:12,508 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44551
2024-01-09 06:57:12,508 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44551
2024-01-09 06:57:12,508 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41293
2024-01-09 06:57:12,508 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,509 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,509 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:12,509 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:57:12,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-66d94jst
2024-01-09 06:57:12,509 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21040522-1bc9-4373-9ad3-f5cce1daca1d
2024-01-09 06:57:12,509 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19fed502-b653-49db-88dd-897bd923e6c0
2024-01-09 06:57:12,509 - distributed.worker - INFO - Starting Worker plugin PreImport-b48e980d-7be0-47e5-9649-fa8495ba916c
2024-01-09 06:57:12,509 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:12,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44507
2024-01-09 06:57:12,548 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44507
2024-01-09 06:57:12,548 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45549
2024-01-09 06:57:12,548 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,548 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,548 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:12,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:57:12,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wi5kr35g
2024-01-09 06:57:12,549 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d599d52-bab9-4c12-9ee0-4980ba9573a2
2024-01-09 06:57:12,549 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d504e746-66a3-49ad-8bc5-9bc21ebe3c69
2024-01-09 06:57:12,549 - distributed.worker - INFO - Starting Worker plugin PreImport-fea7e55e-1e48-4e57-ac84-0d5ca1a4ee82
2024-01-09 06:57:12,550 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:12,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38353
2024-01-09 06:57:12,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38353
2024-01-09 06:57:12,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33457
2024-01-09 06:57:12,597 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,597 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,597 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:12,597 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:57:12,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x1xdhz38
2024-01-09 06:57:12,597 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cef8e5a8-65de-4153-ba7b-e73411fee537
2024-01-09 06:57:12,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29730213-86c4-4d9a-a9f8-0e276b81b886
2024-01-09 06:57:12,597 - distributed.worker - INFO - Starting Worker plugin PreImport-3350e534-366c-4978-bc72-b734aa919686
2024-01-09 06:57:12,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,608 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:57:12,609 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,609 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,609 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:12,610 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34127
2024-01-09 06:57:12,610 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34127
2024-01-09 06:57:12,610 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32903
2024-01-09 06:57:12,610 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,610 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,610 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:12,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45335
2024-01-09 06:57:12,610 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:57:12,610 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r888uriy
2024-01-09 06:57:12,610 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:57:12,610 - distributed.worker - INFO - Starting Worker plugin PreImport-735117e7-ebeb-459d-ad06-f2df2898a8bf
2024-01-09 06:57:12,611 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-36db8502-df8c-4c82-af99-95ecec978dc4
2024-01-09 06:57:12,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b302aa4-c717-492b-8551-7bc5606db9dc
2024-01-09 06:57:12,611 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,611 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,611 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45335
2024-01-09 06:57:12,673 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:12,674 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33473
2024-01-09 06:57:12,674 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33473
2024-01-09 06:57:12,674 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32873
2024-01-09 06:57:12,674 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,674 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,674 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:12,674 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:57:12,674 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r6hnn9wg
2024-01-09 06:57:12,675 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9c4c452-d4bc-4019-adb3-a7c56e5cfc91
2024-01-09 06:57:12,675 - distributed.worker - INFO - Starting Worker plugin PreImport-7aa4f409-e657-4579-88d2-8ece0411f930
2024-01-09 06:57:12,675 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a970d7c-eb42-49f3-bd6c-229971b79d66
2024-01-09 06:57:12,675 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:12,676 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34247
2024-01-09 06:57:12,676 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34247
2024-01-09 06:57:12,676 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36471
2024-01-09 06:57:12,677 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,677 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:12,677 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:57:12,677 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9nj3yd3m
2024-01-09 06:57:12,677 - distributed.worker - INFO - Starting Worker plugin PreImport-633648d2-2149-40be-81b2-9f1147448990
2024-01-09 06:57:12,677 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd118f1b-e263-4478-970a-741c60ae0276
2024-01-09 06:57:12,677 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba4fcb0f-2cbb-400a-9b3f-b33106e8b89c
2024-01-09 06:57:12,678 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,694 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:57:12,695 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,695 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45335
2024-01-09 06:57:12,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:57:12,760 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,760 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45335
2024-01-09 06:57:12,772 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:57:12,772 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-09 06:57:12,773 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34237
2024-01-09 06:57:12,773 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,773 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,773 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34237
2024-01-09 06:57:12,773 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37899
2024-01-09 06:57:12,773 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,773 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,773 - distributed.worker - INFO -               Threads:                          1
2024-01-09 06:57:12,774 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-09 06:57:12,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cof1lfro
2024-01-09 06:57:12,774 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76e938fd-709e-4675-b080-ed604a91af7a
2024-01-09 06:57:12,774 - distributed.worker - INFO - Starting Worker plugin PreImport-ffb948a6-77ff-4ff7-838f-c286f323f6aa
2024-01-09 06:57:12,774 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64b3b8c1-593e-40e5-836f-0b6981eee17a
2024-01-09 06:57:12,774 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45335
2024-01-09 06:57:12,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:57:12,830 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,830 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:57:12,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45335
2024-01-09 06:57:12,832 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,832 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45335
2024-01-09 06:57:12,854 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-09 06:57:12,855 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45335
2024-01-09 06:57:12,855 - distributed.worker - INFO - -------------------------------------------------
2024-01-09 06:57:12,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45335
2024-01-09 06:57:12,868 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44551. Reason: nanny-close
2024-01-09 06:57:12,869 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33271. Reason: nanny-close
2024-01-09 06:57:12,869 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44507. Reason: nanny-close
2024-01-09 06:57:12,870 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34127. Reason: nanny-close
2024-01-09 06:57:12,870 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38353. Reason: nanny-close
2024-01-09 06:57:12,870 - distributed.core - INFO - Connection to tcp://127.0.0.1:45335 has been closed.
2024-01-09 06:57:12,871 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33473. Reason: nanny-close
2024-01-09 06:57:12,871 - distributed.core - INFO - Connection to tcp://127.0.0.1:45335 has been closed.
2024-01-09 06:57:12,871 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34247. Reason: nanny-close
2024-01-09 06:57:12,872 - distributed.core - INFO - Connection to tcp://127.0.0.1:45335 has been closed.
2024-01-09 06:57:12,872 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34237. Reason: nanny-close
2024-01-09 06:57:12,872 - distributed.nanny - INFO - Worker closed
2024-01-09 06:57:12,872 - distributed.nanny - INFO - Worker closed
2024-01-09 06:57:12,872 - distributed.core - INFO - Connection to tcp://127.0.0.1:45335 has been closed.
2024-01-09 06:57:12,872 - distributed.core - INFO - Connection to tcp://127.0.0.1:45335 has been closed.
2024-01-09 06:57:12,873 - distributed.nanny - INFO - Worker closed
2024-01-09 06:57:12,873 - distributed.core - INFO - Connection to tcp://127.0.0.1:45335 has been closed.
2024-01-09 06:57:12,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:45335 has been closed.
2024-01-09 06:57:12,874 - distributed.nanny - INFO - Worker closed
2024-01-09 06:57:12,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:45335 has been closed.
2024-01-09 06:57:12,874 - distributed.nanny - INFO - Worker closed
2024-01-09 06:57:12,875 - distributed.nanny - INFO - Worker closed
2024-01-09 06:57:12,875 - distributed.nanny - INFO - Worker closed
2024-01-09 06:57:12,875 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk PASSED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-09 06:58:41,457 - distributed.worker - WARNING - RMM allocation of 1.00 MiB failed, spill-on-demand couldn't find any device memory to spill.
RMM allocs: 1.00 MiB, <ProxyManager dev_limit=25.60 GiB host_limit=0.98 TiB disk=0 B(0) host=0 B(0) dev=0 B(0)>, traceback:
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/threadpoolexecutor.py", line 57, in _worker
    task.run()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py", line 65, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1541, in <lambda>
    executor, lambda: context.run(func, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2954, in apply_function
    msg = apply_function_simple(function, args, kwargs, time_delay)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2990, in apply_function_simple
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_proxify_host_file.py", line 467, in task
    rmm.DeviceBuffer(size=rmm_pool_size),  # Trigger OOM
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/proxify_host_file.py", line 617, in oom
    traceback.print_stack(file=f)


2024-01-09 06:58:41,678 - distributed.worker - WARNING - Compute Failed
Key:       task-5c302f54392a5ca98adb3c55c8357b0e
Function:  task
args:      ()
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_serializer PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-None] 