[1706082670.332758] [dgx13:84381:0]    ib_mlx5dv_md.c:468  UCX  ERROR mlx5_3: LRU push returned Unsupported operation
[dgx13:84381:0:84381]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  84381) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f215003807d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7f2150035c21]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x27dbc) [0x7f2150035dbc]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x739f8) [0x7f21500e09f8]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7f21500b7d8f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7f21500f3afd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x6da) [0x7f21500f89ea]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7f21500f972f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6c6f0) [0x7f21501a76f0]
 9  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5621695b004c]
10  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5621695963f6]
11  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562169590fb4]
12  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5621695a2469]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x562169593042]
14  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562169590fb4]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5621695a2469]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x562169593042]
17  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5621696456d2]
18  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x562169597c10]
19  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5621696456d2]
20  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x562169597c10]
21  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5621696456d2]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x562169597c10]
23  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5621696456d2]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x562169597c10]
25  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5621696456d2]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x562169597c10]
27  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5621696456d2]
28  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f21652e91e9]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8aa6) [0x7f21652e9aa6]
30  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x56216959a6ac]
31  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x5621695553ff]
32  /opt/conda/envs/gdf/bin/python(+0x136723) [0x562169599723]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x562169597929]
34  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5621695a2712]
35  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5621695924e6]
36  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5621695a2712]
37  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5621695924e6]
38  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5621695a2712]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5621695924e6]
40  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5621695a2712]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5621695924e6]
42  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562169590fb4]
43  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5621695a2469]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x562169593042]
45  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562169590fb4]
46  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x5621695af8cb]
47  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5621695b004c]
48  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x56216967380e]
49  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x56216959a6ac]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5621695963f6]
51  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5621695a2712]
52  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x5621695af9ac]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5621695963f6]
54  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5621695a2712]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5621695924e6]
56  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562169590fb4]
57  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5621695a2469]
58  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5621695924e6]
59  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5621695a2712]
60  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x562169592232]
61  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562169590fb4]
=================================
2024-01-24 07:51:12,134 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40596
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7f07fd3b6240, tag: 0xd27963f08036198, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7f07fd3b6240, tag: 0xd27963f08036198, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:12,135 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40596
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7f16d1156280, tag: 0x2464e55168a4ada0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7f16d1156280, tag: 0x2464e55168a4ada0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:12,135 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40596
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7efcb3da9200, tag: 0x803b8ef176261159, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7efcb3da9200, tag: 0x803b8ef176261159, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:12,135 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40596
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fc3edf46280, tag: 0x559b7c6f71d8eb9d, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fc3edf46280, tag: 0x559b7c6f71d8eb9d, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-24 07:51:12,135 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40596
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f0ebe4b5280, tag: 0x8c3fc9b63efef2a2, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f0ebe4b5280, tag: 0x8c3fc9b63efef2a2, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-24 07:51:12,137 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40596
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7fa01466d180, tag: 0x5eabdaad14574a27, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2857, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7fa01466d180, tag: 0x5eabdaad14574a27, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-24 07:51:12,139 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40596
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f56101fd200, tag: 0x294cb2ec8cee1791, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2857, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f56101fd200, tag: 0x294cb2ec8cee1791, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
[1706082673.085200] [dgx13:84393:0]    ib_mlx5dv_md.c:468  UCX  ERROR mlx5_2: LRU push returned Unsupported operation
[dgx13:84393:0:84393]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  84393) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f082456807d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7f0824565c21]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x27dbc) [0x7f0824565dbc]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x739f8) [0x7f08246109f8]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7f08245e7d8f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7f0824623afd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x6da) [0x7f08246289ea]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7f082462972f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6c6f0) [0x7f08246d76f0]
 9  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5595abc5504c]
10  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5595abc3b3f6]
11  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5595abc35fb4]
12  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5595abc47469]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x5595abc38042]
14  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5595abc35fb4]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5595abc47469]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x5595abc38042]
17  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5595abcea6d2]
18  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x5595abc3cc10]
19  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5595abcea6d2]
20  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x5595abc3cc10]
21  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5595abcea6d2]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x5595abc3cc10]
23  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5595abcea6d2]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x5595abc3cc10]
25  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5595abcea6d2]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x5595abc3cc10]
27  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5595abcea6d2]
28  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f08398371e9]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8aa6) [0x7f0839837aa6]
30  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5595abc3f6ac]
31  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x5595abbfa3ff]
32  /opt/conda/envs/gdf/bin/python(+0x136723) [0x5595abc3e723]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x5595abc3c929]
34  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5595abc47712]
35  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5595abc374e6]
36  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5595abc47712]
37  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5595abc374e6]
38  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5595abc47712]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5595abc374e6]
40  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5595abc47712]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5595abc374e6]
42  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5595abc35fb4]
43  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5595abc47469]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x5595abc38042]
45  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5595abc35fb4]
46  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x5595abc548cb]
47  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5595abc5504c]
48  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x5595abd1880e]
49  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5595abc3f6ac]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5595abc3b3f6]
51  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5595abc47712]
52  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x5595abc549ac]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5595abc3b3f6]
54  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5595abc47712]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5595abc374e6]
56  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5595abc35fb4]
57  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5595abc47469]
58  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5595abc374e6]
59  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5595abc47712]
60  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x5595abc37232]
61  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5595abc35fb4]
=================================
2024-01-24 07:51:14,977 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39686
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #019] ep: 0x7efcb3da9280, tag: 0xe85c42b13cf7bbd, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #019] ep: 0x7efcb3da9280, tag: 0xe85c42b13cf7bbd, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-24 07:51:14,978 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39686
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #019] ep: 0x7f16d1156100, tag: 0xf943a9745678cff6, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #019] ep: 0x7f16d1156100, tag: 0xf943a9745678cff6, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-24 07:51:14,978 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39686
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #016] ep: 0x7f0ebe4b5140, tag: 0x47f252f4d8e7cec7, nbytes: 100002624, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #016] ep: 0x7f0ebe4b5140, tag: 0x47f252f4d8e7cec7, nbytes: 100002624, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:14,978 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39686
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #016] ep: 0x7fc3edf46180, tag: 0x305f6b6decf07695, nbytes: 99997464, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #016] ep: 0x7fc3edf46180, tag: 0x305f6b6decf07695, nbytes: 99997464, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:14,980 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39686
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #016] ep: 0x7f56101fd280, tag: 0xf59c29f68f435fc7, nbytes: 100006248, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #016] ep: 0x7f56101fd280, tag: 0xf59c29f68f435fc7, nbytes: 100006248, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:14,980 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39686
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #031] ep: 0x7fa01466d0c0, tag: 0x323c05312f6f8168, nbytes: 49995676, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #031] ep: 0x7fa01466d0c0, tag: 0x323c05312f6f8168, nbytes: 49995676, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:16,844 - distributed.nanny - WARNING - Restarting worker
[1706082678.348948] [dgx13:84372:0]    ib_mlx5dv_md.c:468  UCX  ERROR mlx5_1: LRU push returned Unsupported operation
[dgx13:84372:0:84372]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  84372) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f0f086c707d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0x51) [0x7f0f086c4c21]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x27dbc) [0x7f0f086c4dbc]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x739f8) [0x7f0f0876f9f8]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xdf) [0x7f0f08746d8f]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x1d) [0x7f0f08782afd]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x6da) [0x7f0f087879ea]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x4f) [0x7f0f0878872f]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x6c6f0) [0x7f0f088366f0]
 9  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55becb23d04c]
10  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55becb2233f6]
11  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55becb21dfb4]
12  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55becb22f469]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55becb220042]
14  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55becb21dfb4]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55becb22f469]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55becb220042]
17  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55becb2d26d2]
18  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x55becb224c10]
19  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55becb2d26d2]
20  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x55becb224c10]
21  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55becb2d26d2]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x55becb224c10]
23  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55becb2d26d2]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x55becb224c10]
25  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55becb2d26d2]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5dc0) [0x55becb224c10]
27  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55becb2d26d2]
28  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f0f1d9831e9]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8aa6) [0x7f0f1d983aa6]
30  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55becb2276ac]
31  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55becb1e23ff]
32  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55becb226723]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55becb224929]
34  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55becb22f712]
35  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55becb21f4e6]
36  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55becb22f712]
37  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55becb21f4e6]
38  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55becb22f712]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55becb21f4e6]
40  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55becb22f712]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55becb21f4e6]
42  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55becb21dfb4]
43  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55becb22f469]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55becb220042]
45  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55becb21dfb4]
46  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55becb23c8cb]
47  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55becb23d04c]
48  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55becb30080e]
49  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55becb2276ac]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55becb2233f6]
51  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55becb22f712]
52  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55becb23c9ac]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55becb2233f6]
54  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55becb22f712]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55becb21f4e6]
56  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55becb21dfb4]
57  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55becb22f469]
58  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55becb21f4e6]
59  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55becb22f712]
60  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55becb21f232]
61  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55becb21dfb4]
=================================
2024-01-24 07:51:20,126 - distributed.nanny - WARNING - Restarting worker
2024-01-24 07:51:20,202 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46587
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #067] ep: 0x7fc3edf461c0, tag: 0xf35457ece801002a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #067] ep: 0x7fc3edf461c0, tag: 0xf35457ece801002a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-24 07:51:20,202 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46587
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #047] ep: 0x7f16d1156240, tag: 0x36d850c0d3bc2024, nbytes: 99953640, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #047] ep: 0x7f16d1156240, tag: 0x36d850c0d3bc2024, nbytes: 99953640, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:20,202 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46587
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #056] ep: 0x7efcb3da91c0, tag: 0x155a3746a3ea41f9, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #056] ep: 0x7efcb3da91c0, tag: 0x155a3746a3ea41f9, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-24 07:51:20,203 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46587
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #092] ep: 0x7fa01466d240, tag: 0x6debf7dcbbf241a3, nbytes: 100004328, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #092] ep: 0x7fa01466d240, tag: 0x6debf7dcbbf241a3, nbytes: 100004328, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-24 07:51:20,203 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46587
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #043] ep: 0x7f56101fd1c0, tag: 0xdf761ffa4779428, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #043] ep: 0x7f56101fd1c0, tag: 0xdf761ffa4779428, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-24 07:51:21,186 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 7)
Function:  _concat
args:      ([                key   payload  _partitions
20992     858528615  17654447            7
20997     839152888  31517982            7
20998        895606  35620250            7
21000     813680714  42851474            7
21005     842916767  18573085            7
...             ...       ...          ...
99977555  805999597  71727687            7
99977651  832222108  15506307            7
99977657  502431709  27228165            7
99974448  833531318  46998195            7
99974449  503358111  94822003            7

[12503560 rows x 3 columns],                 key   payload  _partitions
21067     320470331  78036848            7
21071     968609623  10468524            7
21084     620732964  78463405            7
14410     941462416  12699425            7
62054     112592384  52264810            7
...             ...       ...          ...
99974621  956343049  46538131            7
99974505  520894665  45598155            7
99974507  916612355  36791796            7
99974516  930208644  6
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-01-24 07:51:21,311 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 2)
Function:  _concat
args:      ([                key   payload  _partitions
20994      10483064  98221626            2
21003     838511950   6540561            2
21007     849677067  65745304            2
21009     845222293  26092065            2
21016     109240060  22776545            2
...             ...       ...          ...
99974433  845889194  13925144            2
99974450  836888678  88294512            2
99974451  209682258  72265802            2
99974462  815918212  53908098            2
99974463  831849333  56834795            2

[12500460 rows x 3 columns],                 key   payload  _partitions
21060     921564911  67910004            2
21075     948468084  95315774            2
21080     615853253  28349014            2
14401     905298364  69585570            2
62049     317856783  90639409            2
...             ...       ...          ...
99974504  936292347  37081434            2
99974522  908217543   2640293            2
99974472  941081971  67974842            2
99974483  918733149  7
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-01-24 07:51:21,436 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 5)
Function:  _concat
args:      ([                key   payload  _partitions
21004     308660951  85515726            5
21014     804569408  81528293            5
21017     810512950  46065656            5
25722     303044921  29891928            5
25723     819417321  91544945            5
...             ...       ...          ...
99977654  607653451  16835437            5
99974437  833917326   1077015            5
99974446  805940265  90974462            5
99974456  840738941  50421789            5
99974459  864332141  11525984            5

[12501172 rows x 3 columns],                 key   payload  _partitions
21056     910518691  84513640            5
21073     925724166   3871815            5
21077     214503574  29988609            5
21079     119545007   3361430            5
21083     219782207  80069904            5
...             ...       ...          ...
99974499  958973750  45675789            5
99974478  962814186  85792974            5
99974479  222218561  59940987            5
99974486  963815787  4
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-01-24 07:51:23,672 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 4)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0            11425        4  97463177            4
1            11428        4  97387233            4
2            11432        4   9322537            4
3            11450        4  63514782            4
4            52401        4  26456481            4
...            ...      ...       ...          ...
12499995  99979642        4  30470397            4
12499996  99979749        4  85827540            4
12499997  99979758        4  73490413            4
12499998  99979764        4  61831800            4
12499999  99979771        4  98511770            4

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100052326        4  84689645            4
1         100052338        4  70710007            4
2         100113664        4  73957351            4
3         100052343        4  46554584            4
4         100113673        4  21419373            4
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-01-24 07:51:24,461 - distributed.nanny - WARNING - Restarting worker
2024-01-24 07:51:28,064 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 7)
Function:  _concat
args:      ([                key   payload  _partitions
43008     510417057  23754948            7
31746     861215324  90540806            7
31759     511079697  93633216            7
31760     510871347  56584102            7
123496    868963907  58661506            7
...             ...       ...          ...
99987902  862917534  17599891            7
99987915  865433103  21193928            7
99987921  110119799  17339874            7
99987924  862372701  48915068            7
99987928  836572551   5966607            7

[12503560 rows x 3 columns],                 key   payload  _partitions
31744     951912437  60431881            7
123520    902620041  43416691            7
31756     914330093  61944960            7
123522    620090136   2637526            7
93217     911668125  80483797            7
...             ...       ...          ...
99984648  961041698  29966357            7
99984670  943421669  62359305            7
99984712  115648949  66406300            7
99984721  413370017   
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-01-24 07:51:28,174 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-01-24 07:51:28,175 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-01-24 07:51:28,228 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-01-24 07:51:28,228 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-01-24 07:51:28,389 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 0)
Function:  _concat
args:      ([                key   payload  _partitions
43019     800929588  26446870            0
43020     806219627  71288071            0
43022     508451378    896058            0
43024     836076219   1471704            0
31753     844069800  83982121            0
...             ...       ...          ...
99987886  861931955   6429408            0
99987888  819702156  72960976            0
99987889  808791520  56962055            0
99987892  861458766  44040693            0
99987907  607124427  30947006            0

[12497939 rows x 3 columns],                 key   payload  _partitions
31761     954166200  67401134            0
123538    901654303  64359954            0
31764     937238892  84114854            0
123545    966301020  18174732            0
93239     902099909  75581233            0
...             ...       ...          ...
99984704  945347052  25792121            0
99984719  901626394  10845310            0
99984722  952291921  26517469            0
99984723  967235459  7
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-01-24 07:51:28,590 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 5)
Function:  _concat
args:      ([                key   payload  _partitions
43010     805375978  69199776            5
43011       8699772  33562109            5
43013     307954517  21490608            5
43021     801204952   3916140            5
31744     851912437  60431881            5
...             ...       ...          ...
99987906  858185286   7359734            5
99987914  818937105  27071832            5
99987919  824561712  13231508            5
99987929  610882384   2561505            5
99987934  800666338  10271694            5

[12501172 rows x 3 columns],                 key   payload  _partitions
31750     962319337  71796950            5
123521    224201724  60052997            5
31751     917491052  17818449            5
93216     903009703  16248951            5
20995     910353206  92759830            5
...             ...       ...          ...
99984847  906161411  84897229            5
99984851  723417364  11566594            5
99984649  950072458   4827874            5
99984652   16249037  8
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-01-24 07:51:28,788 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 3)
Function:  _concat
args:      ([                key   payload  _partitions
43009     856279781  81543913            3
43018     843947592  12752940            3
43031     868131844  37407532            3
31755     853009930  43041975            3
123498    806539674   2709294            3
...             ...       ...          ...
99987887  866002536  87022327            3
99987898  107383881  70198254            3
99987903  706999568  64126192            3
99987927  845736333  40792590            3
99987933  302793158  89803346            3

[12504123 rows x 3 columns],                 key   payload  _partitions
31746     961215324  90540806            3
123523    324445279  53443498            3
31757     313993387  67147294            3
93219     623953144  92511233            3
21004     321160951  85515726            3
...             ...       ...          ...
99984707  911464342  49029416            3
99984708  935965506  54752200            3
99984715  954826856  68775416            3
99984720  119244215  1
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
