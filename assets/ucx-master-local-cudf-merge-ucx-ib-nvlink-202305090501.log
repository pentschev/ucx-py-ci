2023-05-09 07:12:59,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:12:59,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:12:59,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:12:59,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:12:59,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:12:59,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:12:59,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:12:59,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:12:59,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:12:59,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:12:59,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:12:59,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:12:59,435 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:12:59,435 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:12:59,441 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:12:59,441 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1683616387.673096] [dgx13:81701:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_1: LRU push returned Unsupported operation
[dgx13:81701:0:81701]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  81701) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f48d1c1edec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7f48d1c1bd28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7f48d1c1be44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x83605) [0x7f48d1cda605]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7f48d1caf069]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7f48d1cf51a0]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7f48d1cfa0be]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7f48d1cf9872]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7f48d1d8f5d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x56001b0c8dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x56001b0c71a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x56001b0add36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56001b0a727a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56001b0b8c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x56001b0a93cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56001b0a727a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56001b0b8c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x56001b0a93cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56001b0cd70e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56001b0ae923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56001b0cd70e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56001b0ae923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56001b0cd70e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56001b0ae923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56001b0cd70e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56001b0ae923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56001b0cd70e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56001b0ae923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56001b0cd70e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f49683f12fe]
30  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x56001b0b12bc]
31  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x56001b064817]
32  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x56001b0aff83]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x56001b0add36]
34  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56001b0b8ef3]
35  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56001b0a881b]
36  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56001b0b8ef3]
37  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56001b0a881b]
38  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56001b0b8ef3]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56001b0a881b]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56001b0b8ef3]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56001b0a881b]
42  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56001b0a727a]
43  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56001b0b8c05]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x56001b0acfa7]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56001b0a727a]
46  /opt/conda/envs/gdf/bin/python(+0x147935) [0x56001b0c6935]
47  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x56001b0c7104]
48  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x56001b18dfc8]
49  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x56001b0b12bc]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x56001b0ac1bb]
51  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56001b0b8ef3]
52  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x56001b0c6c72]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x56001b0ac1bb]
54  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56001b0b8ef3]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56001b0a881b]
56  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56001b0a727a]
57  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56001b0b8c05]
58  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56001b0a881b]
59  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56001b0b8ef3]
60  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x56001b0a8568]
61  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56001b0a727a]
=================================
Task exception was never retrieved
future: <Task finished name='Task-812' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-05-09 07:13:07,835 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34258
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7f944cb66100, tag: 0xd9480de697eecb7, nbytes: 800000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7f944cb66100, tag: 0xd9480de697eecb7, nbytes: 800000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-09 07:13:08,251 - distributed.nanny - WARNING - Restarting worker
[1683616389.013714] [dgx13:81699:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_2: LRU push returned Unsupported operation
[dgx13:81699:0:81699]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  81699) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f52d12a6dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7f52d12a3d28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7f52d12a3e44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x83605) [0x7f52d1362605]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7f52d1337069]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7f52d137d1a0]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7f52d13820be]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7f52d1381872]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7f52d14175d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x56229b90bdc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x56229b90a1a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x56229b8f0d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56229b8ea27a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56229b8fbc05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x56229b8ec3cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56229b8ea27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56229b8fbc05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x56229b8ec3cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56229b91070e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56229b8f1923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56229b91070e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56229b8f1923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56229b91070e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56229b8f1923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56229b91070e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56229b8f1923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56229b91070e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x56229b8f1923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x56229b91070e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f52f3ffe2fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7f52f3ffeb4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x56229b8f42bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x56229b8a7817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x56229b8f2f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x56229b8f0d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56229b8fbef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56229b8eb81b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56229b8fbef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56229b8eb81b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56229b8fbef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56229b8eb81b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56229b8fbef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56229b8eb81b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56229b8ea27a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56229b8fbc05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x56229b8effa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56229b8ea27a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x56229b909935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x56229b90a104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x56229b9d0fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x56229b8f42bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x56229b8ef1bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56229b8fbef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x56229b909c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x56229b8ef1bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56229b8fbef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56229b8eb81b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x56229b8ea27a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x56229b8fbc05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x56229b8eb81b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x56229b8fbef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x56229b8eb568]
=================================
Task exception was never retrieved
future: <Task finished name='Task-1109' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-05-09 07:13:09,227 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47406
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
Task exception was never retrieved
future: <Task finished name='Task-1060' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-05-09 07:13:09,459 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47406
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7f8935040200, tag: 0xb53c04579974d8be, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7f8935040200, tag: 0xb53c04579974d8be, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
Task exception was never retrieved
future: <Task finished name='Task-1085' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1088' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-05-09 07:13:09,547 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47406
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f05ede54140, tag: 0x5686c5cd869be22, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f05ede54140, tag: 0x5686c5cd869be22, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-09 07:13:09,652 - distributed.nanny - WARNING - Restarting worker
2023-05-09 07:13:09,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:13:09,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:13:11,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:13:11,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:13:38,799 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47406
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 318, in connect
    raise OSError(
OSError: Timed out trying to connect to ucx://127.0.0.1:47406 after 30 s
2023-05-09 07:13:39,049 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47406
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 318, in connect
    raise OSError(
OSError: Timed out trying to connect to ucx://127.0.0.1:47406 after 30 s
[1683616424.418474] [dgx13:81680:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_3: LRU push returned Unsupported operation
[dgx13:81680:0:81680]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  81680) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7fb685d2bdec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7fb685d28d28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7fb685d28e44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x83605) [0x7fb685de7605]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7fb685dbc069]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7fb685e021a0]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7fb685e070be]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7fb685e06872]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7fb685e9c5d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x55cb944a5dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x55cb944a41a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55cb9448ad36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55cb9448427a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55cb94495c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55cb944863cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55cb9448427a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55cb94495c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55cb944863cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55cb944aa70e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55cb9448b923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55cb944aa70e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55cb9448b923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55cb944aa70e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55cb9448b923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55cb944aa70e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55cb9448b923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55cb944aa70e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55cb9448b923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55cb944aa70e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fb6aaa852fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7fb6aaa85b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55cb9448e2bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55cb94441817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55cb9448cf83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55cb9448ad36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55cb94495ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55cb9448581b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55cb94495ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55cb9448581b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55cb94495ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55cb9448581b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55cb94495ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55cb9448581b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55cb9448427a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55cb94495c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55cb94489fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55cb9448427a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55cb944a3935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55cb944a4104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55cb9456afc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55cb9448e2bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55cb944891bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55cb94495ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55cb944a3c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55cb944891bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55cb94495ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55cb9448581b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55cb9448427a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55cb94495c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55cb9448581b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55cb94495ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55cb94485568]
=================================
2023-05-09 07:13:44,606 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51826
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #044] ep: 0x7f8ea0570280, tag: 0x5be1344bfc12423c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #044] ep: 0x7f8ea0570280, tag: 0x5be1344bfc12423c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-09 07:13:44,607 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51826
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #059] ep: 0x7f2f648d5180, tag: 0x5bf6a0988047c7cf, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #059] ep: 0x7f2f648d5180, tag: 0x5bf6a0988047c7cf, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-09 07:13:44,608 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51826
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #046] ep: 0x7f944cb66180, tag: 0x784c8fde36567953, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #046] ep: 0x7f944cb66180, tag: 0x784c8fde36567953, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-09 07:13:44,608 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51826
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #039] ep: 0x7f8935040180, tag: 0xa6c9da66537cd23a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #039] ep: 0x7f8935040180, tag: 0xa6c9da66537cd23a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-09 07:13:44,608 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51826
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #046] ep: 0x7f05ede54180, tag: 0xba51855c59b8abf3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #046] ep: 0x7f05ede54180, tag: 0xba51855c59b8abf3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-09 07:13:44,607 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51826
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #015] ep: 0x7fe505017180, tag: 0x8007d2a4d59c634c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #015] ep: 0x7fe505017180, tag: 0x8007d2a4d59c634c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-09 07:13:44,608 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51826
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #010] ep: 0x7fb3b0d9d240, tag: 0xe830d1ae26eb61dc, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #010] ep: 0x7fb3b0d9d240, tag: 0xe830d1ae26eb61dc, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-09 07:13:45,024 - distributed.nanny - WARNING - Restarting worker
2023-05-09 07:13:46,691 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-09 07:13:46,691 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-09 07:13:46,723 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-09 07:13:46,723 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-09 07:13:46,728 - distributed.worker - ERROR - tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2902, in get_data_from_worker
    status = response["status"]
TypeError: tuple indices must be integers or slices, not str
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
