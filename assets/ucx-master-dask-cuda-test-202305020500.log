============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-02 05:36:22,253 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:22,257 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-05-02 05:36:22,260 - distributed.scheduler - INFO - State start
2023-05-02 05:36:22,279 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:22,280 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-02 05:36:22,280 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-05-02 05:36:22,378 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33199'
2023-05-02 05:36:22,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45389'
2023-05-02 05:36:22,399 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37089'
2023-05-02 05:36:22,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45811'
2023-05-02 05:36:23,757 - distributed.scheduler - INFO - Receive client connection: Client-44aa7e89-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:23,770 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42982
2023-05-02 05:36:23,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:23,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:23,868 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:23,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:23,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:23,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:23,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:23,895 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:23,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:23,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:23,945 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:23,952 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-02 05:36:23,974 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42919
2023-05-02 05:36:23,974 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42919
2023-05-02 05:36:23,974 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40513
2023-05-02 05:36:23,974 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-02 05:36:23,974 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:23,974 - distributed.worker - INFO -               Threads:                          4
2023-05-02 05:36:23,974 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-02 05:36:23,974 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-enwf19q1
2023-05-02 05:36:23,974 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ceac8c84-c5c5-4ce4-b973-2899dce17fda
2023-05-02 05:36:23,974 - distributed.worker - INFO - Starting Worker plugin PreImport-e79ad1f1-7d44-46e8-9c30-4b1e4f983ec1
2023-05-02 05:36:23,974 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c89df697-12e7-4368-9d5b-f7311e5c384a
2023-05-02 05:36:23,975 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:23,989 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42919', status: init, memory: 0, processing: 0>
2023-05-02 05:36:23,990 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42919
2023-05-02 05:36:23,990 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43010
2023-05-02 05:36:23,990 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-02 05:36:23,991 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:23,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-02 05:36:24,977 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36765
2023-05-02 05:36:24,977 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36765
2023-05-02 05:36:24,977 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39019
2023-05-02 05:36:24,977 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-02 05:36:24,977 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:24,977 - distributed.worker - INFO -               Threads:                          4
2023-05-02 05:36:24,977 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-02 05:36:24,977 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1j9mnq60
2023-05-02 05:36:24,978 - distributed.worker - INFO - Starting Worker plugin PreImport-a0c414c2-3f13-4fcb-8a4f-0b5ba8362ac4
2023-05-02 05:36:24,978 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-66c6a343-fd74-4315-a452-59dd782462de
2023-05-02 05:36:24,978 - distributed.worker - INFO - Starting Worker plugin RMMSetup-627e1b09-093f-4986-b8b6-ed10ac2dc7dd
2023-05-02 05:36:24,978 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:24,997 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36765', status: init, memory: 0, processing: 0>
2023-05-02 05:36:24,997 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36765
2023-05-02 05:36:24,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47724
2023-05-02 05:36:24,998 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-02 05:36:24,998 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:25,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-02 05:36:25,044 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40245
2023-05-02 05:36:25,044 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40245
2023-05-02 05:36:25,045 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39625
2023-05-02 05:36:25,045 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33577
2023-05-02 05:36:25,045 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-02 05:36:25,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39625
2023-05-02 05:36:25,045 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:25,045 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36325
2023-05-02 05:36:25,045 - distributed.worker - INFO -               Threads:                          4
2023-05-02 05:36:25,045 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-02 05:36:25,045 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-02 05:36:25,045 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:25,045 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-owel8ow2
2023-05-02 05:36:25,045 - distributed.worker - INFO -               Threads:                          4
2023-05-02 05:36:25,045 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-02 05:36:25,045 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8stve7yd
2023-05-02 05:36:25,045 - distributed.worker - INFO - Starting Worker plugin PreImport-4102fa24-ac6f-4ad1-9db4-af4dac367f0d
2023-05-02 05:36:25,045 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1457e9e-999c-43a9-b76c-06c66648e004
2023-05-02 05:36:25,045 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1ae9f34-93ec-4bc7-9d34-9b0256efb44d
2023-05-02 05:36:25,045 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91ae5bb7-b40b-4805-b9fe-cabfb9e61146
2023-05-02 05:36:25,045 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:25,046 - distributed.worker - INFO - Starting Worker plugin PreImport-15ac7c60-8381-4a21-ba14-2deadbbb6b02
2023-05-02 05:36:25,046 - distributed.worker - INFO - Starting Worker plugin RMMSetup-506c8666-678e-4678-b1d8-f4a6a6000b8c
2023-05-02 05:36:25,046 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:25,065 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40245', status: init, memory: 0, processing: 0>
2023-05-02 05:36:25,065 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40245
2023-05-02 05:36:25,065 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47740
2023-05-02 05:36:25,066 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-02 05:36:25,066 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:25,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-02 05:36:25,070 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39625', status: init, memory: 0, processing: 0>
2023-05-02 05:36:25,070 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39625
2023-05-02 05:36:25,070 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47748
2023-05-02 05:36:25,071 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-02 05:36:25,071 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:25,073 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-02 05:36:25,107 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-02 05:36:25,107 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-02 05:36:25,108 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-02 05:36:25,108 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-02 05:36:25,112 - distributed.scheduler - INFO - Remove client Client-44aa7e89-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:25,112 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42982; closing.
2023-05-02 05:36:25,112 - distributed.scheduler - INFO - Remove client Client-44aa7e89-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:25,113 - distributed.scheduler - INFO - Close client connection: Client-44aa7e89-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:25,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45389'. Reason: nanny-close
2023-05-02 05:36:25,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33199'. Reason: nanny-close
2023-05-02 05:36:25,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37089'. Reason: nanny-close
2023-05-02 05:36:25,114 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:25,115 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45811'. Reason: nanny-close
2023-05-02 05:36:25,115 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:25,116 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36765. Reason: nanny-close
2023-05-02 05:36:25,116 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42919. Reason: nanny-close
2023-05-02 05:36:25,117 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:25,117 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:25,117 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40245. Reason: nanny-close
2023-05-02 05:36:25,118 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47724; closing.
2023-05-02 05:36:25,118 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-02 05:36:25,118 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-02 05:36:25,118 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36765', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:25,118 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36765
2023-05-02 05:36:25,118 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39625. Reason: nanny-close
2023-05-02 05:36:25,119 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43010; closing.
2023-05-02 05:36:25,119 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:25,119 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-02 05:36:25,119 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:25,119 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42919', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:25,120 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42919
2023-05-02 05:36:25,120 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36765
2023-05-02 05:36:25,120 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:25,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-02 05:36:25,121 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47740; closing.
2023-05-02 05:36:25,121 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40245', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:25,121 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40245
2023-05-02 05:36:25,122 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47748; closing.
2023-05-02 05:36:25,122 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:25,122 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39625', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:25,122 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39625
2023-05-02 05:36:25,122 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:36:26,131 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:36:26,131 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:36:26,132 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:36:26,132 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-02 05:36:26,133 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-02 05:36:28,154 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:28,159 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44751 instead
  warnings.warn(
2023-05-02 05:36:28,163 - distributed.scheduler - INFO - State start
2023-05-02 05:36:28,189 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:28,190 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:36:28,190 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44751/status
2023-05-02 05:36:28,214 - distributed.scheduler - INFO - Receive client connection: Client-4825a804-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:28,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58548
2023-05-02 05:36:28,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46757'
2023-05-02 05:36:28,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43157'
2023-05-02 05:36:28,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44125'
2023-05-02 05:36:28,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38765'
2023-05-02 05:36:28,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44397'
2023-05-02 05:36:28,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42197'
2023-05-02 05:36:28,533 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46033'
2023-05-02 05:36:28,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41275'
2023-05-02 05:36:30,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:30,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:30,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:30,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:30,210 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:30,210 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:30,217 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:30,217 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:30,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:30,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:30,233 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:30,233 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:30,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:30,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:30,237 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:30,237 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:30,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:30,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:30,324 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:30,327 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:30,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:30,356 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:30,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:30,462 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:33,095 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39201
2023-05-02 05:36:33,096 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39201
2023-05-02 05:36:33,096 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35293
2023-05-02 05:36:33,096 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,096 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,096 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:33,096 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:33,096 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pok60cko
2023-05-02 05:36:33,097 - distributed.worker - INFO - Starting Worker plugin PreImport-b026789e-e430-485e-a90f-2f67576bbe23
2023-05-02 05:36:33,097 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54213dd3-7929-4c1f-8034-2cfdd6f75663
2023-05-02 05:36:33,097 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad6d64db-cbc0-4de9-896d-ffed9933d361
2023-05-02 05:36:33,238 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,262 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39201', status: init, memory: 0, processing: 0>
2023-05-02 05:36:33,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39201
2023-05-02 05:36:33,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58636
2023-05-02 05:36:33,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,264 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:33,533 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41883
2023-05-02 05:36:33,533 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41883
2023-05-02 05:36:33,533 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41387
2023-05-02 05:36:33,533 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,533 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,533 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46311
2023-05-02 05:36:33,533 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:33,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46311
2023-05-02 05:36:33,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:33,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w5ztvf4v
2023-05-02 05:36:33,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46605
2023-05-02 05:36:33,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,534 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,534 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:33,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:33,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_kgph32f
2023-05-02 05:36:33,534 - distributed.worker - INFO - Starting Worker plugin PreImport-f6e70f93-2265-49e2-929b-c857cd781db0
2023-05-02 05:36:33,534 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae06dcd6-fb93-4daf-b6ba-a03089a9a6dc
2023-05-02 05:36:33,534 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26524b30-c52d-46d4-ab34-9f91e71fd43b
2023-05-02 05:36:33,535 - distributed.worker - INFO - Starting Worker plugin PreImport-56bb70e2-2802-478a-89a7-e35a3324e4d9
2023-05-02 05:36:33,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e08393ad-55fd-474a-9584-7b5bffc63e2d
2023-05-02 05:36:33,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80b93dca-a8b7-4bc3-b157-1e9db1e71354
2023-05-02 05:36:33,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33211
2023-05-02 05:36:33,542 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45647
2023-05-02 05:36:33,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33211
2023-05-02 05:36:33,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45647
2023-05-02 05:36:33,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38659
2023-05-02 05:36:33,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33417
2023-05-02 05:36:33,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,542 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,542 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,542 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:33,542 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:33,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:33,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:33,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kl0_d63n
2023-05-02 05:36:33,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mqvhgtk9
2023-05-02 05:36:33,542 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34953
2023-05-02 05:36:33,543 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34953
2023-05-02 05:36:33,543 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41829
2023-05-02 05:36:33,543 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,543 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,543 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:33,543 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:33,543 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2zn6yy8x
2023-05-02 05:36:33,543 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b8dbe07c-9c38-4636-9d1e-f6ae62f3a1c3
2023-05-02 05:36:33,543 - distributed.worker - INFO - Starting Worker plugin PreImport-5ff47964-e8be-4c8f-a355-c0615fac528a
2023-05-02 05:36:33,544 - distributed.worker - INFO - Starting Worker plugin PreImport-c659c798-b410-48fe-bbea-4f9e84140d96
2023-05-02 05:36:33,544 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a5e5584b-fad0-457b-ba4e-0fcb2d99cc22
2023-05-02 05:36:33,544 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2c65824-91e6-43e8-b087-f654f7cd54f7
2023-05-02 05:36:33,544 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0559b7e4-da40-4f48-a03d-78af50e46b3a
2023-05-02 05:36:33,544 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2449e0e1-0dfd-4efc-96d5-37730df2083c
2023-05-02 05:36:33,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46027
2023-05-02 05:36:33,568 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46027
2023-05-02 05:36:33,568 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34403
2023-05-02 05:36:33,568 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,568 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,568 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:33,568 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:33,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-anqcz2c6
2023-05-02 05:36:33,569 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eca618f0-896e-4612-adaf-73b6b9556d4e
2023-05-02 05:36:33,590 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32823
2023-05-02 05:36:33,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32823
2023-05-02 05:36:33,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34093
2023-05-02 05:36:33,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,590 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,590 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:33,591 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:33,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vijj14n7
2023-05-02 05:36:33,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-524b73d4-8800-4ac0-a1aa-d1875ae8b3b4
2023-05-02 05:36:33,712 - distributed.worker - INFO - Starting Worker plugin PreImport-bbecff61-9cf4-4013-a18d-70f2810a83d0
2023-05-02 05:36:33,712 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8550485a-fb2e-4845-83b9-add54de66de9
2023-05-02 05:36:33,712 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,716 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,719 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,719 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,719 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,722 - distributed.worker - INFO - Starting Worker plugin PreImport-9b4c9744-02be-48ef-8098-377dba6501e4
2023-05-02 05:36:33,722 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81eb23df-7d04-45d6-8196-dcad2f9e45ae
2023-05-02 05:36:33,722 - distributed.worker - INFO - Starting Worker plugin PreImport-45b1e542-75fc-42dd-a338-5247e0950613
2023-05-02 05:36:33,722 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85ee4507-ba0c-44f1-9fab-4627bd7e4e30
2023-05-02 05:36:33,722 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,722 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,738 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46027', status: init, memory: 0, processing: 0>
2023-05-02 05:36:33,739 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46027
2023-05-02 05:36:33,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58642
2023-05-02 05:36:33,739 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,739 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:33,744 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46311', status: init, memory: 0, processing: 0>
2023-05-02 05:36:33,745 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46311
2023-05-02 05:36:33,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58670
2023-05-02 05:36:33,745 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,745 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:33,749 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32823', status: init, memory: 0, processing: 0>
2023-05-02 05:36:33,749 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32823
2023-05-02 05:36:33,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58700
2023-05-02 05:36:33,750 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,750 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,750 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45647', status: init, memory: 0, processing: 0>
2023-05-02 05:36:33,750 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45647
2023-05-02 05:36:33,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58658
2023-05-02 05:36:33,751 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,751 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:33,752 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33211', status: init, memory: 0, processing: 0>
2023-05-02 05:36:33,753 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33211
2023-05-02 05:36:33,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58684
2023-05-02 05:36:33,753 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,754 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,754 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41883', status: init, memory: 0, processing: 0>
2023-05-02 05:36:33,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:33,754 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41883
2023-05-02 05:36:33,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58686
2023-05-02 05:36:33,755 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,755 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,755 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34953', status: init, memory: 0, processing: 0>
2023-05-02 05:36:33,756 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34953
2023-05-02 05:36:33,756 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58702
2023-05-02 05:36:33,756 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:33,756 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:33,756 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:33,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:33,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:33,857 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:33,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:33,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:33,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:33,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:33,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:33,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:33,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:33,863 - distributed.scheduler - INFO - Remove client Client-4825a804-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:33,863 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58548; closing.
2023-05-02 05:36:33,863 - distributed.scheduler - INFO - Remove client Client-4825a804-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:33,864 - distributed.scheduler - INFO - Close client connection: Client-4825a804-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:33,865 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44125'. Reason: nanny-close
2023-05-02 05:36:33,866 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:33,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38765'. Reason: nanny-close
2023-05-02 05:36:33,868 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:33,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46033'. Reason: nanny-close
2023-05-02 05:36:33,868 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41883. Reason: nanny-close
2023-05-02 05:36:33,868 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:33,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46757'. Reason: nanny-close
2023-05-02 05:36:33,869 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45647. Reason: nanny-close
2023-05-02 05:36:33,869 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:33,869 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32823. Reason: nanny-close
2023-05-02 05:36:33,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43157'. Reason: nanny-close
2023-05-02 05:36:33,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:33,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44397'. Reason: nanny-close
2023-05-02 05:36:33,870 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39201. Reason: nanny-close
2023-05-02 05:36:33,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:33,870 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42197'. Reason: nanny-close
2023-05-02 05:36:33,870 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58686; closing.
2023-05-02 05:36:33,871 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:33,871 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33211. Reason: nanny-close
2023-05-02 05:36:33,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:33,871 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41883', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:33,871 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41883
2023-05-02 05:36:33,871 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:33,871 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:33,871 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41275'. Reason: nanny-close
2023-05-02 05:36:33,871 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34953. Reason: nanny-close
2023-05-02 05:36:33,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:33,872 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46311. Reason: nanny-close
2023-05-02 05:36:33,872 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:33,872 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:33,872 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:33,872 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58658; closing.
2023-05-02 05:36:33,872 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:33,872 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41883
2023-05-02 05:36:33,873 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58700; closing.
2023-05-02 05:36:33,873 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41883
2023-05-02 05:36:33,873 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:33,873 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46027. Reason: nanny-close
2023-05-02 05:36:33,873 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45647', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:33,873 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41883
2023-05-02 05:36:33,873 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:33,873 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45647
2023-05-02 05:36:33,873 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41883
2023-05-02 05:36:33,874 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32823', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:33,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:33,874 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32823
2023-05-02 05:36:33,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:33,874 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58636; closing.
2023-05-02 05:36:33,875 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:33,875 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39201', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:33,875 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:33,875 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39201
2023-05-02 05:36:33,875 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:33,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58684; closing.
2023-05-02 05:36:33,875 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:33,876 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33211', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:33,876 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33211
2023-05-02 05:36:33,876 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:33,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58670; closing.
2023-05-02 05:36:33,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58702; closing.
2023-05-02 05:36:33,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58642; closing.
2023-05-02 05:36:33,877 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46311', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:33,877 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46311
2023-05-02 05:36:33,877 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34953', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:33,877 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34953
2023-05-02 05:36:33,878 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46027', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:33,878 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46027
2023-05-02 05:36:33,878 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:36:35,586 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:36:35,586 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:36:35,586 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:36:35,587 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:36:35,588 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-02 05:36:37,732 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:37,736 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33679 instead
  warnings.warn(
2023-05-02 05:36:37,739 - distributed.scheduler - INFO - State start
2023-05-02 05:36:37,763 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:37,764 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:36:37,764 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33679/status
2023-05-02 05:36:37,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33581'
2023-05-02 05:36:38,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39451'
2023-05-02 05:36:38,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41391'
2023-05-02 05:36:38,029 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44015'
2023-05-02 05:36:38,038 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35515'
2023-05-02 05:36:38,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38573'
2023-05-02 05:36:38,057 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45785'
2023-05-02 05:36:38,068 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40833'
2023-05-02 05:36:39,072 - distributed.scheduler - INFO - Receive client connection: Client-4dcfea25-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:39,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54962
2023-05-02 05:36:39,749 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:39,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:39,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:39,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:39,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:39,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:39,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:39,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:39,767 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:39,767 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:39,778 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:39,778 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:39,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:39,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:39,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:39,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:39,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:39,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:39,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:39,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:39,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:39,873 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:39,876 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:39,876 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:41,571 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40113
2023-05-02 05:36:41,571 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40113
2023-05-02 05:36:41,571 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40851
2023-05-02 05:36:41,571 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:41,571 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:41,571 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:41,571 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:41,571 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uo46penq
2023-05-02 05:36:41,571 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c297ab9e-2054-440d-8ea0-7e428ff3a4f4
2023-05-02 05:36:41,704 - distributed.worker - INFO - Starting Worker plugin PreImport-1aede2fc-e34c-4e78-ac74-ee3c054d4aae
2023-05-02 05:36:41,704 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fa99d38-ab40-4d49-ad0a-87babbfa7c2b
2023-05-02 05:36:41,705 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:41,812 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46465
2023-05-02 05:36:41,812 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46465
2023-05-02 05:36:41,812 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37833
2023-05-02 05:36:41,812 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:41,813 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:41,813 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:41,813 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:41,813 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pcfaknxw
2023-05-02 05:36:41,813 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10a14d0d-01ad-4882-91b2-57a65cdbd5c5
2023-05-02 05:36:41,868 - distributed.worker - INFO - Starting Worker plugin PreImport-8e38f056-3f34-4171-8887-3c0298b1b72c
2023-05-02 05:36:41,868 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a73d7bd-991e-4671-9018-e6f4c9f42e1b
2023-05-02 05:36:41,869 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:41,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46465', status: init, memory: 0, processing: 0>
2023-05-02 05:36:41,911 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46465
2023-05-02 05:36:41,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54984
2023-05-02 05:36:41,912 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:41,912 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:41,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:41,951 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34731
2023-05-02 05:36:41,951 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34731
2023-05-02 05:36:41,951 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37977
2023-05-02 05:36:41,951 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:41,951 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:41,951 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:41,952 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:41,952 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-13lmb248
2023-05-02 05:36:41,952 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6fbabcf-f9c9-4954-9785-326525838385
2023-05-02 05:36:41,952 - distributed.worker - INFO - Starting Worker plugin PreImport-fde0d5d4-a4ee-44f9-a3e4-7da263bf5692
2023-05-02 05:36:41,952 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ba29f5e-a8cd-4e6e-8192-33513f104c07
2023-05-02 05:36:41,987 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:41,992 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40781
2023-05-02 05:36:41,993 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40781
2023-05-02 05:36:41,993 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39653
2023-05-02 05:36:41,993 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:41,993 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:41,993 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:41,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:41,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_9qx4q6s
2023-05-02 05:36:41,994 - distributed.worker - INFO - Starting Worker plugin RMMSetup-15983d62-01f1-44d9-a381-baf0c311d870
2023-05-02 05:36:41,998 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40113', status: init, memory: 0, processing: 0>
2023-05-02 05:36:41,998 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40113
2023-05-02 05:36:41,999 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54972
2023-05-02 05:36:41,999 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:41,999 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,001 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:42,002 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46855
2023-05-02 05:36:42,002 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46855
2023-05-02 05:36:42,003 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46131
2023-05-02 05:36:42,003 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,003 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,003 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:42,003 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:42,003 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rsyxn5z_
2023-05-02 05:36:42,003 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ae3bc28-0349-47c3-82a1-77d47b4bb86e
2023-05-02 05:36:42,015 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34731', status: init, memory: 0, processing: 0>
2023-05-02 05:36:42,016 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34731
2023-05-02 05:36:42,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55000
2023-05-02 05:36:42,016 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,017 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:42,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38943
2023-05-02 05:36:42,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38943
2023-05-02 05:36:42,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46129
2023-05-02 05:36:42,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,042 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,042 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:42,042 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:42,042 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7dea1fpx
2023-05-02 05:36:42,043 - distributed.worker - INFO - Starting Worker plugin PreImport-64b4986f-3a4f-41d6-a8dd-b80bd5e48fcd
2023-05-02 05:36:42,043 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17144615-11bd-4886-b76f-4128fb48d2db
2023-05-02 05:36:42,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0793899-d9a8-4d97-ba59-25674c532950
2023-05-02 05:36:42,096 - distributed.worker - INFO - Starting Worker plugin PreImport-d16d4267-4ad8-464d-8100-1f90376cf01d
2023-05-02 05:36:42,096 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ef87ae9-d071-4a77-b774-1caf062f98b4
2023-05-02 05:36:42,096 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,100 - distributed.worker - INFO - Starting Worker plugin PreImport-d3ad5112-1855-492b-a011-616a56d2686c
2023-05-02 05:36:42,100 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11b636da-b524-4744-baa4-885fb13743c8
2023-05-02 05:36:42,100 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,110 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40863
2023-05-02 05:36:42,110 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40863
2023-05-02 05:36:42,110 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37269
2023-05-02 05:36:42,110 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,110 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,110 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:42,110 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:42,110 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d8z8dyll
2023-05-02 05:36:42,111 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7009e851-155d-4556-b326-2f727fa35d9b
2023-05-02 05:36:42,118 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,120 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40781', status: init, memory: 0, processing: 0>
2023-05-02 05:36:42,121 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40781
2023-05-02 05:36:42,121 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55008
2023-05-02 05:36:42,121 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,121 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:42,128 - distributed.worker - INFO - Starting Worker plugin PreImport-ae6e95f1-666d-4b4b-975f-c65ca9e67833
2023-05-02 05:36:42,129 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49793ed5-6354-41b9-8b8a-683eece6fce9
2023-05-02 05:36:42,129 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,156 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38943', status: init, memory: 0, processing: 0>
2023-05-02 05:36:42,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38943
2023-05-02 05:36:42,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55038
2023-05-02 05:36:42,157 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,157 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:42,163 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40863', status: init, memory: 0, processing: 0>
2023-05-02 05:36:42,163 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40863
2023-05-02 05:36:42,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55052
2023-05-02 05:36:42,164 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,164 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,167 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:42,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46855', status: init, memory: 0, processing: 0>
2023-05-02 05:36:42,216 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46855
2023-05-02 05:36:42,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55022
2023-05-02 05:36:42,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,217 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:42,231 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40835
2023-05-02 05:36:42,231 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40835
2023-05-02 05:36:42,231 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33009
2023-05-02 05:36:42,231 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,232 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,232 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:42,232 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:42,232 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eecbugzd
2023-05-02 05:36:42,232 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8c15458-c2f4-4a32-b8d4-565dd0155695
2023-05-02 05:36:42,298 - distributed.worker - INFO - Starting Worker plugin PreImport-cdb5bce7-2b60-462f-b898-befc4cfa4c79
2023-05-02 05:36:42,298 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99e5e25a-380f-4764-b915-e3226029a612
2023-05-02 05:36:42,299 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,325 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40835', status: init, memory: 0, processing: 0>
2023-05-02 05:36:42,326 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40835
2023-05-02 05:36:42,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55054
2023-05-02 05:36:42,326 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:42,326 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:42,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:42,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:42,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:42,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:42,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:42,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:42,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:42,370 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:42,375 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:42,380 - distributed.scheduler - INFO - Remove client Client-4dcfea25-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:42,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54962; closing.
2023-05-02 05:36:42,380 - distributed.scheduler - INFO - Remove client Client-4dcfea25-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:42,380 - distributed.scheduler - INFO - Close client connection: Client-4dcfea25-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:42,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44015'. Reason: nanny-close
2023-05-02 05:36:42,383 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:42,383 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33581'. Reason: nanny-close
2023-05-02 05:36:42,385 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:42,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39451'. Reason: nanny-close
2023-05-02 05:36:42,385 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38943. Reason: nanny-close
2023-05-02 05:36:42,386 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:42,386 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41391'. Reason: nanny-close
2023-05-02 05:36:42,386 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:42,386 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40835. Reason: nanny-close
2023-05-02 05:36:42,386 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40113. Reason: nanny-close
2023-05-02 05:36:42,386 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35515'. Reason: nanny-close
2023-05-02 05:36:42,387 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:42,387 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40781. Reason: nanny-close
2023-05-02 05:36:42,387 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38573'. Reason: nanny-close
2023-05-02 05:36:42,387 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:42,387 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45785'. Reason: nanny-close
2023-05-02 05:36:42,387 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34731. Reason: nanny-close
2023-05-02 05:36:42,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55038; closing.
2023-05-02 05:36:42,388 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:42,388 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:42,388 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:42,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38943', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:42,388 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40833'. Reason: nanny-close
2023-05-02 05:36:42,388 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38943
2023-05-02 05:36:42,388 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40863. Reason: nanny-close
2023-05-02 05:36:42,388 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:42,388 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:42,388 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46855. Reason: nanny-close
2023-05-02 05:36:42,389 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:42,389 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:42,389 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:42,389 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:42,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54972; closing.
2023-05-02 05:36:42,389 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:42,389 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46465. Reason: nanny-close
2023-05-02 05:36:42,390 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:42,390 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:42,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40113', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:42,391 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40113
2023-05-02 05:36:42,391 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:42,391 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:42,391 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38943
2023-05-02 05:36:42,391 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38943
2023-05-02 05:36:42,391 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55008; closing.
2023-05-02 05:36:42,391 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:42,392 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:42,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40781', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:42,392 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40781
2023-05-02 05:36:42,393 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55054; closing.
2023-05-02 05:36:42,393 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:42,393 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55000; closing.
2023-05-02 05:36:42,393 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:42,394 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40835', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:42,394 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40835
2023-05-02 05:36:42,394 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34731', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:42,394 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34731
2023-05-02 05:36:42,395 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55022; closing.
2023-05-02 05:36:42,395 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55052; closing.
2023-05-02 05:36:42,396 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54984; closing.
2023-05-02 05:36:42,396 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46855', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:42,396 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46855
2023-05-02 05:36:42,397 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40863', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:42,397 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40863
2023-05-02 05:36:42,397 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46465', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:42,398 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46465
2023-05-02 05:36:42,398 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:36:44,402 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:36:44,403 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:36:44,403 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:36:44,404 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:36:44,405 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-02 05:36:46,640 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:46,645 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44627 instead
  warnings.warn(
2023-05-02 05:36:46,649 - distributed.scheduler - INFO - State start
2023-05-02 05:36:46,678 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:46,679 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:36:46,679 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44627/status
2023-05-02 05:36:47,258 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35731'
2023-05-02 05:36:47,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38595'
2023-05-02 05:36:47,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40373'
2023-05-02 05:36:47,300 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34067'
2023-05-02 05:36:47,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45383'
2023-05-02 05:36:47,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34939'
2023-05-02 05:36:47,329 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40303'
2023-05-02 05:36:47,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42335'
2023-05-02 05:36:48,100 - distributed.scheduler - INFO - Receive client connection: Client-530a9bf8-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:48,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45492
2023-05-02 05:36:49,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:49,038 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:49,052 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:49,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:49,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:49,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:49,071 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:49,071 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:49,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:49,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:49,093 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:49,093 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:49,095 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:49,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:49,110 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:49,110 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:49,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:49,402 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:49,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:49,527 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:49,539 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:49,539 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:49,574 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:49,574 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:52,431 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40715
2023-05-02 05:36:52,432 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40715
2023-05-02 05:36:52,432 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40531
2023-05-02 05:36:52,432 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,432 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,432 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:52,432 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:52,432 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hgy_a9gf
2023-05-02 05:36:52,432 - distributed.worker - INFO - Starting Worker plugin PreImport-94ec4772-d270-4a52-a17c-45d86aabbee2
2023-05-02 05:36:52,432 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-755009a5-33ae-4250-b10d-21db6af5bfe6
2023-05-02 05:36:52,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e5e5a2f-c94f-409c-8950-d9a1dcffa1f4
2023-05-02 05:36:52,444 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39779
2023-05-02 05:36:52,444 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39779
2023-05-02 05:36:52,445 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32875
2023-05-02 05:36:52,445 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,445 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,445 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:52,445 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:52,445 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u0hd99oz
2023-05-02 05:36:52,445 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b343b5ff-0d54-4a96-adf8-2d79d211bf19
2023-05-02 05:36:52,478 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33665
2023-05-02 05:36:52,478 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33665
2023-05-02 05:36:52,478 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46525
2023-05-02 05:36:52,478 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,478 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,478 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:52,478 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:52,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3ydeel36
2023-05-02 05:36:52,479 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9321552-a12f-448e-90dd-64e31cb2438f
2023-05-02 05:36:52,618 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46837
2023-05-02 05:36:52,618 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46837
2023-05-02 05:36:52,618 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37109
2023-05-02 05:36:52,618 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,619 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,619 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:52,619 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:52,619 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-br8ujhv1
2023-05-02 05:36:52,619 - distributed.worker - INFO - Starting Worker plugin PreImport-996c8833-3c6a-4aeb-afaf-e2f290ef3003
2023-05-02 05:36:52,619 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-648ef62a-74e1-4026-b664-a07ad8f910d6
2023-05-02 05:36:52,619 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64c1c566-dfec-4e87-98b9-ae090808abd1
2023-05-02 05:36:52,747 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38895
2023-05-02 05:36:52,747 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38895
2023-05-02 05:36:52,747 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34615
2023-05-02 05:36:52,747 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,747 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,747 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:52,747 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:52,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n3ytqbhv
2023-05-02 05:36:52,748 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1cda1f6-4676-473f-ba2c-4cad393f3174
2023-05-02 05:36:52,748 - distributed.worker - INFO - Starting Worker plugin PreImport-ed73138d-f884-4bf3-8d22-bdfcda861683
2023-05-02 05:36:52,748 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e43da4b4-67ca-4e0b-b4c3-b3b3f6f89938
2023-05-02 05:36:52,787 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,789 - distributed.worker - INFO - Starting Worker plugin PreImport-10d8caec-a714-4c7a-b8cf-1a18791f0ade
2023-05-02 05:36:52,790 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d1ebc36-aa9f-4f34-b908-6fed8f886b29
2023-05-02 05:36:52,790 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,805 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,812 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44071
2023-05-02 05:36:52,812 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44071
2023-05-02 05:36:52,812 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37177
2023-05-02 05:36:52,813 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,813 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,813 - distributed.worker - INFO - Starting Worker plugin PreImport-ba803c4b-d998-45b1-965b-df91af5abc60
2023-05-02 05:36:52,813 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:52,813 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:52,813 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d148ad67-6bc2-4454-afe8-70dc8a874653
2023-05-02 05:36:52,813 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ulxpop9o
2023-05-02 05:36:52,813 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,813 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67b5a669-5f70-4448-adbc-f7fc5fab26cc
2023-05-02 05:36:52,813 - distributed.worker - INFO - Starting Worker plugin PreImport-8ffaded9-ed15-4748-97a5-62ce50e1e7db
2023-05-02 05:36:52,813 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25d04a78-079c-47b7-8187-db502cb3f7b9
2023-05-02 05:36:52,822 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40715', status: init, memory: 0, processing: 0>
2023-05-02 05:36:52,824 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40715
2023-05-02 05:36:52,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45516
2023-05-02 05:36:52,825 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,825 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,825 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33665', status: init, memory: 0, processing: 0>
2023-05-02 05:36:52,826 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33665
2023-05-02 05:36:52,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45520
2023-05-02 05:36:52,827 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,827 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:52,829 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:52,839 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46837', status: init, memory: 0, processing: 0>
2023-05-02 05:36:52,840 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46837
2023-05-02 05:36:52,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45532
2023-05-02 05:36:52,840 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,840 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:52,854 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39779', status: init, memory: 0, processing: 0>
2023-05-02 05:36:52,854 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39779
2023-05-02 05:36:52,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45536
2023-05-02 05:36:52,855 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,855 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:52,943 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,975 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38895', status: init, memory: 0, processing: 0>
2023-05-02 05:36:52,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38895
2023-05-02 05:36:52,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45548
2023-05-02 05:36:52,976 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:52,976 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:52,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:53,009 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:53,039 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44071', status: init, memory: 0, processing: 0>
2023-05-02 05:36:53,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44071
2023-05-02 05:36:53,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45560
2023-05-02 05:36:53,040 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:53,040 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:53,042 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:53,096 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42931
2023-05-02 05:36:53,096 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42931
2023-05-02 05:36:53,096 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35167
2023-05-02 05:36:53,096 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:53,096 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:53,096 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:53,096 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:53,097 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a30zbneo
2023-05-02 05:36:53,097 - distributed.worker - INFO - Starting Worker plugin PreImport-ff510c87-8985-4a7f-a0a5-ec49e02f0f27
2023-05-02 05:36:53,097 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c4dfaee-d9b1-4536-98e2-74715e1be31b
2023-05-02 05:36:53,097 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55d5f3be-ca63-44ee-ac18-70bc87b873eb
2023-05-02 05:36:53,195 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39663
2023-05-02 05:36:53,195 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39663
2023-05-02 05:36:53,195 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33169
2023-05-02 05:36:53,195 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:36:53,195 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:53,195 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:36:53,195 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:36:53,195 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qs5b5x95
2023-05-02 05:36:53,196 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20ef6401-e1d3-4197-a700-daec4d74bca1
2023-05-02 05:36:53,483 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:53,512 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42931', status: init, memory: 0, processing: 0>
2023-05-02 05:36:53,513 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42931
2023-05-02 05:36:53,513 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45572
2023-05-02 05:36:53,513 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:53,513 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:53,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:53,520 - distributed.worker - INFO - Starting Worker plugin PreImport-77e95c2d-40bc-48ef-8634-31545e588510
2023-05-02 05:36:53,520 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64f48ee4-2c46-4ada-85d7-16e5ac979ffc
2023-05-02 05:36:53,520 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:53,547 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39663', status: init, memory: 0, processing: 0>
2023-05-02 05:36:53,548 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39663
2023-05-02 05:36:53,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45578
2023-05-02 05:36:53,548 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:36:53,548 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:36:53,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:36:53,635 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:53,636 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:53,636 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:53,636 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:53,636 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:53,636 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:53,636 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:53,636 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:36:53,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:36:53,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:36:53,653 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:36:53,653 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:36:53,653 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:36:53,653 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:36:53,653 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:36:53,653 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:36:53,661 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:36:53,662 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:36:53,665 - distributed.scheduler - INFO - Remove client Client-530a9bf8-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:53,665 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45492; closing.
2023-05-02 05:36:53,665 - distributed.scheduler - INFO - Remove client Client-530a9bf8-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:53,666 - distributed.scheduler - INFO - Close client connection: Client-530a9bf8-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:53,667 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34067'. Reason: nanny-close
2023-05-02 05:36:53,667 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:53,668 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35731'. Reason: nanny-close
2023-05-02 05:36:53,669 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:53,669 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38895. Reason: nanny-close
2023-05-02 05:36:53,669 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38595'. Reason: nanny-close
2023-05-02 05:36:53,669 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:53,670 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44071. Reason: nanny-close
2023-05-02 05:36:53,670 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40373'. Reason: nanny-close
2023-05-02 05:36:53,670 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:53,671 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33665. Reason: nanny-close
2023-05-02 05:36:53,671 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:53,671 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45548; closing.
2023-05-02 05:36:53,671 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45383'. Reason: nanny-close
2023-05-02 05:36:53,671 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38895', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:53,671 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:53,671 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38895
2023-05-02 05:36:53,671 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:53,672 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34939'. Reason: nanny-close
2023-05-02 05:36:53,672 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40715. Reason: nanny-close
2023-05-02 05:36:53,672 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:53,672 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:53,672 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46837. Reason: nanny-close
2023-05-02 05:36:53,672 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40303'. Reason: nanny-close
2023-05-02 05:36:53,672 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:53,673 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39663. Reason: nanny-close
2023-05-02 05:36:53,673 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:53,673 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42335'. Reason: nanny-close
2023-05-02 05:36:53,673 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38895
2023-05-02 05:36:53,673 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38895
2023-05-02 05:36:53,673 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45560; closing.
2023-05-02 05:36:53,673 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:36:53,673 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38895
2023-05-02 05:36:53,673 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:53,673 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44071', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:53,674 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44071
2023-05-02 05:36:53,674 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38895
2023-05-02 05:36:53,674 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38895
2023-05-02 05:36:53,674 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42931. Reason: nanny-close
2023-05-02 05:36:53,674 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38895
2023-05-02 05:36:53,674 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45520; closing.
2023-05-02 05:36:53,674 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:53,674 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39779. Reason: nanny-close
2023-05-02 05:36:53,674 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:53,674 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33665', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:53,675 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33665
2023-05-02 05:36:53,675 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:53,675 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:53,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45516; closing.
2023-05-02 05:36:53,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45532; closing.
2023-05-02 05:36:53,675 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:53,675 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:53,676 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40715', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:53,676 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:53,676 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40715
2023-05-02 05:36:53,676 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:53,676 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:36:53,676 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46837', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:53,676 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46837
2023-05-02 05:36:53,677 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45578; closing.
2023-05-02 05:36:53,677 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45572; closing.
2023-05-02 05:36:53,677 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:53,677 - distributed.nanny - INFO - Worker closed
2023-05-02 05:36:53,677 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39663', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:53,677 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39663
2023-05-02 05:36:53,678 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42931', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:53,678 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42931
2023-05-02 05:36:53,678 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45536; closing.
2023-05-02 05:36:53,678 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39779', status: closing, memory: 0, processing: 0>
2023-05-02 05:36:53,679 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39779
2023-05-02 05:36:53,679 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:36:55,487 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:36:55,487 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:36:55,488 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:36:55,488 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:36:55,489 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-02 05:36:57,524 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:57,529 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34051 instead
  warnings.warn(
2023-05-02 05:36:57,533 - distributed.scheduler - INFO - State start
2023-05-02 05:36:57,557 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:36:57,558 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:36:57,558 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34051/status
2023-05-02 05:36:57,884 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39673'
2023-05-02 05:36:57,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33023'
2023-05-02 05:36:57,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36753'
2023-05-02 05:36:57,921 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40937'
2023-05-02 05:36:57,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40919'
2023-05-02 05:36:57,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44861'
2023-05-02 05:36:57,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34787'
2023-05-02 05:36:57,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34535'
2023-05-02 05:36:58,301 - distributed.scheduler - INFO - Receive client connection: Client-599f6137-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:36:58,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46626
2023-05-02 05:36:59,614 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:59,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:59,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:59,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:59,667 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:59,667 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:59,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:59,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:59,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:59,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:59,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:59,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:59,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:59,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:59,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:36:59,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:36:59,784 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:59,786 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:59,819 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:59,855 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:59,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:59,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:59,971 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:36:59,992 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:02,180 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37361
2023-05-02 05:37:02,181 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37361
2023-05-02 05:37:02,181 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38235
2023-05-02 05:37:02,181 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,181 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,181 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:02,181 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:02,181 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lfdwmecz
2023-05-02 05:37:02,182 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4bcc99b4-43c2-452f-8aaa-9b845017f3d7
2023-05-02 05:37:02,307 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34009
2023-05-02 05:37:02,308 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34009
2023-05-02 05:37:02,308 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39921
2023-05-02 05:37:02,308 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,308 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,308 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:02,308 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:02,308 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6bvist6i
2023-05-02 05:37:02,309 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4acdd8a9-1801-475e-91fd-4b4d4546c0a5
2023-05-02 05:37:02,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37647
2023-05-02 05:37:02,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37647
2023-05-02 05:37:02,322 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35993
2023-05-02 05:37:02,322 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,322 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,322 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:02,322 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:02,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x861szws
2023-05-02 05:37:02,324 - distributed.worker - INFO - Starting Worker plugin PreImport-5091e3fe-dc63-4853-9abf-6ba22f279ad6
2023-05-02 05:37:02,324 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc6287e7-0747-4102-97ec-ece93c01b1dd
2023-05-02 05:37:02,324 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76729598-705a-4d7b-b0f0-45cb31b47efc
2023-05-02 05:37:02,486 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43123
2023-05-02 05:37:02,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43123
2023-05-02 05:37:02,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41511
2023-05-02 05:37:02,486 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,486 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,486 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:02,486 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:02,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2600brk3
2023-05-02 05:37:02,487 - distributed.worker - INFO - Starting Worker plugin PreImport-1633e1b5-4f56-4a21-ae6a-276387549fc9
2023-05-02 05:37:02,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-608c0f9c-f636-48ca-a5a2-47b7c2f877ee
2023-05-02 05:37:02,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6641b813-48e3-4654-89e8-bc1f43085a16
2023-05-02 05:37:02,508 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36017
2023-05-02 05:37:02,508 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36017
2023-05-02 05:37:02,508 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33973
2023-05-02 05:37:02,508 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,508 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,508 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:02,509 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:02,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-85nb_zzy
2023-05-02 05:37:02,510 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04b536a3-bb6a-47f0-83da-b0fe11d916e9
2023-05-02 05:37:02,528 - distributed.worker - INFO - Starting Worker plugin PreImport-031242c5-6870-4d56-8db9-86a8e9a9d4d3
2023-05-02 05:37:02,528 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0c33782-d2f2-4714-aa03-c9ff27881428
2023-05-02 05:37:02,528 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,631 - distributed.worker - INFO - Starting Worker plugin PreImport-f25abe85-a646-47a3-8454-d12de004f409
2023-05-02 05:37:02,631 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e2c1f47-2b91-47d8-ac6e-f725c5c90a9e
2023-05-02 05:37:02,632 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,660 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33757
2023-05-02 05:37:02,660 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33757
2023-05-02 05:37:02,660 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42961
2023-05-02 05:37:02,660 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,660 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,660 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:02,661 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:02,661 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-isb2tajr
2023-05-02 05:37:02,661 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8dcfcdb1-df3d-4378-ada6-60698c591e0a
2023-05-02 05:37:02,662 - distributed.worker - INFO - Starting Worker plugin PreImport-954dc730-2857-4bf6-bd1b-4ebb350d8b33
2023-05-02 05:37:02,662 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6d5e30a-0970-4d6d-a0c1-09fd05ba8be3
2023-05-02 05:37:02,665 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37361', status: init, memory: 0, processing: 0>
2023-05-02 05:37:02,666 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37361
2023-05-02 05:37:02,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46652
2023-05-02 05:37:02,667 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,667 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:02,684 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,702 - distributed.worker - INFO - Starting Worker plugin PreImport-a1235414-80ae-4f29-9ac6-ee6ab5585374
2023-05-02 05:37:02,702 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5159f094-fbf4-4516-a6bb-796e94f1ce81
2023-05-02 05:37:02,702 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,711 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43123', status: init, memory: 0, processing: 0>
2023-05-02 05:37:02,712 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43123
2023-05-02 05:37:02,712 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46666
2023-05-02 05:37:02,712 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,713 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,714 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:02,730 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36017', status: init, memory: 0, processing: 0>
2023-05-02 05:37:02,731 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36017
2023-05-02 05:37:02,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46682
2023-05-02 05:37:02,732 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,732 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:02,763 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34009', status: init, memory: 0, processing: 0>
2023-05-02 05:37:02,763 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34009
2023-05-02 05:37:02,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46650
2023-05-02 05:37:02,764 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,764 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,767 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:02,778 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,803 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37647', status: init, memory: 0, processing: 0>
2023-05-02 05:37:02,804 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37647
2023-05-02 05:37:02,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46696
2023-05-02 05:37:02,804 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,804 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:02,833 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,928 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45379
2023-05-02 05:37:02,928 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45379
2023-05-02 05:37:02,929 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44681
2023-05-02 05:37:02,929 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,929 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,929 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:02,929 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:02,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-34t1tw4j
2023-05-02 05:37:02,930 - distributed.worker - INFO - Starting Worker plugin PreImport-332b7c65-09ae-4413-b94e-cba2d5bbde26
2023-05-02 05:37:02,930 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f3c95d7-d6a5-4d8c-909a-1875402fab16
2023-05-02 05:37:02,930 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8ee996a-4a15-4274-922a-0ca242ad7f1f
2023-05-02 05:37:02,966 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33757', status: init, memory: 0, processing: 0>
2023-05-02 05:37:02,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33757
2023-05-02 05:37:02,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46702
2023-05-02 05:37:02,968 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:02,968 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:02,971 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:03,214 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:03,249 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45379', status: init, memory: 0, processing: 0>
2023-05-02 05:37:03,250 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45379
2023-05-02 05:37:03,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46718
2023-05-02 05:37:03,251 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:03,251 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:03,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:03,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39981
2023-05-02 05:37:03,330 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39981
2023-05-02 05:37:03,330 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32889
2023-05-02 05:37:03,330 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:03,330 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:03,330 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:03,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:03,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vi7sgqhk
2023-05-02 05:37:03,331 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33edbd1d-b5c6-4b24-b109-5af60616aabe
2023-05-02 05:37:03,331 - distributed.worker - INFO - Starting Worker plugin PreImport-9df72bd1-0340-4471-a746-e4eda7a197aa
2023-05-02 05:37:03,332 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f68e4126-f566-48ef-8dd0-6f9dc15c4754
2023-05-02 05:37:04,114 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:04,140 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39981', status: init, memory: 0, processing: 0>
2023-05-02 05:37:04,141 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39981
2023-05-02 05:37:04,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46770
2023-05-02 05:37:04,141 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:04,141 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:04,143 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:04,234 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:04,234 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:04,234 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:04,234 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:04,234 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:04,234 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:04,235 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:04,242 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:04,246 - distributed.scheduler - INFO - Remove client Client-599f6137-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:04,246 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46626; closing.
2023-05-02 05:37:04,247 - distributed.scheduler - INFO - Remove client Client-599f6137-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:04,247 - distributed.scheduler - INFO - Close client connection: Client-599f6137-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:04,248 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40937'. Reason: nanny-close
2023-05-02 05:37:04,249 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:04,250 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39673'. Reason: nanny-close
2023-05-02 05:37:04,252 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:04,252 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33757. Reason: nanny-close
2023-05-02 05:37:04,252 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33023'. Reason: nanny-close
2023-05-02 05:37:04,253 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:04,253 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39981. Reason: nanny-close
2023-05-02 05:37:04,253 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36753'. Reason: nanny-close
2023-05-02 05:37:04,253 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:04,254 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40919'. Reason: nanny-close
2023-05-02 05:37:04,254 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34009. Reason: nanny-close
2023-05-02 05:37:04,254 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:04,254 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46702; closing.
2023-05-02 05:37:04,254 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:04,254 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37647. Reason: nanny-close
2023-05-02 05:37:04,254 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44861'. Reason: nanny-close
2023-05-02 05:37:04,254 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33757', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:04,255 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:04,255 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33757
2023-05-02 05:37:04,255 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:04,255 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34787'. Reason: nanny-close
2023-05-02 05:37:04,255 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43123. Reason: nanny-close
2023-05-02 05:37:04,255 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:04,256 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34535'. Reason: nanny-close
2023-05-02 05:37:04,256 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:04,256 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36017. Reason: nanny-close
2023-05-02 05:37:04,256 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:04,256 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:04,256 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33757
2023-05-02 05:37:04,256 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46770; closing.
2023-05-02 05:37:04,256 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33757
2023-05-02 05:37:04,256 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33757
2023-05-02 05:37:04,257 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39981', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:04,257 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39981
2023-05-02 05:37:04,257 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33757
2023-05-02 05:37:04,257 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33757
2023-05-02 05:37:04,257 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:04,257 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:04,258 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46696; closing.
2023-05-02 05:37:04,258 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:04,258 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:04,258 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46650; closing.
2023-05-02 05:37:04,258 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37361. Reason: nanny-close
2023-05-02 05:37:04,258 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46666; closing.
2023-05-02 05:37:04,258 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37647', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:04,259 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37647
2023-05-02 05:37:04,259 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34009', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:04,259 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34009
2023-05-02 05:37:04,259 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:04,259 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:04,259 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43123', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:04,259 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45379. Reason: nanny-close
2023-05-02 05:37:04,259 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43123
2023-05-02 05:37:04,260 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:04,260 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:04,260 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46682; closing.
2023-05-02 05:37:04,260 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:04,260 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36017', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:04,260 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36017
2023-05-02 05:37:04,261 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33757
2023-05-02 05:37:04,261 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46652; closing.
2023-05-02 05:37:04,261 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:04,262 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37361', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:04,262 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37361
2023-05-02 05:37:04,263 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:04,263 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46718; closing.
2023-05-02 05:37:04,264 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45379', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:04,264 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45379
2023-05-02 05:37:04,264 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:37:04,266 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:45379'.
2023-05-02 05:37:04,267 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:45379'. Shutting down.
2023-05-02 05:37:04,271 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:06,869 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:37:06,869 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:37:06,869 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:37:06,870 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:37:06,871 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-02 05:37:09,025 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:09,030 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34175 instead
  warnings.warn(
2023-05-02 05:37:09,033 - distributed.scheduler - INFO - State start
2023-05-02 05:37:09,074 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:09,075 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:37:09,075 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34175/status
2023-05-02 05:37:09,204 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46239'
2023-05-02 05:37:10,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:10,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:11,125 - distributed.scheduler - INFO - Receive client connection: Client-60639613-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:11,138 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43634
2023-05-02 05:37:11,172 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:12,245 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34657
2023-05-02 05:37:12,246 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34657
2023-05-02 05:37:12,246 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-05-02 05:37:12,246 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:12,246 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:12,246 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:12,246 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-02 05:37:12,246 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9yxdypoe
2023-05-02 05:37:12,246 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3fcc7cac-49c8-4069-b953-ef5d6511a8b9
2023-05-02 05:37:12,247 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b472880e-c609-478e-92eb-ba7b3fe1c7e1
2023-05-02 05:37:12,247 - distributed.worker - INFO - Starting Worker plugin PreImport-36ec648d-05c8-445d-a5d4-3d74a71a93d6
2023-05-02 05:37:12,248 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:12,286 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34657', status: init, memory: 0, processing: 0>
2023-05-02 05:37:12,288 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34657
2023-05-02 05:37:12,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43652
2023-05-02 05:37:12,289 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:12,289 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:12,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:12,369 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:12,374 - distributed.scheduler - INFO - Remove client Client-60639613-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:12,374 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43634; closing.
2023-05-02 05:37:12,375 - distributed.scheduler - INFO - Remove client Client-60639613-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:12,375 - distributed.scheduler - INFO - Close client connection: Client-60639613-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:12,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46239'. Reason: nanny-close
2023-05-02 05:37:12,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:12,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34657. Reason: nanny-close
2023-05-02 05:37:12,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43652; closing.
2023-05-02 05:37:12,381 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:12,381 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34657', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:12,381 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34657
2023-05-02 05:37:12,381 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:37:12,382 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:13,447 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:37:13,447 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:37:13,447 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:37:13,448 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:37:13,448 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-02 05:37:17,499 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:17,503 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33669 instead
  warnings.warn(
2023-05-02 05:37:17,507 - distributed.scheduler - INFO - State start
2023-05-02 05:37:17,626 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:17,627 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:37:17,627 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33669/status
2023-05-02 05:37:17,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45937'
2023-05-02 05:37:18,486 - distributed.scheduler - INFO - Receive client connection: Client-657f11ee-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:18,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34900
2023-05-02 05:37:19,386 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:19,386 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:19,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:21,126 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39071
2023-05-02 05:37:21,126 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39071
2023-05-02 05:37:21,126 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33287
2023-05-02 05:37:21,126 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:21,126 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:21,126 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:21,126 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-02 05:37:21,126 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iq3bm76c
2023-05-02 05:37:21,127 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d95a3220-b203-4106-a792-91053f2c64c7
2023-05-02 05:37:21,127 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f281482-5e9d-426b-ac64-8d2b9bbd190b
2023-05-02 05:37:21,127 - distributed.worker - INFO - Starting Worker plugin PreImport-ed5c7dbe-935e-4d73-b204-1b98574c6223
2023-05-02 05:37:21,129 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:21,155 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39071', status: init, memory: 0, processing: 0>
2023-05-02 05:37:21,157 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39071
2023-05-02 05:37:21,157 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34914
2023-05-02 05:37:21,157 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:21,157 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:21,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:21,263 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:21,267 - distributed.scheduler - INFO - Remove client Client-657f11ee-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:21,267 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34900; closing.
2023-05-02 05:37:21,267 - distributed.scheduler - INFO - Remove client Client-657f11ee-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:21,268 - distributed.scheduler - INFO - Close client connection: Client-657f11ee-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:21,269 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45937'. Reason: nanny-close
2023-05-02 05:37:21,269 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:21,271 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39071. Reason: nanny-close
2023-05-02 05:37:21,273 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34914; closing.
2023-05-02 05:37:21,273 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:21,273 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39071', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:21,273 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39071
2023-05-02 05:37:21,273 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:37:21,274 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:22,336 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:37:22,336 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:37:22,337 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:37:22,338 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:37:22,339 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-02 05:37:24,541 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:24,546 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37421 instead
  warnings.warn(
2023-05-02 05:37:24,550 - distributed.scheduler - INFO - State start
2023-05-02 05:37:24,629 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:24,630 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:37:24,630 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37421/status
2023-05-02 05:37:28,966 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:37:28,966 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:37:28,967 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:37:28,967 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:37:28,967 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-05-02 05:37:31,234 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:31,239 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35065 instead
  warnings.warn(
2023-05-02 05:37:31,243 - distributed.scheduler - INFO - State start
2023-05-02 05:37:31,517 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:31,518 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-02 05:37:31,518 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35065/status
2023-05-02 05:37:31,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44965'
2023-05-02 05:37:32,538 - distributed.scheduler - INFO - Receive client connection: Client-6d9b8ad2-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:32,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51720
2023-05-02 05:37:33,264 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:33,264 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:33,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:34,087 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37635
2023-05-02 05:37:34,087 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37635
2023-05-02 05:37:34,087 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34065
2023-05-02 05:37:34,087 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-02 05:37:34,087 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:34,088 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:34,088 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-02 05:37:34,088 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g4bq_tou
2023-05-02 05:37:34,088 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3414055a-0ade-4bd6-be7d-586cc3208adc
2023-05-02 05:37:34,088 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-50825eaf-f392-43c7-819c-ccfc707b62b5
2023-05-02 05:37:34,088 - distributed.worker - INFO - Starting Worker plugin PreImport-b18b474d-4b61-410a-be05-30549d89fda8
2023-05-02 05:37:34,088 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:34,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37635', status: init, memory: 0, processing: 0>
2023-05-02 05:37:34,112 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37635
2023-05-02 05:37:34,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51748
2023-05-02 05:37:34,113 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-02 05:37:34,113 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:34,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-02 05:37:34,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:34,197 - distributed.scheduler - INFO - Remove client Client-6d9b8ad2-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:34,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51720; closing.
2023-05-02 05:37:34,198 - distributed.scheduler - INFO - Remove client Client-6d9b8ad2-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:34,199 - distributed.scheduler - INFO - Close client connection: Client-6d9b8ad2-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:34,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44965'. Reason: nanny-close
2023-05-02 05:37:34,200 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:34,201 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37635. Reason: nanny-close
2023-05-02 05:37:34,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-02 05:37:34,203 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51748; closing.
2023-05-02 05:37:34,203 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37635', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:34,203 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37635
2023-05-02 05:37:34,204 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:37:34,204 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:35,166 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:37:35,166 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:37:35,167 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:37:35,168 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-02 05:37:35,168 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-05-02 05:37:37,422 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:37,426 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36161 instead
  warnings.warn(
2023-05-02 05:37:37,430 - distributed.scheduler - INFO - State start
2023-05-02 05:37:37,572 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:37,573 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:37:37,573 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36161/status
2023-05-02 05:37:37,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37391'
2023-05-02 05:37:37,906 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42667'
2023-05-02 05:37:37,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37165'
2023-05-02 05:37:37,924 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41341'
2023-05-02 05:37:37,939 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36361'
2023-05-02 05:37:37,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44655'
2023-05-02 05:37:37,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42063'
2023-05-02 05:37:37,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41343'
2023-05-02 05:37:39,348 - distributed.scheduler - INFO - Receive client connection: Client-715b7fcc-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:39,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58050
2023-05-02 05:37:39,653 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:39,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:39,699 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:39,699 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:39,705 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:39,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:39,709 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:39,709 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:39,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:39,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:39,714 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:39,714 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:39,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:39,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:39,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:39,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:39,802 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:39,898 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:40,011 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:40,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:40,127 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:40,156 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:40,177 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:40,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:43,028 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37519
2023-05-02 05:37:43,028 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37519
2023-05-02 05:37:43,028 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43883
2023-05-02 05:37:43,028 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,028 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,028 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:43,028 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:43,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dathxksu
2023-05-02 05:37:43,029 - distributed.worker - INFO - Starting Worker plugin PreImport-0f6dde16-01b4-4520-b92f-78e4bdd00471
2023-05-02 05:37:43,029 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c554f91-9fa0-474b-b2a7-cb1e28364b95
2023-05-02 05:37:43,029 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ee38028-5eb2-4a03-9150-497462235032
2023-05-02 05:37:43,175 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45687
2023-05-02 05:37:43,175 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45687
2023-05-02 05:37:43,175 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39267
2023-05-02 05:37:43,175 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,175 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,175 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:43,175 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:43,175 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ix8mnbkj
2023-05-02 05:37:43,176 - distributed.worker - INFO - Starting Worker plugin PreImport-d7635848-c0cb-4f1f-b4e3-540684633707
2023-05-02 05:37:43,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85dfbfd8-50df-40a3-99b1-08e2a607c43f
2023-05-02 05:37:43,176 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd521f99-cc33-4a3c-a7c5-10b5748b4bdc
2023-05-02 05:37:43,481 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35711
2023-05-02 05:37:43,481 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35711
2023-05-02 05:37:43,482 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36301
2023-05-02 05:37:43,482 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,482 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,482 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:43,482 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:43,482 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cu4w_5n7
2023-05-02 05:37:43,482 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79b95cab-1125-4aed-94ce-dbb6a792e3e6
2023-05-02 05:37:43,485 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,492 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,516 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43381
2023-05-02 05:37:43,517 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43381
2023-05-02 05:37:43,517 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36773
2023-05-02 05:37:43,517 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,517 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,517 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:43,517 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:43,517 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yn5mbu5d
2023-05-02 05:37:43,517 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e326455d-eb02-4957-9c5a-5eb41e94bd0c
2023-05-02 05:37:43,523 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37519', status: init, memory: 0, processing: 0>
2023-05-02 05:37:43,525 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37519
2023-05-02 05:37:43,525 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58078
2023-05-02 05:37:43,525 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,525 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,526 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45687', status: init, memory: 0, processing: 0>
2023-05-02 05:37:43,526 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45687
2023-05-02 05:37:43,526 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58084
2023-05-02 05:37:43,527 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,527 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:43,530 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:43,606 - distributed.worker - INFO - Starting Worker plugin PreImport-e0a7a024-8cdd-4202-8dcf-d33c66a2d506
2023-05-02 05:37:43,607 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56a0fb02-5fd9-487f-87bd-34cc10ce742a
2023-05-02 05:37:43,607 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,635 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35711', status: init, memory: 0, processing: 0>
2023-05-02 05:37:43,636 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35711
2023-05-02 05:37:43,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58096
2023-05-02 05:37:43,636 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,636 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:43,643 - distributed.worker - INFO - Starting Worker plugin PreImport-06e6f7bc-5b2d-4ae0-b2f6-dc8db1d84355
2023-05-02 05:37:43,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d84cee55-0e6c-4ef4-9bc8-68316f1dd7d0
2023-05-02 05:37:43,643 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,676 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43381', status: init, memory: 0, processing: 0>
2023-05-02 05:37:43,677 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43381
2023-05-02 05:37:43,677 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58110
2023-05-02 05:37:43,678 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,678 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:43,756 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42411
2023-05-02 05:37:43,756 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42411
2023-05-02 05:37:43,756 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41969
2023-05-02 05:37:43,756 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,757 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,757 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:43,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34017
2023-05-02 05:37:43,757 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:43,757 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34017
2023-05-02 05:37:43,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zc5_v5lv
2023-05-02 05:37:43,757 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37809
2023-05-02 05:37:43,757 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,757 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,757 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:43,757 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:43,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9px8tp3n
2023-05-02 05:37:43,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-690a3db4-7590-40d5-984c-5279f717c134
2023-05-02 05:37:43,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69a4d797-14ef-4cf6-8ae8-fe6aae5a50a2
2023-05-02 05:37:43,758 - distributed.worker - INFO - Starting Worker plugin PreImport-86de6ed6-54eb-4dd2-bca1-16fb3749cf82
2023-05-02 05:37:43,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-624e5470-2af7-4573-b8f2-e8ff19647da8
2023-05-02 05:37:43,758 - distributed.worker - INFO - Starting Worker plugin PreImport-67a20bd2-8c86-4732-8263-9e19fbe3ee57
2023-05-02 05:37:43,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4adf3ee6-4e60-43ca-a6d8-14e29f850724
2023-05-02 05:37:43,784 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37217
2023-05-02 05:37:43,784 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37217
2023-05-02 05:37:43,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33817
2023-05-02 05:37:43,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,784 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,784 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:43,784 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:43,785 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dqnngqhz
2023-05-02 05:37:43,785 - distributed.worker - INFO - Starting Worker plugin PreImport-64b328f5-2ec2-4d2f-befb-49967109d2d8
2023-05-02 05:37:43,786 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd3b67ff-0b1c-4eaa-b48f-8136b06c98f4
2023-05-02 05:37:43,786 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca269182-bc1b-470e-b595-5765248f12fc
2023-05-02 05:37:43,811 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34109
2023-05-02 05:37:43,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34109
2023-05-02 05:37:43,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40945
2023-05-02 05:37:43,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,811 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,811 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:43,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-02 05:37:43,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hr004e3u
2023-05-02 05:37:43,812 - distributed.worker - INFO - Starting Worker plugin PreImport-71a2c3e2-35d8-43f2-8f7e-f6fd8dccc29b
2023-05-02 05:37:43,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88166c8c-ba97-43c6-83ce-56cf92bff9ee
2023-05-02 05:37:43,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a7a45b1-2627-4d48-9b7d-83a262f26241
2023-05-02 05:37:43,903 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,904 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,926 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,931 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34017', status: init, memory: 0, processing: 0>
2023-05-02 05:37:43,932 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34017
2023-05-02 05:37:43,932 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58126
2023-05-02 05:37:43,932 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,933 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:43,935 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,937 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42411', status: init, memory: 0, processing: 0>
2023-05-02 05:37:43,938 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42411
2023-05-02 05:37:43,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58130
2023-05-02 05:37:43,938 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,939 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:43,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37217', status: init, memory: 0, processing: 0>
2023-05-02 05:37:43,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37217
2023-05-02 05:37:43,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58136
2023-05-02 05:37:43,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,958 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,958 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34109', status: init, memory: 0, processing: 0>
2023-05-02 05:37:43,959 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34109
2023-05-02 05:37:43,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58152
2023-05-02 05:37:43,959 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:43,959 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:43,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:43,961 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:43,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:43,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:43,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:43,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:43,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:43,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:43,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:43,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-02 05:37:43,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:43,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:43,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:43,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:43,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:43,987 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:43,987 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:43,987 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:43,991 - distributed.scheduler - INFO - Remove client Client-715b7fcc-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:43,991 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58050; closing.
2023-05-02 05:37:43,991 - distributed.scheduler - INFO - Remove client Client-715b7fcc-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:43,992 - distributed.scheduler - INFO - Close client connection: Client-715b7fcc-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:43,993 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37165'. Reason: nanny-close
2023-05-02 05:37:43,993 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:43,994 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44655'. Reason: nanny-close
2023-05-02 05:37:43,994 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:43,995 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42411. Reason: nanny-close
2023-05-02 05:37:43,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37391'. Reason: nanny-close
2023-05-02 05:37:43,995 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:43,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42667'. Reason: nanny-close
2023-05-02 05:37:43,995 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34017. Reason: nanny-close
2023-05-02 05:37:43,996 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:43,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41341'. Reason: nanny-close
2023-05-02 05:37:43,996 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43381. Reason: nanny-close
2023-05-02 05:37:43,996 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:43,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:43,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58130; closing.
2023-05-02 05:37:43,996 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45687. Reason: nanny-close
2023-05-02 05:37:43,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36361'. Reason: nanny-close
2023-05-02 05:37:43,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42411', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:43,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:43,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42411
2023-05-02 05:37:43,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42063'. Reason: nanny-close
2023-05-02 05:37:43,997 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:43,997 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34109. Reason: nanny-close
2023-05-02 05:37:43,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:43,998 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41343'. Reason: nanny-close
2023-05-02 05:37:43,998 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:43,998 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:43,998 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35711. Reason: nanny-close
2023-05-02 05:37:43,998 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:43,998 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37519. Reason: nanny-close
2023-05-02 05:37:43,998 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:43,998 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:43,998 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42411
2023-05-02 05:37:43,999 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42411
2023-05-02 05:37:43,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58126; closing.
2023-05-02 05:37:43,999 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:43,999 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:44,000 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34017', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:44,000 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37217. Reason: nanny-close
2023-05-02 05:37:44,000 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34017
2023-05-02 05:37:44,000 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42411
2023-05-02 05:37:44,000 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42411
2023-05-02 05:37:44,000 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:44,000 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:44,001 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:44,001 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:44,001 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58110; closing.
2023-05-02 05:37:44,001 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58084; closing.
2023-05-02 05:37:44,001 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58152; closing.
2023-05-02 05:37:44,002 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43381', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:44,002 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:44,002 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43381
2023-05-02 05:37:44,002 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:44,002 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:44,002 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45687', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:44,002 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45687
2023-05-02 05:37:44,003 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34109', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:44,003 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34109
2023-05-02 05:37:44,003 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58096; closing.
2023-05-02 05:37:44,004 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:44,004 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58078; closing.
2023-05-02 05:37:44,004 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35711', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:44,004 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35711
2023-05-02 05:37:44,005 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37519', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:44,005 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37519
2023-05-02 05:37:44,006 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58136; closing.
2023-05-02 05:37:44,006 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37217', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:44,006 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37217
2023-05-02 05:37:44,006 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:37:44,007 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58136>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-05-02 05:37:45,611 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:37:45,611 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:37:45,612 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:37:45,612 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:37:45,613 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-05-02 05:37:47,694 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:47,698 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46319 instead
  warnings.warn(
2023-05-02 05:37:47,702 - distributed.scheduler - INFO - State start
2023-05-02 05:37:47,930 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:47,931 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:37:47,932 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46319/status
2023-05-02 05:37:48,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37899'
2023-05-02 05:37:49,612 - distributed.scheduler - INFO - Receive client connection: Client-777d9c38-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:49,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52844
2023-05-02 05:37:49,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:49,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:49,717 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:50,756 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45253
2023-05-02 05:37:50,757 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45253
2023-05-02 05:37:50,757 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40497
2023-05-02 05:37:50,757 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:50,757 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:50,757 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:50,757 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-02 05:37:50,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ajstpjn3
2023-05-02 05:37:50,757 - distributed.worker - INFO - Starting Worker plugin PreImport-32595760-296c-44d0-a94e-2258dae0732c
2023-05-02 05:37:50,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25794e72-51b2-4ee3-b02e-929dcec18ad7
2023-05-02 05:37:50,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a889fc3e-3cba-4f00-a2b0-c50a49990148
2023-05-02 05:37:50,893 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:50,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45253', status: init, memory: 0, processing: 0>
2023-05-02 05:37:50,917 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45253
2023-05-02 05:37:50,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52868
2023-05-02 05:37:50,918 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:50,918 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:50,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:50,957 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:37:50,960 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:50,962 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:50,964 - distributed.scheduler - INFO - Remove client Client-777d9c38-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:50,964 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52844; closing.
2023-05-02 05:37:50,965 - distributed.scheduler - INFO - Remove client Client-777d9c38-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:50,965 - distributed.scheduler - INFO - Close client connection: Client-777d9c38-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:50,966 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37899'. Reason: nanny-close
2023-05-02 05:37:50,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:50,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45253. Reason: nanny-close
2023-05-02 05:37:50,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:50,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52868; closing.
2023-05-02 05:37:50,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45253', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:50,970 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45253
2023-05-02 05:37:50,970 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:37:50,970 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:52,033 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:37:52,033 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:37:52,033 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:37:52,034 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:37:52,035 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-05-02 05:37:54,155 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:54,159 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41799 instead
  warnings.warn(
2023-05-02 05:37:54,163 - distributed.scheduler - INFO - State start
2023-05-02 05:37:54,213 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-02 05:37:54,214 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-02 05:37:54,214 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41799/status
2023-05-02 05:37:54,346 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43427'
2023-05-02 05:37:54,366 - distributed.scheduler - INFO - Receive client connection: Client-7b5523c7-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:54,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57280
2023-05-02 05:37:55,896 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:37:55,896 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:37:55,950 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-02 05:37:56,902 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38917
2023-05-02 05:37:56,902 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38917
2023-05-02 05:37:56,902 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46137
2023-05-02 05:37:56,902 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-02 05:37:56,902 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:56,902 - distributed.worker - INFO -               Threads:                          1
2023-05-02 05:37:56,903 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-02 05:37:56,903 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wk65lk8a
2023-05-02 05:37:56,903 - distributed.worker - INFO - Starting Worker plugin PreImport-e44e9863-f06d-4aba-952a-aa7f085f9a90
2023-05-02 05:37:56,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-68c9795b-83a2-4e40-b2cd-9dffba72be7b
2023-05-02 05:37:56,903 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8752ab63-3bb8-4258-913b-db1860e19c82
2023-05-02 05:37:57,009 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:57,036 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38917', status: init, memory: 0, processing: 0>
2023-05-02 05:37:57,037 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38917
2023-05-02 05:37:57,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57298
2023-05-02 05:37:57,037 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-02 05:37:57,038 - distributed.worker - INFO - -------------------------------------------------
2023-05-02 05:37:57,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-02 05:37:57,139 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-05-02 05:37:57,144 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-02 05:37:57,147 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:57,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-02 05:37:57,150 - distributed.scheduler - INFO - Remove client Client-7b5523c7-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:57,151 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57280; closing.
2023-05-02 05:37:57,151 - distributed.scheduler - INFO - Remove client Client-7b5523c7-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:57,151 - distributed.scheduler - INFO - Close client connection: Client-7b5523c7-e8ab-11ed-a208-d8c49764f6bb
2023-05-02 05:37:57,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43427'. Reason: nanny-close
2023-05-02 05:37:57,152 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-02 05:37:57,153 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38917. Reason: nanny-close
2023-05-02 05:37:57,155 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-02 05:37:57,155 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57298; closing.
2023-05-02 05:37:57,156 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38917', status: closing, memory: 0, processing: 0>
2023-05-02 05:37:57,156 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38917
2023-05-02 05:37:57,156 - distributed.scheduler - INFO - Lost all workers
2023-05-02 05:37:57,156 - distributed.nanny - INFO - Worker closed
2023-05-02 05:37:58,118 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-02 05:37:58,118 - distributed.scheduler - INFO - Scheduler closing...
2023-05-02 05:37:58,119 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-02 05:37:58,120 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-02 05:37:58,120 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33513 instead
  warnings.warn(
2023-05-02 05:38:07,598 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:07,598 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:07,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:07,622 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:07,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:07,631 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:07,637 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:07,637 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:07,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:07,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:07,685 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:07,685 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:07,700 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:07,700 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:07,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:07,734 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38041 instead
  warnings.warn(
2023-05-02 05:38:16,235 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:16,235 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:16,532 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:16,532 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:16,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:16,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:16,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:16,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:16,576 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:16,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:16,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:16,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:16,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:16,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:16,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:16,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45889 instead
  warnings.warn(
2023-05-02 05:38:25,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:25,115 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:25,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:25,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:25,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:25,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:25,319 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:25,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:25,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:25,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:25,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:25,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:25,382 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:25,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:25,419 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:25,419 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37093 instead
  warnings.warn(
2023-05-02 05:38:34,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:34,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:34,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:34,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:34,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:34,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:34,387 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:34,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:34,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:34,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:34,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:34,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:34,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:34,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:34,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:34,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45087 instead
  warnings.warn(
2023-05-02 05:38:43,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:43,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:43,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:43,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:43,870 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:43,871 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:43,872 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:43,872 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:43,875 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:43,875 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:43,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:43,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:43,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:43,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:43,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:43,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41057 instead
  warnings.warn(
2023-05-02 05:38:52,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:52,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:52,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:52,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:52,906 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:52,906 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:52,936 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:52,936 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:52,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:52,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:52,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:52,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:52,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:52,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:38:52,970 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:38:52,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40653 instead
  warnings.warn(
2023-05-02 05:39:03,091 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:03,091 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:03,091 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:03,091 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:03,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:03,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:03,178 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:03,178 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:03,178 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:03,178 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:03,179 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:03,179 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:03,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:03,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:03,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:03,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35167 instead
  warnings.warn(
2023-05-02 05:39:12,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:12,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:12,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:12,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:12,579 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:12,579 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:12,586 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:12,586 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:12,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:12,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:12,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:12,631 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:12,638 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:12,638 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 05:39:12,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 05:39:12,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43275 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44009 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41683 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33793 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33563 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42377 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33915 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44859 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37289 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35947 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43765 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37237 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37237 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38467 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46743 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39855 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45841 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:12,173 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:12,182 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f396fff8670>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:12,199 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:12,208 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f143862ce50>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:12,217 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:12,220 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:12,226 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7875e41e80>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:12,228 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f389ad4d790>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 05:44:14,187 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-05-02 05:44:14,212 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-05-02 05:44:14,230 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-05-02 05:44:14,232 - distributed.nanny - ERROR - Worker process died unexpectedly
