============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-01 05:34:47,993 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:34:47,998 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45865 instead
  warnings.warn(
2023-05-01 05:34:48,001 - distributed.scheduler - INFO - State start
2023-05-01 05:34:48,022 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:34:48,023 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-01 05:34:48,023 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45865/status
2023-05-01 05:34:48,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38717'
2023-05-01 05:34:48,258 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40025'
2023-05-01 05:34:48,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41241'
2023-05-01 05:34:48,270 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41263'
2023-05-01 05:34:49,867 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:49,867 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:49,871 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:49,871 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:49,872 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:49,872 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:49,875 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:49,876 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:49,877 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:49,879 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:49,879 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:49,884 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-01 05:34:49,893 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36827
2023-05-01 05:34:49,893 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36827
2023-05-01 05:34:49,893 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45779
2023-05-01 05:34:49,893 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-01 05:34:49,894 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:49,894 - distributed.worker - INFO -               Threads:                          4
2023-05-01 05:34:49,894 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-01 05:34:49,894 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y719f9a3
2023-05-01 05:34:49,894 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f17de88f-c04b-43b9-99f6-8ce9e661e2bf
2023-05-01 05:34:49,894 - distributed.worker - INFO - Starting Worker plugin PreImport-72aaa78b-cc65-484e-8424-4ff6106d1768
2023-05-01 05:34:49,894 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b56cfc3a-4976-43e9-9d72-1bc1d54666f9
2023-05-01 05:34:49,894 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:49,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36827', status: init, memory: 0, processing: 0>
2023-05-01 05:34:49,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36827
2023-05-01 05:34:49,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55064
2023-05-01 05:34:49,926 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-01 05:34:49,926 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:49,928 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-01 05:34:50,247 - distributed.scheduler - INFO - Receive client connection: Client-e20e054a-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:34:50,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55076
2023-05-01 05:34:50,846 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36433
2023-05-01 05:34:50,846 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36433
2023-05-01 05:34:50,846 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45943
2023-05-01 05:34:50,846 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-01 05:34:50,846 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,846 - distributed.worker - INFO -               Threads:                          4
2023-05-01 05:34:50,846 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-01 05:34:50,846 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ssxn4rka
2023-05-01 05:34:50,847 - distributed.worker - INFO - Starting Worker plugin PreImport-cd58fbec-b69f-4706-8cc5-1409c81dc167
2023-05-01 05:34:50,847 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-268844b1-9b49-49a3-8c2b-988f0667fc7e
2023-05-01 05:34:50,847 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd69fe27-f41c-46e3-8962-bcd48238b52d
2023-05-01 05:34:50,847 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,863 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34317
2023-05-01 05:34:50,863 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34317
2023-05-01 05:34:50,863 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38231
2023-05-01 05:34:50,863 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-01 05:34:50,863 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,863 - distributed.worker - INFO -               Threads:                          4
2023-05-01 05:34:50,863 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-01 05:34:50,863 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p4xtcdb9
2023-05-01 05:34:50,863 - distributed.worker - INFO - Starting Worker plugin PreImport-0790acea-8df6-4ad5-861e-8540751c473a
2023-05-01 05:34:50,864 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c1caf92-c3f0-421e-b161-898551b1ed1b
2023-05-01 05:34:50,864 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff574433-77fa-4ff1-b658-6419c88f33ea
2023-05-01 05:34:50,864 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,866 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36139
2023-05-01 05:34:50,866 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36139
2023-05-01 05:34:50,866 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44351
2023-05-01 05:34:50,866 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-01 05:34:50,866 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,866 - distributed.worker - INFO -               Threads:                          4
2023-05-01 05:34:50,866 - distributed.worker - INFO -                Memory:                 251.95 GiB
2023-05-01 05:34:50,866 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i4712jdu
2023-05-01 05:34:50,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18917594-117c-44e2-aeb9-f68e6d556b59
2023-05-01 05:34:50,867 - distributed.worker - INFO - Starting Worker plugin PreImport-1ff1663c-3a7d-4832-809e-8f40b4462aa3
2023-05-01 05:34:50,867 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1463846f-35e7-4310-99f0-720a9a6a53c0
2023-05-01 05:34:50,867 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,870 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36433', status: init, memory: 0, processing: 0>
2023-05-01 05:34:50,871 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36433
2023-05-01 05:34:50,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55102
2023-05-01 05:34:50,872 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-01 05:34:50,872 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-01 05:34:50,884 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34317', status: init, memory: 0, processing: 0>
2023-05-01 05:34:50,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34317
2023-05-01 05:34:50,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55116
2023-05-01 05:34:50,885 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-01 05:34:50,885 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-01 05:34:50,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36139', status: init, memory: 0, processing: 0>
2023-05-01 05:34:50,897 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36139
2023-05-01 05:34:50,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55122
2023-05-01 05:34:50,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-01 05:34:50,898 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:50,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-01 05:34:50,971 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-01 05:34:50,972 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-01 05:34:50,972 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-01 05:34:50,972 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-01 05:34:50,977 - distributed.scheduler - INFO - Remove client Client-e20e054a-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:34:50,977 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55076; closing.
2023-05-01 05:34:50,978 - distributed.scheduler - INFO - Remove client Client-e20e054a-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:34:50,978 - distributed.scheduler - INFO - Close client connection: Client-e20e054a-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:34:50,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38717'. Reason: nanny-close
2023-05-01 05:34:50,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:50,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41241'. Reason: nanny-close
2023-05-01 05:34:50,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:50,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36139. Reason: nanny-close
2023-05-01 05:34:50,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41263'. Reason: nanny-close
2023-05-01 05:34:50,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:50,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34317. Reason: nanny-close
2023-05-01 05:34:50,982 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40025'. Reason: nanny-close
2023-05-01 05:34:50,982 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:50,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36433. Reason: nanny-close
2023-05-01 05:34:50,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-01 05:34:50,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55122; closing.
2023-05-01 05:34:50,983 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36827. Reason: nanny-close
2023-05-01 05:34:50,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-01 05:34:50,983 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36139', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:50,983 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36139
2023-05-01 05:34:50,984 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:50,984 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55116; closing.
2023-05-01 05:34:50,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-01 05:34:50,984 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:50,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-01 05:34:50,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34317', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:50,985 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34317
2023-05-01 05:34:50,986 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:50,986 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:50,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55102; closing.
2023-05-01 05:34:50,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55064; closing.
2023-05-01 05:34:50,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36433', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:50,987 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36433
2023-05-01 05:34:50,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36827', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:50,987 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36827
2023-05-01 05:34:50,987 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:34:51,995 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:34:51,996 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:34:51,996 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:34:51,997 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-01 05:34:51,998 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-01 05:34:53,837 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:34:53,840 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38905 instead
  warnings.warn(
2023-05-01 05:34:53,844 - distributed.scheduler - INFO - State start
2023-05-01 05:34:53,862 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:34:53,863 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:34:53,864 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38905/status
2023-05-01 05:34:54,194 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36105'
2023-05-01 05:34:54,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36573'
2023-05-01 05:34:54,217 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32885'
2023-05-01 05:34:54,225 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35361'
2023-05-01 05:34:54,233 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36133'
2023-05-01 05:34:54,242 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44317'
2023-05-01 05:34:54,254 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37241'
2023-05-01 05:34:54,267 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38267'
2023-05-01 05:34:55,622 - distributed.scheduler - INFO - Receive client connection: Client-e5985f81-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:34:55,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52960
2023-05-01 05:34:55,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:55,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:55,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:55,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:55,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:55,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:55,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:55,747 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:55,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:55,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:55,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:55,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:55,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:55,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:55,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:55,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:55,924 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:55,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:34:55,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:34:55,944 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:55,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:55,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:55,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:55,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:34:57,012 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45151
2023-05-01 05:34:57,012 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45151
2023-05-01 05:34:57,013 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42839
2023-05-01 05:34:57,013 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,013 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,013 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:34:57,013 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:34:57,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-avh2092w
2023-05-01 05:34:57,013 - distributed.worker - INFO - Starting Worker plugin PreImport-fd1020c4-4e8c-442e-83cc-d0f2b44fe18d
2023-05-01 05:34:57,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46ecd7e3-6327-4827-9f49-0db9bc40aa94
2023-05-01 05:34:57,014 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c304f96e-569e-40f9-8a38-4c6074aaa8e3
2023-05-01 05:34:57,049 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39067
2023-05-01 05:34:57,049 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39067
2023-05-01 05:34:57,049 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46113
2023-05-01 05:34:57,049 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,049 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,049 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:34:57,049 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:34:57,049 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lt2dcdlz
2023-05-01 05:34:57,050 - distributed.worker - INFO - Starting Worker plugin PreImport-9348ae50-2431-4aeb-a124-a1380879273e
2023-05-01 05:34:57,050 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52970a06-1463-4ac2-887e-3885c3e007a2
2023-05-01 05:34:57,050 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32f780d2-69df-4bbe-a7e1-6321fbd1b521
2023-05-01 05:34:57,087 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36013
2023-05-01 05:34:57,087 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36013
2023-05-01 05:34:57,087 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33265
2023-05-01 05:34:57,087 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,087 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,087 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:34:57,087 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:34:57,087 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-svtr8o28
2023-05-01 05:34:57,087 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c9f9db6c-1dc4-48a7-ada5-23519cb9014f
2023-05-01 05:34:57,248 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,284 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,286 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45151', status: init, memory: 0, processing: 0>
2023-05-01 05:34:57,287 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45151
2023-05-01 05:34:57,287 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52982
2023-05-01 05:34:57,288 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,288 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:34:57,316 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39067', status: init, memory: 0, processing: 0>
2023-05-01 05:34:57,317 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39067
2023-05-01 05:34:57,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52994
2023-05-01 05:34:57,318 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,318 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:34:57,329 - distributed.worker - INFO - Starting Worker plugin PreImport-40da19a2-b61b-4b3b-935d-9e2047d1660d
2023-05-01 05:34:57,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9efddc9-84f0-47af-8bc0-cd80dc291952
2023-05-01 05:34:57,329 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,355 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36013', status: init, memory: 0, processing: 0>
2023-05-01 05:34:57,356 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36013
2023-05-01 05:34:57,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53004
2023-05-01 05:34:57,356 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,356 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:34:57,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42897
2023-05-01 05:34:57,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42897
2023-05-01 05:34:57,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36717
2023-05-01 05:34:57,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,634 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,634 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:34:57,634 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:34:57,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xzo2oeea
2023-05-01 05:34:57,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eebdb742-62b9-418e-9cfa-9e7ef4846200
2023-05-01 05:34:57,750 - distributed.worker - INFO - Starting Worker plugin PreImport-682d3cde-fd1e-4a6b-bc1f-ec99d38dee5a
2023-05-01 05:34:57,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c757e20-8bf1-437d-8453-9ddabc737ee1
2023-05-01 05:34:57,751 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,795 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42897', status: init, memory: 0, processing: 0>
2023-05-01 05:34:57,796 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42897
2023-05-01 05:34:57,796 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53010
2023-05-01 05:34:57,797 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,797 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:34:57,863 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35453
2023-05-01 05:34:57,863 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35453
2023-05-01 05:34:57,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35441
2023-05-01 05:34:57,864 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,864 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,864 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:34:57,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:34:57,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ewacap15
2023-05-01 05:34:57,864 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a33669c-6870-4288-a209-b8db1cfcc601
2023-05-01 05:34:57,864 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44731
2023-05-01 05:34:57,864 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44731
2023-05-01 05:34:57,865 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33243
2023-05-01 05:34:57,865 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,865 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,865 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:34:57,865 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:34:57,865 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dhkt53m7
2023-05-01 05:34:57,865 - distributed.worker - INFO - Starting Worker plugin PreImport-4eaa6468-7032-4a53-8ab5-ff9568f293c2
2023-05-01 05:34:57,865 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9e52ef4-156e-45c9-be7b-bd702c0bef59
2023-05-01 05:34:57,866 - distributed.worker - INFO - Starting Worker plugin RMMSetup-884ff240-3dbb-4607-acd0-7ed53f4f6b42
2023-05-01 05:34:57,929 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38675
2023-05-01 05:34:57,929 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38675
2023-05-01 05:34:57,930 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38059
2023-05-01 05:34:57,930 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,930 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,930 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:34:57,930 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:34:57,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fm5uqm2d
2023-05-01 05:34:57,930 - distributed.worker - INFO - Starting Worker plugin PreImport-a44eec98-e5bf-4334-bc02-dadb83a21030
2023-05-01 05:34:57,931 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6320bb61-c21c-4701-9e90-fda97a514f00
2023-05-01 05:34:57,931 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36323
2023-05-01 05:34:57,931 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36323
2023-05-01 05:34:57,931 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32849
2023-05-01 05:34:57,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4d3edbd-ce83-4b71-a134-14aa408dedf6
2023-05-01 05:34:57,931 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:34:57,931 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,931 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:34:57,931 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:34:57,931 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7defcywp
2023-05-01 05:34:57,932 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee03b71a-b5ec-43a5-a57b-811aa317beb5
2023-05-01 05:34:57,936 - distributed.worker - INFO - Starting Worker plugin PreImport-a3eaa819-a82e-4e21-bb69-b1ec1a137c0b
2023-05-01 05:34:57,936 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f5646793-0c7a-4bff-91d6-46d0cd6bb14b
2023-05-01 05:34:57,979 - distributed.worker - INFO - Starting Worker plugin PreImport-9f6ac863-f259-4c03-9109-67bf6238e894
2023-05-01 05:34:57,979 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7de64c0-3b45-4774-9b97-37a564dc31bf
2023-05-01 05:34:57,979 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:57,983 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:58,012 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44731', status: init, memory: 0, processing: 0>
2023-05-01 05:34:58,013 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44731
2023-05-01 05:34:58,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53018
2023-05-01 05:34:58,013 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:34:58,014 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:58,014 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35453', status: init, memory: 0, processing: 0>
2023-05-01 05:34:58,014 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35453
2023-05-01 05:34:58,014 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53012
2023-05-01 05:34:58,015 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:34:58,015 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:58,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:34:58,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:34:58,051 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:58,054 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:58,077 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38675', status: init, memory: 0, processing: 0>
2023-05-01 05:34:58,078 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38675
2023-05-01 05:34:58,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53030
2023-05-01 05:34:58,078 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:34:58,078 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:58,080 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:34:58,082 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36323', status: init, memory: 0, processing: 0>
2023-05-01 05:34:58,083 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36323
2023-05-01 05:34:58,083 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53032
2023-05-01 05:34:58,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:34:58,083 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:34:58,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:34:58,103 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:34:58,103 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:34:58,103 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:34:58,104 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:34:58,104 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:34:58,104 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:34:58,104 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:34:58,104 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:34:58,108 - distributed.scheduler - INFO - Remove client Client-e5985f81-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:34:58,108 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52960; closing.
2023-05-01 05:34:58,109 - distributed.scheduler - INFO - Remove client Client-e5985f81-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:34:58,109 - distributed.scheduler - INFO - Close client connection: Client-e5985f81-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:34:58,110 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35361'. Reason: nanny-close
2023-05-01 05:34:58,111 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:58,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36105'. Reason: nanny-close
2023-05-01 05:34:58,112 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:58,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36573'. Reason: nanny-close
2023-05-01 05:34:58,113 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38675. Reason: nanny-close
2023-05-01 05:34:58,113 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:58,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32885'. Reason: nanny-close
2023-05-01 05:34:58,113 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36323. Reason: nanny-close
2023-05-01 05:34:58,113 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:58,114 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36013. Reason: nanny-close
2023-05-01 05:34:58,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36133'. Reason: nanny-close
2023-05-01 05:34:58,114 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:58,114 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:34:58,114 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53030; closing.
2023-05-01 05:34:58,114 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39067. Reason: nanny-close
2023-05-01 05:34:58,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44317'. Reason: nanny-close
2023-05-01 05:34:58,114 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38675', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:58,114 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:58,115 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38675
2023-05-01 05:34:58,115 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:34:58,115 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45151. Reason: nanny-close
2023-05-01 05:34:58,115 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37241'. Reason: nanny-close
2023-05-01 05:34:58,115 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:58,115 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:34:58,115 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:58,115 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38267'. Reason: nanny-close
2023-05-01 05:34:58,115 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35453. Reason: nanny-close
2023-05-01 05:34:58,116 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:34:58,116 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:34:58,116 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44731. Reason: nanny-close
2023-05-01 05:34:58,116 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:58,116 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38675
2023-05-01 05:34:58,116 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53032; closing.
2023-05-01 05:34:58,116 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:58,116 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53004; closing.
2023-05-01 05:34:58,117 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38675
2023-05-01 05:34:58,117 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36323', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:58,117 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38675
2023-05-01 05:34:58,117 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36323
2023-05-01 05:34:58,117 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:58,117 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:34:58,117 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38675
2023-05-01 05:34:58,117 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36013', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:58,117 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36013
2023-05-01 05:34:58,117 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:34:58,118 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42897. Reason: nanny-close
2023-05-01 05:34:58,118 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:34:58,118 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52994; closing.
2023-05-01 05:34:58,118 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52982; closing.
2023-05-01 05:34:58,119 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:58,119 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39067', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:58,119 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:58,119 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39067
2023-05-01 05:34:58,119 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45151', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:58,119 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45151
2023-05-01 05:34:58,119 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:58,119 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:34:58,120 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53012; closing.
2023-05-01 05:34:58,120 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53018; closing.
2023-05-01 05:34:58,120 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35453', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:58,120 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35453
2023-05-01 05:34:58,120 - distributed.nanny - INFO - Worker closed
2023-05-01 05:34:58,121 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44731', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:58,121 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44731
2023-05-01 05:34:58,121 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53010; closing.
2023-05-01 05:34:58,122 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42897', status: closing, memory: 0, processing: 0>
2023-05-01 05:34:58,122 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42897
2023-05-01 05:34:58,122 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:34:59,527 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:34:59,528 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:34:59,528 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:34:59,529 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:34:59,530 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-01 05:35:01,430 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:01,434 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41013 instead
  warnings.warn(
2023-05-01 05:35:01,437 - distributed.scheduler - INFO - State start
2023-05-01 05:35:01,459 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:01,460 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:35:01,460 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41013/status
2023-05-01 05:35:01,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43369'
2023-05-01 05:35:01,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34831'
2023-05-01 05:35:01,649 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40303'
2023-05-01 05:35:01,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35405'
2023-05-01 05:35:01,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40337'
2023-05-01 05:35:01,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32895'
2023-05-01 05:35:01,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42079'
2023-05-01 05:35:01,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42729'
2023-05-01 05:35:01,802 - distributed.scheduler - INFO - Receive client connection: Client-ea1a21fc-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:01,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53174
2023-05-01 05:35:03,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:03,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:03,216 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:03,217 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:03,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:03,248 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:03,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:03,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:03,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:03,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:03,292 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:03,292 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:03,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:03,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:03,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:03,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:03,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:03,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:03,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:03,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:03,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:03,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:03,379 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:03,396 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:04,975 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44645
2023-05-01 05:35:04,975 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44645
2023-05-01 05:35:04,975 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42951
2023-05-01 05:35:04,975 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:04,975 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:04,975 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:04,975 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:04,975 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k4rfktnb
2023-05-01 05:35:04,976 - distributed.worker - INFO - Starting Worker plugin PreImport-48c51c4a-4c55-4c4e-aec9-423a312e8eb5
2023-05-01 05:35:04,976 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4db2143-3919-438c-8d81-b5734acf383b
2023-05-01 05:35:04,976 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e5abd2fd-2329-430c-aeb6-c41a3074611b
2023-05-01 05:35:04,986 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44645', status: init, memory: 0, processing: 0>
2023-05-01 05:35:05,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44645
2023-05-01 05:35:05,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51086
2023-05-01 05:35:05,027 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,027 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,029 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36833
2023-05-01 05:35:05,029 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36833
2023-05-01 05:35:05,029 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:05,029 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45641
2023-05-01 05:35:05,029 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,029 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,029 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:05,029 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:05,029 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g50qiaur
2023-05-01 05:35:05,030 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ac36399-fe14-4b1f-8b5a-b3eb56ae9589
2023-05-01 05:35:05,038 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34865
2023-05-01 05:35:05,038 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34865
2023-05-01 05:35:05,038 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43611
2023-05-01 05:35:05,038 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,038 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,038 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:05,038 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:05,038 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7m769y3w
2023-05-01 05:35:05,039 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51326bfb-bd5b-40ae-9bba-c02d74614953
2023-05-01 05:35:05,058 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32919
2023-05-01 05:35:05,058 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32919
2023-05-01 05:35:05,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37565
2023-05-01 05:35:05,059 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,059 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,059 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:05,059 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:05,059 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pgeffj45
2023-05-01 05:35:05,059 - distributed.worker - INFO - Starting Worker plugin PreImport-f7d80a47-4a7d-4e7c-8b88-c0d71e7813b1
2023-05-01 05:35:05,059 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-252cbef1-8c92-454e-bba6-826923dd7b62
2023-05-01 05:35:05,059 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d8f0dc9-f299-4351-b025-26829ac83cb4
2023-05-01 05:35:05,059 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,063 - distributed.worker - INFO - Starting Worker plugin PreImport-3c1abe4a-69a9-4e67-87fa-9a5475e7c9c4
2023-05-01 05:35:05,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ef05cc6-3251-449a-8b46-cff5c9a1fffb
2023-05-01 05:35:05,063 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,066 - distributed.worker - INFO - Starting Worker plugin PreImport-6a820bc4-66ee-4708-96cb-24aaf1ba5dcd
2023-05-01 05:35:05,066 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efd05734-e370-4044-a975-fb9eb48c8fe3
2023-05-01 05:35:05,067 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,088 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36833', status: init, memory: 0, processing: 0>
2023-05-01 05:35:05,089 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36833
2023-05-01 05:35:05,089 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51100
2023-05-01 05:35:05,089 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,090 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,091 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34865', status: init, memory: 0, processing: 0>
2023-05-01 05:35:05,091 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34865
2023-05-01 05:35:05,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51108
2023-05-01 05:35:05,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:05,092 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,092 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,093 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32919', status: init, memory: 0, processing: 0>
2023-05-01 05:35:05,094 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32919
2023-05-01 05:35:05,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:05,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51112
2023-05-01 05:35:05,094 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,094 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,096 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:05,101 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39137
2023-05-01 05:35:05,101 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39137
2023-05-01 05:35:05,101 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44137
2023-05-01 05:35:05,101 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,101 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,101 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:05,101 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:05,101 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lca2r9m8
2023-05-01 05:35:05,102 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04c744e2-4271-4b88-b72b-8190c3149fbb
2023-05-01 05:35:05,102 - distributed.worker - INFO - Starting Worker plugin PreImport-077803fe-953a-40f2-8c0e-8e4cd13cfaef
2023-05-01 05:35:05,102 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87629e66-8188-41df-994e-b02efeb20bc5
2023-05-01 05:35:05,107 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,129 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41281
2023-05-01 05:35:05,130 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41281
2023-05-01 05:35:05,130 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46673
2023-05-01 05:35:05,130 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,130 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,130 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:05,130 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:05,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oqdvblgz
2023-05-01 05:35:05,130 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92b22ed4-626a-496c-9003-72dafdff0c0d
2023-05-01 05:35:05,135 - distributed.worker - INFO - Starting Worker plugin PreImport-54019fe0-12e6-446c-8ce9-23252920ca23
2023-05-01 05:35:05,135 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9faa18e1-c7c3-4c88-b9f5-dd7c46397957
2023-05-01 05:35:05,136 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,138 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39137', status: init, memory: 0, processing: 0>
2023-05-01 05:35:05,139 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39137
2023-05-01 05:35:05,139 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51118
2023-05-01 05:35:05,140 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,140 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:05,162 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33887
2023-05-01 05:35:05,162 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33887
2023-05-01 05:35:05,162 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45127
2023-05-01 05:35:05,162 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,162 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,163 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:05,163 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:05,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1sqoeklc
2023-05-01 05:35:05,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-379b4e0f-0f27-4744-bdc5-2c985f97d7fc
2023-05-01 05:35:05,165 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41281', status: init, memory: 0, processing: 0>
2023-05-01 05:35:05,165 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41281
2023-05-01 05:35:05,165 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51132
2023-05-01 05:35:05,166 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,166 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,168 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:05,168 - distributed.worker - INFO - Starting Worker plugin PreImport-557418e6-9ce9-41ff-a88a-ecf8bb6e2bfa
2023-05-01 05:35:05,168 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27dcc4fa-03bd-4eed-80a2-e56eb7a8009e
2023-05-01 05:35:05,169 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,190 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33887', status: init, memory: 0, processing: 0>
2023-05-01 05:35:05,191 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33887
2023-05-01 05:35:05,191 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51136
2023-05-01 05:35:05,192 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,192 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:05,277 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45009
2023-05-01 05:35:05,277 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45009
2023-05-01 05:35:05,277 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40221
2023-05-01 05:35:05,277 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,277 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,277 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:05,278 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:05,278 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wx2qvrk4
2023-05-01 05:35:05,278 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a572ac76-7872-4d7c-bdb7-df55dec89892
2023-05-01 05:35:05,287 - distributed.worker - INFO - Starting Worker plugin PreImport-5c92775a-4bfb-4438-8f17-48cf6dd89736
2023-05-01 05:35:05,287 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-258096df-c1fa-4813-964f-3726c21a25a6
2023-05-01 05:35:05,288 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,310 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45009', status: init, memory: 0, processing: 0>
2023-05-01 05:35:05,311 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45009
2023-05-01 05:35:05,311 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51146
2023-05-01 05:35:05,311 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:05,311 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:05,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:05,408 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:05,408 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:05,408 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:05,408 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:05,409 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:05,409 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:05,409 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:05,409 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:05,413 - distributed.scheduler - INFO - Remove client Client-ea1a21fc-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:05,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53174; closing.
2023-05-01 05:35:05,414 - distributed.scheduler - INFO - Remove client Client-ea1a21fc-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:05,414 - distributed.scheduler - INFO - Close client connection: Client-ea1a21fc-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:05,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34831'. Reason: nanny-close
2023-05-01 05:35:05,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:05,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40337'. Reason: nanny-close
2023-05-01 05:35:05,417 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:05,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43369'. Reason: nanny-close
2023-05-01 05:35:05,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44645. Reason: nanny-close
2023-05-01 05:35:05,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:05,418 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40303'. Reason: nanny-close
2023-05-01 05:35:05,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45009. Reason: nanny-close
2023-05-01 05:35:05,419 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:05,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36833. Reason: nanny-close
2023-05-01 05:35:05,419 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35405'. Reason: nanny-close
2023-05-01 05:35:05,419 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:05,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32919. Reason: nanny-close
2023-05-01 05:35:05,419 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32895'. Reason: nanny-close
2023-05-01 05:35:05,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51086; closing.
2023-05-01 05:35:05,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:05,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:05,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44645', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:05,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:05,420 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44645
2023-05-01 05:35:05,420 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42079'. Reason: nanny-close
2023-05-01 05:35:05,420 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39137. Reason: nanny-close
2023-05-01 05:35:05,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:05,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:05,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42729'. Reason: nanny-close
2023-05-01 05:35:05,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34865. Reason: nanny-close
2023-05-01 05:35:05,421 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:05,421 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:05,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:05,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51146; closing.
2023-05-01 05:35:05,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41281. Reason: nanny-close
2023-05-01 05:35:05,421 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:05,422 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:05,422 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44645
2023-05-01 05:35:05,422 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:05,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45009', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:05,422 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45009
2023-05-01 05:35:05,422 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44645
2023-05-01 05:35:05,422 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33887. Reason: nanny-close
2023-05-01 05:35:05,422 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51100; closing.
2023-05-01 05:35:05,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:05,423 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44645
2023-05-01 05:35:05,423 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44645
2023-05-01 05:35:05,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36833', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:05,423 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36833
2023-05-01 05:35:05,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:05,424 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51112; closing.
2023-05-01 05:35:05,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:05,424 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:05,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32919', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:05,424 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32919
2023-05-01 05:35:05,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:05,425 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51118; closing.
2023-05-01 05:35:05,425 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:05,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39137', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:05,425 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39137
2023-05-01 05:35:05,425 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:05,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51108; closing.
2023-05-01 05:35:05,426 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:05,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51132; closing.
2023-05-01 05:35:05,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51136; closing.
2023-05-01 05:35:05,426 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34865', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:05,426 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-01 05:35:05,427 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41281', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:05,427 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41281
2023-05-01 05:35:05,427 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33887', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:05,428 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33887
2023-05-01 05:35:05,428 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:35:06,883 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:35:06,883 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:35:06,884 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:35:06,885 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:35:06,885 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-01 05:35:08,929 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:08,934 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37281 instead
  warnings.warn(
2023-05-01 05:35:08,937 - distributed.scheduler - INFO - State start
2023-05-01 05:35:08,959 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:08,960 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:35:08,960 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37281/status
2023-05-01 05:35:09,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37363'
2023-05-01 05:35:09,326 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45531'
2023-05-01 05:35:09,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40783'
2023-05-01 05:35:09,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44531'
2023-05-01 05:35:09,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36633'
2023-05-01 05:35:09,357 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43529'
2023-05-01 05:35:09,365 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33401'
2023-05-01 05:35:09,374 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42525'
2023-05-01 05:35:10,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:10,981 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:11,006 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:11,026 - distributed.scheduler - INFO - Receive client connection: Client-ee8efe2c-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:11,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:11,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:11,029 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:11,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:11,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:11,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:11,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:11,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:11,037 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:11,038 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:11,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51310
2023-05-01 05:35:11,054 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:11,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:11,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:11,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:11,059 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:11,060 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:11,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:11,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:11,065 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:11,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:11,097 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:12,683 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44219
2023-05-01 05:35:12,683 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44219
2023-05-01 05:35:12,683 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41689
2023-05-01 05:35:12,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:12,684 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:12,684 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:12,684 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:12,684 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ikdj3trh
2023-05-01 05:35:12,684 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8ed59fb-5386-4e93-805a-e285e6b9856e
2023-05-01 05:35:12,994 - distributed.worker - INFO - Starting Worker plugin PreImport-d9ebc02e-dc1c-4988-8469-a8d794cb0bc0
2023-05-01 05:35:12,995 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6f74676-67b7-4266-bd8d-96bf9809ccf3
2023-05-01 05:35:12,995 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,005 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39525
2023-05-01 05:35:13,005 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39525
2023-05-01 05:35:13,005 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44935
2023-05-01 05:35:13,005 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,005 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,005 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:13,005 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:13,005 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0zeu9hzd
2023-05-01 05:35:13,005 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5725def7-80de-4caa-9720-fbff7b48f9cd
2023-05-01 05:35:13,006 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33129
2023-05-01 05:35:13,006 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33129
2023-05-01 05:35:13,006 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35791
2023-05-01 05:35:13,006 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,006 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,007 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:13,007 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:13,007 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dojpe1sq
2023-05-01 05:35:13,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37f03460-e439-47c4-8383-e0de637ed4ec
2023-05-01 05:35:13,009 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39689
2023-05-01 05:35:13,009 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39689
2023-05-01 05:35:13,009 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39711
2023-05-01 05:35:13,009 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,009 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,009 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:13,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:13,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d16mqmqa
2023-05-01 05:35:13,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1ea1fb0-dc44-4a27-8a40-22479e01cc0c
2023-05-01 05:35:13,010 - distributed.worker - INFO - Starting Worker plugin PreImport-341fb7e3-6eb0-4801-b980-bb818a15c919
2023-05-01 05:35:13,010 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0157710d-8593-47c1-9350-83dbbc89863b
2023-05-01 05:35:13,011 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46425
2023-05-01 05:35:13,011 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46425
2023-05-01 05:35:13,011 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42477
2023-05-01 05:35:13,011 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,011 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,011 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:13,011 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:13,011 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jk8ul1yj
2023-05-01 05:35:13,012 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de01f224-8d77-4cb1-95e8-aa2e9dbebc63
2023-05-01 05:35:13,012 - distributed.worker - INFO - Starting Worker plugin PreImport-4964743e-c1a5-48b6-bfc3-e51e9627d3d1
2023-05-01 05:35:13,012 - distributed.worker - INFO - Starting Worker plugin RMMSetup-610df6fc-9293-4fc8-b7f1-ace8ed1dad13
2023-05-01 05:35:13,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42535
2023-05-01 05:35:13,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42535
2023-05-01 05:35:13,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45111
2023-05-01 05:35:13,024 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39813
2023-05-01 05:35:13,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39813
2023-05-01 05:35:13,024 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39489
2023-05-01 05:35:13,024 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:13,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:13,024 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o0ry403x
2023-05-01 05:35:13,024 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:13,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:13,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pkgryue9
2023-05-01 05:35:13,024 - distributed.worker - INFO - Starting Worker plugin PreImport-e7838e57-95e0-473f-9e32-ad6e4cf82c09
2023-05-01 05:35:13,024 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59afa44a-16df-4c12-95b2-81990dd6203a
2023-05-01 05:35:13,024 - distributed.worker - INFO - Starting Worker plugin PreImport-e7eba606-5606-441c-9d08-d5278f482013
2023-05-01 05:35:13,025 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2a643e4-ecab-40d7-b2fb-15ea89eb5015
2023-05-01 05:35:13,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b96c1375-2253-4612-bc96-6ce915001982
2023-05-01 05:35:13,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9bdf2fbd-0c14-4c7f-a1c3-d970bfa908ec
2023-05-01 05:35:13,030 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44219', status: init, memory: 0, processing: 0>
2023-05-01 05:35:13,031 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44219
2023-05-01 05:35:13,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51328
2023-05-01 05:35:13,032 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,032 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:13,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36215
2023-05-01 05:35:13,053 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36215
2023-05-01 05:35:13,053 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44239
2023-05-01 05:35:13,053 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,053 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,053 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:13,053 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:13,053 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qsfqn5x7
2023-05-01 05:35:13,054 - distributed.worker - INFO - Starting Worker plugin PreImport-6eddf0c4-3ba1-41fc-8cde-2d1a6f4a0842
2023-05-01 05:35:13,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1de71156-32d1-4826-a7ce-b21d44e395e3
2023-05-01 05:35:13,054 - distributed.worker - INFO - Starting Worker plugin RMMSetup-684cd62a-5955-4834-83a8-c5ad051e5502
2023-05-01 05:35:13,245 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,269 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36215', status: init, memory: 0, processing: 0>
2023-05-01 05:35:13,270 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36215
2023-05-01 05:35:13,270 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51340
2023-05-01 05:35:13,270 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,271 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:13,284 - distributed.worker - INFO - Starting Worker plugin PreImport-8d54bb46-3fab-47e7-84ee-a47b3ac499b3
2023-05-01 05:35:13,284 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe315fbd-8a24-4c00-8bd8-6cddb296f16b
2023-05-01 05:35:13,284 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,309 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33129', status: init, memory: 0, processing: 0>
2023-05-01 05:35:13,310 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33129
2023-05-01 05:35:13,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51356
2023-05-01 05:35:13,310 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,310 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,312 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:13,313 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,323 - distributed.worker - INFO - Starting Worker plugin PreImport-18ad9c02-6df2-4516-ac9b-2a48ef8f5f7e
2023-05-01 05:35:13,324 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb022443-729b-4558-be53-dfda01afd154
2023-05-01 05:35:13,325 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,331 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,337 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,337 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,341 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39689', status: init, memory: 0, processing: 0>
2023-05-01 05:35:13,342 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39689
2023-05-01 05:35:13,342 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51360
2023-05-01 05:35:13,342 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,342 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:13,361 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46425', status: init, memory: 0, processing: 0>
2023-05-01 05:35:13,362 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46425
2023-05-01 05:35:13,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51390
2023-05-01 05:35:13,362 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,362 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,364 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39525', status: init, memory: 0, processing: 0>
2023-05-01 05:35:13,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:13,365 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39525
2023-05-01 05:35:13,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51376
2023-05-01 05:35:13,366 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,366 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,369 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42535', status: init, memory: 0, processing: 0>
2023-05-01 05:35:13,369 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:13,369 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42535
2023-05-01 05:35:13,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51404
2023-05-01 05:35:13,370 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,370 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,372 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:13,380 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39813', status: init, memory: 0, processing: 0>
2023-05-01 05:35:13,381 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39813
2023-05-01 05:35:13,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51412
2023-05-01 05:35:13,382 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:13,382 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:13,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:13,393 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:13,393 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:13,394 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:13,394 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:13,394 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:13,394 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:13,394 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:13,394 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:13,404 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:35:13,405 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:35:13,405 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:35:13,405 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:35:13,405 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:35:13,405 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:35:13,405 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:35:13,405 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:35:13,412 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:13,414 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:13,416 - distributed.scheduler - INFO - Remove client Client-ee8efe2c-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:13,416 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51310; closing.
2023-05-01 05:35:13,416 - distributed.scheduler - INFO - Remove client Client-ee8efe2c-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:13,417 - distributed.scheduler - INFO - Close client connection: Client-ee8efe2c-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:13,418 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40783'. Reason: nanny-close
2023-05-01 05:35:13,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:13,419 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44531'. Reason: nanny-close
2023-05-01 05:35:13,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:13,420 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33401'. Reason: nanny-close
2023-05-01 05:35:13,420 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46425. Reason: nanny-close
2023-05-01 05:35:13,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:13,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37363'. Reason: nanny-close
2023-05-01 05:35:13,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39689. Reason: nanny-close
2023-05-01 05:35:13,421 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:13,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45531'. Reason: nanny-close
2023-05-01 05:35:13,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39525. Reason: nanny-close
2023-05-01 05:35:13,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:13,422 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42535. Reason: nanny-close
2023-05-01 05:35:13,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36633'. Reason: nanny-close
2023-05-01 05:35:13,422 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51390; closing.
2023-05-01 05:35:13,422 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:13,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:13,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46425', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:13,423 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36215. Reason: nanny-close
2023-05-01 05:35:13,423 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46425
2023-05-01 05:35:13,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:13,423 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43529'. Reason: nanny-close
2023-05-01 05:35:13,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:13,424 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:13,424 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42525'. Reason: nanny-close
2023-05-01 05:35:13,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33129. Reason: nanny-close
2023-05-01 05:35:13,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:13,424 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:13,424 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:13,424 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46425
2023-05-01 05:35:13,424 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46425
2023-05-01 05:35:13,424 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51360; closing.
2023-05-01 05:35:13,424 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46425
2023-05-01 05:35:13,424 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46425
2023-05-01 05:35:13,425 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:13,425 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:13,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39689', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:13,425 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39689
2023-05-01 05:35:13,425 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46425
2023-05-01 05:35:13,425 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:13,425 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39813. Reason: nanny-close
2023-05-01 05:35:13,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51404; closing.
2023-05-01 05:35:13,426 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44219. Reason: nanny-close
2023-05-01 05:35:13,426 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:13,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51376; closing.
2023-05-01 05:35:13,426 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:13,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51340; closing.
2023-05-01 05:35:13,427 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42535', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:13,427 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42535
2023-05-01 05:35:13,427 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:13,427 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:13,427 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39525', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:13,427 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39525
2023-05-01 05:35:13,427 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36215', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:13,428 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36215
2023-05-01 05:35:13,428 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:13,428 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51356; closing.
2023-05-01 05:35:13,428 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:13,428 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33129', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:13,428 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33129
2023-05-01 05:35:13,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51328; closing.
2023-05-01 05:35:13,429 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:13,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51412; closing.
2023-05-01 05:35:13,430 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44219', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:13,430 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44219
2023-05-01 05:35:13,430 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:13,430 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39813', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:13,430 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39813
2023-05-01 05:35:13,430 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:35:14,835 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:35:14,836 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:35:14,836 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:35:14,837 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:35:14,838 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-01 05:35:16,704 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:16,708 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37399 instead
  warnings.warn(
2023-05-01 05:35:16,712 - distributed.scheduler - INFO - State start
2023-05-01 05:35:16,730 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:16,731 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:35:16,731 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37399/status
2023-05-01 05:35:16,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44797'
2023-05-01 05:35:16,977 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33459'
2023-05-01 05:35:16,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34215'
2023-05-01 05:35:16,996 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44203'
2023-05-01 05:35:17,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38629'
2023-05-01 05:35:17,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43333'
2023-05-01 05:35:17,024 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32961'
2023-05-01 05:35:17,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33715'
2023-05-01 05:35:18,005 - distributed.scheduler - INFO - Receive client connection: Client-f3344041-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:18,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43904
2023-05-01 05:35:18,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:18,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:18,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:18,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:18,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:18,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:18,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:18,575 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:18,575 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:18,637 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:18,637 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:18,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:18,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:18,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:18,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:18,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:18,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:18,647 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:18,647 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:18,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:18,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:18,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:18,688 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:18,696 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:20,479 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44441
2023-05-01 05:35:20,479 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44441
2023-05-01 05:35:20,479 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44165
2023-05-01 05:35:20,479 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,479 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,479 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:20,480 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:20,480 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gughgpno
2023-05-01 05:35:20,480 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c85839a-e558-4c2b-a0cb-649e89361b38
2023-05-01 05:35:20,486 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35535
2023-05-01 05:35:20,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35535
2023-05-01 05:35:20,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37663
2023-05-01 05:35:20,487 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,487 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,487 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:20,487 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:20,487 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-96se2n_y
2023-05-01 05:35:20,487 - distributed.worker - INFO - Starting Worker plugin PreImport-d1f84e6c-411d-4749-a638-44cfa9e45512
2023-05-01 05:35:20,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b75c541b-8c03-414e-833e-ce849c0e790b
2023-05-01 05:35:20,488 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b684f649-7abd-4fb2-a4d9-1f8dc77f6efe
2023-05-01 05:35:20,489 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41621
2023-05-01 05:35:20,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41621
2023-05-01 05:35:20,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34609
2023-05-01 05:35:20,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,489 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,489 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:20,489 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:20,489 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j4ft3c9f
2023-05-01 05:35:20,489 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-397795c1-2961-4aa8-b60b-409647b1d4c7
2023-05-01 05:35:20,489 - distributed.worker - INFO - Starting Worker plugin PreImport-ec600a60-8c0c-425e-8e3f-6045e390d02a
2023-05-01 05:35:20,490 - distributed.worker - INFO - Starting Worker plugin RMMSetup-570d243f-2410-455b-8a68-fd88ee885b7f
2023-05-01 05:35:20,522 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45993
2023-05-01 05:35:20,522 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45993
2023-05-01 05:35:20,522 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39699
2023-05-01 05:35:20,522 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,522 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,522 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:20,522 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:20,522 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lgs5qj_7
2023-05-01 05:35:20,523 - distributed.worker - INFO - Starting Worker plugin PreImport-b612acbd-5bbc-46ef-84bf-f398ee59a74b
2023-05-01 05:35:20,523 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3906e467-cf16-4e16-b3ee-20356f588499
2023-05-01 05:35:20,524 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1cbc87c-2afa-4286-923d-fb155e77753c
2023-05-01 05:35:20,629 - distributed.worker - INFO - Starting Worker plugin PreImport-ce44b5e1-d60f-4c44-a97c-6ae80146f0c1
2023-05-01 05:35:20,629 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bfa18583-6c93-4070-ba46-34e5e4b2db8f
2023-05-01 05:35:20,629 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,630 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,632 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,662 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35535', status: init, memory: 0, processing: 0>
2023-05-01 05:35:20,663 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35535
2023-05-01 05:35:20,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43928
2023-05-01 05:35:20,664 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,664 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:20,669 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,671 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41621', status: init, memory: 0, processing: 0>
2023-05-01 05:35:20,671 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41621
2023-05-01 05:35:20,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43930
2023-05-01 05:35:20,672 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,672 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,673 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44441', status: init, memory: 0, processing: 0>
2023-05-01 05:35:20,673 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44441
2023-05-01 05:35:20,673 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43920
2023-05-01 05:35:20,674 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:20,674 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,677 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:20,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39405
2023-05-01 05:35:20,686 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39405
2023-05-01 05:35:20,686 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33717
2023-05-01 05:35:20,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43737
2023-05-01 05:35:20,687 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,687 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43737
2023-05-01 05:35:20,687 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,687 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37283
2023-05-01 05:35:20,687 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:20,687 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,687 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:20,687 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,687 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tnk3tumz
2023-05-01 05:35:20,687 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:20,687 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:20,687 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3ouk29zg
2023-05-01 05:35:20,687 - distributed.worker - INFO - Starting Worker plugin PreImport-7d6e5bcb-4770-41e5-bfac-78d2b233928a
2023-05-01 05:35:20,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b68dd40-9199-4900-aaef-6eba9fee8f17
2023-05-01 05:35:20,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fa07355-7046-40a2-beb7-0b06ea3d73c5
2023-05-01 05:35:20,687 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c66b2ac1-2ad3-470e-8caf-3a3f7b09ef42
2023-05-01 05:35:20,688 - distributed.worker - INFO - Starting Worker plugin PreImport-59ecbe4f-bf89-45da-8575-7133f630b302
2023-05-01 05:35:20,688 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5df1d9d9-9563-4605-9ef4-a0e5c7a74827
2023-05-01 05:35:20,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37213
2023-05-01 05:35:20,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37213
2023-05-01 05:35:20,688 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34029
2023-05-01 05:35:20,688 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,688 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,688 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:20,688 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:20,688 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6z7lmg28
2023-05-01 05:35:20,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a046905-25ee-413e-80f6-f62c6eac6baf
2023-05-01 05:35:20,689 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40319
2023-05-01 05:35:20,689 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40319
2023-05-01 05:35:20,690 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37121
2023-05-01 05:35:20,690 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,690 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,690 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:20,690 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:20,690 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4cvyg_5e
2023-05-01 05:35:20,690 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ed2018c-a37f-4096-be1d-7f0913150e0f
2023-05-01 05:35:20,699 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45993', status: init, memory: 0, processing: 0>
2023-05-01 05:35:20,700 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45993
2023-05-01 05:35:20,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43938
2023-05-01 05:35:20,700 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,700 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,702 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:20,809 - distributed.worker - INFO - Starting Worker plugin PreImport-910d4171-3ae3-41f0-9cc3-cf8543ba73f9
2023-05-01 05:35:20,809 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc187bc0-68c2-4cc7-a865-f14ba5fa0e91
2023-05-01 05:35:20,809 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,815 - distributed.worker - INFO - Starting Worker plugin PreImport-2c3eeecb-2155-4f24-961c-877e8dff3948
2023-05-01 05:35:20,815 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,815 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12b1e478-33a0-475f-a732-14eee57192ba
2023-05-01 05:35:20,815 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,819 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,836 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40319', status: init, memory: 0, processing: 0>
2023-05-01 05:35:20,837 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40319
2023-05-01 05:35:20,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43950
2023-05-01 05:35:20,837 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,837 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,838 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37213', status: init, memory: 0, processing: 0>
2023-05-01 05:35:20,838 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37213
2023-05-01 05:35:20,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43962
2023-05-01 05:35:20,839 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,839 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,839 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:20,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:20,854 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43737', status: init, memory: 0, processing: 0>
2023-05-01 05:35:20,854 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43737
2023-05-01 05:35:20,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43976
2023-05-01 05:35:20,855 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39405', status: init, memory: 0, processing: 0>
2023-05-01 05:35:20,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39405
2023-05-01 05:35:20,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43960
2023-05-01 05:35:20,856 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,856 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,857 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:20,857 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:20,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:20,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:20,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:20,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:20,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:20,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:20,897 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:20,897 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:20,897 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:20,897 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:20,901 - distributed.scheduler - INFO - Remove client Client-f3344041-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:20,902 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43904; closing.
2023-05-01 05:35:20,902 - distributed.scheduler - INFO - Remove client Client-f3344041-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:20,902 - distributed.scheduler - INFO - Close client connection: Client-f3344041-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:20,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44203'. Reason: nanny-close
2023-05-01 05:35:20,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:20,905 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44797'. Reason: nanny-close
2023-05-01 05:35:20,906 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:20,906 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33459'. Reason: nanny-close
2023-05-01 05:35:20,907 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43737. Reason: nanny-close
2023-05-01 05:35:20,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:20,907 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41621. Reason: nanny-close
2023-05-01 05:35:20,907 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34215'. Reason: nanny-close
2023-05-01 05:35:20,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:20,908 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38629'. Reason: nanny-close
2023-05-01 05:35:20,908 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40319. Reason: nanny-close
2023-05-01 05:35:20,908 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:20,908 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45993. Reason: nanny-close
2023-05-01 05:35:20,908 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43333'. Reason: nanny-close
2023-05-01 05:35:20,909 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:20,909 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:20,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43930; closing.
2023-05-01 05:35:20,909 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32961'. Reason: nanny-close
2023-05-01 05:35:20,909 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:20,909 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:20,909 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39405. Reason: nanny-close
2023-05-01 05:35:20,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41621', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:20,909 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:20,909 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41621
2023-05-01 05:35:20,910 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33715'. Reason: nanny-close
2023-05-01 05:35:20,910 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44441. Reason: nanny-close
2023-05-01 05:35:20,910 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:20,910 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:20,910 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35535. Reason: nanny-close
2023-05-01 05:35:20,910 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:20,910 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43976; closing.
2023-05-01 05:35:20,911 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:20,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43950; closing.
2023-05-01 05:35:20,911 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:20,911 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37213. Reason: nanny-close
2023-05-01 05:35:20,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43737', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:20,912 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43737
2023-05-01 05:35:20,912 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:20,912 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41621
2023-05-01 05:35:20,912 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41621
2023-05-01 05:35:20,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40319', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:20,912 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40319
2023-05-01 05:35:20,912 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41621
2023-05-01 05:35:20,912 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:20,912 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:20,912 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43938; closing.
2023-05-01 05:35:20,913 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41621
2023-05-01 05:35:20,913 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:20,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45993', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:20,913 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45993
2023-05-01 05:35:20,913 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:20,914 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:20,914 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:20,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43920; closing.
2023-05-01 05:35:20,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43928; closing.
2023-05-01 05:35:20,914 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:20,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43960; closing.
2023-05-01 05:35:20,915 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:20,915 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44441', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:20,915 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44441
2023-05-01 05:35:20,915 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35535', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:20,915 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35535
2023-05-01 05:35:20,916 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39405', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:20,916 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39405
2023-05-01 05:35:20,916 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43962; closing.
2023-05-01 05:35:20,916 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37213', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:20,917 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37213
2023-05-01 05:35:20,917 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:35:22,321 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:35:22,321 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:35:22,322 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:35:22,323 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:35:22,324 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-01 05:35:24,226 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:24,231 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40339 instead
  warnings.warn(
2023-05-01 05:35:24,234 - distributed.scheduler - INFO - State start
2023-05-01 05:35:24,259 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:24,260 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:35:24,261 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40339/status
2023-05-01 05:35:24,328 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34307'
2023-05-01 05:35:25,188 - distributed.scheduler - INFO - Receive client connection: Client-f7aa6cc7-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:25,204 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43990
2023-05-01 05:35:25,777 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:25,777 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:26,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:26,824 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43637
2023-05-01 05:35:26,824 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43637
2023-05-01 05:35:26,824 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-05-01 05:35:26,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:26,825 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:26,825 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:26,825 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-01 05:35:26,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iclwx7z_
2023-05-01 05:35:26,825 - distributed.worker - INFO - Starting Worker plugin RMMSetup-962cd019-fc06-452e-b18c-a843f9b3cc85
2023-05-01 05:35:26,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ad154409-51ad-4711-b132-a6d00f462158
2023-05-01 05:35:26,826 - distributed.worker - INFO - Starting Worker plugin PreImport-04d39155-7807-4339-9ecd-bdc549de5d1f
2023-05-01 05:35:26,826 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:26,862 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43637', status: init, memory: 0, processing: 0>
2023-05-01 05:35:26,863 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43637
2023-05-01 05:35:26,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44014
2023-05-01 05:35:26,864 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:26,864 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:26,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:26,943 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:26,946 - distributed.scheduler - INFO - Remove client Client-f7aa6cc7-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:26,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43990; closing.
2023-05-01 05:35:26,947 - distributed.scheduler - INFO - Remove client Client-f7aa6cc7-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:26,947 - distributed.scheduler - INFO - Close client connection: Client-f7aa6cc7-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:26,948 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34307'. Reason: nanny-close
2023-05-01 05:35:26,948 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:26,950 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43637. Reason: nanny-close
2023-05-01 05:35:26,952 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44014; closing.
2023-05-01 05:35:26,952 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:26,953 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43637', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:26,953 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43637
2023-05-01 05:35:26,953 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:35:26,954 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:27,914 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:35:27,915 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:35:27,915 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:35:27,916 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:35:27,916 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-01 05:35:31,598 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:31,602 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39033 instead
  warnings.warn(
2023-05-01 05:35:31,606 - distributed.scheduler - INFO - State start
2023-05-01 05:35:31,626 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:31,627 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:35:31,628 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39033/status
2023-05-01 05:35:31,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45395'
2023-05-01 05:35:31,910 - distributed.scheduler - INFO - Receive client connection: Client-fc01feaa-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:31,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44116
2023-05-01 05:35:33,179 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:33,179 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:33,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:34,260 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36407
2023-05-01 05:35:34,260 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36407
2023-05-01 05:35:34,260 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43511
2023-05-01 05:35:34,260 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:34,260 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:34,261 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:34,261 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-01 05:35:34,261 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ov683usi
2023-05-01 05:35:34,261 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a4de374-23ee-4324-bb17-d116721edf39
2023-05-01 05:35:34,261 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d383f2a8-6ac7-4226-b1c9-de0efeb82b63
2023-05-01 05:35:34,261 - distributed.worker - INFO - Starting Worker plugin PreImport-e7214266-4802-46e8-b8b1-078125f665d9
2023-05-01 05:35:34,263 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:34,287 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36407', status: init, memory: 0, processing: 0>
2023-05-01 05:35:34,288 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36407
2023-05-01 05:35:34,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38306
2023-05-01 05:35:34,289 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:34,289 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:34,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:34,377 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:34,379 - distributed.scheduler - INFO - Remove client Client-fc01feaa-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:34,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44116; closing.
2023-05-01 05:35:34,380 - distributed.scheduler - INFO - Remove client Client-fc01feaa-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:34,380 - distributed.scheduler - INFO - Close client connection: Client-fc01feaa-e7e1-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:34,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45395'. Reason: nanny-close
2023-05-01 05:35:34,382 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:34,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36407. Reason: nanny-close
2023-05-01 05:35:34,385 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38306; closing.
2023-05-01 05:35:34,385 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:34,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36407', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:34,385 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36407
2023-05-01 05:35:34,385 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:35:34,386 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:35,297 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:35:35,298 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:35:35,298 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:35:35,299 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:35:35,299 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-01 05:35:37,224 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:37,228 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46511 instead
  warnings.warn(
2023-05-01 05:35:37,232 - distributed.scheduler - INFO - State start
2023-05-01 05:35:37,266 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:37,267 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:35:37,268 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46511/status
2023-05-01 05:35:40,658 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:35:40,659 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:35:40,659 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:35:40,660 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:35:40,660 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-05-01 05:35:42,582 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:42,586 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35581 instead
  warnings.warn(
2023-05-01 05:35:42,590 - distributed.scheduler - INFO - State start
2023-05-01 05:35:42,614 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:42,615 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-01 05:35:42,615 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35581/status
2023-05-01 05:35:42,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45499'
2023-05-01 05:35:44,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:44,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:44,328 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:45,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44335
2023-05-01 05:35:45,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44335
2023-05-01 05:35:45,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43547
2023-05-01 05:35:45,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-01 05:35:45,055 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:45,055 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:45,055 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-01 05:35:45,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q4pfoql1
2023-05-01 05:35:45,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9ca8fce-9795-4f4a-a689-4d559403ce37
2023-05-01 05:35:45,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ceb0b62f-8cf6-4a50-b6a3-225bcbc6ed09
2023-05-01 05:35:45,056 - distributed.worker - INFO - Starting Worker plugin PreImport-ebd70ca7-01cc-4e1d-8a8b-2f677d3f168c
2023-05-01 05:35:45,056 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:45,078 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44335', status: init, memory: 0, processing: 0>
2023-05-01 05:35:45,094 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44335
2023-05-01 05:35:45,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51212
2023-05-01 05:35:45,094 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-01 05:35:45,094 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:45,096 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-01 05:35:46,689 - distributed.scheduler - INFO - Receive client connection: Client-02930035-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:46,690 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51228
2023-05-01 05:35:46,696 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:46,702 - distributed.scheduler - INFO - Remove client Client-02930035-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:46,702 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51228; closing.
2023-05-01 05:35:46,702 - distributed.scheduler - INFO - Remove client Client-02930035-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:46,703 - distributed.scheduler - INFO - Close client connection: Client-02930035-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:46,703 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45499'. Reason: nanny-close
2023-05-01 05:35:46,704 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:46,705 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44335. Reason: nanny-close
2023-05-01 05:35:46,707 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-01 05:35:46,707 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51212; closing.
2023-05-01 05:35:46,707 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44335', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:46,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44335
2023-05-01 05:35:46,708 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:35:46,708 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:47,620 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:35:47,620 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:35:47,621 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:35:47,621 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-01 05:35:47,622 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-05-01 05:35:49,571 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:49,575 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35389 instead
  warnings.warn(
2023-05-01 05:35:49,579 - distributed.scheduler - INFO - State start
2023-05-01 05:35:49,599 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:49,600 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:35:49,601 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35389/status
2023-05-01 05:35:49,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45229'
2023-05-01 05:35:49,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36835'
2023-05-01 05:35:49,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39509'
2023-05-01 05:35:49,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37281'
2023-05-01 05:35:49,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43217'
2023-05-01 05:35:49,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36701'
2023-05-01 05:35:49,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41603'
2023-05-01 05:35:49,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34903'
2023-05-01 05:35:50,796 - distributed.scheduler - INFO - Receive client connection: Client-06c0176b-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:50,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43608
2023-05-01 05:35:51,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:51,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:51,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:51,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:51,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:51,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:51,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:51,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:51,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:51,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:51,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:51,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:51,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:51,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:51,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:51,691 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:51,695 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:51,700 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:51,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:51,705 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:51,712 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:51,713 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:51,713 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:51,767 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:35:53,356 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41529
2023-05-01 05:35:53,357 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41529
2023-05-01 05:35:53,357 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38393
2023-05-01 05:35:53,357 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,357 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,357 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:53,357 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:53,357 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-imh9r4d2
2023-05-01 05:35:53,357 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b93cf0e-3cfe-456a-b788-f11d1b821ed5
2023-05-01 05:35:53,358 - distributed.worker - INFO - Starting Worker plugin PreImport-523d9691-e74e-4191-a2df-8a0ede11b9fb
2023-05-01 05:35:53,358 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad8b4c88-c932-4cbf-82b3-0e4262b2de68
2023-05-01 05:35:53,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45255
2023-05-01 05:35:53,399 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45255
2023-05-01 05:35:53,399 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43685
2023-05-01 05:35:53,400 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,400 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,400 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:53,400 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:53,400 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zmbh273o
2023-05-01 05:35:53,400 - distributed.worker - INFO - Starting Worker plugin PreImport-915e5e85-c9c4-4bdf-99f2-a17855e7d4d2
2023-05-01 05:35:53,400 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-714c6922-cf0b-458b-9a1e-62b3027139f1
2023-05-01 05:35:53,400 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46651
2023-05-01 05:35:53,400 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46651
2023-05-01 05:35:53,400 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36069
2023-05-01 05:35:53,400 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,400 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,400 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:53,400 - distributed.worker - INFO - Starting Worker plugin RMMSetup-660164c9-6b67-43c9-82d5-ac4b59cd9672
2023-05-01 05:35:53,400 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:53,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gdshrbld
2023-05-01 05:35:53,401 - distributed.worker - INFO - Starting Worker plugin PreImport-90ae1bca-b965-49b4-ae90-f8db806f0b55
2023-05-01 05:35:53,401 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85640fef-2ff5-4898-b32f-93c3cdd19f42
2023-05-01 05:35:53,408 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb730f31-4b5b-44a5-af87-dd48da652168
2023-05-01 05:35:53,534 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,567 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41529', status: init, memory: 0, processing: 0>
2023-05-01 05:35:53,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41529
2023-05-01 05:35:53,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43624
2023-05-01 05:35:53,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,569 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:53,574 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,580 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41305
2023-05-01 05:35:53,580 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41305
2023-05-01 05:35:53,580 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43149
2023-05-01 05:35:53,580 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,580 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,581 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:53,581 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:53,581 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m_uki4sj
2023-05-01 05:35:53,581 - distributed.worker - INFO - Starting Worker plugin PreImport-5f28d966-3491-4c49-b259-e4eaf99438e7
2023-05-01 05:35:53,581 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d8a7c167-6389-4ff8-91c7-b8b350735538
2023-05-01 05:35:53,581 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff103b94-e286-4bde-9e55-2ef73505120c
2023-05-01 05:35:53,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40431
2023-05-01 05:35:53,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40431
2023-05-01 05:35:53,583 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33335
2023-05-01 05:35:53,583 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,583 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,583 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:53,583 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:53,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iajsfqwp
2023-05-01 05:35:53,584 - distributed.worker - INFO - Starting Worker plugin PreImport-eedbda21-bbd4-46e8-80d8-7cd26cae9989
2023-05-01 05:35:53,584 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-076c34a9-d29e-4d9a-a2c0-b053ba28804b
2023-05-01 05:35:53,584 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc6ca95a-68ae-458e-8079-eadfe6a98a42
2023-05-01 05:35:53,594 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,609 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45255', status: init, memory: 0, processing: 0>
2023-05-01 05:35:53,609 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45255
2023-05-01 05:35:53,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43640
2023-05-01 05:35:53,610 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,610 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:53,633 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46651', status: init, memory: 0, processing: 0>
2023-05-01 05:35:53,634 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46651
2023-05-01 05:35:53,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43642
2023-05-01 05:35:53,635 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,635 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:53,664 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37223
2023-05-01 05:35:53,665 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37223
2023-05-01 05:35:53,665 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34663
2023-05-01 05:35:53,665 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,665 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,665 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:53,665 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:53,665 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2hjj5r3_
2023-05-01 05:35:53,665 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8617d7bd-df59-4a9e-8e92-a05496142404
2023-05-01 05:35:53,665 - distributed.worker - INFO - Starting Worker plugin PreImport-fd893881-84d2-493c-8361-e9acc4b40594
2023-05-01 05:35:53,666 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49f5488e-2a1b-4c3b-8f88-a44fa8187b72
2023-05-01 05:35:53,717 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,717 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,746 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41305', status: init, memory: 0, processing: 0>
2023-05-01 05:35:53,747 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41305
2023-05-01 05:35:53,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43658
2023-05-01 05:35:53,747 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,747 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,748 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40431', status: init, memory: 0, processing: 0>
2023-05-01 05:35:53,749 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40431
2023-05-01 05:35:53,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43674
2023-05-01 05:35:53,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:53,749 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,749 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:53,788 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,814 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43291
2023-05-01 05:35:53,814 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43291
2023-05-01 05:35:53,814 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33371
2023-05-01 05:35:53,815 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,815 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,815 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:53,815 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:53,815 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fdn_n479
2023-05-01 05:35:53,815 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6f38be5-69ce-4be8-a671-fef41627a47a
2023-05-01 05:35:53,815 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37223', status: init, memory: 0, processing: 0>
2023-05-01 05:35:53,816 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37223
2023-05-01 05:35:53,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43680
2023-05-01 05:35:53,817 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,817 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:53,894 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39787
2023-05-01 05:35:53,894 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39787
2023-05-01 05:35:53,894 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38095
2023-05-01 05:35:53,894 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,894 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,895 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:35:53,895 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-01 05:35:53,895 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1q7694t6
2023-05-01 05:35:53,895 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a28428ef-3493-4479-9cd2-b7a8c084b67c
2023-05-01 05:35:53,895 - distributed.worker - INFO - Starting Worker plugin PreImport-2093a09b-ebe2-49fb-999a-5fb149190e73
2023-05-01 05:35:53,895 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6870ac2-ae95-4637-a3b8-a5838c4e39f4
2023-05-01 05:35:53,928 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85d8a48d-e408-4457-9b23-041c3fde9f0e
2023-05-01 05:35:53,928 - distributed.worker - INFO - Starting Worker plugin PreImport-d05b9ad9-e10d-47e3-8ca9-f7e160e5301d
2023-05-01 05:35:53,928 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,955 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43291', status: init, memory: 0, processing: 0>
2023-05-01 05:35:53,956 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43291
2023-05-01 05:35:53,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43682
2023-05-01 05:35:53,957 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:53,957 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:53,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:54,008 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:54,038 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39787', status: init, memory: 0, processing: 0>
2023-05-01 05:35:54,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39787
2023-05-01 05:35:54,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43694
2023-05-01 05:35:54,039 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:35:54,040 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:35:54,042 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:35:54,083 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:54,083 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:54,083 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:54,083 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:54,083 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:54,084 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:54,084 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:54,084 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-01 05:35:54,097 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:54,097 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:54,097 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:54,098 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:54,098 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:54,098 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:54,098 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:54,098 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:35:54,103 - distributed.scheduler - INFO - Remove client Client-06c0176b-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:54,104 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43608; closing.
2023-05-01 05:35:54,104 - distributed.scheduler - INFO - Remove client Client-06c0176b-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:54,104 - distributed.scheduler - INFO - Close client connection: Client-06c0176b-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:54,107 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37281'. Reason: nanny-close
2023-05-01 05:35:54,108 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:54,108 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45229'. Reason: nanny-close
2023-05-01 05:35:54,109 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:54,109 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36835'. Reason: nanny-close
2023-05-01 05:35:54,109 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46651. Reason: nanny-close
2023-05-01 05:35:54,109 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:54,110 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39509'. Reason: nanny-close
2023-05-01 05:35:54,110 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43291. Reason: nanny-close
2023-05-01 05:35:54,110 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:54,110 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43217'. Reason: nanny-close
2023-05-01 05:35:54,110 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40431. Reason: nanny-close
2023-05-01 05:35:54,111 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:54,111 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43642; closing.
2023-05-01 05:35:54,111 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:54,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36701'. Reason: nanny-close
2023-05-01 05:35:54,111 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41529. Reason: nanny-close
2023-05-01 05:35:54,111 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46651', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:54,111 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:54,112 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46651
2023-05-01 05:35:54,112 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:54,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41603'. Reason: nanny-close
2023-05-01 05:35:54,112 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39787. Reason: nanny-close
2023-05-01 05:35:54,112 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:54,112 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:54,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34903'. Reason: nanny-close
2023-05-01 05:35:54,112 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:54,113 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:35:54,113 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46651
2023-05-01 05:35:54,113 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:54,113 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46651
2023-05-01 05:35:54,113 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43682; closing.
2023-05-01 05:35:54,113 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46651
2023-05-01 05:35:54,113 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37223. Reason: nanny-close
2023-05-01 05:35:54,113 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46651
2023-05-01 05:35:54,113 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:54,114 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:54,114 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43291', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:54,114 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43291
2023-05-01 05:35:54,114 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:54,114 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41305. Reason: nanny-close
2023-05-01 05:35:54,114 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43674; closing.
2023-05-01 05:35:54,114 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:54,114 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40431', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:54,114 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40431
2023-05-01 05:35:54,114 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45255. Reason: nanny-close
2023-05-01 05:35:54,115 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43624; closing.
2023-05-01 05:35:54,115 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:54,115 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:54,115 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41529', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:54,115 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41529
2023-05-01 05:35:54,116 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:54,116 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43694; closing.
2023-05-01 05:35:54,116 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46651
2023-05-01 05:35:54,116 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:54,116 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39787', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:54,116 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39787
2023-05-01 05:35:54,116 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43680; closing.
2023-05-01 05:35:54,116 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:35:54,117 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:54,117 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37223', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:54,117 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37223
2023-05-01 05:35:54,117 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43658; closing.
2023-05-01 05:35:54,118 - distributed.nanny - INFO - Worker closed
2023-05-01 05:35:54,118 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41305', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:54,118 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41305
2023-05-01 05:35:54,118 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43640; closing.
2023-05-01 05:35:54,119 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45255', status: closing, memory: 0, processing: 0>
2023-05-01 05:35:54,119 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45255
2023-05-01 05:35:54,119 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:35:55,673 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:35:55,674 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:35:55,674 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:35:55,675 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:35:55,675 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-05-01 05:35:57,512 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:57,516 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35465 instead
  warnings.warn(
2023-05-01 05:35:57,520 - distributed.scheduler - INFO - State start
2023-05-01 05:35:57,542 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:35:57,543 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:35:57,543 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35465/status
2023-05-01 05:35:57,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34821'
2023-05-01 05:35:59,353 - distributed.scheduler - INFO - Receive client connection: Client-0b8d084a-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:35:59,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53544
2023-05-01 05:35:59,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:35:59,375 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:35:59,398 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:36:00,025 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34581
2023-05-01 05:36:00,025 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34581
2023-05-01 05:36:00,025 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34467
2023-05-01 05:36:00,025 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:36:00,025 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:36:00,025 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:36:00,025 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-01 05:36:00,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mia0n473
2023-05-01 05:36:00,026 - distributed.worker - INFO - Starting Worker plugin PreImport-be3817fe-aaa0-4c98-87be-ed915a71eeaf
2023-05-01 05:36:00,026 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62112cfd-6263-4ebf-8c3f-a9629ba53f04
2023-05-01 05:36:00,026 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cb39232-7b2c-4ee1-8b47-5480abcd843a
2023-05-01 05:36:00,124 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:36:00,149 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34581', status: init, memory: 0, processing: 0>
2023-05-01 05:36:00,150 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34581
2023-05-01 05:36:00,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53552
2023-05-01 05:36:00,151 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:36:00,151 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:36:00,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:36:00,191 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:36:00,194 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:36:00,196 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:36:00,198 - distributed.scheduler - INFO - Remove client Client-0b8d084a-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:36:00,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53544; closing.
2023-05-01 05:36:00,198 - distributed.scheduler - INFO - Remove client Client-0b8d084a-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:36:00,199 - distributed.scheduler - INFO - Close client connection: Client-0b8d084a-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:36:00,200 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34821'. Reason: nanny-close
2023-05-01 05:36:00,200 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:36:00,201 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34581. Reason: nanny-close
2023-05-01 05:36:00,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:36:00,203 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53552; closing.
2023-05-01 05:36:00,203 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34581', status: closing, memory: 0, processing: 0>
2023-05-01 05:36:00,203 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34581
2023-05-01 05:36:00,204 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:36:00,204 - distributed.nanny - INFO - Worker closed
2023-05-01 05:36:01,166 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:36:01,166 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:36:01,167 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:36:01,167 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:36:01,168 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-05-01 05:36:03,275 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:36:03,280 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42843 instead
  warnings.warn(
2023-05-01 05:36:03,284 - distributed.scheduler - INFO - State start
2023-05-01 05:36:03,308 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-01 05:36:03,309 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-01 05:36:03,310 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42843/status
2023-05-01 05:36:03,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35167'
2023-05-01 05:36:04,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:04,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:04,885 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-01 05:36:05,605 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41881
2023-05-01 05:36:05,605 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41881
2023-05-01 05:36:05,606 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33021
2023-05-01 05:36:05,606 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-01 05:36:05,606 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:36:05,606 - distributed.worker - INFO -               Threads:                          1
2023-05-01 05:36:05,606 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-01 05:36:05,606 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e1tlpxd7
2023-05-01 05:36:05,606 - distributed.worker - INFO - Starting Worker plugin PreImport-2680aafb-9798-4374-a6fa-a5aa588a4b59
2023-05-01 05:36:05,606 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e96f847-e266-4438-b209-6add5a619d4e
2023-05-01 05:36:05,606 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f394fe75-b587-46dd-9459-2c6f892034dc
2023-05-01 05:36:05,719 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:36:05,749 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41881', status: init, memory: 0, processing: 0>
2023-05-01 05:36:05,767 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41881
2023-05-01 05:36:05,767 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41668
2023-05-01 05:36:05,767 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-01 05:36:05,767 - distributed.worker - INFO - -------------------------------------------------
2023-05-01 05:36:05,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-01 05:36:05,922 - distributed.scheduler - INFO - Receive client connection: Client-0ed64d24-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:36:05,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41672
2023-05-01 05:36:05,929 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-05-01 05:36:05,933 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-01 05:36:05,937 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:36:05,938 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-01 05:36:05,941 - distributed.scheduler - INFO - Remove client Client-0ed64d24-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:36:05,941 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41672; closing.
2023-05-01 05:36:05,941 - distributed.scheduler - INFO - Remove client Client-0ed64d24-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:36:05,941 - distributed.scheduler - INFO - Close client connection: Client-0ed64d24-e7e2-11ed-a2fe-d8c49764f6bb
2023-05-01 05:36:05,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35167'. Reason: nanny-close
2023-05-01 05:36:05,943 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-01 05:36:05,944 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41881. Reason: nanny-close
2023-05-01 05:36:05,945 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41668; closing.
2023-05-01 05:36:05,945 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-01 05:36:05,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41881', status: closing, memory: 0, processing: 0>
2023-05-01 05:36:05,946 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41881
2023-05-01 05:36:05,946 - distributed.scheduler - INFO - Lost all workers
2023-05-01 05:36:05,947 - distributed.nanny - INFO - Worker closed
2023-05-01 05:36:07,059 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-01 05:36:07,060 - distributed.scheduler - INFO - Scheduler closing...
2023-05-01 05:36:07,060 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-01 05:36:07,061 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-01 05:36:07,061 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42595 instead
  warnings.warn(
2023-05-01 05:36:15,901 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:15,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:15,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:15,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:15,920 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:15,920 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:15,920 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:15,920 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:15,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:15,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:15,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:15,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:15,928 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:15,928 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:15,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:15,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42513 instead
  warnings.warn(
2023-05-01 05:36:23,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:23,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:23,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:23,939 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:23,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:23,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:23,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:23,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:23,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:23,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:23,997 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:23,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:24,007 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:24,007 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:24,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:24,026 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39649 instead
  warnings.warn(
2023-05-01 05:36:31,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:31,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:31,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:31,480 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:31,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:31,480 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:31,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:31,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:31,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:31,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:31,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:31,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:31,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:31,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:31,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:31,508 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34787 instead
  warnings.warn(
2023-05-01 05:36:40,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:40,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:40,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:40,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:40,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:40,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:40,067 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:40,067 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:40,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:40,091 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:40,094 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:40,094 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:40,101 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:40,101 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:40,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:40,117 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34509 instead
  warnings.warn(
2023-05-01 05:36:49,607 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:49,607 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:49,607 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:49,607 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:49,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:49,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:49,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:49,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:49,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:49,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:49,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:49,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:49,614 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:49,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:49,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:49,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35205 instead
  warnings.warn(
2023-05-01 05:36:58,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:58,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:58,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:58,972 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:58,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:58,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:58,978 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:58,978 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:58,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:58,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:59,003 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:59,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:59,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:59,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:36:59,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:36:59,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44579 instead
  warnings.warn(
2023-05-01 05:37:08,724 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:08,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:09,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:09,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:09,012 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:09,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:09,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:09,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:09,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:09,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:09,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:09,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:09,050 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:09,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:09,063 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:09,063 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38065 instead
  warnings.warn(
2023-05-01 05:37:19,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:19,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:19,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:19,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:19,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:19,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:19,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:19,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:19,164 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:19,164 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:19,170 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:19,170 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:19,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:19,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:37:19,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:37:19,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44053 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39243 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35569 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44949 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43085 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35545 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46851 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33579 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40067 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45691 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44137 instead
  warnings.warn(
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-3266' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33373 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42641 instead
  warnings.warn(
2023-05-01 05:41:00,940 - distributed.worker - WARNING - Compute Failed
Key:       ('check_partitions-ea1c76ae4a173f264dba141b985d8393', 0)
Function:  subgraph_callable-9a5d5e96-cb40-41f4-b4c7-071db52e
args:      (< could not convert arg to str >)
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:94: 2 cudaErrorMemoryAllocation out of memory')"

Process SpawnProcess-22:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 153, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 314, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 599, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3186, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2345, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 349, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 416, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 389, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2208, in _gather
    raise exception.with_traceback(traceback)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/optimization.py", line 990, in __call__
    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/core.py", line 149, in get
    result = _execute_task(task, cache)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/core.py", line 119, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 73, in apply
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 7006, in apply_and_enforce
    df = func(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 99, in check_partitions
    hashes = partitioning_index(df, npartitions)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 776, in partitioning_index
    return hash_object_dispatch(df, index=False) % int(npartitions)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 642, in __call__
    return meth(arg, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/backends.py", line 396, in hash_object_cudf
    return safe_hash(frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/backends.py", line 388, in safe_hash
    return cudf.Series(frame.hash_values(), index=frame.index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 1722, in hash_values
    {None: libcudf.hash.hash([*self._columns], method, seed)},
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "hash.pyx", line 56, in cudf._lib.hash.hash
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:94: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43715 instead
  warnings.warn(
2023-05-01 05:41:19,264 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-01 05:41:19,304 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f9b7a30b6d0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 12 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
