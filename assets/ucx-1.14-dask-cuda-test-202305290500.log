============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-29 06:08:59,527 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:08:59,531 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-05-29 06:08:59,534 - distributed.scheduler - INFO - State start
2023-05-29 06:08:59,552 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:08:59,553 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-29 06:08:59,553 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-05-29 06:08:59,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40057'
2023-05-29 06:08:59,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42361'
2023-05-29 06:08:59,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44197'
2023-05-29 06:08:59,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45915'
2023-05-29 06:09:00,115 - distributed.scheduler - INFO - Receive client connection: Client-4c75cdf6-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:09:00,126 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58784
2023-05-29 06:09:01,166 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dob_f6tj', purging
2023-05-29 06:09:01,167 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0oraw5fy', purging
2023-05-29 06:09:01,167 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y3nu1ef6', purging
2023-05-29 06:09:01,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:01,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:01,174 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-29 06:09:01,185 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35059
2023-05-29 06:09:01,185 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35059
2023-05-29 06:09:01,185 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46031
2023-05-29 06:09:01,186 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-29 06:09:01,186 - distributed.worker - INFO - -------------------------------------------------
2023-05-29 06:09:01,186 - distributed.worker - INFO -               Threads:                          4
2023-05-29 06:09:01,186 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-29 06:09:01,186 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mvpt8bew
2023-05-29 06:09:01,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a06eb32f-5216-4388-b8f8-bbc945868114
2023-05-29 06:09:01,186 - distributed.worker - INFO - Starting Worker plugin PreImport-e80167d9-a415-4789-9179-b1a311973782
2023-05-29 06:09:01,187 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e0ea9a7-01d0-4e0d-ac56-0802a13233e7
2023-05-29 06:09:01,187 - distributed.worker - INFO - -------------------------------------------------
2023-05-29 06:09:01,200 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35059', status: init, memory: 0, processing: 0>
2023-05-29 06:09:01,201 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35059
2023-05-29 06:09:01,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58798
2023-05-29 06:09:01,201 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-29 06:09:01,202 - distributed.worker - INFO - -------------------------------------------------
2023-05-29 06:09:01,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-29 06:09:01,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:01,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:01,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:01,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:01,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:01,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:01,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:01,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:01,349 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:01,977 - distributed.nanny - INFO - Worker process 26293 exited with status 127
2023-05-29 06:09:01,978 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:02,113 - distributed.nanny - INFO - Worker process 26296 exited with status 127
2023-05-29 06:09:02,114 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:02,145 - distributed.nanny - INFO - Worker process 26300 exited with status 127
2023-05-29 06:09:02,146 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:03,473 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-emlhsem8', purging
2023-05-29 06:09:03,474 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o8c5xbow', purging
2023-05-29 06:09:03,474 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m89ci5b5', purging
2023-05-29 06:09:03,475 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:03,475 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:03,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:03,643 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:03,643 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:03,649 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:03,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:03,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:03,771 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:05,441 - distributed.nanny - INFO - Worker process 26334 exited with status 127
2023-05-29 06:09:05,442 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:05,460 - distributed.nanny - INFO - Worker process 26342 exited with status 127
2023-05-29 06:09:05,461 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:05,495 - distributed.nanny - INFO - Worker process 26339 exited with status 127
2023-05-29 06:09:05,496 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:06,917 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e_2lwgix', purging
2023-05-29 06:09:06,917 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ijtmiybv', purging
2023-05-29 06:09:06,918 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w9_mbg0i', purging
2023-05-29 06:09:06,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:06,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:06,925 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:07,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:07,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:07,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:07,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:07,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:07,132 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:08,111 - distributed.scheduler - INFO - Remove client Client-4c75cdf6-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:09:08,111 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58784; closing.
2023-05-29 06:09:08,111 - distributed.scheduler - INFO - Remove client Client-4c75cdf6-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:09:08,112 - distributed.scheduler - INFO - Close client connection: Client-4c75cdf6-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:09:08,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40057'. Reason: nanny-close
2023-05-29 06:09:08,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42361'. Reason: nanny-close
2023-05-29 06:09:08,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44197'. Reason: nanny-close
2023-05-29 06:09:08,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45915'. Reason: nanny-close
2023-05-29 06:09:08,114 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-29 06:09:08,116 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35059. Reason: nanny-close
2023-05-29 06:09:08,117 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58798; closing.
2023-05-29 06:09:08,117 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-29 06:09:08,118 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35059', status: closing, memory: 0, processing: 0>
2023-05-29 06:09:08,118 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35059
2023-05-29 06:09:08,118 - distributed.scheduler - INFO - Lost all workers
2023-05-29 06:09:08,119 - distributed.nanny - INFO - Worker closed
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:09,188 - distributed.nanny - INFO - Worker process 26369 exited with status 127
2023-05-29 06:09:09,270 - distributed.nanny - INFO - Worker process 26372 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:09,478 - distributed.nanny - INFO - Worker process 26366 exited with status 127
2023-05-29 06:09:38,145 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-29 06:09:38,146 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:09:38,147 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:09:38,148 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-29 06:09:38,148 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-29 06:09:40,329 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:09:40,333 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-05-29 06:09:40,336 - distributed.scheduler - INFO - State start
2023-05-29 06:09:40,354 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:09:40,355 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:09:40,355 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:09:40,356 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:09:40,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44997'
2023-05-29 06:09:40,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33241'
2023-05-29 06:09:40,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33245'
2023-05-29 06:09:40,511 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40259'
2023-05-29 06:09:40,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41929'
2023-05-29 06:09:40,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41665'
2023-05-29 06:09:40,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44489'
2023-05-29 06:09:40,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39039'
2023-05-29 06:09:41,995 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g9y93ztv', purging
2023-05-29 06:09:41,996 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-71sfgpxb', purging
2023-05-29 06:09:41,996 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ta3s8f82', purging
2023-05-29 06:09:41,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:41,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:42,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:42,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:42,020 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:42,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:42,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:42,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:42,040 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:42,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:42,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:42,046 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:42,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:42,067 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:42,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:42,080 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:42,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:42,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:42,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:42,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:42,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:42,116 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:42,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:42,140 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:43,839 - distributed.nanny - INFO - Worker process 26590 exited with status 127
2023-05-29 06:09:43,840 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:43,994 - distributed.nanny - INFO - Worker process 26566 exited with status 127
2023-05-29 06:09:43,994 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:44,063 - distributed.nanny - INFO - Worker process 26573 exited with status 127
2023-05-29 06:09:44,064 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:44,084 - distributed.nanny - INFO - Worker process 26587 exited with status 127
2023-05-29 06:09:44,085 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:44,108 - distributed.nanny - INFO - Worker process 26569 exited with status 127
2023-05-29 06:09:44,109 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:44,153 - distributed.nanny - INFO - Worker process 26581 exited with status 127
2023-05-29 06:09:44,154 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:44,178 - distributed.nanny - INFO - Worker process 26578 exited with status 127
2023-05-29 06:09:44,178 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:44,212 - distributed.nanny - INFO - Worker process 26584 exited with status 127
2023-05-29 06:09:44,213 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:45,330 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-02lra9aw', purging
2023-05-29 06:09:45,331 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ce04nsrb', purging
2023-05-29 06:09:45,331 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1qj6t6pd', purging
2023-05-29 06:09:45,332 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ay_c3089', purging
2023-05-29 06:09:45,332 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1vphv2ce', purging
2023-05-29 06:09:45,332 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3yaf9cav', purging
2023-05-29 06:09:45,333 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0i_g211t', purging
2023-05-29 06:09:45,333 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yhy_b3iv', purging
2023-05-29 06:09:45,333 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:45,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:45,357 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:45,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:45,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:45,397 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:45,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:45,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:45,598 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:45,599 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:45,619 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:45,619 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:45,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:45,632 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:45,654 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:45,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:45,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:45,695 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:45,695 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:45,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:45,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:45,732 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:45,733 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:45,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:45,997 - distributed.nanny - INFO - Worker process 26642 exited with status 127
2023-05-29 06:09:45,998 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:47,064 - distributed.nanny - INFO - Worker process 26649 exited with status 127
2023-05-29 06:09:47,065 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:47,476 - distributed.nanny - INFO - Worker process 26657 exited with status 127
2023-05-29 06:09:47,477 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:47,501 - distributed.nanny - INFO - Worker process 26654 exited with status 127
2023-05-29 06:09:47,502 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:47,540 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r2fc_kg9', purging
2023-05-29 06:09:47,540 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5sx1d262', purging
2023-05-29 06:09:47,541 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h6gubb_w', purging
2023-05-29 06:09:47,541 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-849m2qyv', purging
2023-05-29 06:09:47,541 - distributed.nanny - INFO - Worker process 26660 exited with status 127
2023-05-29 06:09:47,541 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yl5tc2ai', purging
2023-05-29 06:09:47,542 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:47,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:47,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:47,567 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:47,613 - distributed.nanny - INFO - Worker process 26664 exited with status 127
2023-05-29 06:09:47,614 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:47,656 - distributed.nanny - INFO - Worker process 26667 exited with status 127
2023-05-29 06:09:47,657 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:47,679 - distributed.nanny - INFO - Worker process 26670 exited with status 127
2023-05-29 06:09:47,680 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:48,061 - distributed.nanny - INFO - Worker process 26710 exited with status 127
2023-05-29 06:09:48,062 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:48,569 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-malvmxxf', purging
2023-05-29 06:09:48,570 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ycorr0vb', purging
2023-05-29 06:09:48,570 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yok2w1u9', purging
2023-05-29 06:09:48,571 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0ec5mw_5', purging
2023-05-29 06:09:48,571 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:48,571 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:48,594 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:48,973 - distributed.nanny - INFO - Worker process 26727 exited with status 127
2023-05-29 06:09:48,974 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:49,108 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ux9yz0rb', purging
2023-05-29 06:09:49,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:49,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:49,118 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:49,118 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:49,133 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:49,142 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:49,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:49,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:49,188 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:49,225 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:49,225 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:49,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:49,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:49,316 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:49,316 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:49,419 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:49,435 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:49,443 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:49,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:49,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:49,714 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:50,044 - distributed.nanny - INFO - Worker process 26733 exited with status 127
2023-05-29 06:09:50,045 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:50,491 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8d7xqlq1', purging
2023-05-29 06:09:50,491 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4ky1a7bz', purging
2023-05-29 06:09:50,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:50,492 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:50,649 - distributed.nanny - INFO - Worker process 26736 exited with status 127
2023-05-29 06:09:50,650 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:50,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:51,099 - distributed.nanny - INFO - Worker process 26740 exited with status 127
2023-05-29 06:09:51,100 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:51,175 - distributed.nanny - INFO - Worker process 26748 exited with status 127
2023-05-29 06:09:51,176 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:51,232 - distributed.nanny - INFO - Worker process 26751 exited with status 127
2023-05-29 06:09:51,233 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:51,257 - distributed.nanny - INFO - Worker process 26755 exited with status 127
2023-05-29 06:09:51,258 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:51,518 - distributed.nanny - INFO - Worker process 26761 exited with status 127
2023-05-29 06:09:51,519 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:51,583 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-06gcuyx5', purging
2023-05-29 06:09:51,583 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qvuv7ibs', purging
2023-05-29 06:09:51,584 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6265zkta', purging
2023-05-29 06:09:51,584 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-37dwr3n8', purging
2023-05-29 06:09:51,584 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rgctp2ep', purging
2023-05-29 06:09:51,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:51,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:51,638 - distributed.nanny - INFO - Worker process 26777 exited with status 127
2023-05-29 06:09:51,639 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:51,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:52,201 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b0w63a5u', purging
2023-05-29 06:09:52,201 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nbby6054', purging
2023-05-29 06:09:52,202 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:52,202 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:52,240 - distributed.nanny - INFO - Worker process 26809 exited with status 127
2023-05-29 06:09:52,241 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:52,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:52,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:52,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:52,868 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:52,868 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:52,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:52,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:52,946 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:52,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:53,211 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:53,211 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:53,290 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:53,290 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:53,539 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:53,709 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:53,716 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:53,719 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:53,728 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:53,767 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:53,887 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:53,887 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:54,031 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:54,934 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40259'. Reason: nanny-close
2023-05-29 06:09:54,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41929'. Reason: nanny-close
2023-05-29 06:09:54,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44997'. Reason: nanny-close
2023-05-29 06:09:54,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33241'. Reason: nanny-close
2023-05-29 06:09:54,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33245'. Reason: nanny-close
2023-05-29 06:09:54,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41665'. Reason: nanny-close
2023-05-29 06:09:54,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44489'. Reason: nanny-close
2023-05-29 06:09:54,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39039'. Reason: nanny-close
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:56,652 - distributed.nanny - INFO - Worker process 26820 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:56,787 - distributed.nanny - INFO - Worker process 26830 exited with status 127
2023-05-29 06:09:56,819 - distributed.nanny - INFO - Worker process 26835 exited with status 127
2023-05-29 06:09:56,843 - distributed.nanny - INFO - Worker process 26838 exited with status 127
2023-05-29 06:09:56,892 - distributed.nanny - INFO - Worker process 26842 exited with status 127
2023-05-29 06:09:56,915 - distributed.nanny - INFO - Worker process 26855 exited with status 127
2023-05-29 06:09:56,979 - distributed.nanny - INFO - Worker process 26867 exited with status 127
2023-05-29 06:09:57,006 - distributed.nanny - INFO - Worker process 26848 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-29 06:10:26,616 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:10:26,619 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40411 instead
  warnings.warn(
2023-05-29 06:10:26,623 - distributed.scheduler - INFO - State start
2023-05-29 06:10:26,641 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:10:26,641 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:10:26,642 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:10:26,642 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:10:26,804 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43827'
2023-05-29 06:10:26,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35289'
2023-05-29 06:10:26,828 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40737'
2023-05-29 06:10:26,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38737'
2023-05-29 06:10:26,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42345'
2023-05-29 06:10:26,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41467'
2023-05-29 06:10:26,853 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42751'
2023-05-29 06:10:26,861 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40223'
2023-05-29 06:10:28,272 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-36ny7z4k', purging
2023-05-29 06:10:28,273 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8n_h8xb2', purging
2023-05-29 06:10:28,273 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d3aftb5k', purging
2023-05-29 06:10:28,273 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bv_y3cc8', purging
2023-05-29 06:10:28,274 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pvgo06ck', purging
2023-05-29 06:10:28,274 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5fkm7uzc', purging
2023-05-29 06:10:28,274 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2zc87bye', purging
2023-05-29 06:10:28,275 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-89vxsm2_', purging
2023-05-29 06:10:28,275 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:28,275 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:28,299 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:28,323 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:28,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:28,347 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:28,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:28,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:28,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:28,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:28,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:28,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:28,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:28,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:28,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:28,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:28,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:28,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:28,548 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:28,556 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:28,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:28,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:28,559 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:28,561 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:28,962 - distributed.nanny - INFO - Worker process 27107 exited with status 127
2023-05-29 06:10:28,963 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:30,289 - distributed.nanny - INFO - Worker process 27093 exited with status 127
2023-05-29 06:10:30,291 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:30,412 - distributed.nanny - INFO - Worker process 27097 exited with status 127
2023-05-29 06:10:30,413 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:30,414 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_s0_aks9', purging
2023-05-29 06:10:30,415 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2kult7hi', purging
2023-05-29 06:10:30,415 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f5y3w8zo', purging
2023-05-29 06:10:30,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:30,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:30,478 - distributed.nanny - INFO - Worker process 27086 exited with status 127
2023-05-29 06:10:30,478 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:30,501 - distributed.nanny - INFO - Worker process 27101 exited with status 127
2023-05-29 06:10:30,502 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:30,525 - distributed.nanny - INFO - Worker process 27110 exited with status 127
2023-05-29 06:10:30,526 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:30,552 - distributed.nanny - INFO - Worker process 27105 exited with status 127
2023-05-29 06:10:30,553 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:30,576 - distributed.nanny - INFO - Worker process 27089 exited with status 127
2023-05-29 06:10:30,577 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:30,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:30,943 - distributed.nanny - INFO - Worker process 27150 exited with status 127
2023-05-29 06:10:30,944 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:31,739 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yd326_p5', purging
2023-05-29 06:10:31,739 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rm6dr5x4', purging
2023-05-29 06:10:31,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ujxdb0j9', purging
2023-05-29 06:10:31,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-44bt99wq', purging
2023-05-29 06:10:31,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ky3pss05', purging
2023-05-29 06:10:31,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mo8x45z9', purging
2023-05-29 06:10:31,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:31,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:31,764 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:31,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:31,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:32,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:32,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:32,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:32,063 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:32,081 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:32,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:32,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:32,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:32,107 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:32,107 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:32,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:32,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:32,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:32,138 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:32,144 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:32,159 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:32,373 - distributed.nanny - INFO - Worker process 27167 exited with status 127
2023-05-29 06:10:32,374 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:32,450 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-21renczk', purging
2023-05-29 06:10:32,451 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:32,451 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:32,661 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:33,463 - distributed.nanny - INFO - Worker process 27177 exited with status 127
2023-05-29 06:10:33,464 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:33,714 - distributed.nanny - INFO - Worker process 27180 exited with status 127
2023-05-29 06:10:33,715 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:33,841 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-eayd7thi', purging
2023-05-29 06:10:33,842 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-temsm43y', purging
2023-05-29 06:10:33,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:33,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:33,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:33,948 - distributed.nanny - INFO - Worker process 27186 exited with status 127
2023-05-29 06:10:33,948 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:34,014 - distributed.nanny - INFO - Worker process 27183 exited with status 127
2023-05-29 06:10:34,015 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:34,039 - distributed.nanny - INFO - Worker process 27189 exited with status 127
2023-05-29 06:10:34,040 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:34,098 - distributed.nanny - INFO - Worker process 27194 exited with status 127
2023-05-29 06:10:34,099 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:34,346 - distributed.nanny - INFO - Worker process 27201 exited with status 127
2023-05-29 06:10:34,347 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:34,453 - distributed.nanny - INFO - Worker process 27237 exited with status 127
2023-05-29 06:10:34,454 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:35,093 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_0la9n1n', purging
2023-05-29 06:10:35,093 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1jeawjnj', purging
2023-05-29 06:10:35,094 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ik3y7qda', purging
2023-05-29 06:10:35,094 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sdonepp2', purging
2023-05-29 06:10:35,094 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-188st004', purging
2023-05-29 06:10:35,095 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z0v354sc', purging
2023-05-29 06:10:35,095 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:35,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:35,120 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:35,290 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:35,290 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:35,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:35,430 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:35,430 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:35,454 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:35,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:35,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:35,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:35,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:35,664 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:35,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:35,697 - distributed.nanny - INFO - Worker process 27255 exited with status 127
2023-05-29 06:10:35,698 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:35,734 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mo0xr_fc', purging
2023-05-29 06:10:35,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:35,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:35,772 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:35,953 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:35,953 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:35,997 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:36,071 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gpbhf6jb', purging
2023-05-29 06:10:36,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:36,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:36,083 - distributed.nanny - INFO - Worker process 27262 exited with status 127
2023-05-29 06:10:36,084 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:36,122 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:36,871 - distributed.nanny - INFO - Worker process 27270 exited with status 127
2023-05-29 06:10:36,872 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:37,264 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1d5edupt', purging
2023-05-29 06:10:37,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:37,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:37,378 - distributed.nanny - INFO - Worker process 27274 exited with status 127
2023-05-29 06:10:37,379 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:37,390 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:37,432 - distributed.nanny - INFO - Worker process 27277 exited with status 127
2023-05-29 06:10:37,433 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:37,525 - distributed.nanny - INFO - Worker process 27281 exited with status 127
2023-05-29 06:10:37,526 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:37,568 - distributed.nanny - INFO - Worker process 27288 exited with status 127
2023-05-29 06:10:37,569 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:37,769 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rk79yvc5', purging
2023-05-29 06:10:37,769 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_jm7ff6y', purging
2023-05-29 06:10:37,770 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ehidtvaf', purging
2023-05-29 06:10:37,770 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hpqnbg93', purging
2023-05-29 06:10:37,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:37,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:37,804 - distributed.nanny - INFO - Worker process 27292 exited with status 127
2023-05-29 06:10:37,805 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:37,826 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:38,132 - distributed.nanny - INFO - Worker process 27321 exited with status 127
2023-05-29 06:10:38,133 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:38,290 - distributed.nanny - INFO - Worker process 27334 exited with status 127
2023-05-29 06:10:38,291 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:38,526 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-41vncx_d', purging
2023-05-29 06:10:38,527 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kscnh16g', purging
2023-05-29 06:10:38,527 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s2dyxfq1', purging
2023-05-29 06:10:38,528 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:38,528 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:38,554 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:38,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:38,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:38,867 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:38,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:38,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:38,988 - distributed.nanny - INFO - Worker process 27348 exited with status 127
2023-05-29 06:10:38,989 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:39,155 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:39,197 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-djpdwctl', purging
2023-05-29 06:10:39,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:39,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:39,224 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:39,228 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:39,228 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:39,260 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:39,495 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4_4oxyok', purging
2023-05-29 06:10:39,495 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:39,496 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:39,498 - distributed.nanny - INFO - Worker process 27360 exited with status 127
2023-05-29 06:10:39,499 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:39,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:39,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:39,882 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:39,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:39,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:40,027 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:40,037 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:40,095 - distributed.nanny - INFO - Worker process 27363 exited with status 127
2023-05-29 06:10:40,096 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:40,506 - distributed.nanny - INFO - Worker process 27369 exited with status 127
2023-05-29 06:10:40,507 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:40,634 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2k76bsok', purging
2023-05-29 06:10:40,634 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vra2o582', purging
2023-05-29 06:10:40,635 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:40,635 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:40,692 - distributed.nanny - INFO - Worker process 27372 exited with status 127
2023-05-29 06:10:40,693 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:40,741 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:41,147 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6zf0d4y9', purging
2023-05-29 06:10:41,148 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:41,148 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:41,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35289'. Reason: nanny-close
2023-05-29 06:10:41,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42345'. Reason: nanny-close
2023-05-29 06:10:41,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43827'. Reason: nanny-close
2023-05-29 06:10:41,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40737'. Reason: nanny-close
2023-05-29 06:10:41,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38737'. Reason: nanny-close
2023-05-29 06:10:41,295 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41467'. Reason: nanny-close
2023-05-29 06:10:41,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42751'. Reason: nanny-close
2023-05-29 06:10:41,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40223'. Reason: nanny-close
2023-05-29 06:10:41,507 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:41,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:41,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:42,076 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:42,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:42,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j476rafy', purging
2023-05-29 06:10:42,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:42,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:42,498 - distributed.nanny - INFO - Worker process 27379 exited with status 127
2023-05-29 06:10:42,536 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:42,544 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:42,621 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:43,689 - distributed.nanny - INFO - Worker process 27389 exited with status 127
2023-05-29 06:10:43,795 - distributed.nanny - INFO - Worker process 27395 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:44,670 - distributed.nanny - INFO - Worker process 27413 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:44,817 - distributed.nanny - INFO - Worker process 27428 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:44,863 - distributed.nanny - INFO - Worker process 27458 exited with status 127
2023-05-29 06:10:44,906 - distributed.nanny - INFO - Worker process 27445 exited with status 127
2023-05-29 06:10:44,936 - distributed.nanny - INFO - Worker process 27452 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-29 06:11:13,134 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:11:13,137 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40441 instead
  warnings.warn(
2023-05-29 06:11:13,141 - distributed.scheduler - INFO - State start
2023-05-29 06:11:13,160 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:11:13,160 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:11:13,161 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:11:13,161 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:11:13,310 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34061'
2023-05-29 06:11:13,325 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35677'
2023-05-29 06:11:13,344 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46879'
2023-05-29 06:11:13,346 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39499'
2023-05-29 06:11:13,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36639'
2023-05-29 06:11:13,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34831'
2023-05-29 06:11:13,369 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37793'
2023-05-29 06:11:13,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45307'
2023-05-29 06:11:14,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1y6u2iki', purging
2023-05-29 06:11:14,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pyh3l7ba', purging
2023-05-29 06:11:14,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s5lhg_i6', purging
2023-05-29 06:11:14,755 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gn2xpx3o', purging
2023-05-29 06:11:14,755 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lggl_asx', purging
2023-05-29 06:11:14,755 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gfsb8axs', purging
2023-05-29 06:11:14,756 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-elsz2pxw', purging
2023-05-29 06:11:14,756 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:14,756 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:14,781 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:14,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:14,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:14,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:14,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:14,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:14,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:14,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:14,885 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:14,885 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:14,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:14,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:14,894 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:14,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:14,900 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:14,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:15,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:15,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:15,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:15,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:15,071 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:15,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:16,263 - distributed.nanny - INFO - Worker process 27666 exited with status 127
2023-05-29 06:11:16,265 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:16,850 - distributed.nanny - INFO - Worker process 27673 exited with status 127
2023-05-29 06:11:16,851 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:16,885 - distributed.nanny - INFO - Worker process 27678 exited with status 127
2023-05-29 06:11:16,886 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:16,938 - distributed.nanny - INFO - Worker process 27681 exited with status 127
2023-05-29 06:11:16,938 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:16,961 - distributed.nanny - INFO - Worker process 27690 exited with status 127
2023-05-29 06:11:16,962 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:16,985 - distributed.nanny - INFO - Worker process 27687 exited with status 127
2023-05-29 06:11:16,985 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:17,009 - distributed.nanny - INFO - Worker process 27684 exited with status 127
2023-05-29 06:11:17,010 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:17,041 - distributed.nanny - INFO - Worker process 27669 exited with status 127
2023-05-29 06:11:17,042 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:17,692 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ux9qxwig', purging
2023-05-29 06:11:17,692 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o2od04aj', purging
2023-05-29 06:11:17,692 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9cncbz1y', purging
2023-05-29 06:11:17,693 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oh299v39', purging
2023-05-29 06:11:17,693 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0hxzfgpi', purging
2023-05-29 06:11:17,693 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fnre7j14', purging
2023-05-29 06:11:17,693 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_zhww7sd', purging
2023-05-29 06:11:17,694 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vgkpqfpx', purging
2023-05-29 06:11:17,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:17,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:17,717 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:18,114 - distributed.nanny - INFO - Worker process 27740 exited with status 127
2023-05-29 06:11:18,114 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:18,362 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hdkjq77i', purging
2023-05-29 06:11:18,363 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:18,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:18,370 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:18,371 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:18,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:18,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:18,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:18,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:18,504 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:18,504 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:18,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:18,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:18,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:18,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:18,595 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:18,596 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:18,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:18,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:18,802 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:18,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:18,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:19,597 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5vzjqml3', purging
2023-05-29 06:11:19,597 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k5bo4ml6', purging
2023-05-29 06:11:19,598 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:19,598 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:19,746 - distributed.nanny - INFO - Worker process 27755 exited with status 127
2023-05-29 06:11:19,747 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:19,773 - distributed.nanny - INFO - Worker process 27752 exited with status 127
2023-05-29 06:11:19,774 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:19,974 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:20,282 - distributed.nanny - INFO - Worker process 27759 exited with status 127
2023-05-29 06:11:20,283 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:20,349 - distributed.nanny - INFO - Worker process 27762 exited with status 127
2023-05-29 06:11:20,350 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:20,373 - distributed.nanny - INFO - Worker process 27771 exited with status 127
2023-05-29 06:11:20,374 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:20,402 - distributed.nanny - INFO - Worker process 27765 exited with status 127
2023-05-29 06:11:20,402 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:20,428 - distributed.nanny - INFO - Worker process 27768 exited with status 127
2023-05-29 06:11:20,429 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:20,762 - distributed.nanny - INFO - Worker process 27787 exited with status 127
2023-05-29 06:11:20,763 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:21,322 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-se_ab_rg', purging
2023-05-29 06:11:21,323 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g7qutn4s', purging
2023-05-29 06:11:21,323 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8ph1lrxb', purging
2023-05-29 06:11:21,323 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7cg7zwa9', purging
2023-05-29 06:11:21,324 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wwkuc3yh', purging
2023-05-29 06:11:21,324 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uv_84wcf', purging
2023-05-29 06:11:21,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:21,325 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:21,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:21,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:21,347 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:21,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:21,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:21,792 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:21,830 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:21,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:21,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:21,877 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:21,943 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jju96ww_', purging
2023-05-29 06:11:21,943 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:21,943 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:21,966 - distributed.nanny - INFO - Worker process 27829 exited with status 127
2023-05-29 06:11:21,966 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:21,983 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:21,983 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:21,983 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:21,984 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:22,156 - distributed.nanny - INFO - Worker process 27832 exited with status 127
2023-05-29 06:11:22,157 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:22,161 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:22,161 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:22,162 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:22,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2l4n_akd', purging
2023-05-29 06:11:22,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:22,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:22,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:22,571 - distributed.nanny - INFO - Worker process 27852 exited with status 127
2023-05-29 06:11:22,572 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:23,367 - distributed.nanny - INFO - Worker process 27842 exited with status 127
2023-05-29 06:11:23,368 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:23,407 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4yzr3ac0', purging
2023-05-29 06:11:23,407 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-djrii9a2', purging
2023-05-29 06:11:23,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:23,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:23,464 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:23,632 - distributed.nanny - INFO - Worker process 27858 exited with status 127
2023-05-29 06:11:23,633 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:23,659 - distributed.nanny - INFO - Worker process 27847 exited with status 127
2023-05-29 06:11:23,660 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:23,690 - distributed.nanny - INFO - Worker process 27855 exited with status 127
2023-05-29 06:11:23,691 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:23,728 - distributed.nanny - INFO - Worker process 27862 exited with status 127
2023-05-29 06:11:23,729 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:23,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5srdfvnk', purging
2023-05-29 06:11:23,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-od816i5g', purging
2023-05-29 06:11:23,732 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-iwyus9i4', purging
2023-05-29 06:11:23,732 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0j0xteau', purging
2023-05-29 06:11:23,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:23,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:23,883 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:24,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:24,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:24,225 - distributed.nanny - INFO - Worker process 27892 exited with status 127
2023-05-29 06:11:24,226 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:24,232 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:24,501 - distributed.nanny - INFO - Worker process 27903 exited with status 127
2023-05-29 06:11:24,502 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:24,659 - distributed.nanny - INFO - Worker process 27913 exited with status 127
2023-05-29 06:11:24,660 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:24,956 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fqdt8gvf', purging
2023-05-29 06:11:24,956 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9phr83o5', purging
2023-05-29 06:11:24,956 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6j135aup', purging
2023-05-29 06:11:24,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:24,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:24,982 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:25,216 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:25,216 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:25,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:25,220 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:25,226 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:25,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:25,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:25,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:25,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:25,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:25,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:25,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:25,794 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-awixl1j2', purging
2023-05-29 06:11:25,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:25,795 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:25,965 - distributed.nanny - INFO - Worker process 27927 exited with status 127
2023-05-29 06:11:25,966 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:25,996 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:26,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:26,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:26,191 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:26,191 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:26,262 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:26,275 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:26,534 - distributed.nanny - INFO - Worker process 27940 exited with status 127
2023-05-29 06:11:26,535 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:26,574 - distributed.nanny - INFO - Worker process 27949 exited with status 127
2023-05-29 06:11:26,575 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:26,779 - distributed.nanny - INFO - Worker process 27937 exited with status 127
2023-05-29 06:11:26,780 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:26,976 - distributed.nanny - INFO - Worker process 27943 exited with status 127
2023-05-29 06:11:26,977 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:27,360 - distributed.nanny - INFO - Worker process 27961 exited with status 127
2023-05-29 06:11:27,361 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:27,399 - distributed.nanny - INFO - Worker process 27971 exited with status 127
2023-05-29 06:11:27,400 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:27,429 - distributed.nanny - INFO - Worker process 27975 exited with status 127
2023-05-29 06:11:27,430 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:27,563 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nyqnvyo4', purging
2023-05-29 06:11:27,563 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8k18ybkq', purging
2023-05-29 06:11:27,564 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-97z_ahja', purging
2023-05-29 06:11:27,564 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u8zvyn09', purging
2023-05-29 06:11:27,564 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lsz1anl1', purging
2023-05-29 06:11:27,565 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4b95nt9i', purging
2023-05-29 06:11:27,565 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jsdfq2gh', purging
2023-05-29 06:11:27,565 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:27,566 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:27,590 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:28,015 - distributed.nanny - INFO - Worker process 28006 exited with status 127
2023-05-29 06:11:28,016 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:28,106 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7fb3733d5eb0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1318, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1348, in _ensure_connected
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-05-29 06:11:28,110 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39499'. Reason: nanny-close
2023-05-29 06:11:28,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36639'. Reason: nanny-close
2023-05-29 06:11:28,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34061'. Reason: nanny-close
2023-05-29 06:11:28,111 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35677'. Reason: nanny-close
2023-05-29 06:11:28,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46879'. Reason: nanny-close
2023-05-29 06:11:28,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34831'. Reason: nanny-close
2023-05-29 06:11:28,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37793'. Reason: nanny-close
2023-05-29 06:11:28,112 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45307'. Reason: nanny-close
2023-05-29 06:11:28,134 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9igmhkfp', purging
2023-05-29 06:11:28,135 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:28,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:28,159 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:28,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:28,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:28,208 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:28,421 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:28,421 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:28,455 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:28,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:28,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:28,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:28,786 - distributed.nanny - INFO - Worker process 28022 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:29,024 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jd97pc4z', purging
2023-05-29 06:11:29,025 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:29,025 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:29,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:29,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:29,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:29,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:29,237 - distributed.nanny - INFO - Worker process 28026 exited with status 127
2023-05-29 06:11:29,242 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:29,289 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:29,293 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:29,506 - distributed.nanny - INFO - Worker process 28033 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:29,750 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h9372cp4', purging
2023-05-29 06:11:29,750 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y_asdyl_', purging
2023-05-29 06:11:29,751 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:29,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:29,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:31,645 - distributed.nanny - INFO - Worker process 28039 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:32,546 - distributed.nanny - INFO - Worker process 28047 exited with status 127
2023-05-29 06:11:32,613 - distributed.nanny - INFO - Worker process 28051 exited with status 127
2023-05-29 06:11:32,855 - distributed.nanny - INFO - Worker process 28054 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:33,021 - distributed.nanny - INFO - Worker process 28065 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-29 06:11:59,841 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:11:59,845 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38859 instead
  warnings.warn(
2023-05-29 06:11:59,848 - distributed.scheduler - INFO - State start
2023-05-29 06:11:59,866 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:11:59,867 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:11:59,867 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:11:59,868 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:12:00,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38145'
2023-05-29 06:12:00,038 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39601'
2023-05-29 06:12:00,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34227'
2023-05-29 06:12:00,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36369'
2023-05-29 06:12:00,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44943'
2023-05-29 06:12:00,068 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37051'
2023-05-29 06:12:00,076 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36749'
2023-05-29 06:12:00,084 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43919'
2023-05-29 06:12:01,468 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ja9hx8wl', purging
2023-05-29 06:12:01,468 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uru29usu', purging
2023-05-29 06:12:01,468 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r1hn1os7', purging
2023-05-29 06:12:01,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-putdtj_g', purging
2023-05-29 06:12:01,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zw3toexb', purging
2023-05-29 06:12:01,469 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:01,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:01,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:01,565 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:01,565 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:01,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:01,589 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:01,589 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:01,589 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:01,589 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:01,590 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:01,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:01,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:01,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:01,621 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:01,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:01,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:01,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:01,756 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:01,756 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:01,757 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:01,784 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:01,785 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:01,792 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:02,228 - distributed.nanny - INFO - Worker process 28289 exited with status 127
2023-05-29 06:12:02,229 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:03,611 - distributed.nanny - INFO - Worker process 28301 exited with status 127
2023-05-29 06:12:03,612 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:03,680 - distributed.nanny - INFO - Worker process 28297 exited with status 127
2023-05-29 06:12:03,680 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:03,707 - distributed.nanny - INFO - Worker process 28293 exited with status 127
2023-05-29 06:12:03,708 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:03,729 - distributed.nanny - INFO - Worker process 28305 exited with status 127
2023-05-29 06:12:03,730 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:03,742 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-461m81xo', purging
2023-05-29 06:12:03,743 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-thpdhjb6', purging
2023-05-29 06:12:03,743 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-leajwdwi', purging
2023-05-29 06:12:03,743 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jkym6yom', purging
2023-05-29 06:12:03,744 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oysry60e', purging
2023-05-29 06:12:03,744 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j75ln4ae', purging
2023-05-29 06:12:03,744 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-78l31y8k', purging
2023-05-29 06:12:03,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:03,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:03,756 - distributed.nanny - INFO - Worker process 28310 exited with status 127
2023-05-29 06:12:03,757 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:03,780 - distributed.nanny - INFO - Worker process 28307 exited with status 127
2023-05-29 06:12:03,781 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:03,796 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:03,852 - distributed.nanny - INFO - Worker process 28286 exited with status 127
2023-05-29 06:12:03,853 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:04,181 - distributed.nanny - INFO - Worker process 28355 exited with status 127
2023-05-29 06:12:04,182 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:05,186 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-re5wzhzn', purging
2023-05-29 06:12:05,187 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yayn84h7', purging
2023-05-29 06:12:05,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:05,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:05,212 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:05,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:05,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:05,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:05,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:05,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:05,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:05,289 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:05,289 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:05,291 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:05,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:05,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:05,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:05,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:05,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:05,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:05,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:05,497 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:05,503 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:05,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:05,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:06,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:06,676 - distributed.nanny - INFO - Worker process 28372 exited with status 127
2023-05-29 06:12:06,677 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:07,069 - distributed.nanny - INFO - Worker process 28381 exited with status 127
2023-05-29 06:12:07,070 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:07,093 - distributed.nanny - INFO - Worker process 28375 exited with status 127
2023-05-29 06:12:07,094 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:07,116 - distributed.nanny - INFO - Worker process 28385 exited with status 127
2023-05-29 06:12:07,117 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:07,141 - distributed.nanny - INFO - Worker process 28378 exited with status 127
2023-05-29 06:12:07,141 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:07,325 - distributed.nanny - INFO - Worker process 28394 exited with status 127
2023-05-29 06:12:07,326 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:07,350 - distributed.nanny - INFO - Worker process 28388 exited with status 127
2023-05-29 06:12:07,350 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:07,488 - distributed.nanny - INFO - Worker process 28400 exited with status 127
2023-05-29 06:12:07,489 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:08,226 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f_oycsgx', purging
2023-05-29 06:12:08,226 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9c4uec0b', purging
2023-05-29 06:12:08,226 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_wqghirw', purging
2023-05-29 06:12:08,227 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nby7_3yc', purging
2023-05-29 06:12:08,227 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e4eoh75o', purging
2023-05-29 06:12:08,227 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f31n9c46', purging
2023-05-29 06:12:08,228 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_oy4t6nx', purging
2023-05-29 06:12:08,228 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r_dqmcjo', purging
2023-05-29 06:12:08,229 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:08,229 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:08,252 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:08,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:08,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:08,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:08,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:08,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:08,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:08,635 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:08,636 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:08,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:08,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:08,672 - distributed.nanny - INFO - Worker process 28450 exited with status 127
2023-05-29 06:12:08,673 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:08,679 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:08,690 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:08,767 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b7vbxsaw', purging
2023-05-29 06:12:08,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:08,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:08,801 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:08,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:08,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:08,995 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:08,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:09,142 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:09,162 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:09,968 - distributed.nanny - INFO - Worker process 28461 exited with status 127
2023-05-29 06:12:09,969 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:09,996 - distributed.nanny - INFO - Worker process 28464 exited with status 127
2023-05-29 06:12:09,997 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:10,133 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4mlklp9k', purging
2023-05-29 06:12:10,133 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qk8azthc', purging
2023-05-29 06:12:10,134 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tresx8q7', purging
2023-05-29 06:12:10,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:10,134 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:10,340 - distributed.nanny - INFO - Worker process 28467 exited with status 127
2023-05-29 06:12:10,341 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:10,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:10,418 - distributed.nanny - INFO - Worker process 28470 exited with status 127
2023-05-29 06:12:10,418 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:10,788 - distributed.nanny - INFO - Worker process 28477 exited with status 127
2023-05-29 06:12:10,789 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:10,826 - distributed.nanny - INFO - Worker process 28481 exited with status 127
2023-05-29 06:12:10,827 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:10,857 - distributed.nanny - INFO - Worker process 28474 exited with status 127
2023-05-29 06:12:10,857 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:10,997 - distributed.nanny - INFO - Worker process 28507 exited with status 127
2023-05-29 06:12:10,998 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:11,510 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-03bn8ges', purging
2023-05-29 06:12:11,510 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-68zy2r2s', purging
2023-05-29 06:12:11,511 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lxd1hc98', purging
2023-05-29 06:12:11,511 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ju0u11by', purging
2023-05-29 06:12:11,512 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s4fyiuno', purging
2023-05-29 06:12:11,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:11,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:11,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:11,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:11,540 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:11,541 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:11,856 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:11,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:11,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:11,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:12,009 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:12,011 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:12,278 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:12,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:12,297 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:12,297 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:12,332 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:12,332 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:12,498 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tq5d8byx', purging
2023-05-29 06:12:12,499 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-try5hbox', purging
2023-05-29 06:12:12,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:12,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:12,562 - distributed.nanny - INFO - Worker process 28538 exited with status 127
2023-05-29 06:12:12,563 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:12,593 - distributed.nanny - INFO - Worker process 28535 exited with status 127
2023-05-29 06:12:12,595 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:12,600 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:12,600 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:12,605 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:12,620 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:13,506 - distributed.nanny - INFO - Worker process 28547 exited with status 127
2023-05-29 06:12:13,507 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:13,532 - distributed.nanny - INFO - Worker process 28552 exited with status 127
2023-05-29 06:12:13,532 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:13,815 - distributed.nanny - INFO - Worker process 28565 exited with status 127
2023-05-29 06:12:13,815 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:13,864 - distributed.nanny - INFO - Worker process 28574 exited with status 127
2023-05-29 06:12:13,865 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:13,892 - distributed.nanny - INFO - Worker process 28562 exited with status 127
2023-05-29 06:12:13,893 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:13,915 - distributed.nanny - INFO - Worker process 28568 exited with status 127
2023-05-29 06:12:13,916 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:14,080 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ivhqyytz', purging
2023-05-29 06:12:14,081 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m218kyzu', purging
2023-05-29 06:12:14,081 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fxic_gas', purging
2023-05-29 06:12:14,081 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nmpgf6i0', purging
2023-05-29 06:12:14,082 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n72ygdji', purging
2023-05-29 06:12:14,082 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w0cbbrep', purging
2023-05-29 06:12:14,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:14,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:14,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:14,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:14,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:14,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:14,789 - distributed.nanny - INFO - Worker process 28614 exited with status 127
2023-05-29 06:12:14,790 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:14,818 - distributed.nanny - INFO - Worker process 28607 exited with status 127
2023-05-29 06:12:14,819 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:15,120 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1ttltzbx', purging
2023-05-29 06:12:15,120 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-brwd8vn2', purging
2023-05-29 06:12:15,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:15,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:15,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:15,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:15,146 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:15,154 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:15,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:15,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:15,499 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:15,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:15,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:15,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:15,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:15,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:15,621 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:15,623 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:15,623 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:15,660 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:15,879 - distributed.nanny - INFO - Worker process 28633 exited with status 127
2023-05-29 06:12:15,880 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:16,430 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-su2b7cvb', purging
2023-05-29 06:12:16,431 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n9zmzk9p', purging
2023-05-29 06:12:16,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:16,432 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:16,453 - distributed.nanny - INFO - Worker process 28636 exited with status 127
2023-05-29 06:12:16,453 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:16,510 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:16,511 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:16,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:16,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:16,729 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7fb3733f23d0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1318, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1348, in _ensure_connected
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-05-29 06:12:16,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34227'. Reason: nanny-close
2023-05-29 06:12:16,732 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36369'. Reason: nanny-close
2023-05-29 06:12:16,732 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36749'. Reason: nanny-close
2023-05-29 06:12:16,732 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38145'. Reason: nanny-close
2023-05-29 06:12:16,732 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39601'. Reason: nanny-close
2023-05-29 06:12:16,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44943'. Reason: nanny-close
2023-05-29 06:12:16,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37051'. Reason: nanny-close
2023-05-29 06:12:16,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43919'. Reason: nanny-close
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:16,962 - distributed.nanny - INFO - Worker process 28646 exited with status 127
2023-05-29 06:12:17,176 - distributed.nanny - INFO - Worker process 28643 exited with status 127
2023-05-29 06:12:17,199 - distributed.nanny - INFO - Worker process 28652 exited with status 127
2023-05-29 06:12:17,221 - distributed.nanny - INFO - Worker process 28649 exited with status 127
2023-05-29 06:12:17,482 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3uyf8k5g', purging
2023-05-29 06:12:17,482 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lxlkcep3', purging
2023-05-29 06:12:17,483 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-guhrcffn', purging
2023-05-29 06:12:17,483 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zw85dyoy', purging
2023-05-29 06:12:17,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:17,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:17,511 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:17,625 - distributed.nanny - INFO - Worker process 28676 exited with status 127
2023-05-29 06:12:17,652 - distributed.nanny - INFO - Worker process 28673 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:18,017 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9_fhqjf1', purging
2023-05-29 06:12:18,018 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5q49jim5', purging
2023-05-29 06:12:18,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:18,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:18,118 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:12:18,140 - distributed.nanny - INFO - Worker process 28707 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:18,555 - distributed.nanny - INFO - Worker process 28717 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-29 06:12:48,554 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:12:48,558 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34345 instead
  warnings.warn(
2023-05-29 06:12:48,561 - distributed.scheduler - INFO - State start
2023-05-29 06:12:48,579 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:12:48,580 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:12:48,580 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:12:48,581 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:12:48,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36629'
2023-05-29 06:12:49,957 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3n42xe8y', purging
2023-05-29 06:12:49,958 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-atfhbk5b', purging
2023-05-29 06:12:49,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:49,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:50,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:50,579 - distributed.nanny - INFO - Worker process 28916 exited with status 127
2023-05-29 06:12:50,580 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:51,908 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ff72zjrq', purging
2023-05-29 06:12:51,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:51,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:52,162 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:52,543 - distributed.nanny - INFO - Worker process 28926 exited with status 127
2023-05-29 06:12:52,544 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:53,850 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wum7zanw', purging
2023-05-29 06:12:53,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:53,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:54,103 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:54,471 - distributed.nanny - INFO - Worker process 28936 exited with status 127
2023-05-29 06:12:54,472 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:55,793 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-grm7tgz0', purging
2023-05-29 06:12:55,794 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:55,794 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:56,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:56,453 - distributed.nanny - INFO - Worker process 28946 exited with status 127
2023-05-29 06:12:56,453 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:57,783 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-85pnzg2o', purging
2023-05-29 06:12:57,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:57,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:58,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:58,611 - distributed.nanny - INFO - Worker process 28956 exited with status 127
2023-05-29 06:12:58,611 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:59,959 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3ofzmsg7', purging
2023-05-29 06:12:59,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:59,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:00,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:00,618 - distributed.nanny - INFO - Worker process 28966 exited with status 127
2023-05-29 06:13:00,619 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:01,993 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qczktse0', purging
2023-05-29 06:13:01,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:01,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:02,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:02,664 - distributed.nanny - INFO - Worker process 28976 exited with status 127
2023-05-29 06:13:02,665 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:04,004 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4x4ln5pz', purging
2023-05-29 06:13:04,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:04,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:04,263 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:04,650 - distributed.nanny - INFO - Worker process 28986 exited with status 127
2023-05-29 06:13:04,651 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:06,026 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b_dhxbfu', purging
2023-05-29 06:13:06,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:06,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:06,291 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:06,684 - distributed.nanny - INFO - Worker process 28996 exited with status 127
2023-05-29 06:13:06,685 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:06,918 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36629'. Reason: nanny-close
2023-05-29 06:13:08,026 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r4kmpbef', purging
2023-05-29 06:13:08,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:08,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:08,284 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:08,683 - distributed.nanny - INFO - Worker process 29006 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-29 06:13:40,301 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:13:40,306 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-05-29 06:13:40,309 - distributed.scheduler - INFO - State start
2023-05-29 06:13:40,332 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:13:40,334 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-29 06:13:40,335 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-05-29 06:13:40,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37697'
2023-05-29 06:13:40,348 - distributed.scheduler - INFO - Receive client connection: Client-f3c9bfe8-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:40,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38646
2023-05-29 06:13:41,877 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p1nbaoic', purging
2023-05-29 06:13:41,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:41,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:42,149 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:42,588 - distributed.nanny - INFO - Worker process 29265 exited with status 127
2023-05-29 06:13:42,589 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:43,951 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6bzd8zk1', purging
2023-05-29 06:13:43,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:43,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:44,218 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:44,783 - distributed.nanny - INFO - Worker process 29275 exited with status 127
2023-05-29 06:13:44,784 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:46,184 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_yj2frsi', purging
2023-05-29 06:13:46,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:46,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:46,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:46,969 - distributed.nanny - INFO - Worker process 29285 exited with status 127
2023-05-29 06:13:46,970 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:48,380 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dexrx82v', purging
2023-05-29 06:13:48,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:48,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:48,654 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:13:48,794 - distributed.scheduler - INFO - Remove client Client-f3c9bfe8-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:48,794 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38646; closing.
2023-05-29 06:13:48,794 - distributed.scheduler - INFO - Remove client Client-f3c9bfe8-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:48,795 - distributed.scheduler - INFO - Close client connection: Client-f3c9bfe8-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:48,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37697'. Reason: nanny-close
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:49,134 - distributed.nanny - INFO - Worker process 29295 exited with status 127
2023-05-29 06:14:18,828 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-29 06:14:18,828 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:14:18,829 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:14:18,829 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-29 06:14:18,830 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-29 06:14:21,007 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:14:21,011 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-05-29 06:14:21,015 - distributed.scheduler - INFO - State start
2023-05-29 06:14:21,035 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:14:21,036 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-29 06:14:21,036 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-05-29 06:29:17,637 - distributed.scheduler - INFO - Receive client connection: Client-236455c4-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:29:17,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53366
2023-05-29 06:29:33,727 - distributed.scheduler - INFO - Remove client Client-236455c4-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:29:33,728 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53366; closing.
2023-05-29 06:29:33,728 - distributed.scheduler - INFO - Remove client Client-236455c4-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:29:33,729 - distributed.scheduler - INFO - Close client connection: Client-236455c4-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:30:04,132 - distributed.scheduler - INFO - Receive client connection: Client-3f1ae7fd-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:30:04,132 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52084
2023-05-29 06:30:14,192 - distributed.scheduler - INFO - Remove client Client-3f1ae7fd-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:30:14,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52084; closing.
2023-05-29 06:30:14,193 - distributed.scheduler - INFO - Remove client Client-3f1ae7fd-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:30:14,193 - distributed.scheduler - INFO - Close client connection: Client-3f1ae7fd-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:30:44,590 - distributed.scheduler - INFO - Receive client connection: Client-573866eb-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:30:44,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36558
2023-05-29 06:30:54,678 - distributed.scheduler - INFO - Remove client Client-573866eb-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:30:54,678 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36558; closing.
2023-05-29 06:30:54,678 - distributed.scheduler - INFO - Remove client Client-573866eb-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:30:54,678 - distributed.scheduler - INFO - Close client connection: Client-573866eb-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:38:52,893 - distributed._signals - INFO - Received signal SIGTERM (15)
2023-05-29 06:38:52,899 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:38:52,901 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:38:52,903 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-29 06:38:52,904 - distributed.scheduler - INFO - End scheduler
