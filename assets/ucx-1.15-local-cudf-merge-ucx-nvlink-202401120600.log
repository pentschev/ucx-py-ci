[dgx13:90657:0:90657] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  90657) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f90931ef07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f90931ef274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f90931ef43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f9134876420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f909326e6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f9093297839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f90931a93df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f90931ac838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f90931f84a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f90931ab5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f909326b8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f909332406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5557df8663f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5557df860fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5557df872469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5557df8624e6]
16  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5557df872712]
17  /opt/conda/envs/gdf/bin/python(+0x14ca83) [0x5557df87fa83]
18  /opt/conda/envs/gdf/bin/python(+0x25819c) [0x5557df98b19c]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x5557df8253ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x5557df869723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x5557df867929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5557df872712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5557df8624e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5557df872712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5557df8624e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5557df872712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5557df8624e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5557df872712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5557df8624e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5557df860fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5557df872469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x5557df863042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5557df860fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x5557df87f8cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5557df88004c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x5557df94380e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5557df86a6ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5557df8663f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5557df872712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x5557df87f9ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5557df8663f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5557df872712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5557df8624e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5557df860fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5557df872469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5557df8624e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5557df872712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x5557df862232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5557df860fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5557df872469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x5557df863042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5557df860fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x5557df860c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5557df860c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5557df90e2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x5557df93b6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x5557df937a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5557df92f87a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5557df92f76c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5557df92e9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5557df902107]
=================================
[dgx13:90652:0:90652] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  90652) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f32f0f8907d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f32f0f89274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f32f0f8943a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f33845fd420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f32f10086b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f32f1031839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f32f0f433df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f32f0f46838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f32f0f924a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f32f0f455dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f32f10058da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f32f10be06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55992e20f3f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55992e209fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55992e21b469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55992e20b4e6]
16  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55992e21b712]
17  /opt/conda/envs/gdf/bin/python(+0x14ca83) [0x55992e228a83]
18  /opt/conda/envs/gdf/bin/python(+0x25819c) [0x55992e33419c]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55992e1ce3ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55992e212723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55992e210929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55992e21b712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55992e20b4e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55992e21b712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55992e20b4e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55992e21b712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55992e20b4e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55992e21b712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55992e20b4e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55992e209fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55992e21b469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55992e20c042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55992e209fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55992e2288cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55992e22904c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55992e2ec80e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55992e2136ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55992e20f3f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55992e21b712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55992e2289ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55992e20f3f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55992e21b712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55992e20b4e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55992e209fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55992e21b469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55992e20b4e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55992e21b712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55992e20b232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55992e209fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55992e21b469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55992e20c042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55992e209fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55992e209c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55992e209c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55992e2b72cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x55992e2e46ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x55992e2e0a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55992e2d887a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55992e2d876c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55992e2d79a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55992e2ab107]
=================================
[dgx13:90662:0:90662] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  90662) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f38d4d8207d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f38d4d82274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f38d4d8243a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f396840d420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f38d4e016b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f38d4e2a839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f38d4d3c3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f38d4d3f838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f38d4d8b4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f38d4d3e5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f38d4dfe8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f38d4eb706a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55d99d30c3f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d99d306fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d99d318469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d99d3084e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55d99d3bb6d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f395e3df1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55d99d3106ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55d99d2cb3ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55d99d30f723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55d99d30d929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d99d318712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d99d3084e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d99d318712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d99d3084e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d99d318712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d99d3084e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d99d318712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d99d3084e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d99d306fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d99d318469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55d99d309042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d99d306fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55d99d3258cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55d99d32604c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55d99d3e980e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55d99d3106ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55d99d30c3f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d99d318712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55d99d3259ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55d99d30c3f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d99d318712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d99d3084e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d99d306fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d99d318469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d99d3084e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d99d318712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55d99d308232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d99d306fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d99d318469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55d99d309042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d99d306fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55d99d306c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55d99d306c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55d99d3b42cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x55d99d3e16ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x55d99d3dda63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55d99d3d587a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55d99d3d576c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55d99d3d49a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55d99d3a8107]
=================================
[dgx13:90678:0:90678] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  90678) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fb2a0ef007d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fb2a0ef0274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fb2a0ef043a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fb342562420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fb2a0f6f6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fb2a0f98839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fb2a0eaa3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fb2a0ead838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fb2a0ef94a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fb2a0eac5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fb2a0f6c8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fb2a102506a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x563ba112d3f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x563ba1127fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x563ba1139469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563ba11294e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x563ba11dc6d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fb3385501e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x563ba11316ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x563ba10ec3ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x563ba1130723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x563ba112e929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x563ba1139712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563ba11294e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x563ba1139712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563ba11294e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x563ba1139712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563ba11294e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x563ba1139712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563ba11294e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x563ba1127fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x563ba1139469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x563ba112a042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x563ba1127fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x563ba11468cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x563ba114704c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x563ba120a80e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x563ba11316ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x563ba112d3f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x563ba1139712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x563ba11469ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x563ba112d3f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x563ba1139712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563ba11294e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x563ba1127fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x563ba1139469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563ba11294e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x563ba1139712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x563ba1129232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x563ba1127fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x563ba1139469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x563ba112a042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x563ba1127fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x563ba1127c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x563ba1127c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x563ba11d52cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x563ba12026ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x563ba11fea63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x563ba11f687a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x563ba11f676c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x563ba11f59a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x563ba11c9107]
=================================
[dgx13:90647:0:90647] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  90647) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fd84342707d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fd843427274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fd84342743a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fd8e4aa5420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fd8434a66b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fd8434cf839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fd8433e13df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fd8433e4838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fd8434304a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fd8433e35dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fd8434a38da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fd84355c06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564991aff3f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564991af9fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564991b0b469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564991afb4e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x564991bae6d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fd8daa901e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x564991b036ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x564991abe3ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x564991b02723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x564991b00929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564991b0b712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564991afb4e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564991b0b712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564991afb4e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564991b0b712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564991afb4e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564991b0b712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564991afb4e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564991af9fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564991b0b469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x564991afc042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564991af9fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x564991b188cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x564991b1904c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x564991bdc80e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x564991b036ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564991aff3f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564991b0b712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x564991b189ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564991aff3f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564991b0b712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564991afb4e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564991af9fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564991b0b469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564991afb4e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564991b0b712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x564991afb232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564991af9fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564991b0b469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x564991afc042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564991af9fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x564991af9c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x564991af9c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x564991ba72cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x564991bd46ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x564991bd0a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x564991bc887a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x564991bc876c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x564991bc79a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x564991b9b107]
=================================
2024-01-12 07:42:58,358 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37583
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f6a2e684180, tag: 0xfab39558dfcf4199, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f6a2e684180, tag: 0xfab39558dfcf4199, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-01-12 07:42:58,359 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43833
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f50ee824200, tag: 0x84daa0471dfa5f2e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f50ee824200, tag: 0x84daa0471dfa5f2e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-01-12 07:42:58,358 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37583
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f900153c1c0, tag: 0x51dbf466c9182bd6, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f900153c1c0, tag: 0x51dbf466c9182bd6, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
Task exception was never retrieved
future: <Task finished name='Task-875' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2024-01-12 07:42:58,362 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43833
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f6a2e684100, tag: 0xec56a105e7a1d980, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f6a2e684100, tag: 0xec56a105e7a1d980, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-12 07:42:58,362 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37583
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f50ee824140, tag: 0x57d9fed13fbcbb81, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2857, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f50ee824140, tag: 0x57d9fed13fbcbb81, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-12 07:42:58,362 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43833
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f900153c200, tag: 0xffc469e737672204, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2857, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f900153c200, tag: 0xffc469e737672204, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-12 07:42:58,388 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:60693
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7f50ee824100, tag: 0x592a88a5a9ddc131, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7f50ee824100, tag: 0x592a88a5a9ddc131, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-12 07:42:58,394 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:60693
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f900153c240, tag: 0x1510ba9b89266963, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f900153c240, tag: 0x1510ba9b89266963, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-12 07:42:58,420 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:60693
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #010] ep: 0x7f6a2e684240, tag: 0x45ca0cf844503dd2, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #010] ep: 0x7f6a2e684240, tag: 0x45ca0cf844503dd2, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-12 07:42:58,442 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:57305 -> ucx://127.0.0.1:40875
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f900153c380, tag: 0xf82952601e049c20, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-01-12 07:42:58,443 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:40279 -> ucx://127.0.0.1:39189
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f6a2e684340, tag: 0x46339d5f83215729, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-01-12 07:42:58,444 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39189
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f900153c180, tag: 0x59132d3140fe3ff4, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f900153c180, tag: 0x59132d3140fe3ff4, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-01-12 07:42:58,444 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40875
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f900153c100, tag: 0xee8b7d2c84782390, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f900153c100, tag: 0xee8b7d2c84782390, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-12 07:42:58,445 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39189
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f6a2e684280, tag: 0xbaadf0d4d8495c2c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f6a2e684280, tag: 0xbaadf0d4d8495c2c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-01-12 07:42:58,445 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40875
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f6a2e684140, tag: 0x44be0ed3a2a4b42a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f6a2e684140, tag: 0x44be0ed3a2a4b42a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-12 07:42:58,469 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:56841 -> ucx://127.0.0.1:39189
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f50ee824380, tag: 0x1a0bbd20a7cdd2fa, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-01-12 07:42:58,470 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39189
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f50ee824240, tag: 0x4af707be143665d0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f50ee824240, tag: 0x4af707be143665d0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
Task exception was never retrieved
future: <Task finished name='Task-869' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2024-01-12 07:42:58,472 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40875
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f50ee8241c0, tag: 0x490d794435b442bf, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f50ee8241c0, tag: 0x490d794435b442bf, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-12 07:43:00,383 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-12 07:43:00,383 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-12 07:43:00,453 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-fc2862d7de17ea8bdcee5a7a96a5fa51', 2)
Function:  subgraph_callable-b7e70352-4f46-4ddd-9ac5-ec944a86
args:      (               key   payload
shuffle                     
2            43011   1134580
2            43024  66076219
2            43026   3253603
2            43038  98264072
2           143905  49464753
...            ...       ...
2        799997681  66572273
2        799997686  25570606
2        799997688  17585944
2        799997693  73944232
2        799997694  16118650

[100000000 rows x 2 columns], '_partitions', 'getitem-1940d3694e23f539df56eb3dad6103f4', ['key'])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:00,489 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-12 07:43:00,489 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-12 07:43:00,678 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 7)
Function:  _concat
args:      ([                key   payload  _partitions
2112      830997470   9087692            7
2115      855907240   5341634            7
2120      508753433  73450230            7
2123      209898715  90103762            7
2127      830469842   9831713            7
...             ...       ...          ...
99976672  812449612  14163739            7
99976673  805995460  24489684            7
99976608  838720139   7629622            7
99976613  850749118  16702978            7
99976618  511658738  68989564            7

[12503560 rows x 3 columns],                 key   payload  _partitions
123553    923418517  76278515            7
123557     19981176  71947971            7
123559    938757397  34464890            7
123577    616922425  74708297            7
22728     323830267  65009920            7
...             ...       ...          ...
99977671  938239835  65816707            7
99977674  961913814  95829150            7
99988258  905228223  56746745            7
99988265  900815941  1
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:00,856 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 1)
Function:  _concat
args:      ([                key   payload  _partitions
2113      815377998  85554309            1
2125      848911361  99307636            1
2134      500328462  75562588            1
2143      843059124  86043155            1
143874    827638635  15142288            1
...             ...       ...          ...
99976682  853249935  71828931            1
99976702  504699660   7130281            1
99976622  859557521  28912490            1
99976625  868399730  64694649            1
99976627  810556639  66716928            1

[12497240 rows x 3 columns],                 key   payload  _partitions
123554    967498576  88961428            1
123556     12900079  83057077            1
123560    220081790  48044231            1
123583    915309370  92173477            1
22723     936756642  18314678            1
...             ...       ...          ...
99988264   13366383  70540494            1
99988270  218967229  84026102            1
99988277   24321645   2417845            1
99988284  322292006  6
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:01,727 - distributed.worker - ERROR - 'data'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2075, in gather_dep
    data=response["data"],
KeyError: 'data'
2024-01-12 07:43:01,731 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:57305
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #135] ep: 0x7f6a2e6841c0, tag: 0xb50ed29c86d9ca85, nbytes: 219, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #135] ep: 0x7f6a2e6841c0, tag: 0xb50ed29c86d9ca85, nbytes: 219, type: <class 'numpy.ndarray'>>: Message truncated")
2024-01-12 07:43:01,731 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56841
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 360, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #113] ep: 0x7f900153c140, tag: 0xdcb0fb57d8b91054, nbytes: 480, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #113] ep: 0x7f900153c140, tag: 0xdcb0fb57d8b91054, nbytes: 480, type: <class 'numpy.ndarray'>>: Message truncated")
2024-01-12 07:43:01,732 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:57305 -> ucx://127.0.0.1:40279
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 641, in send
    self._ep.raise_on_error()
  File "ucp/_libs/ucx_endpoint.pyx", line 353, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXConnectionReset: Endpoint 0x7f900153c3c0 error: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-01-12 07:43:01,887 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 3)
Function:  subgraph_callable-d3fc16c3-9489-416c-99af-a49ea2cb
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0            123590        3  97043370            3
1             62053        3   2499843            3
2            123602        3  15171764            3
3             62069        3  31783233            3
4             62071        3   7164868            3
...             ...      ...       ...          ...
99999995  799998191        3  78206090            3
99999996  799998194        3  54940291            3
99999997  799998089        3  81069218            3
99999998  799998107        3  23290957            3
99999999  799998110        3  56667339            3

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:01,987 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 1)
Function:  subgraph_callable-d3fc16c3-9489-416c-99af-a49ea2cb
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0            123587        1  47151826            1
1             62050        1  16165251            1
2            123592        1   4470515            1
3             62057        1  82313098            1
4            123596        1  52329082            1
...             ...      ...       ...          ...
99999995  799998189        1  27139610            1
99999996  799998082        1  29785618            1
99999997  799998083        1  37089347            1
99999998  799998094        1  78840529            1
99999999  799998100        1  45381574            1

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:02,008 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 6)
Function:  subgraph_callable-d3fc16c3-9489-416c-99af-a49ea2cb
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0            123601        6  63494838            6
1            123603        6  80733837            6
2            124452        6  55265579            6
3            136802        6  27902615            6
4             53891        6  59297904            6
...             ...      ...       ...          ...
99999995  799998205        6  92108826            6
99999996  799998207        6  43052978            6
99999997  799998106        6   1502842            6
99999998  799998108        6   4840486            6
99999999  799998111        6  21975086            6

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:02,055 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 0)
Function:  subgraph_callable-d3fc16c3-9489-416c-99af-a49ea2cb
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0            123584        0  89516539            0
1             62051        0   7474217            0
2            123594        0  18242367            0
3             62066        0  26512835            0
4            123595        0  13664442            0
...             ...      ...       ...          ...
99999995  799998204        0  40331734            0
99999996  799998081        0  59765741            0
99999997  799998101        0  74908683            0
99999998  799998103        0  56811904            0
99999999  799998105        0  46819190            0

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:02,350 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 2)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0           123593        2  45277593            2
1            62052        2  97925488            2
2           123600        2   9314341            2
3            62054        2   7444167            2
4           123609        2  33483008            2
...            ...      ...       ...          ...
12499995  99997681        2  66572273            2
12499996  99997686        2  25570606            2
12499997  99997688        2  17585944            2
12499998  99997693        2  73944232            2
12499999  99997694        2  16118650            2

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100011268        2  60032624            2
1         100011271        2  31628930            2
2         100011272        2  74889503            2
3         100011430        2  46455952            2
4         100011433        2  98329126            2
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:02,844 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 7)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0           123588        7  82750020            7
1            62048        7  19090312            7
2           123589        7  64670725            7
3            62067        7  75877245            7
4           123599        7  46494626            7
...            ...      ...       ...          ...
12499995  99997548        7  69623166            7
12499996  99997552        7  87703134            7
12499997  99997676        7  65523777            7
12499998  99997687        7  48300642            7
12499999  99997689        7  74553536            7

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100011265        7  97356992            7
1         100011266        7  74298396            7
2         100011270        7  11995629            7
3         100011273        7   4232767            7
4         100011286        7  38598600            7
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2024-01-12 07:43:16,597 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
2024-01-12 07:43:16,638 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
