[dgx13:85391:0:85391] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  85391) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f9ba34e307d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f9ba34e3274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f9ba34e343a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f9c48b1a420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f9ba35626b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f9ba358b839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f9ba349d3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f9ba34a0838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f9ba34ec4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f9ba349f5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f9ba355f8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f9ba361806a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561e28d076fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561e28d03094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561e28d14519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561e28d045c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561e28d147c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x561e28d21e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x561e28e2cb2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x561e28cbed05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x561e28d0b7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x561e28d09929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561e28d147c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561e28d045c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561e28d147c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561e28d045c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561e28d147c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561e28d045c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561e28d147c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561e28d045c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561e28d03094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561e28d14519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x561e28d05128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561e28d03094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x561e28d21ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x561e28d2244c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x561e28de510e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x561e28d0c77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561e28d076fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561e28d147c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x561e28d21dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561e28d076fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561e28d147c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561e28d045c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561e28d03094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561e28d14519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561e28d045c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561e28d147c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x561e28d04312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561e28d03094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561e28d14519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x561e28d05128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561e28d03094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x561e28d02d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x561e28d02d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x561e28db007b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x561e28ddcfca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x561e28dd9353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x561e28dd116a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x561e28dd105c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x561e28dd0297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x561e28da3f07]
=================================
2023-12-16 07:07:10,255 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:46651 -> ucx://127.0.0.1:52505
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f720017f100, tag: 0x9699dc9bff37b32a, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
[dgx13:85393:0:85393] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  85393) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fd8d8b9207d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fd8d8b92274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fd8d8b9243a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fd97a1e4420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fd8d8c116b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fd8d8c3a839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fd8d8b4c3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fd8d8b4f838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fd8d8b9b4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fd8d8b4e5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fd8d8c0e8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fd8d8cc706a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5632ab88c6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5632ab888094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5632ab899519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5632ab8895c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x5632ab93c162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fd9701d01e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5632ab89177c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x5632ab843d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x5632ab8907f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x5632ab88e929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5632ab8997c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5632ab8895c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5632ab8997c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5632ab8895c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5632ab8997c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5632ab8895c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5632ab8997c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5632ab8895c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5632ab888094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5632ab899519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x5632ab88a128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5632ab888094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x5632ab8a6ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5632ab8a744c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x5632ab96a10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5632ab89177c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5632ab88c6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5632ab8997c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x5632ab8a6dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5632ab88c6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5632ab8997c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5632ab8895c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5632ab888094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5632ab899519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5632ab8895c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5632ab8997c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x5632ab889312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5632ab888094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5632ab899519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x5632ab88a128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5632ab888094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x5632ab887d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5632ab887d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5632ab93507b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x5632ab961fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x5632ab95e353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5632ab95616a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5632ab95605c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5632ab955297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5632ab928f07]
=================================
[dgx13:85395:0:85395] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  85395) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f9db8a2f07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f9db8a2f274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f9db8a2f43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f9e4c057420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f9db8aae6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f9db8ad7839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f9db89e93df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f9db89ec838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f9db8a384a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f9db89eb5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f9db8aab8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f9db8b6406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560a06c66fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560a06c2094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5560a06d3519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560a06c35c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560a06d37c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x5560a06e0e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x5560a07ebb2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x5560a067dd05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x5560a06ca7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x5560a06c8929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560a06d37c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560a06c35c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560a06d37c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560a06c35c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560a06d37c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560a06c35c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560a06d37c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560a06c35c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560a06c2094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5560a06d3519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x5560a06c4128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560a06c2094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x5560a06e0ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5560a06e144c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x5560a07a410e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5560a06cb77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560a06c66fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560a06d37c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x5560a06e0dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560a06c66fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560a06d37c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560a06c35c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560a06c2094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5560a06d3519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560a06c35c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560a06d37c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x5560a06c3312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560a06c2094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5560a06d3519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x5560a06c4128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560a06c2094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x5560a06c1d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5560a06c1d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5560a076f07b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x5560a079bfca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x5560a0798353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5560a079016a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5560a079005c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5560a078f297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5560a0762f07]
=================================
[dgx13:85402:0:85402] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  85402) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f502874407d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f5028744274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f502874443a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f50bbdb4420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f50287c36b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f50287ec839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f50286fe3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f5028701838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f502874d4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f50287005dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f50287c08da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f502887906a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55eb0a1fb6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eb0a1f7094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55eb0a208519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eb0a1f85c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eb0a2087c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x55eb0a215e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x55eb0a320b2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55eb0a1b2d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55eb0a1ff7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55eb0a1fd929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eb0a2087c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eb0a1f85c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eb0a2087c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eb0a1f85c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eb0a2087c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eb0a1f85c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eb0a2087c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eb0a1f85c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eb0a1f7094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55eb0a208519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55eb0a1f9128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eb0a1f7094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55eb0a215ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55eb0a21644c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55eb0a2d910e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55eb0a20077c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55eb0a1fb6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eb0a2087c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55eb0a215dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55eb0a1fb6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eb0a2087c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eb0a1f85c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eb0a1f7094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55eb0a208519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55eb0a1f85c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55eb0a2087c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55eb0a1f8312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eb0a1f7094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55eb0a208519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55eb0a1f9128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55eb0a1f7094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55eb0a1f6d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55eb0a1f6d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55eb0a2a407b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55eb0a2d0fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55eb0a2cd353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55eb0a2c516a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55eb0a2c505c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55eb0a2c4297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55eb0a297f07]
=================================
2023-12-16 07:07:13,027 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47555 -> ucx://127.0.0.1:43989
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa8a0579140, tag: 0x6243506cab5cebce, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-12-16 07:07:13,057 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:40913 -> ucx://127.0.0.1:53033
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f3e0e673140, tag: 0x7780d897f1a1c4db, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-12-16 07:07:13,088 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47555 -> ucx://127.0.0.1:53033
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa8a0579180, tag: 0x22ce4ed322865ccd, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-12-16 07:07:13,111 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47555 -> ucx://127.0.0.1:40717
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa8a05791c0, tag: 0x51a0bbe8ea315b79, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-12-16 07:07:13,136 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:56605 -> ucx://127.0.0.1:40717
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f40d429a140, tag: 0xd992b3d2a237799c, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-12-16 07:07:14,765 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-12-16 07:07:14,765 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-12-16 07:07:14,769 - distributed.worker - ERROR - tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2861, in get_data_from_worker
    status = response["status"]
TypeError: tuple indices must be integers or slices, not str
2023-12-16 07:07:14,861 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-12-16 07:07:14,862 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-12-16 07:07:14,866 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-adccb87e3fee0e0ff9c5206a34e9fe09', 3)
Function:  subgraph_callable-668f58a1-d530-4e87-a629-df118049
args:      (               key   payload
shuffle                     
0           708215  88374987
0           691712  86503404
0           655647  26203032
0           566902  65572570
0           691721  56440491
...            ...       ...
7        799947820  71762028
7        799932964   2655307
7        799936663   1701614
7        799886016  36481689
7        799985681  10098131

[100002012 rows x 2 columns],                  key   payload
22018      854195119  13726404
22031      812750150   4409989
22036      815341968  75617973
41638      859473686  51533225
22040      102730621  46648532
...              ...       ...
99998677   288990168  59314285
99998685  1539141913  66012037
99983204  1547070148  36163864
99983205  1526978197  89443872
99983215  1537799203  83244808

[100005738 rows x 2 columns], 'simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 'simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-12-16 07:07:14,982 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 1)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           719463  54828608
0           364476  76946720
0           573642  12828762
0           624209  45096884
0           528543   4604311
...            ...       ...
0        799728383  60839617
0        799894559  43374243
0        799920519  48798948
0        799832623  94617463
0        799909842  28173973

[12497508 rows x 2 columns],                key   payload
shuffle                     
1          1037381  51868210
1           926154  25636068
1           300795  42714880
1           612807  50374931
1           645469  81979022
...            ...       ...
1        799898254  30081272
1        799898263  85243281
1        799988268  21756703
1        799904481  66538438
1        799875797  86735921

[12503907 rows x 2 columns],                key   payload
shuffle                     
2           180762   9851005
2           167085  13673146
2            73104  87726884
2          1180729  43980000
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-12-16 07:07:14,983 - distributed.worker - ERROR - 'int' object is not subscriptable
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2861, in get_data_from_worker
    status = response["status"]
TypeError: 'int' object is not subscriptable
2023-12-16 07:07:14,987 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1069, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1784, in get_data
    assert response == "OK", response
AssertionError: {'op': 'get_data', 'keys': {('split-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 5, 2)}, 'who': 'ucx://127.0.0.1:56605', 'reply': True}
2023-12-16 07:07:14,992 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46651
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 364, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #083] ep: 0x7f40d429a240, tag: 0x296ff3210bcdcef5, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #083] ep: 0x7f40d429a240, tag: 0x296ff3210bcdcef5, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated")
2023-12-16 07:07:15,096 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-adccb87e3fee0e0ff9c5206a34e9fe09', 2)
Function:  subgraph_callable-668f58a1-d530-4e87-a629-df118049
args:      (               key   payload
shuffle                     
0           623537  25070528
0           643400  89267822
0           663910  36787935
0           358218  41652580
0           485546  35666490
...            ...       ...
7        799985672  64049393
7        799907465  38586320
7        799894165  71891047
7        799985683  90088906
7        799947281  96763311

[99996471 rows x 2 columns],                  key   payload
22035      504302474  42915611
41663      837381143  58985896
53826      611898246  11278124
53843      860692541  40933759
42538      402318633  55434542
...              ...       ...
99998660  1509344313  49237364
99998662  1532495022  12297505
99983166   697051053  74685456
99983210   797424073  38929695
99983222  1568974451  97337238

[100013945 rows x 2 columns], 'simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 'simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-12-16 07:07:15,188 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-12-16 07:07:15,188 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-12-16 07:07:15,216 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-12-16 07:07:15,217 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2023-12-16 07:07:15,307 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 4)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           656259  37177249
0           658266   5757841
0           624508   1061975
0           393157  85155761
0           642315  62880962
...            ...       ...
0        799938529  78711520
0        799909825  90063452
0        799947640  35310893
0        799881572  53548869
0        799837370   6642203

[12503392 rows x 2 columns],                key   payload
shuffle                     
1          1037963  66301991
1           890123  65683693
1           864607  67329823
1           320524  70415617
1           616870   6376220
...            ...       ...
1        799958192  51448760
1        799878714  15942355
1        799879409  15655353
1        799981291  73833792
1        799958200  98657680

[12500389 rows x 2 columns],                key   payload
shuffle                     
2           172617  39284688
2           219848  67013703
2           236551  41500160
2           179822  13424896
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-12-16 07:07:15,307 - distributed.worker - ERROR - 'int' object is not subscriptable
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2861, in get_data_from_worker
    status = response["status"]
TypeError: 'int' object is not subscriptable
2023-12-16 07:07:15,313 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1069, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1784, in get_data
    assert response == "OK", response
AssertionError: {'op': 'get_data', 'keys': {('split-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 5, 2)}, 'who': 'ucx://127.0.0.1:40913', 'reply': True}
2023-12-16 07:07:15,316 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56605
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 364, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #081] ep: 0x7f3e0e673200, tag: 0xdf0692a1d50d38a1, nbytes: 3192, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #081] ep: 0x7f3e0e673200, tag: 0xdf0692a1d50d38a1, nbytes: 3192, type: <class 'numpy.ndarray'>>: Message truncated")
2023-12-16 07:07:15,316 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:56605 -> ucx://127.0.0.1:40913
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 320, in write
    await self.ep.send(struct.pack("?Q", False, nframes))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 629, in send
    self._ep.raise_on_error()
  File "ucp/_libs/ucx_endpoint.pyx", line 353, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXConnectionReset: Endpoint 0x7f40d429a140 error: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-12-16 07:07:15,500 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  _concat
args:      ([                key   payload
22021     810215257  53716423
22026     821017498  47088277
22027     706812113  75951478
41633     837181309  29638536
22028     859609229  45220130
...             ...       ...
99992032  700402088  45460667
99992035  802555827  84873570
99992051  867120451  28516032
99992059  824191913  54898618
99992063  826472369  14868065

[12497796 rows x 2 columns],                 key   payload
31937     944910701  55504790
31956     616968880  51963656
31963     954546604  51738101
19072     723888491  43523208
19075     964304606  17800168
...             ...       ...
99999713  963588330  92433775
99999720  957349781  92285504
99999732  922961653   4797102
99999736  919913480  69109366
99999741  624445666  10887831

[12497151 rows x 2 columns],                  key   payload
20994       35483064  98221626
20997     1039152888  31517982
21003     1038511950   6540561
21013      331625178  94695249
21014     1004569408  81528293
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-12-16 07:07:15,507 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7)
Function:  _concat
args:      ([                key   payload
22023     855671254  37987686
22046     850343153  46598508
41637     201672386  50776182
41642     867166980  41146580
53834     840700027  75278791
...             ...       ...
99992040  856971549  37192128
99992049  841234854  32565406
99992050  817425849  38211957
99992057  864436877  56064229
99992061  861977000  25437600

[12500893 rows x 2 columns],                 key   payload
31957     926806874  79937192
31964     906624787  71015060
31967     956523250  40869265
19077     943532133  86140831
19079     112554120   1630618
...             ...       ...
99988956  932777826  78474402
99999717  922266742   3685901
99999722  931133159  79657343
99999725  965467456  49261441
99999726  931896239  81281180

[12495890 rows x 2 columns],                  key   payload
21002       27243842  26626038
21009     1045222293  26092065
21010      433336711  12827718
123521     236701724  60052997
124586     528525466   4915726
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-12-16 07:07:15,697 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 5)
Function:  _concat
args:      ([                key   payload
22030     804560282  19444218
22039     108239748  35205287
22041     833988748  23300249
41632     502202172  16981155
41643      11448697  66627752
...             ...       ...
99991917  849869018   3355750
99991921  849306980  99443023
99991925  828639231  32080388
99992047  833230073   1154011
99992054  504292709  64565660

[12498923 rows x 2 columns],                 key   payload
31938     122357104   9560516
31940     943294877  70317547
31941     914889211  19930599
31952     117877046  16645531
19076     918035417  79397563
...             ...       ...
99988989  964507114   1191047
99988936  221785508  91500808
99988959  953224853  18120333
99999719  116577211  28084078
99999728  414240537   8142552

[12501128 rows x 2 columns],                  key   payload
20992     1058528615  17654447
21000     1013680714  42851474
21001     1054468280  46592807
21016      134240060  22776545
3715      1047049228  41575564
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

2023-12-16 07:07:18,538 - distributed.nanny - WARNING - Restarting worker
2023-12-16 07:07:25,983 - distributed.nanny - WARNING - Restarting worker
2023-12-16 07:07:26,236 - distributed.nanny - WARNING - Restarting worker
2023-12-16 07:07:26,305 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
