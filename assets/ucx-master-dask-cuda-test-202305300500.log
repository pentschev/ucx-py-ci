============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-30 06:08:34,596 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:08:34,600 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45159 instead
  warnings.warn(
2023-05-30 06:08:34,603 - distributed.scheduler - INFO - State start
2023-05-30 06:08:34,622 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:08:34,622 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:08:34,623 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:08:34,623 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:08:34,714 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42783'
2023-05-30 06:08:34,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34459'
2023-05-30 06:08:34,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42951'
2023-05-30 06:08:34,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34383'
2023-05-30 06:08:36,084 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hhqf_wi8', purging
2023-05-30 06:08:36,084 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8yg458e4', purging
2023-05-30 06:08:36,085 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nv4nx4jo', purging
2023-05-30 06:08:36,085 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-attu0a1_', purging
2023-05-30 06:08:36,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:36,086 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:36,093 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:36,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:36,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:36,167 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:36,194 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:36,194 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:36,201 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:36,230 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:36,230 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:36,242 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-30 06:08:36,280 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34077
2023-05-30 06:08:36,280 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34077
2023-05-30 06:08:36,280 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42541
2023-05-30 06:08:36,280 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-30 06:08:36,280 - distributed.worker - INFO - -------------------------------------------------
2023-05-30 06:08:36,280 - distributed.worker - INFO -               Threads:                          4
2023-05-30 06:08:36,281 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-30 06:08:36,281 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-71aleszf
2023-05-30 06:08:36,281 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce12e779-f7d5-4434-8bc9-eef253bee699
2023-05-30 06:08:36,281 - distributed.worker - INFO - Starting Worker plugin PreImport-ee5816ad-2ead-42ae-9a05-82fa7a17cc7a
2023-05-30 06:08:36,281 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae793435-5572-405e-86b5-8283f853fe0a
2023-05-30 06:08:36,281 - distributed.worker - INFO - -------------------------------------------------
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:36,814 - distributed.nanny - INFO - Worker process 26498 exited with status 127
2023-05-30 06:08:36,815 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:36,952 - distributed.nanny - INFO - Worker process 26505 exited with status 127
2023-05-30 06:08:36,952 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:36,971 - distributed.nanny - INFO - Worker process 26501 exited with status 127
2023-05-30 06:08:36,972 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:38,261 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v1tl4q1h', purging
2023-05-30 06:08:38,262 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ywqxc14p', purging
2023-05-30 06:08:38,263 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x9rhwuc7', purging
2023-05-30 06:08:38,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:38,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:38,270 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:38,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:38,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:38,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:38,406 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:38,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:38,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:38,963 - distributed.nanny - INFO - Worker process 26536 exited with status 127
2023-05-30 06:08:38,964 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:39,051 - distributed.nanny - INFO - Worker process 26544 exited with status 127
2023-05-30 06:08:39,051 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:39,106 - distributed.nanny - INFO - Worker process 26541 exited with status 127
2023-05-30 06:08:39,106 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:40,417 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-giokubyc', purging
2023-05-30 06:08:40,418 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rg4s53_f', purging
2023-05-30 06:08:40,418 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6_lz5v6o', purging
2023-05-30 06:08:40,419 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:40,419 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:40,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:40,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:40,480 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:40,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:40,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:40,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:40,535 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:42,253 - distributed.nanny - INFO - Worker process 26570 exited with status 127
2023-05-30 06:08:42,254 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:42,273 - distributed.nanny - INFO - Worker process 26574 exited with status 127
2023-05-30 06:08:42,274 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:42,300 - distributed.nanny - INFO - Worker process 26566 exited with status 127
2023-05-30 06:08:42,301 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:43,765 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-alcfo_a0', purging
2023-05-30 06:08:43,765 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jacur56b', purging
2023-05-30 06:08:43,765 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yxdxd18q', purging
2023-05-30 06:08:43,766 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:43,766 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:43,767 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:43,767 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:43,773 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:43,774 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:43,781 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:43,781 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:43,787 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:46,197 - distributed.nanny - INFO - Worker process 26601 exited with status 127
2023-05-30 06:08:46,198 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:46,230 - distributed.nanny - INFO - Worker process 26598 exited with status 127
2023-05-30 06:08:46,231 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:46,258 - distributed.nanny - INFO - Worker process 26604 exited with status 127
2023-05-30 06:08:46,259 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:47,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-54_phu5y', purging
2023-05-30 06:08:47,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v6kbazok', purging
2023-05-30 06:08:47,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ncfuzty5', purging
2023-05-30 06:08:47,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:47,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:47,654 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:47,683 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:47,683 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:47,690 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:47,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:47,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:47,728 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:50,199 - distributed.nanny - INFO - Worker process 26628 exited with status 127
2023-05-30 06:08:50,200 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:50,222 - distributed.nanny - INFO - Worker process 26631 exited with status 127
2023-05-30 06:08:50,222 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:50,246 - distributed.nanny - INFO - Worker process 26634 exited with status 127
2023-05-30 06:08:50,247 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:51,689 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i5ws14ut', purging
2023-05-30 06:08:51,690 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2ofddloy', purging
2023-05-30 06:08:51,690 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_nixi53d', purging
2023-05-30 06:08:51,691 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:51,691 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:51,696 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:51,696 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:51,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:51,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:51,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:51,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:51,708 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:52,474 - distributed.nanny - INFO - Worker process 26661 exited with status 127
2023-05-30 06:08:52,475 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:52,499 - distributed.nanny - INFO - Worker process 26658 exited with status 127
2023-05-30 06:08:52,500 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:52,544 - distributed.nanny - INFO - Worker process 26664 exited with status 127
2023-05-30 06:08:52,545 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:53,943 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s4c45fog', purging
2023-05-30 06:08:53,943 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7cys6ivd', purging
2023-05-30 06:08:53,944 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h56sfhhm', purging
2023-05-30 06:08:53,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:53,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:53,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:53,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:53,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:53,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:53,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:53,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:53,999 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:55,486 - distributed.nanny - INFO - Worker process 26691 exited with status 127
2023-05-30 06:08:55,487 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:55,513 - distributed.nanny - INFO - Worker process 26688 exited with status 127
2023-05-30 06:08:55,514 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:55,543 - distributed.nanny - INFO - Worker process 26694 exited with status 127
2023-05-30 06:08:55,544 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:56,938 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-65n2tfm8', purging
2023-05-30 06:08:56,938 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8a3micra', purging
2023-05-30 06:08:56,939 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pz2j5s8f', purging
2023-05-30 06:08:56,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:56,939 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:56,941 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:56,941 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:56,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:56,945 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:56,946 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:56,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:56,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:57,685 - distributed.nanny - INFO - Worker process 26718 exited with status 127
2023-05-30 06:08:57,686 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:57,706 - distributed.nanny - INFO - Worker process 26721 exited with status 127
2023-05-30 06:08:57,707 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:57,728 - distributed.nanny - INFO - Worker process 26724 exited with status 127
2023-05-30 06:08:57,728 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:59,059 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c7_n6554', purging
2023-05-30 06:08:59,059 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mhdosie3', purging
2023-05-30 06:08:59,060 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-psjujhlz', purging
2023-05-30 06:08:59,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:59,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:59,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:59,095 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:59,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:59,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:08:59,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:08:59,102 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:08:59,102 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:59,719 - distributed.nanny - INFO - Worker process 26751 exited with status 127
2023-05-30 06:08:59,720 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:08:59,830 - distributed.nanny - INFO - Worker process 26748 exited with status 127
2023-05-30 06:08:59,831 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:08:59,855 - distributed.nanny - INFO - Worker process 26754 exited with status 127
2023-05-30 06:08:59,856 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:01,100 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3f9i73tn', purging
2023-05-30 06:09:01,100 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c7vf8xtk', purging
2023-05-30 06:09:01,100 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vn1am9_f', purging
2023-05-30 06:09:01,101 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:01,101 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:01,107 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:01,178 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:01,178 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:01,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:01,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:01,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:01,220 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:01,767 - distributed.nanny - INFO - Worker process 26776 exited with status 127
2023-05-30 06:09:01,768 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:01,886 - distributed.nanny - INFO - Worker process 26781 exited with status 127
2023-05-30 06:09:01,886 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:01,907 - distributed.nanny - INFO - Worker process 26784 exited with status 127
2023-05-30 06:09:01,908 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:03,173 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-use1zwmu', purging
2023-05-30 06:09:03,173 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w3w8xf0n', purging
2023-05-30 06:09:03,173 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6cv_k4n7', purging
2023-05-30 06:09:03,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:03,174 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:03,181 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:03,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:03,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:03,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:03,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:03,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:03,276 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:03,847 - distributed.nanny - INFO - Worker process 26806 exited with status 127
2023-05-30 06:09:03,848 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:03,967 - distributed.nanny - INFO - Worker process 26811 exited with status 127
2023-05-30 06:09:03,968 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:03,989 - distributed.nanny - INFO - Worker process 26814 exited with status 127
2023-05-30 06:09:03,990 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:05,223 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wj14tmsk', purging
2023-05-30 06:09:05,223 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dpfc25gf', purging
2023-05-30 06:09:05,224 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k8vt3uyj', purging
2023-05-30 06:09:05,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:05,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:05,230 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:05,290 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
2023-05-30 06:09:05,292 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42783'. Reason: nanny-close
2023-05-30 06:09:05,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34459'. Reason: nanny-close
2023-05-30 06:09:05,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42951'. Reason: nanny-close
2023-05-30 06:09:05,293 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34383'. Reason: nanny-close
2023-05-30 06:09:05,353 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:05,353 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:05,355 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:05,355 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:05,360 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:05,361 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:05,865 - distributed.nanny - INFO - Worker process 26836 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:05,982 - distributed.nanny - INFO - Worker process 26841 exited with status 127
2023-05-30 06:09:06,002 - distributed.nanny - INFO - Worker process 26844 exited with status 127
2023-05-30 06:09:06,282 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-30 06:09:21,163 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-30 06:09:21,163 - distributed.worker - INFO - -------------------------------------------------
2023-05-30 06:09:21,165 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-30 06:09:21,213 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-30 06:09:21,214 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34077. Reason: nanny-close
2023-05-30 06:09:21,216 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-30 06:09:21,217 - distributed.nanny - INFO - Worker closed
2023-05-30 06:09:21,446 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-30 06:09:22,916 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9di2z2uz', purging
2023-05-30 06:09:22,917 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w09_e__5', purging
2023-05-30 06:09:22,917 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-21f_thrf', purging
2023-05-30 06:09:22,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:22,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:22,924 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-30 06:09:22,936 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45387
2023-05-30 06:09:22,936 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45387
2023-05-30 06:09:22,936 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38535
2023-05-30 06:09:22,936 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-30 06:09:22,936 - distributed.worker - INFO - -------------------------------------------------
2023-05-30 06:09:22,936 - distributed.worker - INFO -               Threads:                          4
2023-05-30 06:09:22,936 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-30 06:09:22,936 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hscg51nw
2023-05-30 06:09:22,937 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0a62ac8-df65-4b84-9a0e-f0f594844e19
2023-05-30 06:09:22,937 - distributed.worker - INFO - Starting Worker plugin PreImport-22c652e8-0d8a-4731-9c55-4a969ec408d9
2023-05-30 06:09:22,937 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c836ebe-ffad-45a2-aa01-4ba43edd9d30
2023-05-30 06:09:22,937 - distributed.worker - INFO - -------------------------------------------------
2023-05-30 06:09:22,954 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-30 06:09:22,954 - distributed.worker - INFO - -------------------------------------------------
2023-05-30 06:09:22,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-30 06:09:28,370 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36947
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-30 06:09:37,448 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:09:37,453 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33777 instead
  warnings.warn(
2023-05-30 06:09:37,457 - distributed.scheduler - INFO - State start
2023-05-30 06:09:38,015 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:09:38,016 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:09:38,017 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:09:38,017 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:09:38,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32833'
2023-05-30 06:09:38,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45557'
2023-05-30 06:09:38,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38453'
2023-05-30 06:09:38,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37829'
2023-05-30 06:09:38,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42911'
2023-05-30 06:09:38,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37259'
2023-05-30 06:09:38,195 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40193'
2023-05-30 06:09:38,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46201'
2023-05-30 06:09:39,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:39,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hscg51nw', purging
2023-05-30 06:09:39,832 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:39,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:39,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:39,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:39,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:39,858 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:39,858 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:39,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:39,859 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:39,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:39,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:39,860 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:39,860 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:39,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:39,892 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:39,899 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:39,900 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:39,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:39,924 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:39,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:39,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:40,136 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:40,144 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:43,878 - distributed.nanny - INFO - Worker process 27056 exited with status 127
2023-05-30 06:09:43,879 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:44,084 - distributed.nanny - INFO - Worker process 27070 exited with status 127
2023-05-30 06:09:44,085 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:44,110 - distributed.nanny - INFO - Worker process 27073 exited with status 127
2023-05-30 06:09:44,111 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:44,149 - distributed.nanny - INFO - Worker process 27052 exited with status 127
2023-05-30 06:09:44,150 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:44,178 - distributed.nanny - INFO - Worker process 27067 exited with status 127
2023-05-30 06:09:44,179 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:44,204 - distributed.nanny - INFO - Worker process 27049 exited with status 127
2023-05-30 06:09:44,204 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:44,236 - distributed.nanny - INFO - Worker process 27060 exited with status 127
2023-05-30 06:09:44,237 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:44,263 - distributed.nanny - INFO - Worker process 27064 exited with status 127
2023-05-30 06:09:44,263 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:45,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fu9c1tt7', purging
2023-05-30 06:09:45,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1rh2o22y', purging
2023-05-30 06:09:45,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bd6kqonq', purging
2023-05-30 06:09:45,577 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_ft8yd_6', purging
2023-05-30 06:09:45,577 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2hre5yul', purging
2023-05-30 06:09:45,577 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ln9knqw4', purging
2023-05-30 06:09:45,578 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-agt5q8gq', purging
2023-05-30 06:09:45,578 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6yv53eld', purging
2023-05-30 06:09:45,579 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:45,579 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:45,604 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:45,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:45,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:45,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:45,770 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:45,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:45,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:45,866 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:45,866 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:45,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:45,898 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:45,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:45,899 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:45,903 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:45,903 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:45,904 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:45,904 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:45,947 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:45,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:46,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:46,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:46,071 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:46,222 - distributed.nanny - INFO - Worker process 27132 exited with status 127
2023-05-30 06:09:46,223 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:47,792 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-04q7ebmx', purging
2023-05-30 06:09:47,793 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:47,793 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:48,008 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:49,457 - distributed.nanny - INFO - Worker process 27135 exited with status 127
2023-05-30 06:09:49,458 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:49,486 - distributed.nanny - INFO - Worker process 27147 exited with status 127
2023-05-30 06:09:49,487 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:49,544 - distributed.nanny - INFO - Worker process 27141 exited with status 127
2023-05-30 06:09:49,545 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:49,573 - distributed.nanny - INFO - Worker process 27138 exited with status 127
2023-05-30 06:09:49,574 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:49,640 - distributed.nanny - INFO - Worker process 27150 exited with status 127
2023-05-30 06:09:49,641 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:49,712 - distributed.nanny - INFO - Worker process 27144 exited with status 127
2023-05-30 06:09:49,713 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:49,829 - distributed.nanny - INFO - Worker process 27153 exited with status 127
2023-05-30 06:09:49,830 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:50,167 - distributed.nanny - INFO - Worker process 27191 exited with status 127
2023-05-30 06:09:50,168 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:51,072 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pva3lgo9', purging
2023-05-30 06:09:51,073 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bt02j5r9', purging
2023-05-30 06:09:51,073 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m28j6ns9', purging
2023-05-30 06:09:51,073 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oxm_sf45', purging
2023-05-30 06:09:51,074 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fledzl1v', purging
2023-05-30 06:09:51,074 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bbjh4nv2', purging
2023-05-30 06:09:51,075 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-klskaz6r', purging
2023-05-30 06:09:51,075 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w0xqzzyu', purging
2023-05-30 06:09:51,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:51,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:51,093 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:51,093 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:51,098 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:51,118 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:51,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:51,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:51,183 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:51,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:51,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:51,225 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:51,225 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:51,253 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:51,254 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:51,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:51,406 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:51,408 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:51,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:51,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:51,704 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:51,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:51,731 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:51,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:51,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32833'. Reason: nanny-close
2023-05-30 06:09:51,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45557'. Reason: nanny-close
2023-05-30 06:09:51,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38453'. Reason: nanny-close
2023-05-30 06:09:51,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37829'. Reason: nanny-close
2023-05-30 06:09:51,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42911'. Reason: nanny-close
2023-05-30 06:09:51,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40193'. Reason: nanny-close
2023-05-30 06:09:51,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37259'. Reason: nanny-close
2023-05-30 06:09:51,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46201'. Reason: nanny-close
2023-05-30 06:09:52,062 - distributed.nanny - INFO - Worker process 27218 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:52,627 - distributed.nanny - INFO - Worker process 27221 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:53,216 - distributed.nanny - INFO - Worker process 27225 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:53,260 - distributed.nanny - INFO - Worker process 27231 exited with status 127
2023-05-30 06:09:53,310 - distributed.nanny - INFO - Worker process 27228 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:53,339 - distributed.nanny - INFO - Worker process 27234 exited with status 127
2023-05-30 06:09:53,379 - distributed.nanny - INFO - Worker process 27239 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:53,443 - distributed.nanny - INFO - Worker process 27245 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-30 06:10:23,741 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:10:23,745 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35267 instead
  warnings.warn(
2023-05-30 06:10:23,749 - distributed.scheduler - INFO - State start
2023-05-30 06:10:23,767 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:10:23,768 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:10:23,768 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:10:23,769 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:10:23,939 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33283'
2023-05-30 06:10:23,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37297'
2023-05-30 06:10:23,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40237'
2023-05-30 06:10:23,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39939'
2023-05-30 06:10:23,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33685'
2023-05-30 06:10:23,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40279'
2023-05-30 06:10:24,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35405'
2023-05-30 06:10:24,009 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46857'
2023-05-30 06:10:25,482 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z0lj_j4k', purging
2023-05-30 06:10:25,482 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g_g6puhb', purging
2023-05-30 06:10:25,482 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cqa0fjpp', purging
2023-05-30 06:10:25,483 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-am7903or', purging
2023-05-30 06:10:25,483 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n5khzcp4', purging
2023-05-30 06:10:25,483 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-72tsq40l', purging
2023-05-30 06:10:25,483 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z1qav8vk', purging
2023-05-30 06:10:25,484 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vyxzq382', purging
2023-05-30 06:10:25,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:25,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:25,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:25,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:25,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:25,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:25,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:25,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:25,686 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:25,686 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:25,686 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:25,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:25,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:25,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:25,691 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:25,691 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:25,875 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:25,916 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:25,919 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:25,919 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:25,919 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:25,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:25,932 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:25,933 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:28,738 - distributed.nanny - INFO - Worker process 27493 exited with status 127
2023-05-30 06:10:28,739 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:28,772 - distributed.nanny - INFO - Worker process 27487 exited with status 127
2023-05-30 06:10:28,773 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:28,832 - distributed.nanny - INFO - Worker process 27480 exited with status 127
2023-05-30 06:10:28,833 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:28,887 - distributed.nanny - INFO - Worker process 27476 exited with status 127
2023-05-30 06:10:28,888 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:28,914 - distributed.nanny - INFO - Worker process 27484 exited with status 127
2023-05-30 06:10:28,915 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:28,946 - distributed.nanny - INFO - Worker process 27490 exited with status 127
2023-05-30 06:10:28,947 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:28,976 - distributed.nanny - INFO - Worker process 27472 exited with status 127
2023-05-30 06:10:28,977 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:29,001 - distributed.nanny - INFO - Worker process 27469 exited with status 127
2023-05-30 06:10:29,002 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:30,318 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c6b94lrx', purging
2023-05-30 06:10:30,318 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ukhsi3ab', purging
2023-05-30 06:10:30,318 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hem2kmot', purging
2023-05-30 06:10:30,319 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lupyn61y', purging
2023-05-30 06:10:30,319 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_6og8_gp', purging
2023-05-30 06:10:30,319 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sao3dcdt', purging
2023-05-30 06:10:30,320 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p4arsic_', purging
2023-05-30 06:10:30,320 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_h7tyn1i', purging
2023-05-30 06:10:30,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:30,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:30,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:30,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:30,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:30,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:30,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:30,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:30,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:30,592 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:30,592 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:30,594 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:30,595 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:30,607 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:30,607 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:30,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:30,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:30,672 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:30,676 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:30,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:30,733 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:30,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:30,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:30,799 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:33,566 - distributed.nanny - INFO - Worker process 27552 exited with status 127
2023-05-30 06:10:33,567 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:33,652 - distributed.nanny - INFO - Worker process 27558 exited with status 127
2023-05-30 06:10:33,653 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:33,676 - distributed.nanny - INFO - Worker process 27567 exited with status 127
2023-05-30 06:10:33,677 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:33,720 - distributed.nanny - INFO - Worker process 27555 exited with status 127
2023-05-30 06:10:33,721 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:33,751 - distributed.nanny - INFO - Worker process 27564 exited with status 127
2023-05-30 06:10:33,751 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:33,779 - distributed.nanny - INFO - Worker process 27570 exited with status 127
2023-05-30 06:10:33,780 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:33,820 - distributed.nanny - INFO - Worker process 27561 exited with status 127
2023-05-30 06:10:33,821 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:33,866 - distributed.nanny - INFO - Worker process 27573 exited with status 127
2023-05-30 06:10:33,867 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:34,985 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-izm_atq9', purging
2023-05-30 06:10:34,986 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xhzy3zaz', purging
2023-05-30 06:10:34,986 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pg5f3t5i', purging
2023-05-30 06:10:34,986 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pc25n0j9', purging
2023-05-30 06:10:34,987 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j8k8jse2', purging
2023-05-30 06:10:34,987 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3ne901k5', purging
2023-05-30 06:10:34,987 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ho494m6b', purging
2023-05-30 06:10:34,988 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bv95srhr', purging
2023-05-30 06:10:34,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:34,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:35,277 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:35,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:35,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:35,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:35,353 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:35,353 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:35,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:35,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:35,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:35,412 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:35,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:35,427 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:35,449 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:35,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:35,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:35,474 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:35,479 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:35,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:35,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:35,530 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:35,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:35,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:38,033 - distributed.nanny - INFO - Worker process 27650 exited with status 127
2023-05-30 06:10:38,035 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:38,105 - distributed.nanny - INFO - Worker process 27632 exited with status 127
2023-05-30 06:10:38,106 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:38,129 - distributed.nanny - INFO - Worker process 27641 exited with status 127
2023-05-30 06:10:38,130 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:38,153 - distributed.nanny - INFO - Worker process 27644 exited with status 127
2023-05-30 06:10:38,154 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:38,180 - distributed.nanny - INFO - Worker process 27635 exited with status 127
2023-05-30 06:10:38,180 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:38,206 - distributed.nanny - INFO - Worker process 27653 exited with status 127
2023-05-30 06:10:38,207 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:38,235 - distributed.nanny - INFO - Worker process 27647 exited with status 127
2023-05-30 06:10:38,236 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:38,260 - distributed.nanny - INFO - Worker process 27638 exited with status 127
2023-05-30 06:10:38,261 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:38,318 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33283'. Reason: nanny-close
2023-05-30 06:10:38,319 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40237'. Reason: nanny-close
2023-05-30 06:10:38,319 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33685'. Reason: nanny-close
2023-05-30 06:10:38,320 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40279'. Reason: nanny-close
2023-05-30 06:10:38,320 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37297'. Reason: nanny-close
2023-05-30 06:10:38,320 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39939'. Reason: nanny-close
2023-05-30 06:10:38,320 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35405'. Reason: nanny-close
2023-05-30 06:10:38,320 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46857'. Reason: nanny-close
2023-05-30 06:10:39,538 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1vvsyuvc', purging
2023-05-30 06:10:39,539 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bpurp_ri', purging
2023-05-30 06:10:39,539 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xif6hddm', purging
2023-05-30 06:10:39,539 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_44k8gz6', purging
2023-05-30 06:10:39,540 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oykiaujl', purging
2023-05-30 06:10:39,540 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0o5r_fwd', purging
2023-05-30 06:10:39,540 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-01qagl14', purging
2023-05-30 06:10:39,540 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hwxriblh', purging
2023-05-30 06:10:39,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:39,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:39,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:39,614 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:39,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:39,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:39,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:39,681 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:39,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:39,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:39,707 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:39,707 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:39,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:39,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:39,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:39,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:39,792 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:39,792 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:39,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:39,843 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:39,852 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:39,854 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:39,854 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:39,859 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:41,241 - distributed.nanny - INFO - Worker process 27715 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:41,650 - distributed.nanny - INFO - Worker process 27711 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:41,692 - distributed.nanny - INFO - Worker process 27721 exited with status 127
2023-05-30 06:10:41,737 - distributed.nanny - INFO - Worker process 27727 exited with status 127
2023-05-30 06:10:41,757 - distributed.nanny - INFO - Worker process 27733 exited with status 127
2023-05-30 06:10:41,778 - distributed.nanny - INFO - Worker process 27718 exited with status 127
2023-05-30 06:10:41,801 - distributed.nanny - INFO - Worker process 27730 exited with status 127
2023-05-30 06:10:41,822 - distributed.nanny - INFO - Worker process 27724 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-30 06:11:10,254 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:11:10,259 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36241 instead
  warnings.warn(
2023-05-30 06:11:10,262 - distributed.scheduler - INFO - State start
2023-05-30 06:11:10,321 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:11:10,322 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:11:10,323 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:11:10,323 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:11:10,554 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35589'
2023-05-30 06:11:10,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35149'
2023-05-30 06:11:10,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38095'
2023-05-30 06:11:10,592 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34969'
2023-05-30 06:11:10,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37339'
2023-05-30 06:11:10,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36525'
2023-05-30 06:11:10,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45267'
2023-05-30 06:11:10,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45369'
2023-05-30 06:11:12,096 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c3ldcjy4', purging
2023-05-30 06:11:12,096 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5atmkbfb', purging
2023-05-30 06:11:12,096 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gcf0pigo', purging
2023-05-30 06:11:12,097 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-orc7k3oa', purging
2023-05-30 06:11:12,097 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yegombne', purging
2023-05-30 06:11:12,097 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6s4tuem5', purging
2023-05-30 06:11:12,098 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xkuo0nkf', purging
2023-05-30 06:11:12,098 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l1evfmgr', purging
2023-05-30 06:11:12,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:12,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:12,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:12,165 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:12,229 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:12,229 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:12,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:12,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:12,280 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:12,291 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:12,300 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:12,342 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:12,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:12,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:12,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:12,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:12,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:12,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:12,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:12,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:12,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:12,428 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:12,435 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:12,475 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:16,293 - distributed.nanny - INFO - Worker process 27977 exited with status 127
2023-05-30 06:11:16,294 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:16,384 - distributed.nanny - INFO - Worker process 27974 exited with status 127
2023-05-30 06:11:16,385 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:16,411 - distributed.nanny - INFO - Worker process 27966 exited with status 127
2023-05-30 06:11:16,412 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:16,441 - distributed.nanny - INFO - Worker process 27983 exited with status 127
2023-05-30 06:11:16,442 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:16,520 - distributed.nanny - INFO - Worker process 27962 exited with status 127
2023-05-30 06:11:16,521 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:16,623 - distributed.nanny - INFO - Worker process 27980 exited with status 127
2023-05-30 06:11:16,624 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:16,647 - distributed.nanny - INFO - Worker process 27959 exited with status 127
2023-05-30 06:11:16,648 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:16,683 - distributed.nanny - INFO - Worker process 27970 exited with status 127
2023-05-30 06:11:16,684 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:17,871 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2a8fyw6x', purging
2023-05-30 06:11:17,872 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mwb5uvz9', purging
2023-05-30 06:11:17,872 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3ox3p7jh', purging
2023-05-30 06:11:17,873 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i7vq38sv', purging
2023-05-30 06:11:17,873 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9rzj1euy', purging
2023-05-30 06:11:17,874 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9yc9_zw3', purging
2023-05-30 06:11:17,874 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-epdplnfx', purging
2023-05-30 06:11:17,874 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6q65423_', purging
2023-05-30 06:11:17,875 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:17,875 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:17,899 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:18,018 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:18,018 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:18,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:18,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:18,093 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:18,093 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:18,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:18,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:18,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:18,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:18,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:18,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:18,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:18,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:18,251 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:18,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:18,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:18,300 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:18,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:18,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:18,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:18,491 - distributed.nanny - INFO - Worker process 28051 exited with status 127
2023-05-30 06:11:18,492 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:19,943 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rzlua9ew', purging
2023-05-30 06:11:19,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:19,945 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:20,553 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:22,259 - distributed.nanny - INFO - Worker process 28054 exited with status 127
2023-05-30 06:11:22,260 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:22,284 - distributed.nanny - INFO - Worker process 28045 exited with status 127
2023-05-30 06:11:22,284 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:22,331 - distributed.nanny - INFO - Worker process 28060 exited with status 127
2023-05-30 06:11:22,331 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:22,356 - distributed.nanny - INFO - Worker process 28048 exited with status 127
2023-05-30 06:11:22,357 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:22,426 - distributed.nanny - INFO - Worker process 28041 exited with status 127
2023-05-30 06:11:22,428 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:22,557 - distributed.nanny - INFO - Worker process 28057 exited with status 127
2023-05-30 06:11:22,558 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:22,606 - distributed.nanny - INFO - Worker process 28063 exited with status 127
2023-05-30 06:11:22,607 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:23,042 - distributed.nanny - INFO - Worker process 28101 exited with status 127
2023-05-30 06:11:23,044 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:23,700 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rwi0sh3n', purging
2023-05-30 06:11:23,701 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-05bl24wf', purging
2023-05-30 06:11:23,701 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t_mat7yu', purging
2023-05-30 06:11:23,701 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u3cvun8i', purging
2023-05-30 06:11:23,702 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-07eiku1r', purging
2023-05-30 06:11:23,702 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-89qcik2f', purging
2023-05-30 06:11:23,702 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-59sx23oc', purging
2023-05-30 06:11:23,703 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0hl5ajo5', purging
2023-05-30 06:11:23,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:23,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:23,727 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:23,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:23,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:23,971 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:23,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:23,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:24,006 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:24,068 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:24,068 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:24,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:24,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:24,103 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:24,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:24,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:24,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:24,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:24,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:24,383 - distributed.nanny - INFO - Worker process 28129 exited with status 127
2023-05-30 06:11:24,384 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:24,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:24,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:24,710 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kmfzkbl_', purging
2023-05-30 06:11:24,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:24,711 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:24,793 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35589'. Reason: nanny-close
2023-05-30 06:11:24,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38095'. Reason: nanny-close
2023-05-30 06:11:24,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34969'. Reason: nanny-close
2023-05-30 06:11:24,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37339'. Reason: nanny-close
2023-05-30 06:11:24,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36525'. Reason: nanny-close
2023-05-30 06:11:24,794 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45267'. Reason: nanny-close
2023-05-30 06:11:24,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35149'. Reason: nanny-close
2023-05-30 06:11:24,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45369'. Reason: nanny-close
2023-05-30 06:11:24,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:25,350 - distributed.nanny - INFO - Worker process 28138 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:26,000 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1916oq9_', purging
2023-05-30 06:11:26,001 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:26,001 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:27,203 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:27,241 - distributed.nanny - INFO - Worker process 28132 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:28,018 - distributed.nanny - INFO - Worker process 28144 exited with status 127
2023-05-30 06:11:28,076 - distributed.nanny - INFO - Worker process 28135 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:28,206 - distributed.nanny - INFO - Worker process 28147 exited with status 127
2023-05-30 06:11:28,254 - distributed.nanny - INFO - Worker process 28141 exited with status 127
2023-05-30 06:11:28,337 - distributed.nanny - INFO - Worker process 28157 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:28,683 - distributed.nanny - INFO - Worker process 28190 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-30 06:11:56,741 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:11:56,744 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35949 instead
  warnings.warn(
2023-05-30 06:11:56,748 - distributed.scheduler - INFO - State start
2023-05-30 06:11:56,766 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:11:56,767 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:11:56,768 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:11:56,768 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:11:56,905 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37229'
2023-05-30 06:11:56,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46083'
2023-05-30 06:11:56,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42531'
2023-05-30 06:11:56,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35103'
2023-05-30 06:11:56,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39421'
2023-05-30 06:11:56,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44143'
2023-05-30 06:11:56,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42851'
2023-05-30 06:11:56,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32785'
2023-05-30 06:11:58,428 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_aanz51x', purging
2023-05-30 06:11:58,429 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3u6_hm5p', purging
2023-05-30 06:11:58,429 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pgc63amh', purging
2023-05-30 06:11:58,429 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ew3o17g4', purging
2023-05-30 06:11:58,430 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4ex1bg0w', purging
2023-05-30 06:11:58,430 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c84ugdgh', purging
2023-05-30 06:11:58,431 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cc23pjsr', purging
2023-05-30 06:11:58,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:58,432 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:58,456 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:58,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:58,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:58,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:58,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:58,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:58,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:58,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:58,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:58,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:58,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:58,537 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:58,537 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:58,540 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:58,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:58,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:58,542 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:58,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:58,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:58,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:58,580 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:58,735 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:00,166 - distributed.nanny - INFO - Worker process 28410 exited with status 127
2023-05-30 06:12:00,167 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:00,599 - distributed.nanny - INFO - Worker process 28413 exited with status 127
2023-05-30 06:12:00,600 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:00,627 - distributed.nanny - INFO - Worker process 28396 exited with status 127
2023-05-30 06:12:00,627 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:00,650 - distributed.nanny - INFO - Worker process 28392 exited with status 127
2023-05-30 06:12:00,650 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:00,673 - distributed.nanny - INFO - Worker process 28400 exited with status 127
2023-05-30 06:12:00,674 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:00,726 - distributed.nanny - INFO - Worker process 28407 exited with status 127
2023-05-30 06:12:00,727 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:00,748 - distributed.nanny - INFO - Worker process 28404 exited with status 127
2023-05-30 06:12:00,748 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:00,772 - distributed.nanny - INFO - Worker process 28389 exited with status 127
2023-05-30 06:12:00,773 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:01,753 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-18pq8jgz', purging
2023-05-30 06:12:01,753 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1fj7ztb4', purging
2023-05-30 06:12:01,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uklzambc', purging
2023-05-30 06:12:01,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j2h0lfeb', purging
2023-05-30 06:12:01,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bw3ptsa8', purging
2023-05-30 06:12:01,755 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c7cjwfoe', purging
2023-05-30 06:12:01,755 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xj1p00st', purging
2023-05-30 06:12:01,755 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1nzm11h6', purging
2023-05-30 06:12:01,756 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:01,756 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:01,778 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:01,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:01,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:02,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:02,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:02,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:02,249 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:02,249 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:02,299 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ewodq8e6', purging
2023-05-30 06:12:02,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:02,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:02,323 - distributed.nanny - INFO - Worker process 28465 exited with status 127
2023-05-30 06:12:02,323 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:02,323 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:02,323 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:02,331 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:02,331 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:02,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:02,358 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:02,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:02,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:02,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:02,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:02,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:02,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:02,525 - distributed.nanny - INFO - Worker process 28475 exited with status 127
2023-05-30 06:12:02,526 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:03,803 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ghza59kr', purging
2023-05-30 06:12:03,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:03,804 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:03,897 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:03,906 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:03,907 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:03,943 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:04,004 - distributed.nanny - INFO - Worker process 28484 exited with status 127
2023-05-30 06:12:04,005 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:04,042 - distributed.nanny - INFO - Worker process 28481 exited with status 127
2023-05-30 06:12:04,043 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:04,073 - distributed.nanny - INFO - Worker process 28478 exited with status 127
2023-05-30 06:12:04,073 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:04,098 - distributed.nanny - INFO - Worker process 28487 exited with status 127
2023-05-30 06:12:04,099 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:04,142 - distributed.nanny - INFO - Worker process 28494 exited with status 127
2023-05-30 06:12:04,143 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:04,166 - distributed.nanny - INFO - Worker process 28490 exited with status 127
2023-05-30 06:12:04,167 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:04,910 - distributed.nanny - INFO - Worker process 28523 exited with status 127
2023-05-30 06:12:04,911 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:04,969 - distributed.nanny - INFO - Worker process 28537 exited with status 127
2023-05-30 06:12:04,970 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:05,591 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i9sy3ymf', purging
2023-05-30 06:12:05,591 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8iyk8ukq', purging
2023-05-30 06:12:05,591 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ab_fhtpr', purging
2023-05-30 06:12:05,592 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x_ccdz_6', purging
2023-05-30 06:12:05,592 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sizdt3ge', purging
2023-05-30 06:12:05,592 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d6y26w2u', purging
2023-05-30 06:12:05,593 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9x5zxott', purging
2023-05-30 06:12:05,593 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mr69h9mj', purging
2023-05-30 06:12:05,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:05,594 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:05,618 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:05,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:05,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:05,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:05,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:05,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:05,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:05,688 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:05,692 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:05,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:05,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:05,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:05,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:05,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:05,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:05,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:06,550 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:06,550 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:06,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:06,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:06,646 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:06,646 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:06,855 - distributed.nanny - INFO - Worker process 28566 exited with status 127
2023-05-30 06:12:06,856 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:07,365 - distributed.nanny - INFO - Worker process 28569 exited with status 127
2023-05-30 06:12:07,365 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:07,757 - distributed.nanny - INFO - Worker process 28575 exited with status 127
2023-05-30 06:12:07,757 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:07,783 - distributed.nanny - INFO - Worker process 28572 exited with status 127
2023-05-30 06:12:07,784 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:07,810 - distributed.nanny - INFO - Worker process 28581 exited with status 127
2023-05-30 06:12:07,811 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:07,862 - distributed.nanny - INFO - Worker process 28578 exited with status 127
2023-05-30 06:12:07,863 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:08,230 - distributed.nanny - INFO - Worker process 28595 exited with status 127
2023-05-30 06:12:08,231 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:08,256 - distributed.nanny - INFO - Worker process 28599 exited with status 127
2023-05-30 06:12:08,257 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:08,441 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fxiw6w9g', purging
2023-05-30 06:12:08,442 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vlxufvvo', purging
2023-05-30 06:12:08,442 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0vdwgpcd', purging
2023-05-30 06:12:08,442 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d0mt2s6y', purging
2023-05-30 06:12:08,443 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gtaafbqm', purging
2023-05-30 06:12:08,443 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6uo5i817', purging
2023-05-30 06:12:08,443 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5vydzlrm', purging
2023-05-30 06:12:08,444 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dhcra38i', purging
2023-05-30 06:12:08,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:08,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:08,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:08,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:08,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:08,850 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:08,881 - distributed.nanny - INFO - Worker process 28641 exited with status 127
2023-05-30 06:12:08,882 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:09,270 - distributed.nanny - INFO - Worker process 28648 exited with status 127
2023-05-30 06:12:09,271 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:09,360 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jdu961zl', purging
2023-05-30 06:12:09,361 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-il1czsab', purging
2023-05-30 06:12:09,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:09,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:09,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:09,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:09,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:09,388 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:09,440 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:09,440 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:09,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:09,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:09,470 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:09,483 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:09,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:09,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:09,777 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:09,777 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:09,894 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:09,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:10,432 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:10,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:10,476 - distributed.nanny - INFO - Worker process 28660 exited with status 127
2023-05-30 06:12:10,477 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:10,515 - distributed.nanny - INFO - Worker process 28657 exited with status 127
2023-05-30 06:12:10,516 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:10,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:10,802 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g975mvlg', purging
2023-05-30 06:12:10,803 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vh4w6bae', purging
2023-05-30 06:12:10,803 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:10,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:10,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:11,131 - distributed.nanny - INFO - Worker process 28663 exited with status 127
2023-05-30 06:12:11,132 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:11,171 - distributed.nanny - INFO - Worker process 28666 exited with status 127
2023-05-30 06:12:11,171 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:11,386 - distributed.nanny - INFO - Worker process 28672 exited with status 127
2023-05-30 06:12:11,387 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:11,626 - distributed.nanny - INFO - Worker process 28675 exited with status 127
2023-05-30 06:12:11,627 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:11,975 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h61cweyh', purging
2023-05-30 06:12:11,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0ssofs2h', purging
2023-05-30 06:12:11,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tf80q0al', purging
2023-05-30 06:12:11,976 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kylhveah', purging
2023-05-30 06:12:11,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:11,977 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:12,015 - distributed.nanny - INFO - Worker process 28693 exited with status 127
2023-05-30 06:12:12,016 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:12,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:12,046 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rsputsk1', purging
2023-05-30 06:12:12,046 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1in9y0w8', purging
2023-05-30 06:12:12,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:12,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:12,053 - distributed.nanny - INFO - Worker process 28699 exited with status 127
2023-05-30 06:12:12,054 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:12,067 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37229'. Reason: nanny-close
2023-05-30 06:12:12,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46083'. Reason: nanny-close
2023-05-30 06:12:12,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42531'. Reason: nanny-close
2023-05-30 06:12:12,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35103'. Reason: nanny-close
2023-05-30 06:12:12,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39421'. Reason: nanny-close
2023-05-30 06:12:12,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44143'. Reason: nanny-close
2023-05-30 06:12:12,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42851'. Reason: nanny-close
2023-05-30 06:12:12,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32785'. Reason: nanny-close
2023-05-30 06:12:12,072 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:12,635 - distributed.nanny - INFO - Worker process 28733 exited with status 127
2023-05-30 06:12:12,654 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ga906ev5', purging
2023-05-30 06:12:12,654 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6uc_dx95', purging
2023-05-30 06:12:12,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:12,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:12,667 - distributed.nanny - INFO - Worker process 28736 exited with status 127
2023-05-30 06:12:12,668 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:12,668 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:12,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:12,691 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:12,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:12,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:13,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:13,122 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:13,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:13,165 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:13,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2kughm5s', purging
2023-05-30 06:12:13,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y46dl17i', purging
2023-05-30 06:12:13,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:13,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:13,501 - distributed.nanny - INFO - Worker process 28752 exited with status 127
2023-05-30 06:12:13,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:13,504 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:13,525 - distributed.nanny - INFO - Worker process 28756 exited with status 127
2023-05-30 06:12:13,553 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:13,555 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:13,884 - distributed.nanny - INFO - Worker process 28762 exited with status 127
2023-05-30 06:12:14,207 - distributed.nanny - INFO - Worker process 28765 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:14,391 - distributed.nanny - INFO - Worker process 28782 exited with status 127
2023-05-30 06:12:14,416 - distributed.nanny - INFO - Worker process 28776 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-30 06:12:44,027 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:12:44,030 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-05-30 06:12:44,033 - distributed.scheduler - INFO - State start
2023-05-30 06:12:44,053 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:12:44,054 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-30 06:12:44,054 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-05-30 06:12:44,063 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35235'
2023-05-30 06:12:44,214 - distributed.scheduler - INFO - Receive client connection: Client-fca35231-feb0-11ed-a60b-d8c49764f6bb
2023-05-30 06:12:44,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35096
2023-05-30 06:12:44,486 - distributed.scheduler - INFO - Receive client connection: Client-fd3e271d-feb0-11ed-a615-d8c49764f6bb
2023-05-30 06:12:44,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35032
2023-05-30 06:12:45,410 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-niw2cpr1', purging
2023-05-30 06:12:45,411 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_t0hft4e', purging
2023-05-30 06:12:45,411 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-no37ymkz', purging
2023-05-30 06:12:45,411 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vm868sqt', purging
2023-05-30 06:12:45,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:45,412 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:45,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:46,060 - distributed.nanny - INFO - Worker process 28999 exited with status 127
2023-05-30 06:12:46,061 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:47,394 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t7lw4o8u', purging
2023-05-30 06:12:47,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:47,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:47,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:48,028 - distributed.nanny - INFO - Worker process 29010 exited with status 127
2023-05-30 06:12:48,029 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:49,403 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y27bgywi', purging
2023-05-30 06:12:49,403 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:49,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:49,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:50,039 - distributed.nanny - INFO - Worker process 29020 exited with status 127
2023-05-30 06:12:50,040 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:51,407 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0sj1veel', purging
2023-05-30 06:12:51,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:51,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:51,666 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:52,043 - distributed.nanny - INFO - Worker process 29030 exited with status 127
2023-05-30 06:12:52,044 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:53,383 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0invjskb', purging
2023-05-30 06:12:53,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:53,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:53,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:54,039 - distributed.nanny - INFO - Worker process 29040 exited with status 127
2023-05-30 06:12:54,039 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:54,260 - distributed.scheduler - INFO - Remove client Client-fca35231-feb0-11ed-a60b-d8c49764f6bb
2023-05-30 06:12:54,261 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35096; closing.
2023-05-30 06:12:54,261 - distributed.scheduler - INFO - Remove client Client-fca35231-feb0-11ed-a60b-d8c49764f6bb
2023-05-30 06:12:54,262 - distributed.scheduler - INFO - Close client connection: Client-fca35231-feb0-11ed-a60b-d8c49764f6bb
2023-05-30 06:12:54,262 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35235'. Reason: nanny-close
2023-05-30 06:12:54,507 - distributed.scheduler - INFO - Remove client Client-fd3e271d-feb0-11ed-a615-d8c49764f6bb
2023-05-30 06:12:54,507 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35032; closing.
2023-05-30 06:12:54,507 - distributed.scheduler - INFO - Remove client Client-fd3e271d-feb0-11ed-a615-d8c49764f6bb
2023-05-30 06:12:54,508 - distributed.scheduler - INFO - Close client connection: Client-fd3e271d-feb0-11ed-a615-d8c49764f6bb
2023-05-30 06:12:55,375 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4vi6cdeh', purging
2023-05-30 06:12:55,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:55,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:55,628 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:56,000 - distributed.nanny - INFO - Worker process 29050 exited with status 127
2023-05-30 06:13:09,763 - distributed.scheduler - INFO - Receive client connection: Client-0ce87c49-feb1-11ed-a5db-d8c49764f6bb
2023-05-30 06:13:09,764 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35434
2023-05-30 06:13:19,784 - distributed.scheduler - INFO - Remove client Client-0ce87c49-feb1-11ed-a5db-d8c49764f6bb
2023-05-30 06:13:19,784 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35434; closing.
2023-05-30 06:13:19,785 - distributed.scheduler - INFO - Remove client Client-0ce87c49-feb1-11ed-a5db-d8c49764f6bb
2023-05-30 06:13:19,785 - distributed.scheduler - INFO - Close client connection: Client-0ce87c49-feb1-11ed-a5db-d8c49764f6bb
2023-05-30 06:13:24,294 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-30 06:13:24,295 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:13:24,295 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:13:24,296 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-30 06:13:24,297 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-30 06:13:27,946 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:13:27,951 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43963 instead
  warnings.warn(
2023-05-30 06:13:27,954 - distributed.scheduler - INFO - State start
2023-05-30 06:13:27,973 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:13:27,974 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:13:27,974 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:13:27,975 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:13:28,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40849'
2023-05-30 06:13:29,511 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-stmcbtv6', purging
2023-05-30 06:13:29,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:29,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:29,766 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:30,284 - distributed.nanny - INFO - Worker process 29308 exited with status 127
2023-05-30 06:13:30,285 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:31,581 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1lzr3mq6', purging
2023-05-30 06:13:31,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:31,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:31,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:32,213 - distributed.nanny - INFO - Worker process 29318 exited with status 127
2023-05-30 06:13:32,214 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:33,503 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k0zhuya3', purging
2023-05-30 06:13:33,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:33,503 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:33,754 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:34,128 - distributed.nanny - INFO - Worker process 29328 exited with status 127
2023-05-30 06:13:34,129 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:35,419 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0ziik7ux', purging
2023-05-30 06:13:35,420 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:35,420 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:35,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:36,033 - distributed.nanny - INFO - Worker process 29338 exited with status 127
2023-05-30 06:13:36,034 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:36,516 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40849'. Reason: nanny-close
2023-05-30 06:13:37,326 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2db0z7c2', purging
2023-05-30 06:13:37,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:37,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:37,576 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:37,936 - distributed.nanny - INFO - Worker process 29348 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-30 06:14:08,256 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:14:08,260 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37263 instead
  warnings.warn(
2023-05-30 06:14:08,263 - distributed.scheduler - INFO - State start
2023-05-30 06:14:08,281 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:14:08,282 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:14:08,282 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:14:08,283 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
