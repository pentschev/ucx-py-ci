============================= test session starts ==============================
platform linux -- Python 3.9.17, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1183 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-08-23 05:37:51,835 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:37:51,839 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37159 instead
  warnings.warn(
2023-08-23 05:37:51,843 - distributed.scheduler - INFO - State start
2023-08-23 05:37:51,970 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:37:51,971 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-08-23 05:37:51,972 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37159/status
2023-08-23 05:37:52,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42707'
2023-08-23 05:37:52,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39355'
2023-08-23 05:37:52,087 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39755'
2023-08-23 05:37:52,095 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33213'
2023-08-23 05:37:53,049 - distributed.scheduler - INFO - Receive client connection: Client-32b79b3c-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:37:53,065 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35432
2023-08-23 05:37:53,822 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:37:53,822 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:37:53,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:37:53,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:37:53,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:37:53,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:37:53,827 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:37:53,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:37:53,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:37:53,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:37:53,830 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:37:53,833 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-08-23 05:37:53,844 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43293
2023-08-23 05:37:53,844 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43293
2023-08-23 05:37:53,844 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34467
2023-08-23 05:37:53,844 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-23 05:37:53,844 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:53,844 - distributed.worker - INFO -               Threads:                          4
2023-08-23 05:37:53,844 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-23 05:37:53,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-1lvndzc0
2023-08-23 05:37:53,845 - distributed.worker - INFO - Starting Worker plugin PreImport-100f1f24-4189-4244-b54d-81b61f42fd8c
2023-08-23 05:37:53,845 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef25b404-14d8-4e1c-b6db-a4ba0ba6b77e
2023-08-23 05:37:53,845 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-176fcf12-8d5e-4c9e-8f3d-303f3f002f8b
2023-08-23 05:37:53,845 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:54,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43293', status: init, memory: 0, processing: 0>
2023-08-23 05:37:54,478 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43293
2023-08-23 05:37:54,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35450
2023-08-23 05:37:54,479 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:37:54,480 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-23 05:37:54,480 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:54,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-23 05:37:55,129 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33029
2023-08-23 05:37:55,129 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33029
2023-08-23 05:37:55,130 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45481
2023-08-23 05:37:55,130 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-23 05:37:55,130 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,130 - distributed.worker - INFO -               Threads:                          4
2023-08-23 05:37:55,130 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-23 05:37:55,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-3uj0109_
2023-08-23 05:37:55,131 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca3fd48b-647a-4c7c-b00d-92afca7c1ad2
2023-08-23 05:37:55,131 - distributed.worker - INFO - Starting Worker plugin PreImport-e1439779-a8fb-49c2-b8d7-23581f2745fe
2023-08-23 05:37:55,131 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39a3ac81-f17d-48c4-939d-937b8d3f1e53
2023-08-23 05:37:55,132 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,140 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33039
2023-08-23 05:37:55,140 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33039
2023-08-23 05:37:55,140 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32991
2023-08-23 05:37:55,140 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-23 05:37:55,141 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,141 - distributed.worker - INFO -               Threads:                          4
2023-08-23 05:37:55,141 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-23 05:37:55,141 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-d56g7fgy
2023-08-23 05:37:55,141 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54ce0a39-0fa8-4cda-8f24-c8316e054298
2023-08-23 05:37:55,142 - distributed.worker - INFO - Starting Worker plugin PreImport-f3748b9e-4ac5-480c-a112-0e97cd4dc1bc
2023-08-23 05:37:55,142 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6406e1a6-db13-4c37-bce7-f77addc9e2e8
2023-08-23 05:37:55,141 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39573
2023-08-23 05:37:55,142 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,142 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39573
2023-08-23 05:37:55,142 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42911
2023-08-23 05:37:55,142 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-23 05:37:55,143 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,143 - distributed.worker - INFO -               Threads:                          4
2023-08-23 05:37:55,143 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-23 05:37:55,143 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gxvkzdvn
2023-08-23 05:37:55,143 - distributed.worker - INFO - Starting Worker plugin PreImport-5104b047-9062-4df4-850b-45b62db9833e
2023-08-23 05:37:55,144 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-154ac6bc-6fa6-47fd-8f3e-79a42cb60e10
2023-08-23 05:37:55,144 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf559842-60c3-4f7d-b9f1-68b2e149f39e
2023-08-23 05:37:55,144 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,169 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33039', status: init, memory: 0, processing: 0>
2023-08-23 05:37:55,169 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33039
2023-08-23 05:37:55,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56424
2023-08-23 05:37:55,170 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:37:55,171 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33029', status: init, memory: 0, processing: 0>
2023-08-23 05:37:55,171 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-23 05:37:55,171 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,171 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33029
2023-08-23 05:37:55,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56414
2023-08-23 05:37:55,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-23 05:37:55,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:37:55,174 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39573', status: init, memory: 0, processing: 0>
2023-08-23 05:37:55,174 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-23 05:37:55,175 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39573
2023-08-23 05:37:55,175 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,175 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56426
2023-08-23 05:37:55,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:37:55,176 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-23 05:37:55,176 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:37:55,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-23 05:37:55,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-23 05:37:55,250 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-23 05:37:55,250 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-23 05:37:55,250 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-23 05:37:55,251 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-23 05:37:55,255 - distributed.scheduler - INFO - Remove client Client-32b79b3c-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:37:55,255 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35432; closing.
2023-08-23 05:37:55,256 - distributed.scheduler - INFO - Remove client Client-32b79b3c-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:37:55,256 - distributed.scheduler - INFO - Close client connection: Client-32b79b3c-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:37:55,257 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42707'. Reason: nanny-close
2023-08-23 05:37:55,257 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:37:55,258 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39355'. Reason: nanny-close
2023-08-23 05:37:55,258 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:37:55,259 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39573. Reason: nanny-close
2023-08-23 05:37:55,259 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39755'. Reason: nanny-close
2023-08-23 05:37:55,259 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33039. Reason: nanny-close
2023-08-23 05:37:55,259 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:37:55,260 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33213'. Reason: nanny-close
2023-08-23 05:37:55,260 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:37:55,260 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33029. Reason: nanny-close
2023-08-23 05:37:55,260 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-23 05:37:55,260 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56426; closing.
2023-08-23 05:37:55,261 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39573', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769075.2612615')
2023-08-23 05:37:55,261 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43293. Reason: nanny-close
2023-08-23 05:37:55,261 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-23 05:37:55,262 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56424; closing.
2023-08-23 05:37:55,262 - distributed.nanny - INFO - Worker closed
2023-08-23 05:37:55,262 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769075.2627456')
2023-08-23 05:37:55,262 - distributed.nanny - INFO - Worker closed
2023-08-23 05:37:55,263 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-23 05:37:55,263 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35450; closing.
2023-08-23 05:37:55,263 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56414; closing.
2023-08-23 05:37:55,263 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-23 05:37:55,264 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43293', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769075.2640936')
2023-08-23 05:37:55,264 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33029', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769075.2645319')
2023-08-23 05:37:55,264 - distributed.scheduler - INFO - Lost all workers
2023-08-23 05:37:55,264 - distributed.nanny - INFO - Worker closed
2023-08-23 05:37:55,265 - distributed.nanny - INFO - Worker closed
2023-08-23 05:37:56,423 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:37:56,424 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:37:56,424 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:37:56,425 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-08-23 05:37:56,425 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-08-23 05:37:58,575 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:37:58,580 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38135 instead
  warnings.warn(
2023-08-23 05:37:58,583 - distributed.scheduler - INFO - State start
2023-08-23 05:37:58,604 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:37:58,605 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-08-23 05:37:58,605 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:37:58,606 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 608, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3908, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 801, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-08-23 05:37:58,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46585'
2023-08-23 05:37:58,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39111'
2023-08-23 05:37:58,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38189'
2023-08-23 05:37:58,757 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46447'
2023-08-23 05:37:58,759 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46707'
2023-08-23 05:37:58,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37043'
2023-08-23 05:37:58,778 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46283'
2023-08-23 05:37:58,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38733'
2023-08-23 05:38:00,606 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:00,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:00,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:00,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:00,611 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:00,615 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:00,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:00,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:00,629 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:00,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:00,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:00,678 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:00,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:00,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:00,690 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:00,708 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:00,708 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:00,713 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:00,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:00,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:00,722 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:00,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:00,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:00,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:03,089 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42469
2023-08-23 05:38:03,090 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42469
2023-08-23 05:38:03,090 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39857
2023-08-23 05:38:03,090 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,090 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,090 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:03,091 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:03,091 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-54tag8rh
2023-08-23 05:38:03,091 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d728e43-3127-4034-8e1c-9b5a79f5a334
2023-08-23 05:38:03,091 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb2c0c76-b58c-4317-ac6d-952c174d12fe
2023-08-23 05:38:03,235 - distributed.worker - INFO - Starting Worker plugin PreImport-a8bfc7ad-0400-495d-b559-73f45f9a8333
2023-08-23 05:38:03,236 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,293 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:03,294 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,294 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:03,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40747
2023-08-23 05:38:03,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40747
2023-08-23 05:38:03,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33617
2023-08-23 05:38:03,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,535 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,535 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:03,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:03,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tkvepxmw
2023-08-23 05:38:03,536 - distributed.worker - INFO - Starting Worker plugin PreImport-65a39889-3c91-4d80-b8d7-5a7050e3cb3e
2023-08-23 05:38:03,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96680fe6-2bd4-4abc-ae7a-afc1d597a032
2023-08-23 05:38:03,537 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64661d1b-8e4e-4470-b0fb-7010ea725cd3
2023-08-23 05:38:03,537 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37135
2023-08-23 05:38:03,538 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37135
2023-08-23 05:38:03,538 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39627
2023-08-23 05:38:03,538 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,538 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,538 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:03,538 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:03,538 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mrgkp4ri
2023-08-23 05:38:03,539 - distributed.worker - INFO - Starting Worker plugin PreImport-c4072f72-8e46-4d83-bd7a-c1d0c4edc32a
2023-08-23 05:38:03,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e7a1a49-eb4d-4ab5-8dae-ca4f6a34544a
2023-08-23 05:38:03,539 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52ffc6b6-50f6-46ad-b670-458213886944
2023-08-23 05:38:03,545 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44089
2023-08-23 05:38:03,546 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44089
2023-08-23 05:38:03,546 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35227
2023-08-23 05:38:03,546 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,546 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,546 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:03,546 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:03,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vwnft3qz
2023-08-23 05:38:03,547 - distributed.worker - INFO - Starting Worker plugin PreImport-8e90c580-3b42-4901-b47a-5ac123c59129
2023-08-23 05:38:03,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2dba8569-a819-4054-93c8-2d8332f43a32
2023-08-23 05:38:03,550 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38003
2023-08-23 05:38:03,551 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38003
2023-08-23 05:38:03,551 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46879
2023-08-23 05:38:03,551 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,551 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,551 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:03,551 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:03,551 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fgwqw1r6
2023-08-23 05:38:03,552 - distributed.worker - INFO - Starting Worker plugin PreImport-54c1c166-cb84-4b0b-84c6-71166c4a6e2a
2023-08-23 05:38:03,552 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6cd8c89-bcdd-4666-a0ff-0e97b36c4ebc
2023-08-23 05:38:03,552 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3923bf9-70af-4349-a134-3c9ccb6ad951
2023-08-23 05:38:03,553 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43257
2023-08-23 05:38:03,555 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43257
2023-08-23 05:38:03,555 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37459
2023-08-23 05:38:03,555 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,555 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,555 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:03,555 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:03,555 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_nys2vtf
2023-08-23 05:38:03,556 - distributed.worker - INFO - Starting Worker plugin PreImport-8a1b2225-db30-409a-bc9e-30a92bd35ab5
2023-08-23 05:38:03,556 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3aea705c-19dd-422b-9d49-10522c210fcc
2023-08-23 05:38:03,557 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dcc56b7b-9060-4e99-adad-27bae42a3137
2023-08-23 05:38:03,557 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40647
2023-08-23 05:38:03,559 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40647
2023-08-23 05:38:03,559 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46583
2023-08-23 05:38:03,559 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,559 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,559 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:03,559 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:03,559 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vc75qf3k
2023-08-23 05:38:03,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6cd8850e-105f-43b1-8e14-62a914c5b270
2023-08-23 05:38:03,561 - distributed.worker - INFO - Starting Worker plugin PreImport-ba4b840c-b41c-4887-a785-0fdbf5bda8d6
2023-08-23 05:38:03,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-867f93d3-e7a6-4cc2-859a-9e5088b495bc
2023-08-23 05:38:03,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44445
2023-08-23 05:38:03,591 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44445
2023-08-23 05:38:03,592 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33429
2023-08-23 05:38:03,592 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,592 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,592 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:03,592 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:03,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jd9il9h_
2023-08-23 05:38:03,594 - distributed.worker - INFO - Starting Worker plugin PreImport-e0042fa1-5487-483a-9714-c5b9e5fd5ee4
2023-08-23 05:38:03,594 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-086835b7-58e0-4f73-aff6-1ea10cf24b9a
2023-08-23 05:38:03,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e012e28-a934-4293-b2d4-f867f6a40df3
2023-08-23 05:38:03,736 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,736 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,736 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,737 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5d3b070-e071-4c97-9753-a747d2ced679
2023-08-23 05:38:03,737 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,737 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,737 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,738 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,771 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:03,772 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,772 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:03,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:03,775 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,775 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:03,779 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:03,780 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,780 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,781 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:03,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:03,782 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,782 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:03,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,784 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:03,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:03,787 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:03,788 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,788 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,788 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:03,789 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:03,789 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:03,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:03,791 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:03,888 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:03,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:03,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:03,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:03,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:03,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:03,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:03,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:04,174 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46585'. Reason: nanny-close
2023-08-23 05:38:04,174 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:04,175 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39111'. Reason: nanny-close
2023-08-23 05:38:04,175 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:04,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38189'. Reason: nanny-close
2023-08-23 05:38:04,175 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37135. Reason: nanny-close
2023-08-23 05:38:04,176 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:04,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46447'. Reason: nanny-close
2023-08-23 05:38:04,176 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40647. Reason: nanny-close
2023-08-23 05:38:04,177 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:04,177 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42469. Reason: nanny-close
2023-08-23 05:38:04,177 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46707'. Reason: nanny-close
2023-08-23 05:38:04,177 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44445. Reason: nanny-close
2023-08-23 05:38:04,177 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:04,178 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37043'. Reason: nanny-close
2023-08-23 05:38:04,178 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:04,179 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:04,179 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:04,179 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:04,179 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46283'. Reason: nanny-close
2023-08-23 05:38:04,179 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44089. Reason: nanny-close
2023-08-23 05:38:04,179 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:04,180 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:04,180 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38733'. Reason: nanny-close
2023-08-23 05:38:04,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38003. Reason: nanny-close
2023-08-23 05:38:04,180 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:04,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43257. Reason: nanny-close
2023-08-23 05:38:04,181 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:04,181 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:04,181 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40747. Reason: nanny-close
2023-08-23 05:38:04,181 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:04,181 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:04,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:04,183 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:04,183 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:04,183 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:04,184 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:04,184 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:04,184 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:04,185 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-08-23 05:38:07,655 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:38:07,660 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39361 instead
  warnings.warn(
2023-08-23 05:38:07,663 - distributed.scheduler - INFO - State start
2023-08-23 05:38:07,687 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:38:07,688 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-23 05:38:07,689 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39361/status
2023-08-23 05:38:07,885 - distributed.scheduler - INFO - Receive client connection: Client-3c14a7ff-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:38:07,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60506
2023-08-23 05:38:07,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42047'
2023-08-23 05:38:08,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44091'
2023-08-23 05:38:08,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36775'
2023-08-23 05:38:08,063 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45255'
2023-08-23 05:38:08,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43333'
2023-08-23 05:38:08,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41355'
2023-08-23 05:38:08,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36185'
2023-08-23 05:38:08,115 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33423'
2023-08-23 05:38:09,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:09,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:09,796 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:09,877 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:09,877 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:09,883 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:09,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:09,884 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:09,889 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:10,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:10,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:10,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:10,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:10,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:10,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:10,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:10,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:10,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:10,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:10,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:10,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:10,111 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:10,111 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:10,114 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:11,713 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41225
2023-08-23 05:38:11,713 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41225
2023-08-23 05:38:11,713 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43205
2023-08-23 05:38:11,713 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:11,713 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:11,713 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:11,714 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:11,714 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-to53wlqy
2023-08-23 05:38:11,714 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4bfaa65e-f319-4d19-868a-d7c05c6eb8e0
2023-08-23 05:38:11,714 - distributed.worker - INFO - Starting Worker plugin PreImport-b0284d36-3528-4724-97c6-3c465ca7b86c
2023-08-23 05:38:11,714 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20a14f1f-5b75-405c-908f-ce1dd29cd7dc
2023-08-23 05:38:11,779 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:11,806 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41225', status: init, memory: 0, processing: 0>
2023-08-23 05:38:11,809 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41225
2023-08-23 05:38:11,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60580
2023-08-23 05:38:11,810 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:11,810 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:11,811 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:11,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:12,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37951
2023-08-23 05:38:12,749 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37951
2023-08-23 05:38:12,749 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44623
2023-08-23 05:38:12,749 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,749 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,749 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:12,749 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:12,749 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t0qyares
2023-08-23 05:38:12,749 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35817
2023-08-23 05:38:12,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de85e8b4-381b-47bb-b68d-c6e9cfbed64a
2023-08-23 05:38:12,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35817
2023-08-23 05:38:12,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37537
2023-08-23 05:38:12,750 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,750 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,750 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:12,750 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:12,750 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ee94g4g1
2023-08-23 05:38:12,750 - distributed.worker - INFO - Starting Worker plugin PreImport-bca4879d-c05d-4088-9978-3299de6e76b3
2023-08-23 05:38:12,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f4c44e8-0f43-49f1-863a-c8d0fd3150a5
2023-08-23 05:38:12,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-989cbda2-bb26-4f07-8416-07286713735b
2023-08-23 05:38:12,751 - distributed.worker - INFO - Starting Worker plugin PreImport-6ff92d1f-8d52-490f-86cc-d8579a59f1ae
2023-08-23 05:38:12,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5575e124-06ef-486e-af68-3bdf050dd258
2023-08-23 05:38:12,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33115
2023-08-23 05:38:12,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33115
2023-08-23 05:38:12,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40279
2023-08-23 05:38:12,764 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,764 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,764 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:12,764 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:12,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q4pby124
2023-08-23 05:38:12,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee323ae3-f56f-483c-bf3b-e41d27274d31
2023-08-23 05:38:12,765 - distributed.worker - INFO - Starting Worker plugin PreImport-c9c964f4-62ed-4de0-b6e9-4f7e7cd9c138
2023-08-23 05:38:12,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cd7240a-a110-4b35-a65e-d423a85f0d7d
2023-08-23 05:38:12,782 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36417
2023-08-23 05:38:12,783 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36417
2023-08-23 05:38:12,783 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34985
2023-08-23 05:38:12,783 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,783 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,783 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:12,783 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:12,783 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-136u4bbf
2023-08-23 05:38:12,784 - distributed.worker - INFO - Starting Worker plugin PreImport-11d2d2b3-e5e9-4a7d-bf2e-f940767653ac
2023-08-23 05:38:12,784 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbe412af-dd35-44e2-9e08-e6c5f43092d2
2023-08-23 05:38:12,784 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29da251f-d19d-4ee6-9c4d-6fd56c3cd57b
2023-08-23 05:38:12,787 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38359
2023-08-23 05:38:12,788 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38359
2023-08-23 05:38:12,788 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45719
2023-08-23 05:38:12,788 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,788 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,788 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:12,788 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:12,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hk6jk7id
2023-08-23 05:38:12,789 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41640212-f872-4dd5-8d42-c9e735e1b50f
2023-08-23 05:38:12,789 - distributed.worker - INFO - Starting Worker plugin PreImport-966a048a-d438-43a4-bfd5-dd33fc3b0392
2023-08-23 05:38:12,789 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68d3bc93-60eb-42b0-aca2-d90334154b6b
2023-08-23 05:38:12,818 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35637
2023-08-23 05:38:12,820 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35637
2023-08-23 05:38:12,820 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35209
2023-08-23 05:38:12,820 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,820 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,820 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:12,821 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:12,821 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zwo9xntc
2023-08-23 05:38:12,819 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40485
2023-08-23 05:38:12,821 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40485
2023-08-23 05:38:12,821 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36603
2023-08-23 05:38:12,821 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,821 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,821 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:12,822 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:12,822 - distributed.worker - INFO - Starting Worker plugin PreImport-a90943c3-e1dc-484f-a4e6-31b64414966a
2023-08-23 05:38:12,822 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l3978aui
2023-08-23 05:38:12,822 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85c46405-bbca-496b-b171-fe0c6bde2cfd
2023-08-23 05:38:12,822 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5bd4a3f-4a56-42da-87d6-ea1d59ec9e87
2023-08-23 05:38:12,823 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09250aa9-5fe3-4b0f-83ee-b58bba4395a9
2023-08-23 05:38:12,823 - distributed.worker - INFO - Starting Worker plugin PreImport-a807cb7e-d729-498b-80d4-c3ca88d28b5a
2023-08-23 05:38:12,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a58b4fa0-d6a9-4c4e-aa42-702f53d3cdf3
2023-08-23 05:38:12,828 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,830 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,832 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,832 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,833 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,837 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,838 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33115', status: init, memory: 0, processing: 0>
2023-08-23 05:38:12,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33115
2023-08-23 05:38:12,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60598
2023-08-23 05:38:12,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:12,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36417', status: init, memory: 0, processing: 0>
2023-08-23 05:38:12,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,860 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36417
2023-08-23 05:38:12,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60618
2023-08-23 05:38:12,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:12,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:12,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,862 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:12,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37951', status: init, memory: 0, processing: 0>
2023-08-23 05:38:12,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37951
2023-08-23 05:38:12,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60602
2023-08-23 05:38:12,869 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35637', status: init, memory: 0, processing: 0>
2023-08-23 05:38:12,870 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:12,870 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35637
2023-08-23 05:38:12,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60656
2023-08-23 05:38:12,871 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40485', status: init, memory: 0, processing: 0>
2023-08-23 05:38:12,871 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:12,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,871 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,871 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40485
2023-08-23 05:38:12,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60668
2023-08-23 05:38:12,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,871 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:12,872 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38359', status: init, memory: 0, processing: 0>
2023-08-23 05:38:12,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:12,873 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,873 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38359
2023-08-23 05:38:12,873 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60642
2023-08-23 05:38:12,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:12,874 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35817', status: init, memory: 0, processing: 0>
2023-08-23 05:38:12,874 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35817
2023-08-23 05:38:12,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60626
2023-08-23 05:38:12,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:12,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:12,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,876 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,876 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:12,877 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:12,877 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:12,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:12,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:12,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:12,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:12,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:12,926 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:12,926 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:12,926 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:12,926 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:12,926 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:38:12,931 - distributed.scheduler - INFO - Remove client Client-3c14a7ff-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:38:12,931 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60506; closing.
2023-08-23 05:38:12,931 - distributed.scheduler - INFO - Remove client Client-3c14a7ff-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:38:12,931 - distributed.scheduler - INFO - Close client connection: Client-3c14a7ff-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:38:12,932 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42047'. Reason: nanny-close
2023-08-23 05:38:12,933 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:12,934 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44091'. Reason: nanny-close
2023-08-23 05:38:12,934 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:12,934 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36417. Reason: nanny-close
2023-08-23 05:38:12,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36775'. Reason: nanny-close
2023-08-23 05:38:12,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:12,935 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35637. Reason: nanny-close
2023-08-23 05:38:12,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45255'. Reason: nanny-close
2023-08-23 05:38:12,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:12,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38359. Reason: nanny-close
2023-08-23 05:38:12,936 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43333'. Reason: nanny-close
2023-08-23 05:38:12,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:12,936 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:12,937 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60618; closing.
2023-08-23 05:38:12,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35817. Reason: nanny-close
2023-08-23 05:38:12,937 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41355'. Reason: nanny-close
2023-08-23 05:38:12,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:12,937 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:12,937 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36417', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769092.937507')
2023-08-23 05:38:12,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33115. Reason: nanny-close
2023-08-23 05:38:12,937 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36185'. Reason: nanny-close
2023-08-23 05:38:12,938 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:12,938 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40485. Reason: nanny-close
2023-08-23 05:38:12,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33423'. Reason: nanny-close
2023-08-23 05:38:12,938 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:12,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:12,938 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:12,939 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37951. Reason: nanny-close
2023-08-23 05:38:12,939 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:12,939 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41225. Reason: nanny-close
2023-08-23 05:38:12,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:12,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:12,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:12,940 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60656; closing.
2023-08-23 05:38:12,940 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:12,941 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:12,941 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60642; closing.
2023-08-23 05:38:12,941 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:12,941 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35637', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769092.941724')
2023-08-23 05:38:12,941 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:12,942 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:12,942 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60626; closing.
2023-08-23 05:38:12,942 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:12,943 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:12,943 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38359', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769092.9435544')
2023-08-23 05:38:12,943 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:12,944 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35817', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769092.9441223')
2023-08-23 05:38:12,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60598; closing.
2023-08-23 05:38:12,945 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60668; closing.
2023-08-23 05:38:12,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769092.9457278')
2023-08-23 05:38:12,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769092.946105')
2023-08-23 05:38:12,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60602; closing.
2023-08-23 05:38:12,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60580; closing.
2023-08-23 05:38:12,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37951', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769092.9471545')
2023-08-23 05:38:12,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41225', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769092.9475453')
2023-08-23 05:38:12,947 - distributed.scheduler - INFO - Lost all workers
2023-08-23 05:38:14,400 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:38:14,401 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:38:14,401 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:38:14,403 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-23 05:38:14,403 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-08-23 05:38:16,729 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:38:16,734 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46031 instead
  warnings.warn(
2023-08-23 05:38:16,737 - distributed.scheduler - INFO - State start
2023-08-23 05:38:16,759 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:38:16,759 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-08-23 05:38:16,760 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:38:16,761 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 608, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3908, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 801, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-08-23 05:38:16,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37475'
2023-08-23 05:38:16,996 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40401'
2023-08-23 05:38:17,009 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44413'
2023-08-23 05:38:17,029 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40443'
2023-08-23 05:38:17,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35025'
2023-08-23 05:38:17,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39911'
2023-08-23 05:38:17,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32937'
2023-08-23 05:38:17,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37967'
2023-08-23 05:38:18,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:18,891 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:18,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:18,919 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:18,919 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:18,925 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:18,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:18,939 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:18,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:18,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:18,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:18,970 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:18,970 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:18,971 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:18,975 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:18,995 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:18,995 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:19,000 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:19,011 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:19,011 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:19,016 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:19,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:19,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:19,081 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:23,669 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35307
2023-08-23 05:38:23,670 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35307
2023-08-23 05:38:23,670 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40031
2023-08-23 05:38:23,670 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,670 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,670 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:23,670 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:23,670 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-achn59j7
2023-08-23 05:38:23,671 - distributed.worker - INFO - Starting Worker plugin PreImport-ede16e28-bf2e-46ab-828a-cfee0967009a
2023-08-23 05:38:23,671 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc4b4460-91a1-4edc-ac6a-a5c1c6331ed1
2023-08-23 05:38:23,674 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5671173-fcdb-4f48-a31f-a3d0b0db7187
2023-08-23 05:38:23,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44195
2023-08-23 05:38:23,679 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44195
2023-08-23 05:38:23,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38695
2023-08-23 05:38:23,679 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,679 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,679 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:23,680 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:23,680 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5alif693
2023-08-23 05:38:23,681 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e7a464c-1dab-419f-91ed-b82b48bf5401
2023-08-23 05:38:23,681 - distributed.worker - INFO - Starting Worker plugin PreImport-1f95bb5e-3ba6-4f36-99fb-94898d91bcc2
2023-08-23 05:38:23,681 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ec7840a-9456-4908-850c-7079c97dbcd5
2023-08-23 05:38:23,701 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42379
2023-08-23 05:38:23,701 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42379
2023-08-23 05:38:23,701 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40109
2023-08-23 05:38:23,701 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,702 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,702 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:23,702 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:23,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y2fkkr9x
2023-08-23 05:38:23,702 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd84274d-25b8-45f7-9cb0-ecbed3162da8
2023-08-23 05:38:23,706 - distributed.worker - INFO - Starting Worker plugin PreImport-83e6be0b-3f74-4576-9399-b0b83662c1f6
2023-08-23 05:38:23,707 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8da03e1b-c9f2-4805-b441-e133ae570d59
2023-08-23 05:38:23,746 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44919
2023-08-23 05:38:23,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44919
2023-08-23 05:38:23,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42947
2023-08-23 05:38:23,747 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,747 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,747 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:23,747 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:23,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-alo7nntf
2023-08-23 05:38:23,747 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da6715eb-e3b4-4a1f-96fc-39c57b1886f5
2023-08-23 05:38:23,748 - distributed.worker - INFO - Starting Worker plugin PreImport-05d61216-5cde-4d77-9550-aa190536947d
2023-08-23 05:38:23,748 - distributed.worker - INFO - Starting Worker plugin RMMSetup-36720aa7-53d3-4457-8e3a-2d33c0d353ce
2023-08-23 05:38:23,768 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43383
2023-08-23 05:38:23,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43383
2023-08-23 05:38:23,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38371
2023-08-23 05:38:23,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,769 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,769 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:23,770 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:23,770 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1m3irao0
2023-08-23 05:38:23,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1e1ab94-650b-405c-a840-36b8cf693d86
2023-08-23 05:38:23,770 - distributed.worker - INFO - Starting Worker plugin PreImport-becacbc3-8e97-4855-a1af-74b8b40e4cc0
2023-08-23 05:38:23,770 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38ca2df5-fe2a-4d0c-b02f-b0e5ff504eea
2023-08-23 05:38:23,781 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34223
2023-08-23 05:38:23,782 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34223
2023-08-23 05:38:23,782 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35281
2023-08-23 05:38:23,782 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,782 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,782 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:23,782 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:23,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-euxrnlji
2023-08-23 05:38:23,782 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-006cd9ad-7bc2-4070-922e-f737864805f7
2023-08-23 05:38:23,783 - distributed.worker - INFO - Starting Worker plugin PreImport-0f34367d-d01a-4bf4-81a1-de2c81857298
2023-08-23 05:38:23,783 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9b4d641-86cf-44ae-87a8-c91bad27460a
2023-08-23 05:38:23,783 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42315
2023-08-23 05:38:23,783 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42315
2023-08-23 05:38:23,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41525
2023-08-23 05:38:23,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,784 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,784 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:23,784 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:23,784 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vw38mug3
2023-08-23 05:38:23,784 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd8629e2-a8da-4d6e-ab07-c837ba2ee463
2023-08-23 05:38:23,785 - distributed.worker - INFO - Starting Worker plugin PreImport-65a7f159-9d3e-4f5e-a8bb-4acb671e3ad1
2023-08-23 05:38:23,785 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b6788dc-704a-4a84-94b2-e8bf566783c4
2023-08-23 05:38:23,785 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46689
2023-08-23 05:38:23,786 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46689
2023-08-23 05:38:23,786 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42477
2023-08-23 05:38:23,786 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,786 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,787 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:23,787 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:23,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m_z9q6o9
2023-08-23 05:38:23,787 - distributed.worker - INFO - Starting Worker plugin PreImport-85a794ce-d933-4d27-8890-40a003e746b6
2023-08-23 05:38:23,788 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb95e9ae-cff1-4c1b-8606-ac803fcee909
2023-08-23 05:38:23,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-93f4679f-f63c-43c0-9cdf-1596b6d25093
2023-08-23 05:38:23,871 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:23,921 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:23,921 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:23,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:24,003 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,014 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,016 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,023 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,028 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,035 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,036 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,045 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:24,046 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:24,046 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,047 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:24,047 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:24,047 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:24,048 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:24,049 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:24,049 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:24,049 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,050 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:24,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:24,060 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:24,060 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:24,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:24,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:24,067 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:24,067 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,068 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:24,068 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,069 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:24,069 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:24,080 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:24,081 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:24,081 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:24,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:32,480 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37475'. Reason: nanny-close
2023-08-23 05:38:32,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:32,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40401'. Reason: nanny-close
2023-08-23 05:38:32,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:32,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44413'. Reason: nanny-close
2023-08-23 05:38:32,483 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:32,483 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46689. Reason: nanny-close
2023-08-23 05:38:32,483 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35307. Reason: nanny-close
2023-08-23 05:38:32,483 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40443'. Reason: nanny-close
2023-08-23 05:38:32,483 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:32,483 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43383. Reason: nanny-close
2023-08-23 05:38:32,484 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35025'. Reason: nanny-close
2023-08-23 05:38:32,484 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:32,484 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34223. Reason: nanny-close
2023-08-23 05:38:32,484 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39911'. Reason: nanny-close
2023-08-23 05:38:32,484 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:32,485 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32937'. Reason: nanny-close
2023-08-23 05:38:32,485 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44195. Reason: nanny-close
2023-08-23 05:38:32,485 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:32,485 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:32,485 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37967'. Reason: nanny-close
2023-08-23 05:38:32,485 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42379. Reason: nanny-close
2023-08-23 05:38:32,486 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:32,486 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:32,486 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:32,486 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44919. Reason: nanny-close
2023-08-23 05:38:32,486 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:32,487 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42315. Reason: nanny-close
2023-08-23 05:38:32,487 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:32,487 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:32,488 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:32,488 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:32,488 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:32,488 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:32,488 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:32,489 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:32,489 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:32,490 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:32,490 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:32,490 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-08-23 05:38:35,924 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:38:35,928 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42005 instead
  warnings.warn(
2023-08-23 05:38:35,931 - distributed.scheduler - INFO - State start
2023-08-23 05:38:35,952 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:38:35,953 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-23 05:38:35,953 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42005/status
2023-08-23 05:38:35,971 - distributed.scheduler - INFO - Receive client connection: Client-4d331e32-4177-11ee-b13c-d8c49764f6bb
2023-08-23 05:38:35,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58510
2023-08-23 05:38:36,104 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35567'
2023-08-23 05:38:36,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44995'
2023-08-23 05:38:36,127 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42573'
2023-08-23 05:38:36,142 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44141'
2023-08-23 05:38:36,144 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40163'
2023-08-23 05:38:36,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39389'
2023-08-23 05:38:36,161 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45843'
2023-08-23 05:38:36,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45873'
2023-08-23 05:38:37,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:37,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:37,865 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:37,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:37,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:37,953 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:37,953 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:37,955 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:37,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:37,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:37,959 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:37,960 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:37,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:37,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:37,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:37,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:37,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:37,971 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:37,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:37,972 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:37,977 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:38,022 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:38,022 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:38,026 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:38,189 - distributed.scheduler - INFO - Receive client connection: Client-4ce84a0a-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:38:38,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58638
2023-08-23 05:38:39,743 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45371
2023-08-23 05:38:39,745 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45371
2023-08-23 05:38:39,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32833
2023-08-23 05:38:39,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:39,745 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:39,745 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:39,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:39,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ppvr4yka
2023-08-23 05:38:39,746 - distributed.worker - INFO - Starting Worker plugin PreImport-6aefcafa-5211-443f-8cbb-0f4a63de0c50
2023-08-23 05:38:39,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d20cae8-0a78-480e-9a50-328b035744b4
2023-08-23 05:38:39,746 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c996b25-0d12-42d8-b0b3-d97ecf8253a3
2023-08-23 05:38:40,291 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,330 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45371', status: init, memory: 0, processing: 0>
2023-08-23 05:38:40,331 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45371
2023-08-23 05:38:40,331 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58646
2023-08-23 05:38:40,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:40,333 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:40,333 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:40,364 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:40,368 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:40,370 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:40,374 - distributed.scheduler - INFO - Remove client Client-4d331e32-4177-11ee-b13c-d8c49764f6bb
2023-08-23 05:38:40,374 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58510; closing.
2023-08-23 05:38:40,374 - distributed.scheduler - INFO - Remove client Client-4d331e32-4177-11ee-b13c-d8c49764f6bb
2023-08-23 05:38:40,375 - distributed.scheduler - INFO - Close client connection: Client-4d331e32-4177-11ee-b13c-d8c49764f6bb
2023-08-23 05:38:40,785 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45137
2023-08-23 05:38:40,786 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45137
2023-08-23 05:38:40,786 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34267
2023-08-23 05:38:40,786 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:40,787 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,787 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:40,787 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:40,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3_kmjji3
2023-08-23 05:38:40,787 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3091b863-0bbf-41a3-a65d-4150390f0e7b
2023-08-23 05:38:40,788 - distributed.worker - INFO - Starting Worker plugin PreImport-16d8d60d-29d6-454c-8a0c-b0eaf4ddbdaa
2023-08-23 05:38:40,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0d699a5-17f9-45c2-b30c-602b233b7d54
2023-08-23 05:38:40,802 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44891
2023-08-23 05:38:40,803 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44891
2023-08-23 05:38:40,803 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38249
2023-08-23 05:38:40,803 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:40,803 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,803 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:40,803 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:40,803 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iswdfosg
2023-08-23 05:38:40,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd5d4a69-f343-4a63-a6cb-0c07d4d6a289
2023-08-23 05:38:40,804 - distributed.worker - INFO - Starting Worker plugin PreImport-fe0b5104-1507-48db-813d-653516f7c344
2023-08-23 05:38:40,804 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea536fc7-9549-484c-b950-e35e50b3aab7
2023-08-23 05:38:40,805 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44165
2023-08-23 05:38:40,806 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44165
2023-08-23 05:38:40,806 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37969
2023-08-23 05:38:40,806 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:40,806 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,806 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:40,807 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:40,807 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i0dn9w94
2023-08-23 05:38:40,807 - distributed.worker - INFO - Starting Worker plugin PreImport-b68f1978-05a2-41a0-87e4-051cdfe495dc
2023-08-23 05:38:40,807 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf1ccc49-5722-454d-8b1d-929f4a7d637a
2023-08-23 05:38:40,808 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4e6f858-d208-4edf-9a75-f808bd44b5c8
2023-08-23 05:38:40,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42697
2023-08-23 05:38:40,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42697
2023-08-23 05:38:40,873 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37093
2023-08-23 05:38:40,873 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:40,873 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,873 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:40,873 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:40,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eipvcnwg
2023-08-23 05:38:40,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f432d02-2ee5-4003-b859-cd6c0d75a6a5
2023-08-23 05:38:40,874 - distributed.worker - INFO - Starting Worker plugin PreImport-ce9a419a-9eb6-443b-b0dc-70be7d6d4449
2023-08-23 05:38:40,874 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d39522a1-fa83-4d4f-bf0e-f714575d05d1
2023-08-23 05:38:40,878 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41111
2023-08-23 05:38:40,879 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41111
2023-08-23 05:38:40,879 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33637
2023-08-23 05:38:40,879 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:40,879 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,879 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:40,879 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:40,879 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-txz68kng
2023-08-23 05:38:40,880 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d1d36ed0-af81-40b3-b35e-7614daf5a1e7
2023-08-23 05:38:40,880 - distributed.worker - INFO - Starting Worker plugin PreImport-d87ec68f-9978-4659-9b80-48ee1c45edae
2023-08-23 05:38:40,880 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37f4567c-7be6-483a-ae40-b0cc32485e56
2023-08-23 05:38:40,887 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42173
2023-08-23 05:38:40,887 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42173
2023-08-23 05:38:40,887 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39531
2023-08-23 05:38:40,888 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:40,888 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,888 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:40,888 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:40,888 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o1lslluj
2023-08-23 05:38:40,888 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02390eeb-9dc3-4ac0-a267-ce468f1bd262
2023-08-23 05:38:40,890 - distributed.worker - INFO - Starting Worker plugin PreImport-50cf3ba3-9832-4700-bc8f-e58f0cbd6660
2023-08-23 05:38:40,891 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1f31123-c32d-4fcb-9ec3-7c0ff2587c49
2023-08-23 05:38:40,890 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37621
2023-08-23 05:38:40,892 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37621
2023-08-23 05:38:40,892 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44921
2023-08-23 05:38:40,892 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:40,892 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:40,892 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:40,893 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:40,893 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dhcbnoqb
2023-08-23 05:38:40,894 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6690872b-7324-4055-9cd7-a705edf64865
2023-08-23 05:38:40,894 - distributed.worker - INFO - Starting Worker plugin PreImport-71653176-a592-4923-85fb-2ce6230bdbc0
2023-08-23 05:38:40,901 - distributed.worker - INFO - Starting Worker plugin RMMSetup-399d6b89-3b4d-49b0-9d2a-d9571d45fae8
2023-08-23 05:38:41,044 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,045 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,065 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,075 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44891', status: init, memory: 0, processing: 0>
2023-08-23 05:38:41,076 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44891
2023-08-23 05:38:41,076 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58656
2023-08-23 05:38:41,077 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38235', status: init, memory: 0, processing: 0>
2023-08-23 05:38:41,077 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:41,077 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38235
2023-08-23 05:38:41,077 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58654
2023-08-23 05:38:41,078 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:41,078 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,079 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:41,093 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45137', status: init, memory: 0, processing: 0>
2023-08-23 05:38:41,093 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,094 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45137
2023-08-23 05:38:41,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58664
2023-08-23 05:38:41,095 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:41,096 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:41,096 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,096 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,098 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:41,101 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,103 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44165', status: init, memory: 0, processing: 0>
2023-08-23 05:38:41,111 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44165
2023-08-23 05:38:41,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58672
2023-08-23 05:38:41,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:41,113 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:41,113 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:41,124 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42697', status: init, memory: 0, processing: 0>
2023-08-23 05:38:41,125 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42697
2023-08-23 05:38:41,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58688
2023-08-23 05:38:41,126 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41111', status: init, memory: 0, processing: 0>
2023-08-23 05:38:41,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:41,127 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41111
2023-08-23 05:38:41,127 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:41,127 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58698
2023-08-23 05:38:41,127 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,127 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58654; closing.
2023-08-23 05:38:41,127 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38235', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.127841')
2023-08-23 05:38:41,128 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:41,128 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:41,129 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:41,129 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:41,139 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37621', status: init, memory: 0, processing: 0>
2023-08-23 05:38:41,139 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37621
2023-08-23 05:38:41,139 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58724
2023-08-23 05:38:41,140 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:41,141 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:41,141 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,142 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42173', status: init, memory: 0, processing: 0>
2023-08-23 05:38:41,143 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:41,143 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42173
2023-08-23 05:38:41,143 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58708
2023-08-23 05:38:41,144 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:38:41,145 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:38:41,145 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:41,147 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:38:41,193 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,193 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,193 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,193 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,193 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,194 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,194 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,194 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:41,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:41,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:41,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:41,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:41,205 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:41,205 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:41,205 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:38:41,211 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,212 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:38:41,215 - distributed.scheduler - INFO - Remove client Client-4ce84a0a-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:38:41,215 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58638; closing.
2023-08-23 05:38:41,215 - distributed.scheduler - INFO - Remove client Client-4ce84a0a-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:38:41,215 - distributed.scheduler - INFO - Close client connection: Client-4ce84a0a-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:38:41,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35567'. Reason: nanny-close
2023-08-23 05:38:41,217 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:41,218 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44995'. Reason: nanny-close
2023-08-23 05:38:41,218 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45371. Reason: nanny-close
2023-08-23 05:38:41,220 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58646; closing.
2023-08-23 05:38:41,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:41,221 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45371', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.2210522')
2023-08-23 05:38:41,218 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:41,222 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:41,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42573'. Reason: nanny-close
2023-08-23 05:38:41,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44165. Reason: nanny-close
2023-08-23 05:38:41,224 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:41,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44141'. Reason: nanny-close
2023-08-23 05:38:41,225 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:41,225 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44891. Reason: nanny-close
2023-08-23 05:38:41,225 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40163'. Reason: nanny-close
2023-08-23 05:38:41,226 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:41,226 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42697. Reason: nanny-close
2023-08-23 05:38:41,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:41,226 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58672; closing.
2023-08-23 05:38:41,226 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39389'. Reason: nanny-close
2023-08-23 05:38:41,226 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:41,226 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.2268028')
2023-08-23 05:38:41,227 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42173. Reason: nanny-close
2023-08-23 05:38:41,227 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:41,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45843'. Reason: nanny-close
2023-08-23 05:38:41,227 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:41,227 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:41,227 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45137. Reason: nanny-close
2023-08-23 05:38:41,227 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45873'. Reason: nanny-close
2023-08-23 05:38:41,228 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:38:41,228 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:41,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58656; closing.
2023-08-23 05:38:41,228 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37621. Reason: nanny-close
2023-08-23 05:38:41,228 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:41,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58688; closing.
2023-08-23 05:38:41,228 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41111. Reason: nanny-close
2023-08-23 05:38:41,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44891', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.2291598')
2023-08-23 05:38:41,229 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:41,229 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:41,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.2297525')
2023-08-23 05:38:41,230 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:41,230 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58708; closing.
2023-08-23 05:38:41,230 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:41,230 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:38:41,231 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42173', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.2315927')
2023-08-23 05:38:41,231 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:41,231 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:41,232 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58664; closing.
2023-08-23 05:38:41,232 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58724; closing.
2023-08-23 05:38:41,232 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:41,232 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58698; closing.
2023-08-23 05:38:41,232 - distributed.nanny - INFO - Worker closed
2023-08-23 05:38:41,232 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.23281')
2023-08-23 05:38:41,233 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.233251')
2023-08-23 05:38:41,233 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41111', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769121.2335665')
2023-08-23 05:38:41,233 - distributed.scheduler - INFO - Lost all workers
2023-08-23 05:38:42,495 - distributed.scheduler - INFO - Receive client connection: Client-51d4eb5a-4177-11ee-b13c-d8c49764f6bb
2023-08-23 05:38:42,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58726
2023-08-23 05:38:42,734 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:38:42,734 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:38:42,735 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:38:42,736 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-23 05:38:42,737 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-08-23 05:38:44,704 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:38:44,708 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46201 instead
  warnings.warn(
2023-08-23 05:38:44,712 - distributed.scheduler - INFO - State start
2023-08-23 05:38:44,733 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:38:44,734 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-08-23 05:38:44,734 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:38:44,735 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 608, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3908, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 801, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-08-23 05:38:45,041 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44427'
2023-08-23 05:38:45,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45399'
2023-08-23 05:38:45,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41619'
2023-08-23 05:38:45,082 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37069'
2023-08-23 05:38:45,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33825'
2023-08-23 05:38:45,094 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36975'
2023-08-23 05:38:45,103 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36813'
2023-08-23 05:38:45,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37063'
2023-08-23 05:38:46,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:46,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:46,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:46,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:46,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:46,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:46,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:46,964 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:46,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:46,966 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:46,967 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:46,967 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:46,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:46,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:46,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:46,980 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:46,980 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:46,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:46,984 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:46,985 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:46,989 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:47,002 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:38:47,002 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:38:47,007 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:38:49,991 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44425
2023-08-23 05:38:49,992 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44425
2023-08-23 05:38:49,992 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45455
2023-08-23 05:38:49,992 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:49,992 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:49,992 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:49,992 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:49,992 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5fwdrztj
2023-08-23 05:38:49,993 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-500e22d8-5eda-48eb-a5e7-a61004c2a6cd
2023-08-23 05:38:49,993 - distributed.worker - INFO - Starting Worker plugin RMMSetup-792e8374-cfd1-41ff-9ec4-213674ea1ca1
2023-08-23 05:38:50,002 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38425
2023-08-23 05:38:50,003 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38425
2023-08-23 05:38:50,003 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36343
2023-08-23 05:38:50,003 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:50,003 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,003 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:50,004 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:50,004 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l4qxisai
2023-08-23 05:38:50,004 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c56ffa3-7106-491a-a842-7eb3421982cc
2023-08-23 05:38:50,004 - distributed.worker - INFO - Starting Worker plugin PreImport-f6c895a4-7d06-437a-b842-6b29b2033300
2023-08-23 05:38:50,005 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30c61d11-b215-4915-83f7-8d135c63da80
2023-08-23 05:38:50,003 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42645
2023-08-23 05:38:50,005 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42645
2023-08-23 05:38:50,005 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39065
2023-08-23 05:38:50,005 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:50,005 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,005 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:50,005 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:50,005 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iqjv76l7
2023-08-23 05:38:50,006 - distributed.worker - INFO - Starting Worker plugin PreImport-d111c4bd-008b-4357-a08c-499a978e6581
2023-08-23 05:38:50,006 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dda077e5-d1dc-431e-9f89-a8c07cdf9ede
2023-08-23 05:38:50,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f490e70b-c6ba-4080-9c88-acd1fc15e20b
2023-08-23 05:38:50,008 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38095
2023-08-23 05:38:50,009 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38095
2023-08-23 05:38:50,009 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34319
2023-08-23 05:38:50,009 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:50,009 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,009 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:50,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:50,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fqp6swg1
2023-08-23 05:38:50,010 - distributed.worker - INFO - Starting Worker plugin PreImport-c04c507d-ffe2-429d-8708-34ef58e276e3
2023-08-23 05:38:50,010 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03124db7-70c7-4b60-9a05-3771d9ccf72a
2023-08-23 05:38:50,010 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e07ddbd-2d17-48ec-bc16-d24bcde0baf5
2023-08-23 05:38:50,028 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40659
2023-08-23 05:38:50,029 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40659
2023-08-23 05:38:50,029 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36983
2023-08-23 05:38:50,029 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:50,028 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43277
2023-08-23 05:38:50,029 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,029 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43277
2023-08-23 05:38:50,029 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42969
2023-08-23 05:38:50,029 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:50,029 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:50,029 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,029 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:50,029 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ajxlfsr1
2023-08-23 05:38:50,029 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:50,029 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:50,029 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rdlhbdea
2023-08-23 05:38:50,030 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-729f132d-e83f-45f2-9e56-00764a3c794b
2023-08-23 05:38:50,030 - distributed.worker - INFO - Starting Worker plugin PreImport-3941693c-8e08-49f3-89f4-68ce32c10f42
2023-08-23 05:38:50,030 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64a2f965-31a5-4744-ae7e-7800be3f6926
2023-08-23 05:38:50,030 - distributed.worker - INFO - Starting Worker plugin PreImport-7356e55f-71f8-441d-b1fd-3118ec30c55e
2023-08-23 05:38:50,031 - distributed.worker - INFO - Starting Worker plugin RMMSetup-499bcfb4-06be-4a08-8114-59e4aff047cd
2023-08-23 05:38:50,031 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37849
2023-08-23 05:38:50,032 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37849
2023-08-23 05:38:50,032 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46265
2023-08-23 05:38:50,032 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:50,032 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,032 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:50,032 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:50,033 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xglfr1da
2023-08-23 05:38:50,033 - distributed.worker - INFO - Starting Worker plugin PreImport-0844509b-5679-41cd-9509-f6e77ff3931c
2023-08-23 05:38:50,033 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbc74dda-ea55-4507-aae7-b29eb1b5c2b2
2023-08-23 05:38:50,033 - distributed.worker - INFO - Starting Worker plugin RMMSetup-686451c9-aa32-408d-9d3d-be7719bc2796
2023-08-23 05:38:50,034 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42647
2023-08-23 05:38:50,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42647
2023-08-23 05:38:50,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37519
2023-08-23 05:38:50,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:38:50,035 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,035 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:38:50,035 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:38:50,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5v_w_6za
2023-08-23 05:38:50,036 - distributed.worker - INFO - Starting Worker plugin PreImport-c0df4501-8e7d-4231-93f0-123667e0aad5
2023-08-23 05:38:50,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0002fc0-f643-4583-af90-e0488e506319
2023-08-23 05:38:50,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-289f2db8-f63f-4031-a044-1dd08d5d9fb6
2023-08-23 05:38:50,181 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,187 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,187 - distributed.worker - INFO - Starting Worker plugin PreImport-7d661a67-b4e1-490a-aa71-37b4c5e11dba
2023-08-23 05:38:50,188 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,188 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,188 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,188 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-868f5def-e832-42b4-a874-5e262a9eefaa
2023-08-23 05:38:50,188 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,189 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:38:50,191 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:39:19,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44427'. Reason: nanny-close
2023-08-23 05:39:19,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45399'. Reason: nanny-close
2023-08-23 05:39:19,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41619'. Reason: nanny-close
2023-08-23 05:39:19,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37069'. Reason: nanny-close
2023-08-23 05:39:19,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33825'. Reason: nanny-close
2023-08-23 05:39:19,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36975'. Reason: nanny-close
2023-08-23 05:39:19,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36813'. Reason: nanny-close
2023-08-23 05:39:19,117 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37063'. Reason: nanny-close
2023-08-23 05:39:20,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:39:20,188 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:39:20,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:39:20,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:39:20,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:39:20,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:39:20,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:39:20,192 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-08-23 05:39:52,271 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:39:52,276 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39255 instead
  warnings.warn(
2023-08-23 05:39:52,280 - distributed.scheduler - INFO - State start
2023-08-23 05:39:52,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-l4qxisai', purging
2023-08-23 05:39:52,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5fwdrztj', purging
2023-08-23 05:39:52,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fqp6swg1', purging
2023-08-23 05:39:52,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5v_w_6za', purging
2023-08-23 05:39:52,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xglfr1da', purging
2023-08-23 05:39:52,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ajxlfsr1', purging
2023-08-23 05:39:52,284 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rdlhbdea', purging
2023-08-23 05:39:52,284 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-iqjv76l7', purging
2023-08-23 05:39:52,334 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:39:52,335 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-23 05:39:52,335 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39255/status
2023-08-23 05:39:52,441 - distributed.scheduler - INFO - Receive client connection: Client-7a681607-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:39:52,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45526
2023-08-23 05:39:52,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45353'
2023-08-23 05:39:54,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:39:54,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:39:55,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:39:58,288 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37519
2023-08-23 05:39:58,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37519
2023-08-23 05:39:58,289 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-08-23 05:39:58,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:39:58,289 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:39:58,289 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:39:58,290 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-23 05:39:58,290 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ukda3gin
2023-08-23 05:39:58,290 - distributed.worker - INFO - Starting Worker plugin PreImport-92e7d442-d63d-4661-809a-6eaf2a12921e
2023-08-23 05:39:58,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6adbe588-88ba-4167-8137-16e50f161e2e
2023-08-23 05:39:58,291 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33edcb5b-e1ce-46ba-8830-dd4e3ca200ed
2023-08-23 05:39:58,291 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:39:58,324 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37519', status: init, memory: 0, processing: 0>
2023-08-23 05:39:58,325 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37519
2023-08-23 05:39:58,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50394
2023-08-23 05:39:58,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:39:58,327 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:39:58,327 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:39:58,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:39:58,355 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:39:58,359 - distributed.scheduler - INFO - Remove client Client-7a681607-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:39:58,359 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45526; closing.
2023-08-23 05:39:58,359 - distributed.scheduler - INFO - Remove client Client-7a681607-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:39:58,359 - distributed.scheduler - INFO - Close client connection: Client-7a681607-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:39:58,361 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45353'. Reason: nanny-close
2023-08-23 05:39:58,362 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:39:58,364 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37519. Reason: nanny-close
2023-08-23 05:39:58,366 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:39:58,366 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50394; closing.
2023-08-23 05:39:58,366 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37519', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769198.3664806')
2023-08-23 05:39:58,366 - distributed.scheduler - INFO - Lost all workers
2023-08-23 05:39:58,367 - distributed.nanny - INFO - Worker closed
2023-08-23 05:39:59,628 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:39:59,628 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:39:59,629 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:39:59,630 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-23 05:39:59,631 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-08-23 05:40:04,545 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:04,549 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38411 instead
  warnings.warn(
2023-08-23 05:40:04,553 - distributed.scheduler - INFO - State start
2023-08-23 05:40:04,577 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:04,578 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-23 05:40:04,579 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38411/status
2023-08-23 05:40:04,689 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34481'
2023-08-23 05:40:06,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:06,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:06,622 - distributed.scheduler - INFO - Receive client connection: Client-8198d371-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:06,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46508
2023-08-23 05:40:07,098 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:08,419 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43759
2023-08-23 05:40:08,420 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43759
2023-08-23 05:40:08,420 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34483
2023-08-23 05:40:08,420 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:08,420 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:08,420 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:08,421 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-23 05:40:08,421 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mf4n5d2t
2023-08-23 05:40:08,421 - distributed.worker - INFO - Starting Worker plugin PreImport-e0955ab8-3fef-469f-8f00-0cb52f7f5d2a
2023-08-23 05:40:08,423 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8657cd27-cbf4-4091-b61d-6304a70658e7
2023-08-23 05:40:08,424 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be467dea-9eed-47d9-a95f-26d6f649c889
2023-08-23 05:40:08,425 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:08,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43759', status: init, memory: 0, processing: 0>
2023-08-23 05:40:08,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43759
2023-08-23 05:40:08,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46532
2023-08-23 05:40:08,484 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:08,485 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:08,485 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:08,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:08,712 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:08,716 - distributed.scheduler - INFO - Remove client Client-8198d371-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:08,716 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46508; closing.
2023-08-23 05:40:08,716 - distributed.scheduler - INFO - Remove client Client-8198d371-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:08,717 - distributed.scheduler - INFO - Close client connection: Client-8198d371-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:08,718 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34481'. Reason: nanny-close
2023-08-23 05:40:08,718 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:08,720 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43759. Reason: nanny-close
2023-08-23 05:40:08,722 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46532; closing.
2023-08-23 05:40:08,723 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:08,723 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769208.7233114')
2023-08-23 05:40:08,723 - distributed.scheduler - INFO - Lost all workers
2023-08-23 05:40:08,725 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:10,537 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:40:10,537 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:40:10,538 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:40:10,538 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-23 05:40:10,539 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-08-23 05:40:12,909 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:12,914 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39575 instead
  warnings.warn(
2023-08-23 05:40:12,919 - distributed.scheduler - INFO - State start
2023-08-23 05:40:12,945 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:12,946 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-23 05:40:12,947 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39575/status
2023-08-23 05:40:18,845 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:40:18,845 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:40:18,846 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:40:18,847 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-23 05:40:18,847 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-08-23 05:40:21,292 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:21,298 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39213 instead
  warnings.warn(
2023-08-23 05:40:21,303 - distributed.scheduler - INFO - State start
2023-08-23 05:40:21,329 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:21,330 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-08-23 05:40:21,331 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39213/status
2023-08-23 05:40:21,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39061'
2023-08-23 05:40:23,501 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:23,502 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:23,506 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:25,151 - distributed.scheduler - INFO - Receive client connection: Client-8b8f9e6c-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:25,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60682
2023-08-23 05:40:26,153 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36513
2023-08-23 05:40:26,154 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36513
2023-08-23 05:40:26,154 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33391
2023-08-23 05:40:26,154 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-23 05:40:26,154 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:26,154 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:26,154 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-23 05:40:26,154 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gf2iw4hn
2023-08-23 05:40:26,155 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-68abf6b1-b3d5-4232-8ffc-e7a0195b89ca
2023-08-23 05:40:26,155 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eccc3239-ade6-46cd-ac73-ff9e12b46c52
2023-08-23 05:40:26,156 - distributed.worker - INFO - Starting Worker plugin PreImport-b344587a-a82a-4b2f-9376-e9cd8e91a659
2023-08-23 05:40:26,156 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:26,188 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36513', status: init, memory: 0, processing: 0>
2023-08-23 05:40:26,189 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36513
2023-08-23 05:40:26,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60702
2023-08-23 05:40:26,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:26,192 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-23 05:40:26,192 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:26,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-23 05:40:26,197 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:26,200 - distributed.scheduler - INFO - Remove client Client-8b8f9e6c-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:26,200 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60682; closing.
2023-08-23 05:40:26,200 - distributed.scheduler - INFO - Remove client Client-8b8f9e6c-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:26,201 - distributed.scheduler - INFO - Close client connection: Client-8b8f9e6c-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:26,202 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39061'. Reason: nanny-close
2023-08-23 05:40:26,202 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:26,204 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36513. Reason: nanny-close
2023-08-23 05:40:26,207 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60702; closing.
2023-08-23 05:40:26,207 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-23 05:40:26,207 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769226.2076454')
2023-08-23 05:40:26,208 - distributed.scheduler - INFO - Lost all workers
2023-08-23 05:40:26,209 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:27,770 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:40:27,771 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:40:27,772 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:40:27,773 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-08-23 05:40:27,773 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-08-23 05:40:30,641 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:30,646 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34957 instead
  warnings.warn(
2023-08-23 05:40:30,651 - distributed.scheduler - INFO - State start
2023-08-23 05:40:30,676 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:30,677 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-23 05:40:30,678 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34957/status
2023-08-23 05:40:30,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43361'
2023-08-23 05:40:31,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32963'
2023-08-23 05:40:31,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44121'
2023-08-23 05:40:31,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42129'
2023-08-23 05:40:31,042 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39039'
2023-08-23 05:40:31,061 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38279'
2023-08-23 05:40:31,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38261'
2023-08-23 05:40:31,096 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36131'
2023-08-23 05:40:31,326 - distributed.scheduler - INFO - Receive client connection: Client-91088ebb-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:31,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49862
2023-08-23 05:40:33,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:33,221 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:33,229 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:33,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:33,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:33,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:33,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:33,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:33,291 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:33,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:33,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:33,312 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:33,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:33,508 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:33,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:33,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:33,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:33,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:33,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:33,577 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:33,577 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:33,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:33,738 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:33,748 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:38,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46223
2023-08-23 05:40:38,270 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46223
2023-08-23 05:40:38,270 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45133
2023-08-23 05:40:38,270 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,270 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,270 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:38,270 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:40:38,270 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tzs14ict
2023-08-23 05:40:38,271 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b0b9f14-90c1-4635-a7fe-66be1e28ab27
2023-08-23 05:40:38,271 - distributed.worker - INFO - Starting Worker plugin PreImport-6d5c5d24-2046-42bb-ab09-77bea63e5a02
2023-08-23 05:40:38,272 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87c7ffa1-6cf6-44c9-b72b-3774c06ae8ed
2023-08-23 05:40:38,308 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33569
2023-08-23 05:40:38,309 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33569
2023-08-23 05:40:38,309 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34859
2023-08-23 05:40:38,309 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,309 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,309 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:38,310 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:40:38,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-259pnab7
2023-08-23 05:40:38,310 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab268f51-74d3-4a80-ad8e-9de057f04dc8
2023-08-23 05:40:38,310 - distributed.worker - INFO - Starting Worker plugin PreImport-18fba2a3-754a-4ff8-9228-f6680a624bd2
2023-08-23 05:40:38,310 - distributed.worker - INFO - Starting Worker plugin RMMSetup-608da3f8-fee8-4311-8ce2-726ed1b08dcc
2023-08-23 05:40:38,326 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43385
2023-08-23 05:40:38,327 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43385
2023-08-23 05:40:38,327 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42615
2023-08-23 05:40:38,327 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,327 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,327 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:38,328 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:40:38,328 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oq4b62k2
2023-08-23 05:40:38,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73f318c8-460e-43d2-b8fb-08a29fae1549
2023-08-23 05:40:38,328 - distributed.worker - INFO - Starting Worker plugin PreImport-28b9457c-171a-4d26-98a8-8fca17d20ad3
2023-08-23 05:40:38,328 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4629110-e887-496d-8ef3-e9be0575f618
2023-08-23 05:40:38,333 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37051
2023-08-23 05:40:38,334 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37051
2023-08-23 05:40:38,334 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32775
2023-08-23 05:40:38,334 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,335 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,335 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:38,335 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:40:38,335 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ggdsx5j6
2023-08-23 05:40:38,335 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc8de11e-3c5e-4cb0-afa6-6fdb1b5174d9
2023-08-23 05:40:38,335 - distributed.worker - INFO - Starting Worker plugin PreImport-c785ae4f-66e4-4893-ac6f-aa3dbb93431d
2023-08-23 05:40:38,335 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7dcf7b4f-e135-4f09-b8c4-7bbe6168bf66
2023-08-23 05:40:38,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40217
2023-08-23 05:40:38,347 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40217
2023-08-23 05:40:38,347 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34931
2023-08-23 05:40:38,347 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,347 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,347 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:38,347 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:40:38,347 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xcxocgfy
2023-08-23 05:40:38,348 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-054a2443-7ed7-403a-8815-10065fd865f8
2023-08-23 05:40:38,348 - distributed.worker - INFO - Starting Worker plugin PreImport-fb207ba9-b1cc-48e5-b75b-4444a9a5939e
2023-08-23 05:40:38,349 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a6a4fc9-8f8e-4161-aa53-f427e9933411
2023-08-23 05:40:38,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32777
2023-08-23 05:40:38,354 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32777
2023-08-23 05:40:38,354 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37665
2023-08-23 05:40:38,354 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,354 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,354 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:38,355 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:40:38,355 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ema03ayn
2023-08-23 05:40:38,356 - distributed.worker - INFO - Starting Worker plugin PreImport-9feaad98-347b-47dc-b593-69dd678e6bad
2023-08-23 05:40:38,356 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7840243-cc34-4ee3-ab0d-8761b8f542a2
2023-08-23 05:40:38,356 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fdbbeb5f-087e-4973-b2d8-d75257335b6e
2023-08-23 05:40:38,394 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46675
2023-08-23 05:40:38,396 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46675
2023-08-23 05:40:38,396 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46307
2023-08-23 05:40:38,396 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,396 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,396 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:38,396 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:40:38,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_lpegffc
2023-08-23 05:40:38,397 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7cecb8d6-5f46-4113-84b6-2cb1f7625d66
2023-08-23 05:40:38,398 - distributed.worker - INFO - Starting Worker plugin PreImport-db68658e-3d96-4145-ae3b-99c86b128214
2023-08-23 05:40:38,398 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e5e2a38-b317-4e62-ba91-8c2eef926dfe
2023-08-23 05:40:38,404 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36743
2023-08-23 05:40:38,406 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36743
2023-08-23 05:40:38,406 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42851
2023-08-23 05:40:38,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,406 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,406 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:38,407 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-23 05:40:38,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c5s3t6yn
2023-08-23 05:40:38,408 - distributed.worker - INFO - Starting Worker plugin PreImport-36eb1f1e-5841-41bc-8cf4-00fb88df2483
2023-08-23 05:40:38,408 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de64bcf9-7877-403f-b4e3-9dd7abdb982e
2023-08-23 05:40:38,408 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5093fdb1-b804-45cb-b6f5-c0a19b50d662
2023-08-23 05:40:38,549 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,561 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,577 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,577 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,577 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,577 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,577 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,578 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43385', status: init, memory: 0, processing: 0>
2023-08-23 05:40:38,587 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43385
2023-08-23 05:40:38,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44328
2023-08-23 05:40:38,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:38,589 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,589 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:38,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33569', status: init, memory: 0, processing: 0>
2023-08-23 05:40:38,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33569
2023-08-23 05:40:38,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44340
2023-08-23 05:40:38,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:38,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,600 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:38,614 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40217', status: init, memory: 0, processing: 0>
2023-08-23 05:40:38,614 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40217
2023-08-23 05:40:38,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44348
2023-08-23 05:40:38,615 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37051', status: init, memory: 0, processing: 0>
2023-08-23 05:40:38,615 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:38,616 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37051
2023-08-23 05:40:38,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44354
2023-08-23 05:40:38,616 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,616 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:38,618 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,618 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,618 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:38,619 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:38,620 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32777', status: init, memory: 0, processing: 0>
2023-08-23 05:40:38,620 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32777
2023-08-23 05:40:38,620 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44372
2023-08-23 05:40:38,621 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36743', status: init, memory: 0, processing: 0>
2023-08-23 05:40:38,622 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36743
2023-08-23 05:40:38,621 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:38,622 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44364
2023-08-23 05:40:38,622 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46675', status: init, memory: 0, processing: 0>
2023-08-23 05:40:38,622 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,622 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,623 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46675
2023-08-23 05:40:38,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44356
2023-08-23 05:40:38,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:38,624 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,624 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,624 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:38,624 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:38,625 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,625 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:38,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46223', status: init, memory: 0, processing: 0>
2023-08-23 05:40:38,626 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46223
2023-08-23 05:40:38,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44380
2023-08-23 05:40:38,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:38,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:38,629 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:38,629 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:38,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:38,685 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:40:38,685 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:40:38,685 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:40:38,686 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:40:38,686 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:40:38,686 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:40:38,686 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:40:38,686 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-23 05:40:38,701 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:38,701 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:38,701 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:38,701 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:38,701 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:38,701 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:38,702 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:38,702 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:38,707 - distributed.scheduler - INFO - Remove client Client-91088ebb-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:38,707 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49862; closing.
2023-08-23 05:40:38,707 - distributed.scheduler - INFO - Remove client Client-91088ebb-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:38,708 - distributed.scheduler - INFO - Close client connection: Client-91088ebb-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:38,709 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43361'. Reason: nanny-close
2023-08-23 05:40:38,710 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:38,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32963'. Reason: nanny-close
2023-08-23 05:40:38,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:38,711 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32777. Reason: nanny-close
2023-08-23 05:40:38,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44121'. Reason: nanny-close
2023-08-23 05:40:38,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:38,712 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42129'. Reason: nanny-close
2023-08-23 05:40:38,712 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36743. Reason: nanny-close
2023-08-23 05:40:38,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:38,713 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37051. Reason: nanny-close
2023-08-23 05:40:38,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39039'. Reason: nanny-close
2023-08-23 05:40:38,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:38,713 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40217. Reason: nanny-close
2023-08-23 05:40:38,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38279'. Reason: nanny-close
2023-08-23 05:40:38,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:38,713 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:38,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44372; closing.
2023-08-23 05:40:38,714 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38261'. Reason: nanny-close
2023-08-23 05:40:38,714 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46223. Reason: nanny-close
2023-08-23 05:40:38,714 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:38,714 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769238.7146535')
2023-08-23 05:40:38,714 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36131'. Reason: nanny-close
2023-08-23 05:40:38,714 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:38,714 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46675. Reason: nanny-close
2023-08-23 05:40:38,714 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:38,714 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:38,715 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43385. Reason: nanny-close
2023-08-23 05:40:38,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44354; closing.
2023-08-23 05:40:38,715 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:38,715 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:38,715 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33569. Reason: nanny-close
2023-08-23 05:40:38,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37051', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769238.7158163')
2023-08-23 05:40:38,716 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:38,716 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:38,716 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:38,716 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:38,717 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44348; closing.
2023-08-23 05:40:38,717 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:38,717 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:38,717 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44364; closing.
2023-08-23 05:40:38,718 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:38,718 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:38,718 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:38,718 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:38,717 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44354>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-23 05:40:38,719 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:38,719 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40217', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769238.7195303')
2023-08-23 05:40:38,719 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36743', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769238.7199314')
2023-08-23 05:40:38,720 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44380; closing.
2023-08-23 05:40:38,720 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46223', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769238.7209373')
2023-08-23 05:40:38,721 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44356; closing.
2023-08-23 05:40:38,721 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44328; closing.
2023-08-23 05:40:38,722 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769238.7220693')
2023-08-23 05:40:38,722 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769238.7225015')
2023-08-23 05:40:38,722 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44340; closing.
2023-08-23 05:40:38,723 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33569', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769238.7233293')
2023-08-23 05:40:38,723 - distributed.scheduler - INFO - Lost all workers
2023-08-23 05:40:38,723 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44340>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-23 05:40:40,579 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:40:40,579 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:40:40,580 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:40:40,581 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-23 05:40:40,581 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-08-23 05:40:43,039 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:43,044 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41073 instead
  warnings.warn(
2023-08-23 05:40:43,049 - distributed.scheduler - INFO - State start
2023-08-23 05:40:43,074 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:43,075 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-23 05:40:43,076 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41073/status
2023-08-23 05:40:43,390 - distributed.scheduler - INFO - Receive client connection: Client-98973132-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:43,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44476
2023-08-23 05:40:43,478 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46371'
2023-08-23 05:40:45,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:45,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:45,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:46,799 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45503
2023-08-23 05:40:46,799 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45503
2023-08-23 05:40:46,800 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39785
2023-08-23 05:40:46,800 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:46,800 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:46,800 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:46,800 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-23 05:40:46,800 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7xbzga8e
2023-08-23 05:40:46,800 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0140c8b2-1bb2-40e2-84bb-1a3b5943c54a
2023-08-23 05:40:46,801 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d75e08d-ac6a-4ebf-bbd5-9c4cbf340e8d
2023-08-23 05:40:46,934 - distributed.worker - INFO - Starting Worker plugin PreImport-3cc75855-3a1f-4e09-9ee7-6977cfe1af4d
2023-08-23 05:40:46,935 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:46,976 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45503', status: init, memory: 0, processing: 0>
2023-08-23 05:40:46,977 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45503
2023-08-23 05:40:46,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35514
2023-08-23 05:40:46,978 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-23 05:40:46,979 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-23 05:40:46,979 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:46,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-23 05:40:47,031 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-23 05:40:47,035 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:47,037 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-23 05:40:47,040 - distributed.scheduler - INFO - Remove client Client-98973132-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:47,040 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44476; closing.
2023-08-23 05:40:47,040 - distributed.scheduler - INFO - Remove client Client-98973132-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:47,041 - distributed.scheduler - INFO - Close client connection: Client-98973132-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:47,042 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46371'. Reason: nanny-close
2023-08-23 05:40:47,043 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-23 05:40:47,044 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45503. Reason: nanny-close
2023-08-23 05:40:47,046 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-23 05:40:47,046 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35514; closing.
2023-08-23 05:40:47,047 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692769247.0469754')
2023-08-23 05:40:47,047 - distributed.scheduler - INFO - Lost all workers
2023-08-23 05:40:47,048 - distributed.nanny - INFO - Worker closed
2023-08-23 05:40:48,660 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:40:48,661 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:40:48,661 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:40:48,662 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-23 05:40:48,663 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-08-23 05:40:51,776 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:51,782 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45113 instead
  warnings.warn(
2023-08-23 05:40:51,786 - distributed.scheduler - INFO - State start
2023-08-23 05:40:51,810 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-23 05:40:51,812 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-23 05:40:51,812 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45113/status
2023-08-23 05:40:51,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37517'
2023-08-23 05:40:54,022 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-23 05:40:54,024 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-23 05:40:54,034 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-23 05:40:55,428 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36107
2023-08-23 05:40:55,429 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36107
2023-08-23 05:40:55,430 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39939
2023-08-23 05:40:55,430 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-23 05:40:55,430 - distributed.worker - INFO - -------------------------------------------------
2023-08-23 05:40:55,430 - distributed.worker - INFO -               Threads:                          1
2023-08-23 05:40:55,430 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-23 05:40:55,430 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ztiyq6lq
2023-08-23 05:40:55,431 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06c5ea5c-7ea3-49b2-854c-c5e3d8312266
2023-08-23 05:40:55,431 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afd36ef9-d1be-4c9d-b08d-d0ada24d2bd3
2023-08-23 05:40:55,443 - distributed.scheduler - INFO - Receive client connection: Client-9d748f35-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:40:55,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41808
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1888, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-08-23 05:40:55,782 - distributed.worker - INFO - Starting Worker plugin PreImport-b3fd7810-0b4a-4d3c-b9f2-45bfc9224eb3
2023-08-23 05:40:55,782 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36107. Reason: failure-to-start-<class 'MemoryError'>
2023-08-23 05:40:55,783 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-08-23 05:40:55,786 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 608, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1888, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 929, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 622, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-08-23 05:40:55,830 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 608, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1888, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 432, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 724, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 865, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 929, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 622, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-08-23 05:40:55,833 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37517'. Reason: nanny-instantiate-failed
2023-08-23 05:40:55,834 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-08-23 05:40:56,339 - distributed.nanny - INFO - Worker process 64678 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 608, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1888, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 608, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 358, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 432, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 724, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 865, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 929, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 622, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-08-23 05:41:05,483 - distributed.scheduler - INFO - Remove client Client-9d748f35-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:41:05,484 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41808; closing.
2023-08-23 05:41:05,484 - distributed.scheduler - INFO - Remove client Client-9d748f35-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:41:05,485 - distributed.scheduler - INFO - Close client connection: Client-9d748f35-4177-11ee-afc6-d8c49764f6bb
2023-08-23 05:41:05,486 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-23 05:41:05,486 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-08-23 05:41:05,487 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-23 05:41:05,489 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-23 05:41:05,489 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout FAILED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44217 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41351 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40893 instead
  warnings.warn(
2023-08-23 05:41:55,365 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:55,540 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:55,547 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:55,574 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f0f3a732af0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:55,750 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:55,757 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:55,792 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f84c64d9ac0>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:56,017 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:56,031 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:56,061 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1353, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1600, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1526, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: CancelledError()
2023-08-23 05:41:56,065 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f4a19cdfac0>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:56,234 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:41:56,246 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://10.33.225.163:48997'.
2023-08-23 05:41:56,249 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://10.33.225.163:48997'. Shutting down.
2023-08-23 05:41:59,461 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
Process SpawnProcess-4:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 154, in _test_ucx_infiniband_nvlink
    res = res.sum().compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 381, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 666, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3278, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2403, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2265, in _gather
    raise exception.with_traceback(traceback)
distributed.scheduler.KilledWorker: Attempted to run task original-array-eab7b03031df3e09d519494ace48ee5f on 3 different workers, but all those workers died while running it. The last worker that attempt to run the task was ucx://10.33.225.163:48997. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34997 instead
  warnings.warn(
2023-08-23 05:42:09,979 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,170 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,183 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,222 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fa71dfe4ac0>>, <Task finished name='Task-19' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-19' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,394 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,406 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,444 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://10.33.225.163:44005'.
2023-08-23 05:42:10,446 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://10.33.225.163:44005'. Shutting down.
2023-08-23 05:42:10,450 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7faa6d824ac0>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,611 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,637 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,658 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://10.33.225.163:48659'.
2023-08-23 05:42:10,658 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://10.33.225.163:48659'. Shutting down.
2023-08-23 05:42:10,665 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f8ee58f0ac0>>, <Task finished name='Task-19' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-19' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,864 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:10,926 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fa68b6e9ac0>>, <Task finished name='Task-21' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-21' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
Process SpawnProcess-5:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 154, in _test_ucx_infiniband_nvlink
    res = res.sum().compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 381, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 666, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3278, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2403, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2265, in _gather
    raise exception.with_traceback(traceback)
distributed.scheduler.KilledWorker: Attempted to run task original-array-c72be40b05baf37ef3ffeb76eb309450 on 3 different workers, but all those workers died while running it. The last worker that attempt to run the task was ucx://10.33.225.163:55349. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36657 instead
  warnings.warn(
2023-08-23 05:42:21,237 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,439 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,455 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,488 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe3b4979ac0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,649 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,666 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,697 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fae43498ac0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,847 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,857 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:21,914 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://10.33.225.163:45200'.
2023-08-23 05:42:21,915 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://10.33.225.163:45200'. Shutting down.
2023-08-23 05:42:21,918 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe38bc3bac0>>, <Task finished name='Task-18' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-18' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:22,068 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:22,108 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fd21db82ac0>>, <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 154, in _test_ucx_infiniband_nvlink
    res = res.sum().compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 381, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 666, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3278, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2403, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2265, in _gather
    raise exception.with_traceback(traceback)
distributed.scheduler.KilledWorker: Attempted to run task original-array-01b3c5781a59a350dad70dc963c94d25 on 3 different workers, but all those workers died while running it. The last worker that attempt to run the task was ucx://10.33.225.163:52617. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42615 instead
  warnings.warn(
2023-08-23 05:42:33,959 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,165 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,179 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,211 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f8ce588aac0>>, <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,383 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,401 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,418 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f884d20cac0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,617 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,655 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,683 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://10.33.225.163:56162'.
2023-08-23 05:42:34,684 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://10.33.225.163:56162'. Shutting down.
2023-08-23 05:42:34,688 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fce403ffac0>>, <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-20' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,861 - distributed.core - ERROR - cannot reshape array of size 5002 into shape (10000,)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
2023-08-23 05:42:34,909 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f48e3389af0>>, <Task finished name='Task-18' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-18' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=ValueError('cannot reshape array of size 5002 into shape (10000,)')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/core/numeric.py", line 1875, in _frombuffer
    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
ValueError: cannot reshape array of size 5002 into shape (10000,)
Process SpawnProcess-7:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 154, in _test_ucx_infiniband_nvlink
    res = res.sum().compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 381, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 666, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3278, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2403, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2265, in _gather
    raise exception.with_traceback(traceback)
distributed.scheduler.KilledWorker: Attempted to run task original-array-ff2542232b442f395c6373e51cf658e2 on 3 different workers, but all those workers died while running it. The last worker that attempt to run the task was ucx://10.33.225.163:56984. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40293 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38763 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38861 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42605 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42799 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38519 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35975 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44751 instead
  warnings.warn(
2023-08-23 05:44:03,614 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-23 05:44:03,623 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:45933'.
2023-08-23 05:44:03,623 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:45933'. Shutting down.
2023-08-23 05:44:03,627 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fddde1515e0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 965, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-23 05:44:05,630 - distributed.nanny - ERROR - Worker process died unexpectedly
