============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-07-22 05:39:11,544 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:11,548 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33335 instead
  warnings.warn(
2023-07-22 05:39:11,551 - distributed.scheduler - INFO - State start
2023-07-22 05:39:11,606 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:11,607 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-22 05:39:11,607 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33335/status
2023-07-22 05:39:11,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44069'
2023-07-22 05:39:11,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43251'
2023-07-22 05:39:11,720 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35815'
2023-07-22 05:39:11,727 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37217'
2023-07-22 05:39:11,908 - distributed.scheduler - INFO - Receive client connection: Client-14f76b2f-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:11,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54106
2023-07-22 05:39:13,241 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:13,241 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:13,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:13,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:13,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:13,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:13,249 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:13,249 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:13,249 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:13,254 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:13,254 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:13,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-07-22 05:39:13,268 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46833
2023-07-22 05:39:13,268 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46833
2023-07-22 05:39:13,268 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36855
2023-07-22 05:39:13,268 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:39:13,268 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:13,268 - distributed.worker - INFO -               Threads:                          4
2023-07-22 05:39:13,268 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-22 05:39:13,268 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jzg7d8y8
2023-07-22 05:39:13,269 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84830ce7-7342-4531-9e67-b7162eb1d9ed
2023-07-22 05:39:13,269 - distributed.worker - INFO - Starting Worker plugin PreImport-6fb53282-cdd3-40a9-8c0f-759888ffe30e
2023-07-22 05:39:13,269 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a21264e6-c28e-4ea5-aee5-dcc8f08a1db5
2023-07-22 05:39:13,269 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:13,282 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46833', status: init, memory: 0, processing: 0>
2023-07-22 05:39:13,283 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46833
2023-07-22 05:39:13,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54128
2023-07-22 05:39:13,284 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:39:13,284 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:13,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:39:14,456 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42611
2023-07-22 05:39:14,456 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42611
2023-07-22 05:39:14,456 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34453
2023-07-22 05:39:14,456 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:39:14,456 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,456 - distributed.worker - INFO -               Threads:                          4
2023-07-22 05:39:14,456 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-22 05:39:14,456 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wajmnsvc
2023-07-22 05:39:14,456 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38ff20a8-c9ec-4004-85cd-0e53f1e4eb79
2023-07-22 05:39:14,457 - distributed.worker - INFO - Starting Worker plugin PreImport-f6a11063-64e9-45d2-8048-43dd936bd5c3
2023-07-22 05:39:14,457 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-910cbd9c-d43f-4415-b4a8-a7a759a61fe2
2023-07-22 05:39:14,457 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,459 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40627
2023-07-22 05:39:14,459 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40627
2023-07-22 05:39:14,459 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38879
2023-07-22 05:39:14,459 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:39:14,459 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,459 - distributed.worker - INFO -               Threads:                          4
2023-07-22 05:39:14,459 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-22 05:39:14,459 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bcmw1zkj
2023-07-22 05:39:14,459 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a209fa0-e6b3-4107-a1bd-2ca82ff6ed1d
2023-07-22 05:39:14,460 - distributed.worker - INFO - Starting Worker plugin PreImport-bebe7a7f-f1c7-48f0-8194-9b0f5e18dc4a
2023-07-22 05:39:14,460 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e1edacf-a618-4146-9d0f-be33c7dd8760
2023-07-22 05:39:14,460 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40627', status: init, memory: 0, processing: 0>
2023-07-22 05:39:14,477 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40627
2023-07-22 05:39:14,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54150
2023-07-22 05:39:14,478 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:39:14,478 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,480 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:39:14,483 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42611', status: init, memory: 0, processing: 0>
2023-07-22 05:39:14,484 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42611
2023-07-22 05:39:14,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54142
2023-07-22 05:39:14,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:39:14,484 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:39:14,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35985
2023-07-22 05:39:14,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35985
2023-07-22 05:39:14,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45921
2023-07-22 05:39:14,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:39:14,495 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,495 - distributed.worker - INFO -               Threads:                          4
2023-07-22 05:39:14,495 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-22 05:39:14,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-19rxgzoe
2023-07-22 05:39:14,496 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fca93a6-9324-4b83-b563-90a541c52ac2
2023-07-22 05:39:14,496 - distributed.worker - INFO - Starting Worker plugin PreImport-56e7da62-a839-4ca2-9c1d-12bb9d26ed1f
2023-07-22 05:39:14,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7336609e-358b-4fe8-85d0-009092be538f
2023-07-22 05:39:14,496 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,515 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35985', status: init, memory: 0, processing: 0>
2023-07-22 05:39:14,516 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35985
2023-07-22 05:39:14,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54152
2023-07-22 05:39:14,517 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:39:14,517 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:14,519 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:39:14,602 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-22 05:39:14,603 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-22 05:39:14,604 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-22 05:39:14,604 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-22 05:39:14,610 - distributed.scheduler - INFO - Remove client Client-14f76b2f-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:14,610 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54106; closing.
2023-07-22 05:39:14,610 - distributed.scheduler - INFO - Remove client Client-14f76b2f-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:14,611 - distributed.scheduler - INFO - Close client connection: Client-14f76b2f-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:14,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43251'. Reason: nanny-close
2023-07-22 05:39:14,612 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:14,613 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44069'. Reason: nanny-close
2023-07-22 05:39:14,613 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:14,614 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42611. Reason: nanny-close
2023-07-22 05:39:14,614 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35815'. Reason: nanny-close
2023-07-22 05:39:14,614 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:14,614 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40627. Reason: nanny-close
2023-07-22 05:39:14,614 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37217'. Reason: nanny-close
2023-07-22 05:39:14,615 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:14,615 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35985. Reason: nanny-close
2023-07-22 05:39:14,615 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:39:14,615 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54142; closing.
2023-07-22 05:39:14,616 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46833. Reason: nanny-close
2023-07-22 05:39:14,616 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42611', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:14,616 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42611
2023-07-22 05:39:14,616 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:39:14,617 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:14,617 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:39:14,617 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54150; closing.
2023-07-22 05:39:14,617 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:14,617 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42611
2023-07-22 05:39:14,618 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40627', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:14,618 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40627
2023-07-22 05:39:14,618 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:39:14,618 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:14,618 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54152; closing.
2023-07-22 05:39:14,619 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35985', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:14,619 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35985
2023-07-22 05:39:14,619 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54128; closing.
2023-07-22 05:39:14,619 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:14,620 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46833', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:14,620 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46833
2023-07-22 05:39:14,620 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:39:15,678 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:39:15,678 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:39:15,679 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:39:15,680 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-22 05:39:15,680 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-07-22 05:39:17,569 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:17,573 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34347 instead
  warnings.warn(
2023-07-22 05:39:17,577 - distributed.scheduler - INFO - State start
2023-07-22 05:39:17,596 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:17,597 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:39:17,597 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34347/status
2023-07-22 05:39:17,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36801'
2023-07-22 05:39:17,820 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41517'
2023-07-22 05:39:17,829 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33789'
2023-07-22 05:39:17,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39213'
2023-07-22 05:39:17,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43879'
2023-07-22 05:39:17,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46465'
2023-07-22 05:39:17,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43647'
2023-07-22 05:39:17,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38761'
2023-07-22 05:39:18,167 - distributed.scheduler - INFO - Receive client connection: Client-18997cc8-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:18,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50294
2023-07-22 05:39:19,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:19,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:19,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:19,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:19,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:19,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:19,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:19,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:19,525 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:19,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:19,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:19,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:19,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:19,553 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:19,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:19,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:19,562 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:19,562 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:19,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:19,594 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:19,594 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:19,594 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:19,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:19,650 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:21,980 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33847
2023-07-22 05:39:21,980 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33847
2023-07-22 05:39:21,980 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45363
2023-07-22 05:39:21,980 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:21,980 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:21,980 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:21,981 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:21,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jafxfauw
2023-07-22 05:39:21,982 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19c5ba3c-975d-4cc6-b6ac-3bc639d2ae0e
2023-07-22 05:39:22,101 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-159c66ed-9b45-42eb-a667-cb0d77185086
2023-07-22 05:39:22,101 - distributed.worker - INFO - Starting Worker plugin PreImport-4f15f477-8014-4022-93b4-eb03e1baa219
2023-07-22 05:39:22,101 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,141 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33847', status: init, memory: 0, processing: 0>
2023-07-22 05:39:22,142 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33847
2023-07-22 05:39:22,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34718
2023-07-22 05:39:22,142 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,143 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,145 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:22,220 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34707
2023-07-22 05:39:22,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34707
2023-07-22 05:39:22,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39587
2023-07-22 05:39:22,220 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,220 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,220 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:22,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:22,221 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hij7mklw
2023-07-22 05:39:22,221 - distributed.worker - INFO - Starting Worker plugin PreImport-feb92540-d6a9-4250-8b68-9becf8cb64e4
2023-07-22 05:39:22,221 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca9d135d-ec98-4909-bed6-ed1619fcc7c2
2023-07-22 05:39:22,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0edd795b-ace4-4d08-b56f-7e9351ced83c
2023-07-22 05:39:22,247 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36259
2023-07-22 05:39:22,247 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36259
2023-07-22 05:39:22,247 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39945
2023-07-22 05:39:22,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,248 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,248 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:22,248 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:22,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lgyw2sg7
2023-07-22 05:39:22,248 - distributed.worker - INFO - Starting Worker plugin PreImport-0b053070-482a-4b47-a333-39804fa3459b
2023-07-22 05:39:22,248 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89afb685-0ed1-4ce3-a378-88d9924dab85
2023-07-22 05:39:22,249 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfe0abbb-4e01-4ca7-ad7a-9d236e7eee4e
2023-07-22 05:39:22,365 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,398 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,400 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34707', status: init, memory: 0, processing: 0>
2023-07-22 05:39:22,401 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34707
2023-07-22 05:39:22,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34728
2023-07-22 05:39:22,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,401 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:22,426 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36259', status: init, memory: 0, processing: 0>
2023-07-22 05:39:22,426 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36259
2023-07-22 05:39:22,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34734
2023-07-22 05:39:22,427 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,427 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:22,435 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43057
2023-07-22 05:39:22,435 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43057
2023-07-22 05:39:22,435 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34119
2023-07-22 05:39:22,435 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,435 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,435 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:22,435 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:22,435 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f2t39ztb
2023-07-22 05:39:22,436 - distributed.worker - INFO - Starting Worker plugin PreImport-9f0a6830-c238-438f-80a4-9be9b7ac952a
2023-07-22 05:39:22,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-601a0fa4-cd21-4ccb-b282-d6163852148e
2023-07-22 05:39:22,436 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8874635-3c36-40e8-ab97-317d893e1381
2023-07-22 05:39:22,437 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46343
2023-07-22 05:39:22,437 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46343
2023-07-22 05:39:22,437 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40037
2023-07-22 05:39:22,437 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,437 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,437 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:22,437 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:22,437 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ngerfd8_
2023-07-22 05:39:22,438 - distributed.worker - INFO - Starting Worker plugin PreImport-6716dc07-6703-429e-bdac-ac5e6e5669bb
2023-07-22 05:39:22,438 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0f2e914-b5e1-4292-a1cb-1d64054fbf63
2023-07-22 05:39:22,438 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b95b00c-6c7e-483e-a838-8f921e30f5cb
2023-07-22 05:39:22,440 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45537
2023-07-22 05:39:22,440 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45537
2023-07-22 05:39:22,440 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42359
2023-07-22 05:39:22,440 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46569
2023-07-22 05:39:22,441 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36547
2023-07-22 05:39:22,441 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42359
2023-07-22 05:39:22,441 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,441 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46569
2023-07-22 05:39:22,441 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,441 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39431
2023-07-22 05:39:22,441 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43031
2023-07-22 05:39:22,441 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:22,441 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,441 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,441 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,441 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:22,441 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,441 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q6lmwz7t
2023-07-22 05:39:22,441 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:22,441 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:22,441 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:22,441 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:22,441 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jj0sb4k0
2023-07-22 05:39:22,441 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-39bijbfq
2023-07-22 05:39:22,441 - distributed.worker - INFO - Starting Worker plugin PreImport-c2df8396-438e-495c-9f8b-2867f49cb45d
2023-07-22 05:39:22,442 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3819555-3153-4669-890d-413bbdb224ab
2023-07-22 05:39:22,442 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e542d66-4062-43b5-97c0-872615ca2b83
2023-07-22 05:39:22,442 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f1654dc-b60f-4093-870b-d4264fb5b417
2023-07-22 05:39:22,442 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94d94025-4fdd-4362-9fee-e564e71d31c1
2023-07-22 05:39:22,442 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48d29ff3-1045-4436-bc87-60edc1ecc861
2023-07-22 05:39:22,558 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,563 - distributed.worker - INFO - Starting Worker plugin PreImport-0c55a1ac-702a-4664-b98d-68fe4674a35f
2023-07-22 05:39:22,564 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,569 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,573 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,575 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98626073-913b-400f-ade8-5d0b43edaf16
2023-07-22 05:39:22,576 - distributed.worker - INFO - Starting Worker plugin PreImport-a2bf8c29-8ece-4cd6-ae1b-39a6c3c6df40
2023-07-22 05:39:22,576 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,587 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43057', status: init, memory: 0, processing: 0>
2023-07-22 05:39:22,588 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43057
2023-07-22 05:39:22,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34746
2023-07-22 05:39:22,589 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,589 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:22,592 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46569', status: init, memory: 0, processing: 0>
2023-07-22 05:39:22,592 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46569
2023-07-22 05:39:22,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34762
2023-07-22 05:39:22,593 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,593 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,593 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46343', status: init, memory: 0, processing: 0>
2023-07-22 05:39:22,594 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46343
2023-07-22 05:39:22,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34764
2023-07-22 05:39:22,594 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,595 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:22,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:22,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45537', status: init, memory: 0, processing: 0>
2023-07-22 05:39:22,597 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45537
2023-07-22 05:39:22,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34766
2023-07-22 05:39:22,598 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,598 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,600 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:22,607 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42359', status: init, memory: 0, processing: 0>
2023-07-22 05:39:22,607 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42359
2023-07-22 05:39:22,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34780
2023-07-22 05:39:22,608 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:22,608 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:22,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:22,647 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:22,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:22,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:22,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:22,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:22,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:22,648 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:22,649 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:22,653 - distributed.scheduler - INFO - Remove client Client-18997cc8-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:22,653 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50294; closing.
2023-07-22 05:39:22,653 - distributed.scheduler - INFO - Remove client Client-18997cc8-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:22,653 - distributed.scheduler - INFO - Close client connection: Client-18997cc8-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:22,654 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33789'. Reason: nanny-close
2023-07-22 05:39:22,655 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:22,655 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43879'. Reason: nanny-close
2023-07-22 05:39:22,656 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:22,656 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34707. Reason: nanny-close
2023-07-22 05:39:22,656 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36801'. Reason: nanny-close
2023-07-22 05:39:22,657 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:22,657 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46569. Reason: nanny-close
2023-07-22 05:39:22,657 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41517'. Reason: nanny-close
2023-07-22 05:39:22,657 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:22,658 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36259. Reason: nanny-close
2023-07-22 05:39:22,658 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39213'. Reason: nanny-close
2023-07-22 05:39:22,658 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46465'. Reason: nanny-close
2023-07-22 05:39:22,658 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:22,658 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45537. Reason: nanny-close
2023-07-22 05:39:22,658 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43647'. Reason: nanny-close
2023-07-22 05:39:22,658 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34728; closing.
2023-07-22 05:39:22,658 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:22,659 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:22,659 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34707', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:22,659 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34707
2023-07-22 05:39:22,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38761'. Reason: nanny-close
2023-07-22 05:39:22,659 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43057. Reason: nanny-close
2023-07-22 05:39:22,659 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:22,659 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:22,659 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:22,659 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46343. Reason: nanny-close
2023-07-22 05:39:22,660 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:22,660 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:22,660 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34707
2023-07-22 05:39:22,660 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34707
2023-07-22 05:39:22,660 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:22,661 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34762; closing.
2023-07-22 05:39:22,661 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:22,661 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:22,661 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34734; closing.
2023-07-22 05:39:22,661 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33847. Reason: nanny-close
2023-07-22 05:39:22,661 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:22,661 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34707
2023-07-22 05:39:22,661 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34707
2023-07-22 05:39:22,661 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:22,661 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42359. Reason: nanny-close
2023-07-22 05:39:22,661 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46569', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:22,662 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:22,662 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46569
2023-07-22 05:39:22,662 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36259', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:22,662 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36259
2023-07-22 05:39:22,662 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:22,662 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:22,663 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34766; closing.
2023-07-22 05:39:22,663 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45537', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:22,663 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:22,663 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45537
2023-07-22 05:39:22,663 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:22,663 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34764; closing.
2023-07-22 05:39:22,664 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34746; closing.
2023-07-22 05:39:22,664 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:22,664 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46343', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:22,664 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46343
2023-07-22 05:39:22,665 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43057', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:22,665 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43057
2023-07-22 05:39:22,665 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:22,665 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34718; closing.
2023-07-22 05:39:22,666 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33847', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:22,667 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33847
2023-07-22 05:39:22,669 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34780; closing.
2023-07-22 05:39:22,670 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42359', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:22,670 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42359
2023-07-22 05:39:22,670 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:39:24,072 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:39:24,072 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:39:24,073 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:39:24,074 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:39:24,074 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-07-22 05:39:25,945 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:25,949 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41571 instead
  warnings.warn(
2023-07-22 05:39:25,953 - distributed.scheduler - INFO - State start
2023-07-22 05:39:25,972 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:25,973 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:39:25,974 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41571/status
2023-07-22 05:39:26,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36161'
2023-07-22 05:39:26,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46565'
2023-07-22 05:39:26,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44063'
2023-07-22 05:39:26,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37735'
2023-07-22 05:39:26,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42117'
2023-07-22 05:39:26,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40697'
2023-07-22 05:39:26,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39887'
2023-07-22 05:39:26,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40037'
2023-07-22 05:39:27,288 - distributed.scheduler - INFO - Receive client connection: Client-1d9ae5bf-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:27,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34954
2023-07-22 05:39:27,708 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:27,709 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:27,732 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:27,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:27,847 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:27,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:27,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:27,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:27,849 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:27,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:27,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:27,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:27,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:27,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:27,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:27,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:27,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:28,042 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:28,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:28,047 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:28,048 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:28,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:28,058 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:28,058 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:29,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33043
2023-07-22 05:39:29,399 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33043
2023-07-22 05:39:29,399 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42515
2023-07-22 05:39:29,399 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:29,400 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:29,400 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:29,400 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:29,400 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-35chehn5
2023-07-22 05:39:29,400 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ecefe489-1cb4-4eb6-9021-a13de62a8cc5
2023-07-22 05:39:29,431 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b3e8ef5-05db-4afa-b1d7-1b04e6b52691
2023-07-22 05:39:29,432 - distributed.worker - INFO - Starting Worker plugin PreImport-0592b2bb-4819-420a-abf8-47629f0f6200
2023-07-22 05:39:29,432 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:29,463 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33043', status: init, memory: 0, processing: 0>
2023-07-22 05:39:29,464 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33043
2023-07-22 05:39:29,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34964
2023-07-22 05:39:29,465 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:29,465 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:29,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:30,510 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34811
2023-07-22 05:39:30,510 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34811
2023-07-22 05:39:30,510 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42287
2023-07-22 05:39:30,510 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,510 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,511 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:30,511 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:30,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ha7udzaz
2023-07-22 05:39:30,511 - distributed.worker - INFO - Starting Worker plugin PreImport-1945ddd3-5064-4cd6-8071-a90767aab4d6
2023-07-22 05:39:30,511 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b78c978-608f-41d1-85f6-25d26e039550
2023-07-22 05:39:30,512 - distributed.worker - INFO - Starting Worker plugin RMMSetup-333c85ea-eace-4a15-a423-92ab214d67ef
2023-07-22 05:39:30,512 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43017
2023-07-22 05:39:30,512 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43017
2023-07-22 05:39:30,512 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32855
2023-07-22 05:39:30,512 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,512 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,513 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:30,513 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:30,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n342d85f
2023-07-22 05:39:30,513 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b467f60-79bf-4b9e-a121-8a7ba29e0c10
2023-07-22 05:39:30,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38311
2023-07-22 05:39:30,514 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38311
2023-07-22 05:39:30,514 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44717
2023-07-22 05:39:30,514 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,514 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,514 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:30,514 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:30,514 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-776mg3k5
2023-07-22 05:39:30,515 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9ca9f73-7140-4a73-9788-f785cd6d7019
2023-07-22 05:39:30,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce4ff4dc-a86c-4399-98e6-d4de7d07a455
2023-07-22 05:39:30,524 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,525 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7dbe487b-3adc-406b-ba1f-a65578c9c887
2023-07-22 05:39:30,526 - distributed.worker - INFO - Starting Worker plugin PreImport-3a2d50f0-6412-4a72-a685-73615eaa9cbf
2023-07-22 05:39:30,526 - distributed.worker - INFO - Starting Worker plugin PreImport-5475b595-9798-4eb7-8ef2-11370558bf62
2023-07-22 05:39:30,526 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,526 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,550 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38311', status: init, memory: 0, processing: 0>
2023-07-22 05:39:30,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38311
2023-07-22 05:39:30,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49084
2023-07-22 05:39:30,551 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,551 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:30,562 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43017', status: init, memory: 0, processing: 0>
2023-07-22 05:39:30,563 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43017
2023-07-22 05:39:30,563 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49090
2023-07-22 05:39:30,564 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,564 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,564 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34811', status: init, memory: 0, processing: 0>
2023-07-22 05:39:30,565 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34811
2023-07-22 05:39:30,565 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49070
2023-07-22 05:39:30,565 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,565 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:30,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:30,658 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33957
2023-07-22 05:39:30,658 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33957
2023-07-22 05:39:30,658 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33431
2023-07-22 05:39:30,658 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,658 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,658 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:30,658 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:30,658 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tdexdy9u
2023-07-22 05:39:30,658 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45885
2023-07-22 05:39:30,659 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45885
2023-07-22 05:39:30,659 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37725
2023-07-22 05:39:30,659 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,659 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04bcc53a-1c88-44a9-aca8-ad07d01af577
2023-07-22 05:39:30,659 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,659 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:30,659 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:30,659 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8i_wot8n
2023-07-22 05:39:30,659 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af4c5a11-9c63-483c-8620-9baef5a3b98b
2023-07-22 05:39:30,661 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45973
2023-07-22 05:39:30,661 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45973
2023-07-22 05:39:30,661 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41903
2023-07-22 05:39:30,661 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,661 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,661 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:30,661 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:30,661 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-58nyh28o
2023-07-22 05:39:30,662 - distributed.worker - INFO - Starting Worker plugin RMMSetup-06b9d08f-1848-4bc5-a8d4-02066ab525f3
2023-07-22 05:39:30,669 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45453
2023-07-22 05:39:30,669 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45453
2023-07-22 05:39:30,669 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34835
2023-07-22 05:39:30,669 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,669 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,669 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:30,669 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:30,669 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ot2slo6b
2023-07-22 05:39:30,670 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe236463-e2ce-40b2-ad86-e3248bb0749f
2023-07-22 05:39:30,679 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67e001e0-56ac-4204-85e9-3edcb2ad9434
2023-07-22 05:39:30,679 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d301678-27a1-490e-9535-8564dfa0eea5
2023-07-22 05:39:30,679 - distributed.worker - INFO - Starting Worker plugin PreImport-c602dafd-0632-4589-8e45-c8213df0bd66
2023-07-22 05:39:30,679 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,679 - distributed.worker - INFO - Starting Worker plugin PreImport-46350c69-cc1a-4934-95ab-2a9ca010f29d
2023-07-22 05:39:30,680 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2120aaa9-ff30-4ee5-babb-cfbf6fbebf6a
2023-07-22 05:39:30,680 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,680 - distributed.worker - INFO - Starting Worker plugin PreImport-1460f53d-b99e-407b-9bc2-6bc4ae50e70a
2023-07-22 05:39:30,680 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,681 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4de9be9-cfd4-49f6-9208-e6d6b45e2be0
2023-07-22 05:39:30,682 - distributed.worker - INFO - Starting Worker plugin PreImport-ae327cd4-1fea-493a-9f47-bd3cc5212499
2023-07-22 05:39:30,682 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,703 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33957', status: init, memory: 0, processing: 0>
2023-07-22 05:39:30,703 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33957
2023-07-22 05:39:30,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49106
2023-07-22 05:39:30,704 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,704 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:30,706 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45973', status: init, memory: 0, processing: 0>
2023-07-22 05:39:30,707 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45973
2023-07-22 05:39:30,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49122
2023-07-22 05:39:30,707 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,707 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,709 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45885', status: init, memory: 0, processing: 0>
2023-07-22 05:39:30,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:30,709 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45885
2023-07-22 05:39:30,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49128
2023-07-22 05:39:30,710 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,710 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,712 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45453', status: init, memory: 0, processing: 0>
2023-07-22 05:39:30,712 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:30,713 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45453
2023-07-22 05:39:30,713 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49132
2023-07-22 05:39:30,713 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:30,713 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:30,716 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:30,806 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:30,806 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:30,806 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:30,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:30,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:30,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:30,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:30,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:30,811 - distributed.scheduler - INFO - Remove client Client-1d9ae5bf-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:30,811 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34954; closing.
2023-07-22 05:39:30,811 - distributed.scheduler - INFO - Remove client Client-1d9ae5bf-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:30,812 - distributed.scheduler - INFO - Close client connection: Client-1d9ae5bf-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:30,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44063'. Reason: nanny-close
2023-07-22 05:39:30,813 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:30,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37735'. Reason: nanny-close
2023-07-22 05:39:30,814 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:30,815 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34811. Reason: nanny-close
2023-07-22 05:39:30,815 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39887'. Reason: nanny-close
2023-07-22 05:39:30,815 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:30,815 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36161'. Reason: nanny-close
2023-07-22 05:39:30,815 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43017. Reason: nanny-close
2023-07-22 05:39:30,816 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:30,816 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33957. Reason: nanny-close
2023-07-22 05:39:30,816 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46565'. Reason: nanny-close
2023-07-22 05:39:30,816 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:30,817 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33043. Reason: nanny-close
2023-07-22 05:39:30,817 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42117'. Reason: nanny-close
2023-07-22 05:39:30,817 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:30,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49070; closing.
2023-07-22 05:39:30,817 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:30,817 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34811', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:30,817 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34811
2023-07-22 05:39:30,817 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40697'. Reason: nanny-close
2023-07-22 05:39:30,817 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45453. Reason: nanny-close
2023-07-22 05:39:30,817 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:30,817 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:30,818 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:30,818 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40037'. Reason: nanny-close
2023-07-22 05:39:30,818 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45885. Reason: nanny-close
2023-07-22 05:39:30,818 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:30,818 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:30,818 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45973. Reason: nanny-close
2023-07-22 05:39:30,818 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34811
2023-07-22 05:39:30,819 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34811
2023-07-22 05:39:30,819 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:30,819 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:30,819 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49090; closing.
2023-07-22 05:39:30,819 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:30,819 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49106; closing.
2023-07-22 05:39:30,819 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38311. Reason: nanny-close
2023-07-22 05:39:30,819 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34811
2023-07-22 05:39:30,820 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34811
2023-07-22 05:39:30,820 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43017', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:30,820 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43017
2023-07-22 05:39:30,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:30,820 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34811
2023-07-22 05:39:30,820 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33957', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:30,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:30,820 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:30,820 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33957
2023-07-22 05:39:30,821 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:30,821 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:30,821 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34964; closing.
2023-07-22 05:39:30,821 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:30,821 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:30,822 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33043', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:30,822 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33043
2023-07-22 05:39:30,822 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:30,822 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:30,822 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49132; closing.
2023-07-22 05:39:30,822 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49122; closing.
2023-07-22 05:39:30,823 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49128; closing.
2023-07-22 05:39:30,823 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49084; closing.
2023-07-22 05:39:30,823 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45453', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:30,824 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45453
2023-07-22 05:39:30,824 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45973', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:30,824 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45973
2023-07-22 05:39:30,825 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45885', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:30,825 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45885
2023-07-22 05:39:30,825 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38311', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:30,825 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38311
2023-07-22 05:39:30,826 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:39:32,180 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:39:32,180 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:39:32,181 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:39:32,182 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:39:32,182 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-07-22 05:39:33,936 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:33,940 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38047 instead
  warnings.warn(
2023-07-22 05:39:33,944 - distributed.scheduler - INFO - State start
2023-07-22 05:39:33,962 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:33,963 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:39:33,963 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38047/status
2023-07-22 05:39:33,993 - distributed.scheduler - INFO - Receive client connection: Client-22669eab-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:34,006 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49214
2023-07-22 05:39:34,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44717'
2023-07-22 05:39:34,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40783'
2023-07-22 05:39:34,123 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36971'
2023-07-22 05:39:34,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46199'
2023-07-22 05:39:34,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43959'
2023-07-22 05:39:34,150 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44979'
2023-07-22 05:39:34,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41597'
2023-07-22 05:39:34,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42607'
2023-07-22 05:39:35,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:35,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:35,696 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:35,753 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:35,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:35,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:35,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:35,780 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:35,780 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:35,780 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:35,781 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:35,781 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:35,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:35,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:35,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:35,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:35,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:35,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:35,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:35,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:35,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:35,973 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:35,974 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:35,974 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:37,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33223
2023-07-22 05:39:37,541 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33223
2023-07-22 05:39:37,541 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36003
2023-07-22 05:39:37,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:37,542 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:37,542 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:37,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:37,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h5tpqtsb
2023-07-22 05:39:37,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b843bd6-4a63-4999-acaf-dfa5b67a7ca3
2023-07-22 05:39:37,871 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a37af0d1-e125-4b22-9436-28eb830fd5aa
2023-07-22 05:39:37,872 - distributed.worker - INFO - Starting Worker plugin PreImport-566b9bef-d94a-4a9c-b515-f473c729babc
2023-07-22 05:39:37,873 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:37,918 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33223', status: init, memory: 0, processing: 0>
2023-07-22 05:39:37,921 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33223
2023-07-22 05:39:37,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49296
2023-07-22 05:39:37,922 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:37,922 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:37,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:38,392 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36433
2023-07-22 05:39:38,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36433
2023-07-22 05:39:38,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37271
2023-07-22 05:39:38,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,392 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,392 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:38,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:38,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jciyprcn
2023-07-22 05:39:38,393 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41099
2023-07-22 05:39:38,393 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41099
2023-07-22 05:39:38,393 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43237
2023-07-22 05:39:38,393 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,393 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6ccfab4-3176-495a-89f2-2168de5d7301
2023-07-22 05:39:38,393 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,393 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:38,393 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:38,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tiw_804_
2023-07-22 05:39:38,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc4325a6-2746-4c45-a5d3-0fc6275c4891
2023-07-22 05:39:38,394 - distributed.worker - INFO - Starting Worker plugin PreImport-64e85416-d670-4d25-b720-83844343b9bb
2023-07-22 05:39:38,395 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d517454-0477-4a44-ba83-3046256107ee
2023-07-22 05:39:38,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9bf3226b-4294-4753-8550-127b26a7af3c
2023-07-22 05:39:38,412 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46271
2023-07-22 05:39:38,412 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46271
2023-07-22 05:39:38,412 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43589
2023-07-22 05:39:38,412 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,412 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,412 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:38,412 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:38,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_a6kgho8
2023-07-22 05:39:38,413 - distributed.worker - INFO - Starting Worker plugin PreImport-f6fbb04f-4c0b-4d25-be21-e84e14e310d4
2023-07-22 05:39:38,413 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9cd38af-c669-4b48-9350-615566174f86
2023-07-22 05:39:38,413 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e172cb3e-6e7a-4bfb-9741-d70560392161
2023-07-22 05:39:38,497 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36511
2023-07-22 05:39:38,497 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35933
2023-07-22 05:39:38,497 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35933
2023-07-22 05:39:38,497 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36511
2023-07-22 05:39:38,497 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40959
2023-07-22 05:39:38,497 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41299
2023-07-22 05:39:38,497 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,497 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,497 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,497 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,497 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:38,497 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:38,497 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:38,497 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:38,498 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0mvseo0u
2023-07-22 05:39:38,498 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ltri0ylh
2023-07-22 05:39:38,498 - distributed.worker - INFO - Starting Worker plugin PreImport-7ed507ff-9a06-4749-9e33-91096e41e149
2023-07-22 05:39:38,498 - distributed.worker - INFO - Starting Worker plugin PreImport-a6ef754c-0993-4b29-8521-bde2ac86927c
2023-07-22 05:39:38,498 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98d812ea-b9ca-4872-a723-07a090309e44
2023-07-22 05:39:38,498 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f0858db-067e-482c-857e-9058d102f80f
2023-07-22 05:39:38,498 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b093e14-f51f-4374-ad33-f15752c1602e
2023-07-22 05:39:38,498 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91947cfc-5f09-44e4-94cc-750c976c1eb2
2023-07-22 05:39:38,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42807
2023-07-22 05:39:38,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42807
2023-07-22 05:39:38,500 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43187
2023-07-22 05:39:38,500 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,500 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,500 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:38,500 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:38,500 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hrvzj_wl
2023-07-22 05:39:38,500 - distributed.worker - INFO - Starting Worker plugin PreImport-7f2bee98-6ebc-4a54-a1a9-af4286e372c3
2023-07-22 05:39:38,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-234d5ee0-268c-499b-9de5-69457ea9441b
2023-07-22 05:39:38,501 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c48bb6b-a850-49c3-8b22-98d8ba15fd45
2023-07-22 05:39:38,501 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41505
2023-07-22 05:39:38,502 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41505
2023-07-22 05:39:38,502 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34705
2023-07-22 05:39:38,502 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,502 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,502 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:38,502 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:38,502 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oendbfpa
2023-07-22 05:39:38,502 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9fd60015-947c-42b2-a503-cdd510cbba82
2023-07-22 05:39:38,567 - distributed.worker - INFO - Starting Worker plugin PreImport-df394b9d-fbb1-4bd1-a1f9-6371b190003e
2023-07-22 05:39:38,567 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,570 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,576 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,602 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36433', status: init, memory: 0, processing: 0>
2023-07-22 05:39:38,602 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36433
2023-07-22 05:39:38,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49308
2023-07-22 05:39:38,603 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,603 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41099', status: init, memory: 0, processing: 0>
2023-07-22 05:39:38,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41099
2023-07-22 05:39:38,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49320
2023-07-22 05:39:38,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:38,606 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,606 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46271', status: init, memory: 0, processing: 0>
2023-07-22 05:39:38,607 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46271
2023-07-22 05:39:38,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49328
2023-07-22 05:39:38,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,608 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:38,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:38,679 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,679 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,681 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-74c74de6-bc59-40e3-8ad5-d67e07d72fba
2023-07-22 05:39:38,681 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,681 - distributed.worker - INFO - Starting Worker plugin PreImport-9db01195-1455-460d-b2c8-27a8a2817f8d
2023-07-22 05:39:38,682 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,709 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36511', status: init, memory: 0, processing: 0>
2023-07-22 05:39:38,710 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36511
2023-07-22 05:39:38,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49342
2023-07-22 05:39:38,710 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,711 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,713 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:38,716 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41505', status: init, memory: 0, processing: 0>
2023-07-22 05:39:38,716 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41505
2023-07-22 05:39:38,716 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49362
2023-07-22 05:39:38,717 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,717 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,717 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35933', status: init, memory: 0, processing: 0>
2023-07-22 05:39:38,718 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35933
2023-07-22 05:39:38,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49344
2023-07-22 05:39:38,718 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,719 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,719 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42807', status: init, memory: 0, processing: 0>
2023-07-22 05:39:38,719 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42807
2023-07-22 05:39:38,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49350
2023-07-22 05:39:38,720 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:38,720 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:38,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:38,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:38,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:38,786 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:38,786 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:38,786 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:38,786 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:38,786 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:38,786 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:38,786 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:38,786 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:38,796 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:39:38,797 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:39:38,797 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:39:38,797 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:39:38,797 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:39:38,797 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:39:38,797 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:39:38,797 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:39:38,803 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:39:38,805 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:39:38,808 - distributed.scheduler - INFO - Remove client Client-22669eab-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:38,808 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49214; closing.
2023-07-22 05:39:38,808 - distributed.scheduler - INFO - Remove client Client-22669eab-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:38,809 - distributed.scheduler - INFO - Close client connection: Client-22669eab-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:38,809 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36971'. Reason: nanny-close
2023-07-22 05:39:38,810 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:38,810 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46199'. Reason: nanny-close
2023-07-22 05:39:38,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:38,811 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36433. Reason: nanny-close
2023-07-22 05:39:38,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41597'. Reason: nanny-close
2023-07-22 05:39:38,811 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:38,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41099. Reason: nanny-close
2023-07-22 05:39:38,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44717'. Reason: nanny-close
2023-07-22 05:39:38,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:38,812 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42807. Reason: nanny-close
2023-07-22 05:39:38,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40783'. Reason: nanny-close
2023-07-22 05:39:38,813 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:38,813 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49308; closing.
2023-07-22 05:39:38,813 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:38,813 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43959'. Reason: nanny-close
2023-07-22 05:39:38,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36511. Reason: nanny-close
2023-07-22 05:39:38,813 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:38,813 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36433', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:38,813 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36433
2023-07-22 05:39:38,813 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:38,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44979'. Reason: nanny-close
2023-07-22 05:39:38,814 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33223. Reason: nanny-close
2023-07-22 05:39:38,814 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:38,814 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46271. Reason: nanny-close
2023-07-22 05:39:38,814 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42607'. Reason: nanny-close
2023-07-22 05:39:38,814 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:38,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:38,815 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:38,815 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36433
2023-07-22 05:39:38,815 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:38,815 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49320; closing.
2023-07-22 05:39:38,815 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36433
2023-07-22 05:39:38,816 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41099', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:38,816 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41099
2023-07-22 05:39:38,816 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36433
2023-07-22 05:39:38,816 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35933. Reason: nanny-close
2023-07-22 05:39:38,816 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36433
2023-07-22 05:39:38,816 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36433
2023-07-22 05:39:38,816 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:38,816 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49350; closing.
2023-07-22 05:39:38,816 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41505. Reason: nanny-close
2023-07-22 05:39:38,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:38,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:38,816 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:38,817 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42807', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:38,817 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42807
2023-07-22 05:39:38,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49342; closing.
2023-07-22 05:39:38,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49296; closing.
2023-07-22 05:39:38,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49328; closing.
2023-07-22 05:39:38,817 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:38,818 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:38,818 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36511', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:38,818 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:38,818 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36511
2023-07-22 05:39:38,818 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:38,818 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33223', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:38,818 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33223
2023-07-22 05:39:38,818 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:38,819 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46271', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:38,819 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46271
2023-07-22 05:39:38,819 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:38,819 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49344; closing.
2023-07-22 05:39:38,820 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35933', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:38,820 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35933
2023-07-22 05:39:38,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49362; closing.
2023-07-22 05:39:38,820 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:38,820 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41505', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:38,821 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41505
2023-07-22 05:39:38,821 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:39:40,276 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:39:40,277 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:39:40,277 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:39:40,278 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:39:40,279 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-07-22 05:39:42,155 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:42,159 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41749 instead
  warnings.warn(
2023-07-22 05:39:42,163 - distributed.scheduler - INFO - State start
2023-07-22 05:39:42,182 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:42,183 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:39:42,184 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41749/status
2023-07-22 05:39:42,379 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35399'
2023-07-22 05:39:42,399 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46241'
2023-07-22 05:39:42,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46227'
2023-07-22 05:39:42,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35845'
2023-07-22 05:39:42,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36549'
2023-07-22 05:39:42,427 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45235'
2023-07-22 05:39:42,435 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33649'
2023-07-22 05:39:42,444 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38703'
2023-07-22 05:39:42,544 - distributed.scheduler - INFO - Receive client connection: Client-27462727-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:42,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33916
2023-07-22 05:39:44,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:44,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:44,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:44,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:44,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:44,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:44,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:44,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:44,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:44,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:44,101 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:44,101 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:44,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:44,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:44,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:44,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:44,835 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:44,863 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:44,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:44,868 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:44,869 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:44,869 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:44,870 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:44,870 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:47,741 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44879
2023-07-22 05:39:47,741 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44879
2023-07-22 05:39:47,741 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44243
2023-07-22 05:39:47,741 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:47,741 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:47,741 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:47,742 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:47,742 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7it5h3ay
2023-07-22 05:39:47,742 - distributed.worker - INFO - Starting Worker plugin RMMSetup-baa53146-ba27-4be1-bc14-9d555cf7a8b2
2023-07-22 05:39:47,742 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35501
2023-07-22 05:39:47,742 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35501
2023-07-22 05:39:47,742 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44239
2023-07-22 05:39:47,742 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:47,742 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:47,742 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:47,743 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:47,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n1zsh4ej
2023-07-22 05:39:47,743 - distributed.worker - INFO - Starting Worker plugin PreImport-21f44a5d-4bcb-4910-8189-945e54d389f5
2023-07-22 05:39:47,743 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c4855a3-273f-4c9a-86c4-bc3a52b6cf28
2023-07-22 05:39:47,743 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aba75c3b-125a-46af-8156-da639411a18f
2023-07-22 05:39:47,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43061
2023-07-22 05:39:47,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43061
2023-07-22 05:39:47,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36053
2023-07-22 05:39:47,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:47,745 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:47,745 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:47,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:47,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2pd2obn9
2023-07-22 05:39:47,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62c62d34-3072-4ee0-baf4-ad5a876f8970
2023-07-22 05:39:47,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36919
2023-07-22 05:39:47,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36919
2023-07-22 05:39:47,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40013
2023-07-22 05:39:47,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42613
2023-07-22 05:39:47,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40013
2023-07-22 05:39:47,750 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:47,750 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:47,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46245
2023-07-22 05:39:47,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46777
2023-07-22 05:39:47,750 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:47,750 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:47,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46777
2023-07-22 05:39:47,750 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:47,750 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:47,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36313
2023-07-22 05:39:47,750 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:47,750 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l_kummv3
2023-07-22 05:39:47,750 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:47,750 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:47,750 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:47,750 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ie8v1cb5
2023-07-22 05:39:47,750 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:47,750 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:47,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dzeav5g_
2023-07-22 05:39:47,751 - distributed.worker - INFO - Starting Worker plugin PreImport-0ef3bb1d-506d-4207-bee2-e2406a258743
2023-07-22 05:39:47,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85a2feaf-ed72-4a40-9315-3fa93c588d55
2023-07-22 05:39:47,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11b5ac62-b107-4c3d-b358-48e3d4f77ba0
2023-07-22 05:39:47,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49693136-c514-4dd2-b8c3-5400fa8f18f7
2023-07-22 05:39:47,751 - distributed.worker - INFO - Starting Worker plugin PreImport-20a2b912-3438-45b7-9001-51a3c7ecaeec
2023-07-22 05:39:47,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-073cc19c-8af4-4b62-b229-2d40663d11a6
2023-07-22 05:39:47,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a16f6ad9-3b1a-4132-963f-f42423684945
2023-07-22 05:39:47,751 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41737
2023-07-22 05:39:47,751 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41737
2023-07-22 05:39:47,751 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32845
2023-07-22 05:39:47,751 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:47,751 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:47,752 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:47,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:47,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ue89e308
2023-07-22 05:39:47,752 - distributed.worker - INFO - Starting Worker plugin PreImport-466c4fd0-2daa-4d1b-b39d-771f99138c51
2023-07-22 05:39:47,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe6fe773-ba17-4e86-9a31-8904e3f06463
2023-07-22 05:39:47,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-341708f3-c629-48ad-ae34-7b2e26ea503c
2023-07-22 05:39:47,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c48d6ff4-eb6e-4068-a8db-876dccd8bcf6
2023-07-22 05:39:47,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46649
2023-07-22 05:39:47,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46649
2023-07-22 05:39:47,825 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33743
2023-07-22 05:39:47,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:47,826 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:47,826 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:47,826 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:39:47,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o99hbnpc
2023-07-22 05:39:47,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c59081eb-fbc1-4076-9073-54fb44b95cb5
2023-07-22 05:39:47,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aea2e0ee-0872-4e5b-a4bd-92b922a7bd7a
2023-07-22 05:39:48,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25c8b555-0bf5-4409-b0c1-9d6f59901bd0
2023-07-22 05:39:48,032 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,032 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,032 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,032 - distributed.worker - INFO - Starting Worker plugin PreImport-9542acff-2d1b-4f36-9c69-409abb90af53
2023-07-22 05:39:48,033 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,056 - distributed.worker - INFO - Starting Worker plugin PreImport-72c4b7bf-e5f1-408f-8d89-bc969959b353
2023-07-22 05:39:48,056 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,058 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,059 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee621e74-d2ce-45a9-8eea-5ce9244373aa
2023-07-22 05:39:48,059 - distributed.worker - INFO - Starting Worker plugin PreImport-cf293070-97ac-465f-9148-8a8f28f69a35
2023-07-22 05:39:48,059 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,060 - distributed.worker - INFO - Starting Worker plugin PreImport-402c5323-c6a9-43f3-86d3-87c9c1bd23c4
2023-07-22 05:39:48,060 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,068 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36919', status: init, memory: 0, processing: 0>
2023-07-22 05:39:48,071 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36919
2023-07-22 05:39:48,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33942
2023-07-22 05:39:48,071 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:48,071 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,071 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41737', status: init, memory: 0, processing: 0>
2023-07-22 05:39:48,072 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41737
2023-07-22 05:39:48,072 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33936
2023-07-22 05:39:48,073 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:48,073 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35501', status: init, memory: 0, processing: 0>
2023-07-22 05:39:48,073 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,073 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35501
2023-07-22 05:39:48,074 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33958
2023-07-22 05:39:48,074 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44879', status: init, memory: 0, processing: 0>
2023-07-22 05:39:48,074 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:48,074 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,075 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:48,075 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44879
2023-07-22 05:39:48,075 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33960
2023-07-22 05:39:48,075 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:48,076 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,076 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:48,077 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:48,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:48,088 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40013', status: init, memory: 0, processing: 0>
2023-07-22 05:39:48,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40013
2023-07-22 05:39:48,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33964
2023-07-22 05:39:48,089 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:48,089 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,089 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43061', status: init, memory: 0, processing: 0>
2023-07-22 05:39:48,090 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43061
2023-07-22 05:39:48,090 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33986
2023-07-22 05:39:48,091 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:48,091 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:48,092 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46649', status: init, memory: 0, processing: 0>
2023-07-22 05:39:48,092 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46649
2023-07-22 05:39:48,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34000
2023-07-22 05:39:48,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:48,093 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:48,093 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,094 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46777', status: init, memory: 0, processing: 0>
2023-07-22 05:39:48,095 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46777
2023-07-22 05:39:48,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33970
2023-07-22 05:39:48,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:48,095 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:48,095 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:48,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:48,133 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:48,133 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:48,133 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:48,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:48,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:48,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:48,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:48,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:39:48,138 - distributed.scheduler - INFO - Remove client Client-27462727-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:48,138 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33916; closing.
2023-07-22 05:39:48,139 - distributed.scheduler - INFO - Remove client Client-27462727-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:48,139 - distributed.scheduler - INFO - Close client connection: Client-27462727-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:48,140 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35845'. Reason: nanny-close
2023-07-22 05:39:48,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35399'. Reason: nanny-close
2023-07-22 05:39:48,141 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:48,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46241'. Reason: nanny-close
2023-07-22 05:39:48,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:48,142 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46649. Reason: nanny-close
2023-07-22 05:39:48,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46227'. Reason: nanny-close
2023-07-22 05:39:48,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:48,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36549'. Reason: nanny-close
2023-07-22 05:39:48,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41737. Reason: nanny-close
2023-07-22 05:39:48,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45235'. Reason: nanny-close
2023-07-22 05:39:48,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33649'. Reason: nanny-close
2023-07-22 05:39:48,143 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:48,144 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36919. Reason: nanny-close
2023-07-22 05:39:48,144 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:48,144 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34000; closing.
2023-07-22 05:39:48,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38703'. Reason: nanny-close
2023-07-22 05:39:48,144 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46649', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:48,144 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:48,144 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46649
2023-07-22 05:39:48,145 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35501. Reason: nanny-close
2023-07-22 05:39:48,145 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:48,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:48,146 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44879. Reason: nanny-close
2023-07-22 05:39:48,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:48,146 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46649
2023-07-22 05:39:48,147 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46649
2023-07-22 05:39:48,147 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:48,147 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46649
2023-07-22 05:39:48,147 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46649
2023-07-22 05:39:48,147 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:48,147 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:48,147 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:48,147 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:48,147 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33936; closing.
2023-07-22 05:39:48,148 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40013. Reason: nanny-close
2023-07-22 05:39:48,147 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:48,148 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46649
2023-07-22 05:39:48,148 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43061. Reason: nanny-close
2023-07-22 05:39:48,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41737', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:48,148 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41737
2023-07-22 05:39:48,149 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:48,149 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46777. Reason: nanny-close
2023-07-22 05:39:48,149 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33942; closing.
2023-07-22 05:39:48,149 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:48,149 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33958; closing.
2023-07-22 05:39:48,149 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:48,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36919', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:48,150 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36919
2023-07-22 05:39:48,150 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:48,150 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:48,150 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35501', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:48,150 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35501
2023-07-22 05:39:48,150 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:48,150 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:48,150 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33960; closing.
2023-07-22 05:39:48,151 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:48,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44879', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:48,151 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44879
2023-07-22 05:39:48,151 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:48,152 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33964; closing.
2023-07-22 05:39:48,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40013', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:48,152 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40013
2023-07-22 05:39:48,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33986; closing.
2023-07-22 05:39:48,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33970; closing.
2023-07-22 05:39:48,153 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43061', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:48,154 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43061
2023-07-22 05:39:48,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46777', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:48,154 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46777
2023-07-22 05:39:48,154 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:39:48,154 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33986>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-22 05:39:48,156 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33970>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-22 05:39:49,608 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:39:49,608 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:39:49,608 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:39:49,609 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:39:49,610 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-07-22 05:39:51,617 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:51,621 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33819 instead
  warnings.warn(
2023-07-22 05:39:51,624 - distributed.scheduler - INFO - State start
2023-07-22 05:39:51,643 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:51,644 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:39:51,645 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33819/status
2023-07-22 05:39:51,648 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44383'
2023-07-22 05:39:52,829 - distributed.scheduler - INFO - Receive client connection: Client-2cd618fd-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:52,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42522
2023-07-22 05:39:53,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:53,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-22 05:39:53,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:54,578 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44651
2023-07-22 05:39:54,578 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44651
2023-07-22 05:39:54,578 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-07-22 05:39:54,578 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:39:54,578 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:54,578 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:39:54,578 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:39:54,578 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rm3_jsyq
2023-07-22 05:39:54,578 - distributed.worker - INFO - Starting Worker plugin RMMSetup-839b922e-ab3d-4518-8ead-d330c2a13404
2023-07-22 05:39:54,579 - distributed.worker - INFO - Starting Worker plugin PreImport-99dc1c5b-43bf-4b70-8210-dc46ebd39584
2023-07-22 05:39:54,579 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb12f0dc-d550-4725-bf8a-d36ce61085ae
2023-07-22 05:39:54,579 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:54,601 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44651', status: init, memory: 0, processing: 0>
2023-07-22 05:39:54,602 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44651
2023-07-22 05:39:54,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42544
2023-07-22 05:39:54,603 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:39:54,603 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:54,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:39:54,775 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:39:54,778 - distributed.scheduler - INFO - Remove client Client-2cd618fd-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:54,778 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42522; closing.
2023-07-22 05:39:54,778 - distributed.scheduler - INFO - Remove client Client-2cd618fd-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:54,779 - distributed.scheduler - INFO - Close client connection: Client-2cd618fd-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:54,779 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44383'. Reason: nanny-close
2023-07-22 05:39:54,780 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:54,781 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44651. Reason: nanny-close
2023-07-22 05:39:54,783 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42544; closing.
2023-07-22 05:39:54,783 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:39:54,783 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44651', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:54,783 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44651
2023-07-22 05:39:54,784 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:39:54,784 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:55,896 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:39:55,897 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:39:55,897 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:39:55,898 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:39:55,899 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-07-22 05:39:59,678 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:59,682 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38915 instead
  warnings.warn(
2023-07-22 05:39:59,687 - distributed.scheduler - INFO - State start
2023-07-22 05:39:59,712 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:59,713 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:39:59,713 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:39:59,714 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-22 05:39:59,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40331'
2023-07-22 05:40:01,290 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:01,290 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-22 05:40:02,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:03,814 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41371
2023-07-22 05:40:03,815 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41371
2023-07-22 05:40:03,815 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38777
2023-07-22 05:40:03,815 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,815 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,815 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,815 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:40:03,815 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-37epdxa4
2023-07-22 05:40:03,815 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7772db90-6b40-47b7-9644-738f0105a1c2
2023-07-22 05:40:03,816 - distributed.worker - INFO - Starting Worker plugin PreImport-ebd03491-af7d-43bf-811f-2b5671914ef8
2023-07-22 05:40:03,816 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eef7349e-a7cb-4102-bfea-04a4493cf4f3
2023-07-22 05:40:03,817 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,840 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,841 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:03,883 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:03,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40331'. Reason: nanny-close
2023-07-22 05:40:03,888 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:03,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41371. Reason: nanny-close
2023-07-22 05:40:03,891 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:03,893 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-07-22 05:40:06,593 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:06,598 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36061 instead
  warnings.warn(
2023-07-22 05:40:06,601 - distributed.scheduler - INFO - State start
2023-07-22 05:40:06,622 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:06,623 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:40:06,623 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36061/status
2023-07-22 05:40:06,695 - distributed.scheduler - INFO - Receive client connection: Client-366dfc3c-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:06,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60428
2023-07-22 05:40:11,930 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:40:11,931 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:11,931 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:11,933 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:40:11,933 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-07-22 05:40:13,930 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:13,934 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-07-22 05:40:13,937 - distributed.scheduler - INFO - State start
2023-07-22 05:40:13,957 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:13,958 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-22 05:40:13,959 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-07-22 05:40:14,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46371'
2023-07-22 05:40:14,392 - distributed.scheduler - INFO - Receive client connection: Client-3a2c85b1-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:14,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46154
2023-07-22 05:40:15,552 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:15,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:15,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:16,454 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34943
2023-07-22 05:40:16,454 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34943
2023-07-22 05:40:16,454 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33931
2023-07-22 05:40:16,454 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:40:16,454 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:16,454 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:16,454 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:40:16,454 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k9ticmgf
2023-07-22 05:40:16,454 - distributed.worker - INFO - Starting Worker plugin PreImport-ca0c5b39-5829-46de-9027-c8547fe5bcf8
2023-07-22 05:40:16,454 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c97a4957-b529-4d59-971b-a29e48d96ab7
2023-07-22 05:40:16,455 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-992497a5-5dcc-46f2-b7d9-af4bf051bfa2
2023-07-22 05:40:16,455 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:16,508 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34943', status: init, memory: 0, processing: 0>
2023-07-22 05:40:16,509 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34943
2023-07-22 05:40:16,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46176
2023-07-22 05:40:16,509 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:40:16,510 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:16,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:40:16,548 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:16,551 - distributed.scheduler - INFO - Remove client Client-3a2c85b1-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:16,551 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46154; closing.
2023-07-22 05:40:16,552 - distributed.scheduler - INFO - Remove client Client-3a2c85b1-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:16,552 - distributed.scheduler - INFO - Close client connection: Client-3a2c85b1-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:16,553 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46371'. Reason: nanny-close
2023-07-22 05:40:16,554 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:16,555 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34943. Reason: nanny-close
2023-07-22 05:40:16,556 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:40:16,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46176; closing.
2023-07-22 05:40:16,557 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34943', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:16,557 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34943
2023-07-22 05:40:16,557 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:40:16,557 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:17,368 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:40:17,369 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:17,369 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:17,370 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-22 05:40:17,370 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-07-22 05:40:19,147 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:19,151 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40915 instead
  warnings.warn(
2023-07-22 05:40:19,154 - distributed.scheduler - INFO - State start
2023-07-22 05:40:19,173 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:19,174 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:40:19,174 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40915/status
2023-07-22 05:40:19,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37779'
2023-07-22 05:40:19,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38721'
2023-07-22 05:40:19,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37419'
2023-07-22 05:40:19,633 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39525'
2023-07-22 05:40:19,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43999'
2023-07-22 05:40:19,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34675'
2023-07-22 05:40:19,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32941'
2023-07-22 05:40:19,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46257'
2023-07-22 05:40:19,691 - distributed.scheduler - INFO - Receive client connection: Client-3d5936cb-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:19,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60406
2023-07-22 05:40:19,830 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36529', status: init, memory: 0, processing: 0>
2023-07-22 05:40:19,832 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36529
2023-07-22 05:40:19,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60424
2023-07-22 05:40:19,837 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43589', status: init, memory: 0, processing: 0>
2023-07-22 05:40:19,837 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43589
2023-07-22 05:40:19,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60426
2023-07-22 05:40:20,154 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43521', status: init, memory: 0, processing: 0>
2023-07-22 05:40:20,154 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43521
2023-07-22 05:40:20,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36472
2023-07-22 05:40:21,194 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:21,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:21,219 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:21,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:21,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:21,288 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34837', status: init, memory: 0, processing: 0>
2023-07-22 05:40:21,289 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34837
2023-07-22 05:40:21,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36510
2023-07-22 05:40:21,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:21,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:21,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:21,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:21,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:21,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:21,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:21,359 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:21,359 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:21,360 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:21,360 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:21,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:21,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:21,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:21,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:21,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:21,549 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:21,549 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:21,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:23,164 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45635
2023-07-22 05:40:23,164 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45635
2023-07-22 05:40:23,164 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41495
2023-07-22 05:40:23,165 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:23,165 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:23,165 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:23,165 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:23,165 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9p5_7f8f
2023-07-22 05:40:23,165 - distributed.worker - INFO - Starting Worker plugin PreImport-400a9297-b54c-40c1-b429-7e2cb0b7fd57
2023-07-22 05:40:23,165 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5d8db9f-be22-4405-bfbd-6fd383d15570
2023-07-22 05:40:23,166 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c1c8077-69df-4409-9f3e-bd9fc8dc484a
2023-07-22 05:40:23,397 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42403', status: init, memory: 0, processing: 0>
2023-07-22 05:40:23,398 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42403
2023-07-22 05:40:23,398 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36516
2023-07-22 05:40:23,442 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:23,475 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45635', status: init, memory: 0, processing: 0>
2023-07-22 05:40:23,475 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45635
2023-07-22 05:40:23,476 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36518
2023-07-22 05:40:23,476 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:23,476 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:23,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:24,056 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34473
2023-07-22 05:40:24,056 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34473
2023-07-22 05:40:24,056 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43013
2023-07-22 05:40:24,056 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,057 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,057 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:24,057 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:24,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7rvlrc_6
2023-07-22 05:40:24,058 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4adc26f0-0dea-43b5-bed0-48862cb3ee56
2023-07-22 05:40:24,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e56b573e-4154-4ff9-b4f8-343f2efa8075
2023-07-22 05:40:24,066 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45499
2023-07-22 05:40:24,066 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45499
2023-07-22 05:40:24,066 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43537
2023-07-22 05:40:24,066 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,066 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,066 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:24,066 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:24,066 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pr7_h41_
2023-07-22 05:40:24,067 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6c057ab-0d0f-4eba-9fc4-5aef95421b24
2023-07-22 05:40:24,071 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46405
2023-07-22 05:40:24,072 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46405
2023-07-22 05:40:24,072 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46719
2023-07-22 05:40:24,072 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,072 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,072 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:24,072 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:24,072 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h0z3mb4l
2023-07-22 05:40:24,072 - distributed.worker - INFO - Starting Worker plugin PreImport-ae62f2d4-2ccb-4b62-b854-729efa1c1de0
2023-07-22 05:40:24,073 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54db715b-17fa-4419-aff0-fb31d2dbb45c
2023-07-22 05:40:24,073 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68698330-0b96-4f69-8705-762f48ad5889
2023-07-22 05:40:24,189 - distributed.worker - INFO - Starting Worker plugin PreImport-975fcbd6-c208-4434-a5b2-ca537c35a39a
2023-07-22 05:40:24,190 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,202 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8f17352-5cb4-46ef-a5ec-09c7bab8e1c2
2023-07-22 05:40:24,202 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,202 - distributed.worker - INFO - Starting Worker plugin PreImport-66c26133-3ce3-459f-b11d-b0acd81506f0
2023-07-22 05:40:24,202 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,223 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34473', status: init, memory: 0, processing: 0>
2023-07-22 05:40:24,224 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34473
2023-07-22 05:40:24,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36530
2023-07-22 05:40:24,225 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,225 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:24,232 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46405', status: init, memory: 0, processing: 0>
2023-07-22 05:40:24,233 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46405
2023-07-22 05:40:24,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36534
2023-07-22 05:40:24,233 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,233 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:24,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45499', status: init, memory: 0, processing: 0>
2023-07-22 05:40:24,237 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45499
2023-07-22 05:40:24,237 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36550
2023-07-22 05:40:24,238 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,238 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,238 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37425
2023-07-22 05:40:24,239 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37425
2023-07-22 05:40:24,239 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34471
2023-07-22 05:40:24,239 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,239 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,239 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:24,239 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:24,239 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t_y9jdtp
2023-07-22 05:40:24,239 - distributed.worker - INFO - Starting Worker plugin PreImport-bb0811a1-3476-4e89-bc2d-bd79f606d570
2023-07-22 05:40:24,240 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f6b806b-d587-4e3d-9f49-3b095c8e74a0
2023-07-22 05:40:24,240 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f7c3b8e-e219-4fe3-91eb-819af61b8b83
2023-07-22 05:40:24,241 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:24,677 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,830 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43307', status: init, memory: 0, processing: 0>
2023-07-22 05:40:24,831 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43307
2023-07-22 05:40:24,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36588
2023-07-22 05:40:24,936 - distributed.scheduler - INFO - Receive client connection: Client-366dfc3c-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:24,937 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36616
2023-07-22 05:40:24,976 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37425', status: init, memory: 0, processing: 0>
2023-07-22 05:40:24,977 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37425
2023-07-22 05:40:24,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36590
2023-07-22 05:40:24,978 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,978 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,978 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45593', status: init, memory: 0, processing: 0>
2023-07-22 05:40:24,979 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45593
2023-07-22 05:40:24,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36600
2023-07-22 05:40:24,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:24,997 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36613
2023-07-22 05:40:24,997 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36613
2023-07-22 05:40:24,997 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34821
2023-07-22 05:40:24,997 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,997 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,997 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:24,997 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:24,998 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-srjf8d76
2023-07-22 05:40:24,998 - distributed.worker - INFO - Starting Worker plugin PreImport-f1be73e4-d55d-4015-94bc-bc81b9a8c9fa
2023-07-22 05:40:24,998 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e8396126-3b3b-4883-bcef-ca02a168a661
2023-07-22 05:40:24,998 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4399cd3c-2b7b-4b7f-9fa6-4cbef458c051
2023-07-22 05:40:25,006 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41193
2023-07-22 05:40:25,006 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41193
2023-07-22 05:40:25,006 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34677
2023-07-22 05:40:25,007 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:25,007 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:25,007 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:25,007 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:25,007 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jc0wxv64
2023-07-22 05:40:25,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efd891fa-fc2a-4463-8f8a-d4a2413ecaa4
2023-07-22 05:40:25,009 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33883
2023-07-22 05:40:25,009 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33883
2023-07-22 05:40:25,009 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37179
2023-07-22 05:40:25,009 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:25,009 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:25,009 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:25,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:25,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r_pm5136
2023-07-22 05:40:25,010 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a2c99a2-8b93-4be2-92e4-8973a7dd8f15
2023-07-22 05:40:25,010 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24206fd4-2084-4cf1-bb79-cfe29c78e216
2023-07-22 05:40:25,187 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:25,195 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-339b60e4-b1dc-49df-bca4-aefd8abdc6a5
2023-07-22 05:40:25,196 - distributed.worker - INFO - Starting Worker plugin PreImport-4951560a-9b43-4e12-b649-2c05fb3ab6f4
2023-07-22 05:40:25,196 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:25,202 - distributed.worker - INFO - Starting Worker plugin PreImport-24e56965-00e8-4b0b-9725-ce780f4b138c
2023-07-22 05:40:25,203 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:25,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36613', status: init, memory: 0, processing: 0>
2023-07-22 05:40:25,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36613
2023-07-22 05:40:25,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36624
2023-07-22 05:40:25,218 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:25,218 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:25,218 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41193', status: init, memory: 0, processing: 0>
2023-07-22 05:40:25,219 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41193
2023-07-22 05:40:25,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36628
2023-07-22 05:40:25,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:25,219 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:25,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:25,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:25,241 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33883', status: init, memory: 0, processing: 0>
2023-07-22 05:40:25,242 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33883
2023-07-22 05:40:25,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36640
2023-07-22 05:40:25,242 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:25,243 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:25,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:27,052 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35967', status: init, memory: 0, processing: 0>
2023-07-22 05:40:27,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35967
2023-07-22 05:40:27,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36752
2023-07-22 05:40:27,431 - distributed.scheduler - INFO - Remove client Client-366dfc3c-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:27,431 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36616; closing.
2023-07-22 05:40:27,431 - distributed.scheduler - INFO - Remove client Client-366dfc3c-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:27,432 - distributed.scheduler - INFO - Close client connection: Client-366dfc3c-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:27,438 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36588; closing.
2023-07-22 05:40:27,438 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43307', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,438 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,440 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,440 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,441 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,441 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,441 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60424; closing.
2023-07-22 05:40:27,441 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,441 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36516; closing.
2023-07-22 05:40:27,441 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,441 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,442 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,443 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36529', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,443 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,444 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42403', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,444 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,444 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36510; closing.
2023-07-22 05:40:27,445 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34837', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,445 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,446 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36752; closing.
2023-07-22 05:40:27,446 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36600; closing.
2023-07-22 05:40:27,446 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60426; closing.
2023-07-22 05:40:27,446 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36472; closing.
2023-07-22 05:40:27,447 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35967', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,447 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,448 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45593', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,448 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,448 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43589', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,448 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,449 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43521', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,449 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,450 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36529
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42403
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,451 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34837
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35967
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45593
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43589
2023-07-22 05:40:27,452 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-07-22 05:40:27,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:27,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:27,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:27,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:27,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:27,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:27,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:27,501 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:27,514 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:27,514 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:27,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:27,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:27,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:27,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:27,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:27,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:27,519 - distributed.scheduler - INFO - Remove client Client-3d5936cb-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:27,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60406; closing.
2023-07-22 05:40:27,520 - distributed.scheduler - INFO - Remove client Client-3d5936cb-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:27,520 - distributed.scheduler - INFO - Close client connection: Client-3d5936cb-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:27,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34675'. Reason: nanny-close
2023-07-22 05:40:27,522 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,522 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37779'. Reason: nanny-close
2023-07-22 05:40:27,522 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,523 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34473. Reason: nanny-close
2023-07-22 05:40:27,523 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38721'. Reason: nanny-close
2023-07-22 05:40:27,523 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,523 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37419'. Reason: nanny-close
2023-07-22 05:40:27,523 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33883. Reason: nanny-close
2023-07-22 05:40:27,524 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,524 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39525'. Reason: nanny-close
2023-07-22 05:40:27,524 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45635. Reason: nanny-close
2023-07-22 05:40:27,524 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,524 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46405. Reason: nanny-close
2023-07-22 05:40:27,524 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43999'. Reason: nanny-close
2023-07-22 05:40:27,525 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36530; closing.
2023-07-22 05:40:27,525 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,525 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,525 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34473', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,525 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34473
2023-07-22 05:40:27,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32941'. Reason: nanny-close
2023-07-22 05:40:27,525 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45499. Reason: nanny-close
2023-07-22 05:40:27,525 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,526 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,526 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46257'. Reason: nanny-close
2023-07-22 05:40:27,526 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37425. Reason: nanny-close
2023-07-22 05:40:27,526 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,526 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,526 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,526 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36613. Reason: nanny-close
2023-07-22 05:40:27,526 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34473
2023-07-22 05:40:27,526 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36640; closing.
2023-07-22 05:40:27,526 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,527 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41193. Reason: nanny-close
2023-07-22 05:40:27,527 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,527 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33883', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,527 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34473
2023-07-22 05:40:27,527 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33883
2023-07-22 05:40:27,527 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,527 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34473
2023-07-22 05:40:27,527 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36518; closing.
2023-07-22 05:40:27,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36534; closing.
2023-07-22 05:40:27,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45635', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,528 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45635
2023-07-22 05:40:27,528 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34473
2023-07-22 05:40:27,529 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,529 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46405', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,529 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46405
2023-07-22 05:40:27,529 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,529 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,530 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,530 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36550; closing.
2023-07-22 05:40:27,530 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,530 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36624; closing.
2023-07-22 05:40:27,531 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45499', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,531 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45499
2023-07-22 05:40:27,531 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36613', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,531 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36613
2023-07-22 05:40:27,531 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36590; closing.
2023-07-22 05:40:27,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36628; closing.
2023-07-22 05:40:27,533 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37425', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,533 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37425
2023-07-22 05:40:27,533 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41193', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:27,533 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41193
2023-07-22 05:40:27,533 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:40:29,339 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:40:29,340 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:29,340 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:29,342 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:40:29,342 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-07-22 05:40:31,292 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:31,297 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37635 instead
  warnings.warn(
2023-07-22 05:40:31,300 - distributed.scheduler - INFO - State start
2023-07-22 05:40:31,320 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:31,320 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:31,321 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:31,321 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-22 05:40:31,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39739'
2023-07-22 05:40:32,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:32,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:32,982 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:34,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43905
2023-07-22 05:40:34,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43905
2023-07-22 05:40:34,381 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37733
2023-07-22 05:40:34,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:34,381 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:34,381 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:34,381 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:40:34,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ctfnkmdh
2023-07-22 05:40:34,382 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9f4bfec-537f-4f28-b6ad-a5852701136a
2023-07-22 05:40:34,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e601fc2d-3ddf-4688-8c60-30386445c302
2023-07-22 05:40:35,159 - distributed.worker - INFO - Starting Worker plugin PreImport-da612c9f-6665-470b-b5a6-e588399a35ab
2023-07-22 05:40:35,160 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:35,200 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:35,200 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:35,205 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:35,224 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:35,234 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39739'. Reason: nanny-close
2023-07-22 05:40:35,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:35,236 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43905. Reason: nanny-close
2023-07-22 05:40:35,238 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:35,239 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits Fatal Python error: init_import_site: Failed to import the site module
Python runtime state: initialized
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site.py", line 589, in <module>
    main()
  File "/opt/conda/envs/gdf/lib/python3.9/site.py", line 576, in main
    known_paths = addsitepackages(known_paths)
  File "/opt/conda/envs/gdf/lib/python3.9/site.py", line 359, in addsitepackages
    addsitedir(sitedir, known_paths)
  File "/opt/conda/envs/gdf/lib/python3.9/site.py", line 208, in addsitedir
    addpackage(sitedir, name, known_paths)
  File "/opt/conda/envs/gdf/lib/python3.9/site.py", line 169, in addpackage
    exec(line)
  File "<string>", line 1, in <module>
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/util.py", line 2, in <module>
    from . import abc
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/abc.py", line 17, in <module>
    from typing import Protocol, runtime_checkable
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 1682, in <module>
    CT_co = TypeVar('CT_co', covariant=True, bound=type)
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 628, in __init__
    def __init__(self, name, *constraints, bound=None,
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 5, in <module>
    from dask.__main__ import main
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 1, in <module>
    from dask import config, datasets
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/datasets.py", line 3, in <module>
    from dask.utils import import_required
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 10, in <module>
    import tempfile
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 978, in get_code
  File "<frozen importlib._bootstrap_external>", line 647, in _compile_bytecode
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38865 instead
  warnings.warn(
2023-07-22 05:40:46,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:46,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:46,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:46,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:46,759 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:46,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:46,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:46,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:46,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:46,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:46,807 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:46,808 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:46,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:46,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:46,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:46,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41789 instead
  warnings.warn(
2023-07-22 05:40:57,550 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:57,551 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:57,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:57,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:57,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:57,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:57,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:57,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:57,668 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:57,668 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:57,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:57,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:57,688 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:57,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:57,707 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:57,707 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37825 instead
  warnings.warn(
2023-07-22 05:41:06,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:06,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:06,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:06,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:06,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:06,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:06,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:06,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:06,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:06,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:06,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:06,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:06,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:06,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:06,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:06,488 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41443 instead
  warnings.warn(
2023-07-22 05:41:15,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:15,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:15,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:15,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:15,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:15,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:15,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:15,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:15,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:15,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:15,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:15,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:15,801 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:15,801 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:15,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:15,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37479 instead
  warnings.warn(
2023-07-22 05:41:28,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:28,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:28,076 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:28,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:28,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:28,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:28,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:28,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:28,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:28,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:28,115 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:28,116 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:28,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:28,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:28,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:28,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41329 instead
  warnings.warn(
2023-07-22 05:41:39,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:39,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:39,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:39,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:39,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:39,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:39,144 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:39,144 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:39,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:39,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:39,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:39,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:39,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:39,200 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:39,249 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:39,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37863 instead
  warnings.warn(
2023-07-22 05:41:50,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:50,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:50,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:50,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:50,674 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:50,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:50,708 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:50,709 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:50,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:50,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:50,744 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:50,744 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:50,747 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:50,747 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:50,779 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:50,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39025 instead
  warnings.warn(
2023-07-22 05:42:05,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:05,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:05,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:05,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:05,276 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:05,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:05,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:05,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:05,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:05,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:05,352 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:05,352 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:05,355 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:05,355 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:05,387 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:05,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37615 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36089 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33053 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36565 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46513 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46115 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41445 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42935 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36885 instead
  warnings.warn(
2023-07-22 05:44:48,932 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-50d53a0f-6efe-4e8d-a2b6-94ae17c87920
Function:  _run_coroutine_on_worker
args:      (187051713161327690888953012035185641338, <function shuffle_task at 0x7fdde8df7ca0>, ('explicit-comms-shuffle-74a4759807e1039752830c55168de2af', {0: {"('explicit-comms-shuffle-a8f29e1da98b432e474f0374532c3c7f', 0)"}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(999, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_UNKNOWN')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
