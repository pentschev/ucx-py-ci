============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.2, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-06-23 05:42:41,358 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:42:41,362 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45285 instead
  warnings.warn(
2023-06-23 05:42:41,365 - distributed.scheduler - INFO - State start
2023-06-23 05:42:41,391 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:42:41,392 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-23 05:42:41,393 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45285/status
2023-06-23 05:42:41,446 - distributed.scheduler - INFO - Receive client connection: Client-c501b0e5-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:42:41,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41016
2023-06-23 05:42:41,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36405'
2023-06-23 05:42:41,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45069'
2023-06-23 05:42:41,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44375'
2023-06-23 05:42:41,589 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34635'
2023-06-23 05:42:42,275 - distributed.scheduler - INFO - Receive client connection: Client-c4145cb0-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:42:42,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41072
2023-06-23 05:42:43,304 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:43,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:43,312 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:43,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:43,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:43,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:43,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:43,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:43,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:43,354 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:43,354 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:43,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-06-23 05:42:43,376 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42685
2023-06-23 05:42:43,376 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42685
2023-06-23 05:42:43,376 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43095
2023-06-23 05:42:43,376 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:42:43,376 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:43,376 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:42:43,377 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:42:43,377 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8o90vffn
2023-06-23 05:42:43,377 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8212f497-e52e-4d08-9052-3b6b2acc325d
2023-06-23 05:42:43,377 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-479d4fee-69a5-4e3e-998f-03955ef17939
2023-06-23 05:42:43,377 - distributed.worker - INFO - Starting Worker plugin PreImport-d8e1b001-55b9-4b75-a3de-2f9487588578
2023-06-23 05:42:43,377 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:43,391 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42685', status: init, memory: 0, processing: 0>
2023-06-23 05:42:43,392 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42685
2023-06-23 05:42:43,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41118
2023-06-23 05:42:43,392 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:42:43,392 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:43,394 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:42:44,838 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34203
2023-06-23 05:42:44,839 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34203
2023-06-23 05:42:44,839 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35271
2023-06-23 05:42:44,839 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:42:44,839 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,839 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:42:44,839 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:42:44,839 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3cwidov7
2023-06-23 05:42:44,839 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a4a7fa9-9df9-4387-b376-5b3d4ea2461e
2023-06-23 05:42:44,840 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9a8cec2-141d-41d2-8f39-b75ce0ccab54
2023-06-23 05:42:44,840 - distributed.worker - INFO - Starting Worker plugin PreImport-cb2fae69-00cf-4374-ac0a-65fce179bbd8
2023-06-23 05:42:44,840 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34203', status: init, memory: 0, processing: 0>
2023-06-23 05:42:44,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34203
2023-06-23 05:42:44,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41138
2023-06-23 05:42:44,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:42:44,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:42:44,942 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35159
2023-06-23 05:42:44,943 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35159
2023-06-23 05:42:44,943 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35231
2023-06-23 05:42:44,943 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:42:44,943 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,943 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:42:44,943 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:42:44,943 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uz2a8ly3
2023-06-23 05:42:44,943 - distributed.worker - INFO - Starting Worker plugin PreImport-9635cb5a-2b6c-4921-8c16-1b913df64728
2023-06-23 05:42:44,943 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ff4b03f-9c9b-493d-8849-ae52589d2fbd
2023-06-23 05:42:44,943 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9515690b-64d2-45ec-a7d2-5425fda5e6b4
2023-06-23 05:42:44,944 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,960 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41567
2023-06-23 05:42:44,961 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41567
2023-06-23 05:42:44,961 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38587
2023-06-23 05:42:44,961 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:42:44,961 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,961 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:42:44,961 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:42:44,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9c79_mez
2023-06-23 05:42:44,961 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31cdce2a-14ee-4b86-8de6-6bd3e97d6076
2023-06-23 05:42:44,961 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ece086e4-264f-4979-a62e-d7099c612594
2023-06-23 05:42:44,962 - distributed.worker - INFO - Starting Worker plugin PreImport-373361c1-14b6-4221-956f-c2c1777e09b6
2023-06-23 05:42:44,965 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,973 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35159', status: init, memory: 0, processing: 0>
2023-06-23 05:42:44,973 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35159
2023-06-23 05:42:44,973 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41142
2023-06-23 05:42:44,975 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:42:44,975 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:42:44,993 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41567', status: init, memory: 0, processing: 0>
2023-06-23 05:42:44,994 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41567
2023-06-23 05:42:44,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41150
2023-06-23 05:42:44,994 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:42:44,995 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:44,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:42:45,028 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37847', status: init, memory: 0, processing: 0>
2023-06-23 05:42:45,028 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37847
2023-06-23 05:42:45,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41162
2023-06-23 05:42:46,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35563', status: init, memory: 0, processing: 0>
2023-06-23 05:42:46,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35563
2023-06-23 05:42:46,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41220
2023-06-23 05:42:46,609 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40777', status: init, memory: 0, processing: 0>
2023-06-23 05:42:46,609 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40777
2023-06-23 05:42:46,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41226
2023-06-23 05:42:46,646 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45921', status: init, memory: 0, processing: 0>
2023-06-23 05:42:46,647 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45921
2023-06-23 05:42:46,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41232
2023-06-23 05:42:51,529 - distributed.scheduler - INFO - Remove client Client-c501b0e5-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:42:51,529 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41016; closing.
2023-06-23 05:42:51,529 - distributed.scheduler - INFO - Remove client Client-c501b0e5-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:42:51,530 - distributed.scheduler - INFO - Close client connection: Client-c501b0e5-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:42:51,536 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41226; closing.
2023-06-23 05:42:51,536 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40777', status: closing, memory: 0, processing: 0>
2023-06-23 05:42:51,536 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40777
2023-06-23 05:42:51,538 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40777
2023-06-23 05:42:51,538 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41220; closing.
2023-06-23 05:42:51,538 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40777
2023-06-23 05:42:51,538 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40777
2023-06-23 05:42:51,538 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41232; closing.
2023-06-23 05:42:51,538 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40777
2023-06-23 05:42:51,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35563', status: closing, memory: 0, processing: 0>
2023-06-23 05:42:51,539 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35563
2023-06-23 05:42:51,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45921', status: closing, memory: 0, processing: 0>
2023-06-23 05:42:51,540 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45921
2023-06-23 05:42:51,540 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41162; closing.
2023-06-23 05:42:51,540 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37847', status: closing, memory: 0, processing: 0>
2023-06-23 05:42:51,541 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37847
2023-06-23 05:42:51,543 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35563
2023-06-23 05:42:51,543 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45921
2023-06-23 05:42:51,543 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35563
2023-06-23 05:42:51,543 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37847
2023-06-23 05:42:51,543 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45921
2023-06-23 05:42:51,543 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35563
2023-06-23 05:42:51,543 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37847
2023-06-23 05:42:51,543 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35563
2023-06-23 05:42:51,544 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45921
2023-06-23 05:42:51,544 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45921
2023-06-23 05:42:51,544 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37847
2023-06-23 05:42:51,544 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37847
2023-06-23 05:42:51,633 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-23 05:42:51,633 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-23 05:42:51,633 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-23 05:42:51,634 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-23 05:42:51,638 - distributed.scheduler - INFO - Remove client Client-c4145cb0-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:42:51,638 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41072; closing.
2023-06-23 05:42:51,639 - distributed.scheduler - INFO - Remove client Client-c4145cb0-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:42:51,639 - distributed.scheduler - INFO - Close client connection: Client-c4145cb0-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:42:51,640 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36405'. Reason: nanny-close
2023-06-23 05:42:51,641 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:42:51,641 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45069'. Reason: nanny-close
2023-06-23 05:42:51,642 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:42:51,642 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44375'. Reason: nanny-close
2023-06-23 05:42:51,642 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41567. Reason: nanny-close
2023-06-23 05:42:51,643 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:42:51,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34635'. Reason: nanny-close
2023-06-23 05:42:51,643 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:42:51,643 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34203. Reason: nanny-close
2023-06-23 05:42:51,644 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35159. Reason: nanny-close
2023-06-23 05:42:51,645 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41150; closing.
2023-06-23 05:42:51,645 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42685. Reason: nanny-close
2023-06-23 05:42:51,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:42:51,645 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41567', status: closing, memory: 0, processing: 0>
2023-06-23 05:42:51,645 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41567
2023-06-23 05:42:51,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:42:51,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:42:51,646 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41138; closing.
2023-06-23 05:42:51,647 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34203', status: closing, memory: 0, processing: 0>
2023-06-23 05:42:51,647 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41567
2023-06-23 05:42:51,647 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34203
2023-06-23 05:42:51,647 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41142; closing.
2023-06-23 05:42:51,647 - distributed.nanny - INFO - Worker closed
2023-06-23 05:42:51,647 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:42:51,647 - distributed.nanny - INFO - Worker closed
2023-06-23 05:42:51,648 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35159', status: closing, memory: 0, processing: 0>
2023-06-23 05:42:51,648 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35159
2023-06-23 05:42:51,648 - distributed.nanny - INFO - Worker closed
2023-06-23 05:42:51,648 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41118; closing.
2023-06-23 05:42:51,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42685', status: closing, memory: 0, processing: 0>
2023-06-23 05:42:51,649 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42685
2023-06-23 05:42:51,649 - distributed.nanny - INFO - Worker closed
2023-06-23 05:42:51,649 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:42:52,757 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:42:52,757 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:42:52,758 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:42:52,759 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-23 05:42:52,759 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-06-23 05:42:54,781 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:42:54,787 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44345 instead
  warnings.warn(
2023-06-23 05:42:54,791 - distributed.scheduler - INFO - State start
2023-06-23 05:42:54,812 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:42:54,813 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:42:54,814 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:42:54,814 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:42:55,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34433'
2023-06-23 05:42:55,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38965'
2023-06-23 05:42:55,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36645'
2023-06-23 05:42:55,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34647'
2023-06-23 05:42:55,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42553'
2023-06-23 05:42:55,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32975'
2023-06-23 05:42:55,193 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34961'
2023-06-23 05:42:55,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40307'
2023-06-23 05:42:56,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,989 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:57,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:57,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:57,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,154 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,204 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,224 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,228 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,234 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,241 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:04,466 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36075
2023-06-23 05:43:04,466 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36075
2023-06-23 05:43:04,466 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39567
2023-06-23 05:43:04,466 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,466 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,466 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,466 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,466 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gbc8h6zh
2023-06-23 05:43:04,467 - distributed.worker - INFO - Starting Worker plugin PreImport-7e587957-c0b6-4ef9-849d-778387ac7480
2023-06-23 05:43:04,467 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b97afd8c-4893-4874-9965-996316ad3547
2023-06-23 05:43:04,475 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46561
2023-06-23 05:43:04,475 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46561
2023-06-23 05:43:04,475 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44089
2023-06-23 05:43:04,475 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,475 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,475 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,476 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_v5flqzv
2023-06-23 05:43:04,476 - distributed.worker - INFO - Starting Worker plugin PreImport-c2131ba4-db99-428d-b229-820f2deb418e
2023-06-23 05:43:04,476 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1604caa9-34e1-461c-8a06-f6d5a5732e6d
2023-06-23 05:43:04,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34357
2023-06-23 05:43:04,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34357
2023-06-23 05:43:04,536 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33305
2023-06-23 05:43:04,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,536 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,536 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1vy108r4
2023-06-23 05:43:04,537 - distributed.worker - INFO - Starting Worker plugin PreImport-b1704599-e247-4174-998d-98f1f2def999
2023-06-23 05:43:04,537 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4634f783-cc18-49bd-bcbf-28b4ee791896
2023-06-23 05:43:04,537 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c703f1b-88d8-4f78-b53c-72f46cdc87c1
2023-06-23 05:43:04,543 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34727
2023-06-23 05:43:04,544 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34727
2023-06-23 05:43:04,544 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46369
2023-06-23 05:43:04,544 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,544 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,544 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,544 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,544 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c83gu4cm
2023-06-23 05:43:04,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9484285-093b-40f4-aeca-346474937f60
2023-06-23 05:43:04,545 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86c32606-b1cc-471a-869f-7b19fa023524
2023-06-23 05:43:04,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43469
2023-06-23 05:43:04,549 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43469
2023-06-23 05:43:04,549 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35611
2023-06-23 05:43:04,549 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,549 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,549 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-alqe8cce
2023-06-23 05:43:04,550 - distributed.worker - INFO - Starting Worker plugin PreImport-497d717f-1b88-425e-be99-8a526c51d760
2023-06-23 05:43:04,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d31aeb84-9de8-48fa-b17d-e125be1ded3a
2023-06-23 05:43:04,550 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee120d26-f66a-4589-92b3-da58ff5962cc
2023-06-23 05:43:04,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40731
2023-06-23 05:43:04,579 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40731
2023-06-23 05:43:04,579 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34149
2023-06-23 05:43:04,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,579 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,579 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,580 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,580 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h152k3vb
2023-06-23 05:43:04,580 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd61af18-375d-4f3e-b6b6-e28eef3dc885
2023-06-23 05:43:04,581 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e844a0b3-4d91-45c6-a67c-11b675933e2d
2023-06-23 05:43:04,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44633
2023-06-23 05:43:04,592 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44633
2023-06-23 05:43:04,592 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44775
2023-06-23 05:43:04,593 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,593 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,593 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,593 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6bin3zdf
2023-06-23 05:43:04,593 - distributed.worker - INFO - Starting Worker plugin PreImport-8208d5ae-5ad6-4d28-b76f-1be234944a15
2023-06-23 05:43:04,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5d2288c-f129-42b1-8b82-2ddab80eeaee
2023-06-23 05:43:04,602 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38861
2023-06-23 05:43:04,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38861
2023-06-23 05:43:04,603 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45975
2023-06-23 05:43:04,603 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,603 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,603 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,603 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rfk8ukcm
2023-06-23 05:43:04,604 - distributed.worker - INFO - Starting Worker plugin PreImport-396e19d7-b528-4457-a698-1f53b0fc4a73
2023-06-23 05:43:04,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce4008b9-8d7a-491e-aed7-cb3432159ce1
2023-06-23 05:43:04,774 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00e71279-22eb-4633-ba43-492d0bff2d74
2023-06-23 05:43:04,775 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,797 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,799 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3184926-1335-4808-9f09-d17511c214b2
2023-06-23 05:43:04,799 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,812 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,812 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,825 - distributed.worker - INFO - Starting Worker plugin PreImport-286a6a09-9690-4e4d-a58c-66a09657e024
2023-06-23 05:43:04,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,828 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,829 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,838 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,838 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01d28d51-fd87-4144-8592-211940cf3adb
2023-06-23 05:43:04,859 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,859 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d86d96fb-1e52-4438-960b-e691dfbd432a
2023-06-23 05:43:04,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,861 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,867 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,889 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,897 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,898 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,903 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,903 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-23 05:43:05,059 - distributed.worker - INFO - Starting Worker plugin PreImport-e927144e-1a03-4586-b820-be346531fd98
2023-06-23 05:43:05,060 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40731. Reason: worker-close
2023-06-23 05:43:05,060 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:43:05,063 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:05,069 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:05,072 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36645'. Reason: nanny-instantiate-failed
2023-06-23 05:43:05,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:43:05,455 - distributed.nanny - INFO - Worker process 52065 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:43:05,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52082 parent=51892 started daemon>
2023-06-23 05:43:05,461 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52079 parent=51892 started daemon>
2023-06-23 05:43:05,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52075 parent=51892 started daemon>
2023-06-23 05:43:05,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52074 parent=51892 started daemon>
2023-06-23 05:43:05,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52069 parent=51892 started daemon>
2023-06-23 05:43:05,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52061 parent=51892 started daemon>
2023-06-23 05:43:05,462 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52058 parent=51892 started daemon>
2023-06-23 05:43:06,376 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52074 exit status was already read will report exitcode 255
2023-06-23 05:43:06,634 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52082 exit status was already read will report exitcode 255
2023-06-23 05:43:16,148 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7fc48eebea30>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1318, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1348, in _ensure_connected
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-06-23 05:43:18,036 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:18,041 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36107 instead
  warnings.warn(
2023-06-23 05:43:18,045 - distributed.scheduler - INFO - State start
2023-06-23 05:43:18,155 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:18,156 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:43:18,156 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:43:18,157 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:43:18,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35605'
2023-06-23 05:43:18,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42539'
2023-06-23 05:43:18,413 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41571'
2023-06-23 05:43:18,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43123'
2023-06-23 05:43:18,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39641'
2023-06-23 05:43:18,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44419'
2023-06-23 05:43:18,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40985'
2023-06-23 05:43:18,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45395'
2023-06-23 05:43:19,623 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43123'. Reason: nanny-close
2023-06-23 05:43:19,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35605'. Reason: nanny-close
2023-06-23 05:43:19,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42539'. Reason: nanny-close
2023-06-23 05:43:19,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41571'. Reason: nanny-close
2023-06-23 05:43:19,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39641'. Reason: nanny-close
2023-06-23 05:43:19,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44419'. Reason: nanny-close
2023-06-23 05:43:19,625 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40985'. Reason: nanny-close
2023-06-23 05:43:19,625 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45395'. Reason: nanny-close
2023-06-23 05:43:20,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c83gu4cm', purging
2023-06-23 05:43:20,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1vy108r4', purging
2023-06-23 05:43:20,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gbc8h6zh', purging
2023-06-23 05:43:20,244 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-alqe8cce', purging
2023-06-23 05:43:20,244 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_v5flqzv', purging
2023-06-23 05:43:20,244 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6bin3zdf', purging
2023-06-23 05:43:20,245 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rfk8ukcm', purging
2023-06-23 05:43:20,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:20,245 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:20,273 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:20,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:20,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:20,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:20,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:20,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:20,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:20,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:20,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:20,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:20,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:20,345 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:20,358 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:20,359 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:20,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:20,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:20,384 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:20,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:20,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:20,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:20,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:20,526 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:24,964 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42455
2023-06-23 05:43:24,964 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42455
2023-06-23 05:43:24,964 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39783
2023-06-23 05:43:24,964 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:24,964 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:24,964 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:24,964 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:24,964 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-spl34pqu
2023-06-23 05:43:24,965 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-917dca8b-30ad-447c-93aa-e2ac5c57b98a
2023-06-23 05:43:24,965 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7765a46-f341-4f26-a181-a79282a67e1c
2023-06-23 05:43:25,017 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45491
2023-06-23 05:43:25,017 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45491
2023-06-23 05:43:25,017 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46171
2023-06-23 05:43:25,017 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,018 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,018 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:25,018 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:25,018 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1mpfwiei
2023-06-23 05:43:25,018 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea6651aa-1605-4a94-bdea-b95bcdce2802
2023-06-23 05:43:25,018 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bba80bfe-6f10-447b-aed9-627dea12432d
2023-06-23 05:43:25,077 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41995
2023-06-23 05:43:25,077 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41995
2023-06-23 05:43:25,077 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33741
2023-06-23 05:43:25,077 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,078 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,078 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:25,078 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:25,078 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pzlbxrot
2023-06-23 05:43:25,079 - distributed.worker - INFO - Starting Worker plugin PreImport-910174b2-bbd1-4905-915c-18fb43c2ea92
2023-06-23 05:43:25,079 - distributed.worker - INFO - Starting Worker plugin RMMSetup-728b3952-59cc-4506-b2a5-2857809d5f58
2023-06-23 05:43:25,166 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42565
2023-06-23 05:43:25,166 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42565
2023-06-23 05:43:25,166 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37171
2023-06-23 05:43:25,166 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,166 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,166 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:25,166 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:25,166 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-020sz4ln
2023-06-23 05:43:25,167 - distributed.worker - INFO - Starting Worker plugin PreImport-8d76161e-3eca-4964-b97f-03399f58f0ce
2023-06-23 05:43:25,167 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37952b34-4fc8-4efd-beae-19ff59a4c81e
2023-06-23 05:43:25,216 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33011
2023-06-23 05:43:25,217 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33011
2023-06-23 05:43:25,217 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45069
2023-06-23 05:43:25,217 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,217 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,217 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:25,217 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:25,217 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2imr8wsr
2023-06-23 05:43:25,218 - distributed.worker - INFO - Starting Worker plugin PreImport-b885b80f-9b59-49cb-9693-98d7105f6582
2023-06-23 05:43:25,218 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ca017af-2c6e-41d0-a99b-4e591177aa37
2023-06-23 05:43:25,221 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38303
2023-06-23 05:43:25,221 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38303
2023-06-23 05:43:25,221 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43623
2023-06-23 05:43:25,221 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,221 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,221 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:25,221 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:25,221 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fth95jwz
2023-06-23 05:43:25,222 - distributed.worker - INFO - Starting Worker plugin PreImport-119345db-5ae2-45c4-99e3-db0793360675
2023-06-23 05:43:25,222 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc607381-1388-4793-9039-42011d8017f2
2023-06-23 05:43:25,227 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36311
2023-06-23 05:43:25,228 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36311
2023-06-23 05:43:25,228 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43085
2023-06-23 05:43:25,228 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,228 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,228 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:25,228 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:25,228 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2r1ldt72
2023-06-23 05:43:25,229 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7dd9f43b-7847-4d88-916c-eb64a8c90283
2023-06-23 05:43:25,229 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d1f0187-c3d5-43f1-bffc-171042c6ea25
2023-06-23 05:43:25,230 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42719
2023-06-23 05:43:25,230 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42719
2023-06-23 05:43:25,230 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41929
2023-06-23 05:43:25,230 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,230 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,230 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:25,230 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:25,230 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sfcvlpk1
2023-06-23 05:43:25,231 - distributed.worker - INFO - Starting Worker plugin PreImport-2b01a86e-e3c3-4ec3-828b-0a67dde2470c
2023-06-23 05:43:25,231 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7358b65b-415f-4f7c-bc0f-f2d747be6f77
2023-06-23 05:43:25,232 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4325f4d8-313c-4d68-87b8-625245c045f6
2023-06-23 05:43:25,247 - distributed.worker - INFO - Starting Worker plugin PreImport-db5a7fa2-cff2-43f3-8320-383f0ba3545c
2023-06-23 05:43:25,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,249 - distributed.worker - INFO - Starting Worker plugin PreImport-a0057f97-23a5-40ac-9c25-e8dfdc36ae1a
2023-06-23 05:43:25,250 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,252 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd1c87c7-17d2-401c-83d7-ca51b04b6dea
2023-06-23 05:43:25,252 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,255 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f5cd816-aba1-473e-9bad-e620806c5220
2023-06-23 05:43:25,255 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-92a4ba82-7daf-4eb3-b914-e0c0a1548c83
2023-06-23 05:43:25,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cfd32045-0961-4c56-8d4b-89ac53bbbc32
2023-06-23 05:43:25,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,257 - distributed.worker - INFO - Starting Worker plugin PreImport-d35ec863-dfe6-4060-8bc1-a2c0e8d91eea
2023-06-23 05:43:25,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,273 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,274 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,275 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:25,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:25,280 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,280 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,281 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,282 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,282 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:25,285 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,285 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:25,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:25,288 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,288 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:25,291 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,291 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,292 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:25,292 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:25,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:25,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:25,321 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:25,322 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:25,322 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:25,323 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:25,323 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:25,323 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45491. Reason: nanny-close
2023-06-23 05:43:25,323 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42455. Reason: nanny-close
2023-06-23 05:43:25,324 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:25,324 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42719. Reason: nanny-close
2023-06-23 05:43:25,324 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36311. Reason: nanny-close
2023-06-23 05:43:25,324 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:25,324 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41995. Reason: nanny-close
2023-06-23 05:43:25,325 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38303. Reason: nanny-close
2023-06-23 05:43:25,325 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33011. Reason: nanny-close
2023-06-23 05:43:25,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:25,326 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:25,326 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:25,326 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:25,327 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:25,327 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:25,327 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:25,328 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:25,328 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:25,328 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:25,329 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:25,329 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:25,329 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:25,330 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:25,330 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:25,331 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42565. Reason: nanny-close
2023-06-23 05:43:25,333 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42455
2023-06-23 05:43:25,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:25,335 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:26,056 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-06-23 05:43:26,392 - distributed.nanny - WARNING - Restarting worker
2023-06-23 05:43:26,393 - distributed.nanny - WARNING - Restarting worker
2023-06-23 05:43:26,404 - distributed.nanny - WARNING - Restarting worker
2023-06-23 05:43:26,413 - distributed.nanny - WARNING - Restarting worker
2023-06-23 05:43:26,422 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-06-23 05:43:26,458 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-06-23 05:43:26,575 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-06-23 05:43:26,591 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52484 parent=52166 started daemon>
2023-06-23 05:43:26,591 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52481 parent=52166 started daemon>
2023-06-23 05:43:26,591 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52478 parent=52166 started daemon>
2023-06-23 05:43:26,591 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52475 parent=52166 started daemon>
2023-06-23 05:43:26,591 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52472 parent=52166 started daemon>
2023-06-23 05:43:26,591 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52469 parent=52166 started daemon>
2023-06-23 05:43:26,591 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52466 parent=52166 started daemon>
2023-06-23 05:43:26,591 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52463 parent=52166 started daemon>
2023-06-23 05:43:26,594 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52466 exit status was already read will report exitcode 255
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-06-23 05:43:29,056 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:29,061 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42275 instead
  warnings.warn(
2023-06-23 05:43:29,065 - distributed.scheduler - INFO - State start
2023-06-23 05:43:29,376 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:29,377 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:43:29,378 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:43:29,378 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:43:29,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33849'
2023-06-23 05:43:29,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45289'
2023-06-23 05:43:29,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40685'
2023-06-23 05:43:29,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45631'
2023-06-23 05:43:29,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41413'
2023-06-23 05:43:29,680 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36803'
2023-06-23 05:43:29,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45861'
2023-06-23 05:43:29,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45221'
2023-06-23 05:43:31,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:31,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:31,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:31,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:31,492 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:31,499 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:31,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:31,512 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:31,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:31,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:31,532 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:31,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:31,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:31,549 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:31,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:31,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:31,553 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:31,565 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:31,565 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:31,581 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:31,581 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:31,625 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:31,671 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:31,672 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:35,636 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40325
2023-06-23 05:43:35,636 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40325
2023-06-23 05:43:35,636 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45667
2023-06-23 05:43:35,636 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:35,636 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:35,636 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:35,636 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:35,636 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_ogt_q86
2023-06-23 05:43:35,637 - distributed.worker - INFO - Starting Worker plugin PreImport-53a0d89d-a1a6-45fc-a732-4774d2e3f1d8
2023-06-23 05:43:35,637 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d2b549d-63bc-4b06-a509-977ea6fbc398
2023-06-23 05:43:35,800 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40619
2023-06-23 05:43:35,800 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40619
2023-06-23 05:43:35,800 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46169
2023-06-23 05:43:35,800 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:35,800 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:35,800 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:35,801 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:35,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-akqeeb0o
2023-06-23 05:43:35,801 - distributed.worker - INFO - Starting Worker plugin PreImport-fc011486-5c93-40c7-9d08-163bf1916167
2023-06-23 05:43:35,801 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49ab8f2b-1458-4ee9-84cd-504cec11512c
2023-06-23 05:43:35,973 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43399
2023-06-23 05:43:35,973 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43399
2023-06-23 05:43:35,974 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40667
2023-06-23 05:43:35,974 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:35,974 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:35,974 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:35,974 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:35,974 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xfh1jr8u
2023-06-23 05:43:35,974 - distributed.worker - INFO - Starting Worker plugin PreImport-569275fe-f45d-4bee-8007-a55578104d34
2023-06-23 05:43:35,974 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3915c26e-02bc-417e-aaea-d4e02125d9a8
2023-06-23 05:43:35,975 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be86f045-88fb-44b8-9c85-8901cf0912d8
2023-06-23 05:43:36,018 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38397
2023-06-23 05:43:36,018 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38397
2023-06-23 05:43:36,018 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40937
2023-06-23 05:43:36,018 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,018 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,018 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:36,019 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:36,019 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pxyao54o
2023-06-23 05:43:36,019 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb6d5b9d-4802-4ca4-882c-bd646f79fd24
2023-06-23 05:43:36,020 - distributed.worker - INFO - Starting Worker plugin PreImport-23482b8e-176d-4776-9db5-a2db85ee56d4
2023-06-23 05:43:36,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e429bd74-3771-4881-8930-eb4cda45109a
2023-06-23 05:43:36,077 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39875
2023-06-23 05:43:36,077 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39875
2023-06-23 05:43:36,077 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38415
2023-06-23 05:43:36,077 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,077 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,077 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:36,077 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:36,077 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y7pa5cja
2023-06-23 05:43:36,078 - distributed.worker - INFO - Starting Worker plugin PreImport-6f978755-a465-484d-bf7b-07c6cdb693cc
2023-06-23 05:43:36,078 - distributed.worker - INFO - Starting Worker plugin RMMSetup-996847b5-4a20-45b2-b206-ca2010d906cd
2023-06-23 05:43:36,122 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32865
2023-06-23 05:43:36,122 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32865
2023-06-23 05:43:36,122 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43297
2023-06-23 05:43:36,122 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,122 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,122 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:36,122 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:36,122 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7c3lmepg
2023-06-23 05:43:36,123 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa0aaddb-7f02-4ffa-bccc-361d6e484bfc
2023-06-23 05:43:36,123 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d4d7992-dbd0-4315-ad98-1440b4c43baa
2023-06-23 05:43:36,139 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43471
2023-06-23 05:43:36,139 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43471
2023-06-23 05:43:36,139 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36493
2023-06-23 05:43:36,139 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,139 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,139 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:36,139 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:36,139 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ikbpn90s
2023-06-23 05:43:36,140 - distributed.worker - INFO - Starting Worker plugin PreImport-7069174a-33f6-4d2f-82c2-6f84967ed392
2023-06-23 05:43:36,140 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a46164d-3234-45db-a583-06fcb0462b87
2023-06-23 05:43:36,140 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43497
2023-06-23 05:43:36,140 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43497
2023-06-23 05:43:36,140 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41207
2023-06-23 05:43:36,140 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,140 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,140 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:36,140 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:36,141 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qhkv0z9q
2023-06-23 05:43:36,142 - distributed.worker - INFO - Starting Worker plugin PreImport-ef7086d1-a0e4-4e94-9f4e-0213d76f5122
2023-06-23 05:43:36,142 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-92856480-2d33-4b15-a882-52da264b345f
2023-06-23 05:43:36,142 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee9a4052-8e30-4d85-a4e3-e006d2612312
2023-06-23 05:43:36,164 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f506eb1-104e-4304-a4dc-b06bfdde8431
2023-06-23 05:43:36,164 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,188 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-19eed2ea-e222-406c-950e-98862d7fa7fa
2023-06-23 05:43:36,190 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,219 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:36,242 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,242 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:36,289 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3805c871-3470-4d68-b845-36bb5303cac8
2023-06-23 05:43:36,318 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,320 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,320 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:36,343 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d390a495-16ed-46b0-bdd2-980886cc3d65
2023-06-23 05:43:36,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,346 - distributed.worker - INFO - Starting Worker plugin PreImport-df257a57-e866-4d5a-bcb8-f9645cf2cdef
2023-06-23 05:43:36,346 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,350 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,351 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,351 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:36,386 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,386 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,388 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,389 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:36,390 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:36,390 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:36,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:36,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 93, in setup
    rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 305, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory
2023-06-23 05:43:36,482 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38397. Reason: worker-close
2023-06-23 05:43:36,482 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:43:36,487 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 93, in setup
    rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 305, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:36,537 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 93, in setup
    rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 305, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:36,541 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45631'. Reason: nanny-instantiate-failed
2023-06-23 05:43:36,541 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:43:36,930 - distributed.nanny - INFO - Worker process 52666 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 93, in setup
    rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 305, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:43:36,937 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52678 parent=52488 started daemon>
2023-06-23 05:43:36,937 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52675 parent=52488 started daemon>
2023-06-23 05:43:36,937 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52672 parent=52488 started daemon>
2023-06-23 05:43:36,937 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52669 parent=52488 started daemon>
2023-06-23 05:43:36,938 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52661 parent=52488 started daemon>
2023-06-23 05:43:36,938 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52657 parent=52488 started daemon>
2023-06-23 05:43:36,938 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52654 parent=52488 started daemon>
2023-06-23 05:43:37,390 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52672 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-06-23 05:43:45,333 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:45,338 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39619 instead
  warnings.warn(
2023-06-23 05:43:45,342 - distributed.scheduler - INFO - State start
2023-06-23 05:43:45,364 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:45,365 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:43:45,365 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:43:45,366 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:43:45,662 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46741'
2023-06-23 05:43:45,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42155'
2023-06-23 05:43:45,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37557'
2023-06-23 05:43:45,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39267'
2023-06-23 05:43:45,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39455'
2023-06-23 05:43:45,709 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45193'
2023-06-23 05:43:45,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46511'
2023-06-23 05:43:45,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38099'
2023-06-23 05:43:47,451 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7c3lmepg', purging
2023-06-23 05:43:47,451 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_ogt_q86', purging
2023-06-23 05:43:47,451 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y7pa5cja', purging
2023-06-23 05:43:47,452 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qhkv0z9q', purging
2023-06-23 05:43:47,452 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-akqeeb0o', purging
2023-06-23 05:43:47,452 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xfh1jr8u', purging
2023-06-23 05:43:47,452 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ikbpn90s', purging
2023-06-23 05:43:47,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:47,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:47,459 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:47,459 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:47,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:47,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:47,465 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:47,466 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:47,481 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:47,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:47,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:47,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:47,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:47,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:47,532 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:47,532 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:47,534 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:47,534 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:47,559 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:47,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:47,562 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:47,578 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:47,579 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:47,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:51,188 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35467
2023-06-23 05:43:51,188 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35467
2023-06-23 05:43:51,188 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35447
2023-06-23 05:43:51,188 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,188 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:51,188 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:51,188 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nmbslhhx
2023-06-23 05:43:51,189 - distributed.worker - INFO - Starting Worker plugin PreImport-ab9c8d4c-5dbe-4c33-ab1e-ad9ddddd95b4
2023-06-23 05:43:51,189 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2de1419-3980-4920-a599-c343da438449
2023-06-23 05:43:51,189 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9510162-19c9-40ae-9801-49016f86cb0e
2023-06-23 05:43:51,220 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36555
2023-06-23 05:43:51,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36555
2023-06-23 05:43:51,221 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42507
2023-06-23 05:43:51,221 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,221 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,221 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:51,221 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:51,221 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-60l6lfdl
2023-06-23 05:43:51,221 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ad6c952-76b4-46c2-b876-29024fea0d8f
2023-06-23 05:43:51,222 - distributed.worker - INFO - Starting Worker plugin PreImport-18bebb37-f69e-47f7-80e0-bf3922e2cf58
2023-06-23 05:43:51,222 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e93112b-71e6-420e-bf24-be4e6667f7cd
2023-06-23 05:43:51,302 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44063
2023-06-23 05:43:51,303 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44063
2023-06-23 05:43:51,303 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34871
2023-06-23 05:43:51,303 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,303 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,303 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:51,303 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:51,303 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sh_hwbqm
2023-06-23 05:43:51,303 - distributed.worker - INFO - Starting Worker plugin PreImport-8e6aed98-46c9-4bcc-9176-8d7964b8872c
2023-06-23 05:43:51,304 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1cf81c15-1b78-4d90-955b-5dba1406497c
2023-06-23 05:43:51,417 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44743
2023-06-23 05:43:51,417 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44743
2023-06-23 05:43:51,417 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45069
2023-06-23 05:43:51,417 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,417 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:51,417 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:51,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uukkwdwy
2023-06-23 05:43:51,418 - distributed.worker - INFO - Starting Worker plugin PreImport-181c123e-cc4a-4205-878e-4a91e2b37cd3
2023-06-23 05:43:51,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a34df25-483f-4904-8491-63a0d15b1508
2023-06-23 05:43:51,422 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ab844ec4-9ca7-4ad5-b324-bee8ae4bcc8b
2023-06-23 05:43:51,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37081
2023-06-23 05:43:51,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37081
2023-06-23 05:43:51,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40435
2023-06-23 05:43:51,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40263
2023-06-23 05:43:51,457 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40263
2023-06-23 05:43:51,457 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,457 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:51,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44381
2023-06-23 05:43:51,458 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:51,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,458 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c3yymc0z
2023-06-23 05:43:51,458 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,458 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:51,458 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:51,458 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l1ydcwo2
2023-06-23 05:43:51,458 - distributed.worker - INFO - Starting Worker plugin PreImport-7f81676c-c77c-4f70-96ba-d8f04eb2797e
2023-06-23 05:43:51,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8c57287-14f3-49c5-bbdd-f7b58393d9aa
2023-06-23 05:43:51,458 - distributed.worker - INFO - Starting Worker plugin PreImport-2f891e8b-fe3b-4955-b189-b3e4da5538a9
2023-06-23 05:43:51,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-033f9786-3b46-4952-8484-81268e9418b1
2023-06-23 05:43:51,465 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40085
2023-06-23 05:43:51,465 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40085
2023-06-23 05:43:51,466 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36819
2023-06-23 05:43:51,466 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,466 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,466 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:51,466 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:51,466 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d3r30f3g
2023-06-23 05:43:51,466 - distributed.worker - INFO - Starting Worker plugin PreImport-d69169e7-7017-4520-aa17-4aab507cf5fa
2023-06-23 05:43:51,467 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8fcde06-7d0f-4c86-9a31-cd295cafeccb
2023-06-23 05:43:51,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32815
2023-06-23 05:43:51,468 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32815
2023-06-23 05:43:51,468 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37873
2023-06-23 05:43:51,468 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,468 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,468 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:51,468 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:51,469 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tza6c7rw
2023-06-23 05:43:51,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-105ce6dd-08c3-48a6-8506-44003f8b8e53
2023-06-23 05:43:51,470 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90cd8002-ed7d-4dda-b4cc-a93ef03015ab
2023-06-23 05:43:51,608 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,612 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b5ac765-de24-478b-b492-c5fe6bcf07ae
2023-06-23 05:43:51,612 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b4ce8fd-cad4-4053-b129-b544a8afee85
2023-06-23 05:43:51,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ae3afe2-92d2-4fd6-8088-747421ae8340
2023-06-23 05:43:51,627 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,627 - distributed.worker - INFO - Starting Worker plugin PreImport-90bbb42d-b6f5-42e2-8b23-a6f9f4dab42d
2023-06-23 05:43:51,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7efec688-7a6b-4980-bace-1a640eea3ddc
2023-06-23 05:43:51,628 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,628 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,628 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,633 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,640 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,640 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:51,648 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,648 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:51,657 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,657 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:51,661 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,661 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:51,666 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,666 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:51,677 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,677 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:51,681 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:51,681 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:51,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 903, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-23 05:43:51,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36555. Reason: worker-close
2023-06-23 05:43:51,860 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:43:51,865 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 903, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:51,912 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 903, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:51,915 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39267'. Reason: nanny-instantiate-failed
2023-06-23 05:43:51,916 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:43:52,354 - distributed.nanny - INFO - Worker process 52940 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 903, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:43:52,359 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52952 parent=52762 started daemon>
2023-06-23 05:43:52,359 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52949 parent=52762 started daemon>
2023-06-23 05:43:52,359 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52946 parent=52762 started daemon>
2023-06-23 05:43:52,359 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52943 parent=52762 started daemon>
2023-06-23 05:43:52,360 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52935 parent=52762 started daemon>
2023-06-23 05:43:52,360 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52931 parent=52762 started daemon>
2023-06-23 05:43:52,360 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52928 parent=52762 started daemon>
2023-06-23 05:43:53,200 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52946 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-06-23 05:44:04,759 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:04,764 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43807 instead
  warnings.warn(
2023-06-23 05:44:04,768 - distributed.scheduler - INFO - State start
2023-06-23 05:44:04,790 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:04,791 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:04,791 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:04,792 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:44:04,896 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43561'
2023-06-23 05:44:06,677 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sh_hwbqm', purging
2023-06-23 05:44:06,678 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nmbslhhx', purging
2023-06-23 05:44:06,678 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d3r30f3g', purging
2023-06-23 05:44:06,679 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uukkwdwy', purging
2023-06-23 05:44:06,679 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l1ydcwo2', purging
2023-06-23 05:44:06,679 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tza6c7rw', purging
2023-06-23 05:44:06,679 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c3yymc0z', purging
2023-06-23 05:44:06,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:06,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
2023-06-23 05:44:07,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:08,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39921
2023-06-23 05:44:08,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39921
2023-06-23 05:44:08,531 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-06-23 05:44:08,531 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:08,531 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:08,531 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:08,531 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:44:08,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-31g2tey7
2023-06-23 05:44:08,532 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72383259-80fa-4ee5-9591-4b9eea5e0ed4
2023-06-23 05:44:08,532 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0da247e8-af90-4a9b-8233-e1eaf26df0a1
2023-06-23 05:44:08,532 - distributed.worker - INFO - Starting Worker plugin PreImport-f1e85f5a-ad15-4294-97f0-d41ddf7ffef3
2023-06-23 05:44:08,533 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:08,575 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:08,575 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:08,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:08,633 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:44:08,639 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43561'. Reason: nanny-close
2023-06-23 05:44:08,639 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:44:08,641 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39921. Reason: nanny-close
2023-06-23 05:44:08,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:44:08,645 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-06-23 05:44:13,791 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:13,796 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43047 instead
  warnings.warn(
2023-06-23 05:44:13,801 - distributed.scheduler - INFO - State start
2023-06-23 05:44:13,824 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:13,825 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:13,825 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:13,826 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:44:13,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33317'
2023-06-23 05:44:15,586 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:15,586 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
2023-06-23 05:44:16,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-23 05:44:16,791 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34599
2023-06-23 05:44:16,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34599
2023-06-23 05:44:16,792 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37081
2023-06-23 05:44:16,792 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:16,792 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:16,792 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:16,792 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:44:16,792 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xigh0g75
2023-06-23 05:44:16,792 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1fdc18c-fd30-405c-a9e3-056d1ee899c7
2023-06-23 05:44:16,793 - distributed.worker - INFO - Starting Worker plugin RMMSetup-404d3b70-189c-432b-964c-0c59be88fc1d
2023-06-23 05:44:16,793 - distributed.worker - INFO - Starting Worker plugin PreImport-3e76ad5a-bd96-4c96-b463-085762a56831
2023-06-23 05:44:16,794 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:18,136 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:18,137 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:18,138 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:18,208 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:44:18,210 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:44:18,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33317'. Reason: nanny-close
2023-06-23 05:44:18,215 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:44:18,216 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34599. Reason: nanny-close
2023-06-23 05:44:18,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:44:18,218 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-06-23 05:44:20,561 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:20,566 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43711 instead
  warnings.warn(
2023-06-23 05:44:20,570 - distributed.scheduler - INFO - State start
2023-06-23 05:44:20,591 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:20,591 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:20,592 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:20,592 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-06-23 05:44:25,739 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:25,743 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34021 instead
  warnings.warn(
2023-06-23 05:44:25,747 - distributed.scheduler - INFO - State start
2023-06-23 05:44:25,768 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:25,769 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-23 05:44:25,769 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34021/status
2023-06-23 05:44:26,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39635'
2023-06-23 05:44:26,831 - distributed.scheduler - INFO - Receive client connection: Client-024c7206-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:26,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55316
2023-06-23 05:44:27,876 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:27,876 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:27,884 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-23 05:44:28,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36219
2023-06-23 05:44:28,345 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36219
2023-06-23 05:44:28,345 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46619
2023-06-23 05:44:28,345 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:44:28,345 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:28,345 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:28,345 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:44:28,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1s9tjqqb
2023-06-23 05:44:28,345 - distributed.worker - INFO - Starting Worker plugin PreImport-6e49a2dc-f144-412b-8a85-01b4fccbdffe
2023-06-23 05:44:28,345 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb8d15cc-2106-450a-a196-b56e23c39e8e
2023-06-23 05:44:28,346 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-299d1b71-9465-4b18-af1d-ec095604db91
2023-06-23 05:44:28,346 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:28,361 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36219', status: init, memory: 0, processing: 0>
2023-06-23 05:44:28,362 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36219
2023-06-23 05:44:28,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55330
2023-06-23 05:44:28,363 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:44:28,363 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:28,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:44:28,383 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:44:28,386 - distributed.scheduler - INFO - Remove client Client-024c7206-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:28,387 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55316; closing.
2023-06-23 05:44:28,387 - distributed.scheduler - INFO - Remove client Client-024c7206-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:28,387 - distributed.scheduler - INFO - Close client connection: Client-024c7206-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:28,388 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39635'. Reason: nanny-close
2023-06-23 05:44:28,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:44:28,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36219. Reason: nanny-close
2023-06-23 05:44:28,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:44:28,396 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55330; closing.
2023-06-23 05:44:28,397 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36219', status: closing, memory: 0, processing: 0>
2023-06-23 05:44:28,397 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36219
2023-06-23 05:44:28,397 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:28,398 - distributed.nanny - INFO - Worker closed
2023-06-23 05:44:29,004 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:44:29,004 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:29,005 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:29,005 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-23 05:44:29,006 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-06-23 05:44:31,310 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:31,315 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37003 instead
  warnings.warn(
2023-06-23 05:44:31,319 - distributed.scheduler - INFO - State start
2023-06-23 05:44:31,340 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:31,342 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:44:31,342 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37003/status
2023-06-23 05:44:31,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44315'
2023-06-23 05:44:31,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44105'
2023-06-23 05:44:31,686 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40245'
2023-06-23 05:44:31,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40911'
2023-06-23 05:44:31,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33253'
2023-06-23 05:44:31,722 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40403'
2023-06-23 05:44:31,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38953'
2023-06-23 05:44:31,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41519'
2023-06-23 05:44:32,906 - distributed.scheduler - INFO - Receive client connection: Client-05797499-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:32,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36034
2023-06-23 05:44:33,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:33,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:33,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:33,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:33,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:33,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:33,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:33,454 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:33,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:33,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:33,455 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:33,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:33,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:33,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:33,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:33,496 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:33,497 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:33,503 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:33,507 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:33,507 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:33,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:33,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:33,579 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:33,579 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:36,194 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40937
2023-06-23 05:44:36,195 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40937
2023-06-23 05:44:36,195 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34589
2023-06-23 05:44:36,195 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:36,195 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:36,195 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:36,195 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:36,195 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wshy4ev_
2023-06-23 05:44:36,196 - distributed.worker - INFO - Starting Worker plugin PreImport-9aa824a5-eac6-45fa-91d8-7a137da9cdd2
2023-06-23 05:44:36,197 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd0aaf43-1bb8-404b-8830-bd6f46f1f949
2023-06-23 05:44:36,197 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0135a36f-c2a7-441b-8500-14ae6c330d09
2023-06-23 05:44:36,632 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:36,670 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40937', status: init, memory: 0, processing: 0>
2023-06-23 05:44:36,671 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40937
2023-06-23 05:44:36,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36060
2023-06-23 05:44:36,672 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:36,672 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:36,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-23 05:44:36,833 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41491
2023-06-23 05:44:36,833 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41491
2023-06-23 05:44:36,833 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34727
2023-06-23 05:44:36,833 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:36,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:36,833 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:36,833 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:36,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s4vaxduy
2023-06-23 05:44:36,834 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea1335a8-9d3f-4f6d-9006-bc27b03438c0
2023-06-23 05:44:36,834 - distributed.worker - INFO - Starting Worker plugin PreImport-330efce6-9456-4e43-810a-40b21d5046c0
2023-06-23 05:44:36,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20b7b062-217f-4a24-89ec-a0a489d24636
2023-06-23 05:44:36,842 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38511
2023-06-23 05:44:36,843 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38511
2023-06-23 05:44:36,843 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42469
2023-06-23 05:44:36,843 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:36,843 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:36,843 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:36,843 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:36,843 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-njf9ao7k
2023-06-23 05:44:36,843 - distributed.worker - INFO - Starting Worker plugin PreImport-7b6f5d08-a10f-4a25-8f1c-bb4915e4a83a
2023-06-23 05:44:36,844 - distributed.worker - INFO - Starting Worker plugin RMMSetup-240b46f0-6295-4e14-abdd-1776c97dcf72
2023-06-23 05:44:37,177 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-425c556b-1781-4889-8a08-839c5ec6d126
2023-06-23 05:44:37,182 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,222 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38511', status: init, memory: 0, processing: 0>
2023-06-23 05:44:37,223 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38511
2023-06-23 05:44:37,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36070
2023-06-23 05:44:37,224 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,224 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:37,306 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38517
2023-06-23 05:44:37,306 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38517
2023-06-23 05:44:37,306 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41641
2023-06-23 05:44:37,306 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,307 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,307 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:37,307 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:37,307 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rn8gz3u2
2023-06-23 05:44:37,307 - distributed.worker - INFO - Starting Worker plugin PreImport-8d0eb367-0682-4911-a07f-695092b226ca
2023-06-23 05:44:37,307 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94b07ea5-f683-48ce-b197-ab3bdf32018a
2023-06-23 05:44:37,317 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33009
2023-06-23 05:44:37,317 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33009
2023-06-23 05:44:37,318 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34627
2023-06-23 05:44:37,318 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,318 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,318 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:37,318 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:37,318 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-43bw10b6
2023-06-23 05:44:37,319 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e027c83-d99c-4085-9cbd-548d42b554ed
2023-06-23 05:44:37,319 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4dca68b-a852-49b1-8ef6-faaa78ab2e3b
2023-06-23 05:44:37,327 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42605
2023-06-23 05:44:37,327 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42605
2023-06-23 05:44:37,327 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46339
2023-06-23 05:44:37,328 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,328 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,328 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:37,328 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:37,328 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9lgpwilq
2023-06-23 05:44:37,328 - distributed.worker - INFO - Starting Worker plugin PreImport-441bf60a-7a5e-4857-95fb-e4858bb32abb
2023-06-23 05:44:37,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3c19008-737a-405c-a50e-b31964e3ae82
2023-06-23 05:44:37,328 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c57170bc-5779-4e54-a729-14b58cc2c17d
2023-06-23 05:44:37,330 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36371
2023-06-23 05:44:37,330 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36371
2023-06-23 05:44:37,331 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39937
2023-06-23 05:44:37,331 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,331 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,331 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:37,331 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:37,331 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-km1d6_xv
2023-06-23 05:44:37,332 - distributed.worker - INFO - Starting Worker plugin PreImport-fa18b524-d226-40b2-b13d-898fac3ffc37
2023-06-23 05:44:37,332 - distributed.worker - INFO - Starting Worker plugin RMMSetup-673ae4f8-c14c-4aad-9230-437d6a92a2b4
2023-06-23 05:44:37,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44873
2023-06-23 05:44:37,374 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44873
2023-06-23 05:44:37,374 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33811
2023-06-23 05:44:37,374 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,374 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,374 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:37,374 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:37,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f_ro_n04
2023-06-23 05:44:37,374 - distributed.worker - INFO - Starting Worker plugin PreImport-d291b349-0d02-49bf-a925-0a3084a92eb6
2023-06-23 05:44:37,375 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6e1d370-4be5-4f57-815f-8cd3e5a4a44f
2023-06-23 05:44:37,543 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-132108e5-0644-4909-9f7f-97b6a60945ed
2023-06-23 05:44:37,544 - distributed.worker - INFO - Starting Worker plugin PreImport-25e97799-54b4-43bd-a506-d21c3e847492
2023-06-23 05:44:37,544 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,544 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be857bf1-5cc7-4482-8f1c-e5e1dd788400
2023-06-23 05:44:37,550 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,550 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,576 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33009', status: init, memory: 0, processing: 0>
2023-06-23 05:44:37,576 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33009
2023-06-23 05:44:37,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36084
2023-06-23 05:44:37,577 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,577 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:37,583 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbfa0c28-102f-41e5-9c02-006ac6d8cf05
2023-06-23 05:44:37,584 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42605', status: init, memory: 0, processing: 0>
2023-06-23 05:44:37,586 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42605
2023-06-23 05:44:37,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36100
2023-06-23 05:44:37,587 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,587 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,587 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36371', status: init, memory: 0, processing: 0>
2023-06-23 05:44:37,588 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36371
2023-06-23 05:44:37,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36114
2023-06-23 05:44:37,588 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,588 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38517', status: init, memory: 0, processing: 0>
2023-06-23 05:44:37,588 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,589 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38517
2023-06-23 05:44:37,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36098
2023-06-23 05:44:37,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:37,590 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,590 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:37,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:37,775 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44873', status: init, memory: 0, processing: 0>
2023-06-23 05:44:37,775 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44873
2023-06-23 05:44:37,776 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36120
2023-06-23 05:44:37,776 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:37,776 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:37,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-23 05:44:38,126 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41491. Reason: worker-close
2023-06-23 05:44:38,126 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:44:38,128 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:44:38,131 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:44:38,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44315'. Reason: nanny-instantiate-failed
2023-06-23 05:44:38,135 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:44:38,155 - distributed.nanny - INFO - Worker process 54008 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:44:38,160 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54032 parent=53842 started daemon>
2023-06-23 05:44:38,160 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54029 parent=53842 started daemon>
2023-06-23 05:44:38,160 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54026 parent=53842 started daemon>
2023-06-23 05:44:38,161 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54023 parent=53842 started daemon>
2023-06-23 05:44:38,161 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54019 parent=53842 started daemon>
2023-06-23 05:44:38,161 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54015 parent=53842 started daemon>
2023-06-23 05:44:38,161 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54011 parent=53842 started daemon>
2023-06-23 05:44:38,200 - distributed.core - INFO - Connection to tcp://127.0.0.1:36084 has been closed.
2023-06-23 05:44:38,201 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33009', status: running, memory: 0, processing: 0>
2023-06-23 05:44:38,201 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33009
2023-06-23 05:44:38,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:36100 has been closed.
2023-06-23 05:44:38,203 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42605', status: running, memory: 0, processing: 0>
2023-06-23 05:44:38,203 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42605
2023-06-23 05:44:38,204 - distributed.core - INFO - Connection to tcp://127.0.0.1:36120 has been closed.
2023-06-23 05:44:38,204 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44873', status: running, memory: 0, processing: 0>
2023-06-23 05:44:38,204 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44873
2023-06-23 05:44:38,205 - distributed.core - INFO - Connection to tcp://127.0.0.1:36114 has been closed.
2023-06-23 05:44:38,205 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36371', status: running, memory: 0, processing: 0>
2023-06-23 05:44:38,205 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36371
2023-06-23 05:44:38,205 - distributed.core - INFO - Connection to tcp://127.0.0.1:36060 has been closed.
2023-06-23 05:44:38,205 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40937', status: running, memory: 0, processing: 0>
2023-06-23 05:44:38,206 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40937
2023-06-23 05:44:38,206 - distributed.core - INFO - Connection to tcp://127.0.0.1:36070 has been closed.
2023-06-23 05:44:38,206 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38511', status: running, memory: 0, processing: 0>
2023-06-23 05:44:38,206 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38511
2023-06-23 05:44:38,211 - distributed.core - INFO - Connection to tcp://127.0.0.1:36098 has been closed.
2023-06-23 05:44:38,211 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38517', status: running, memory: 0, processing: 0>
2023-06-23 05:44:38,211 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38517
2023-06-23 05:44:38,211 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:38,959 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 54019 exit status was already read will report exitcode 255
2023-06-23 05:44:38,990 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 54026 exit status was already read will report exitcode 255
2023-06-23 05:44:43,234 - distributed.scheduler - INFO - Receive client connection: Client-0da6a0a4-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:43,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35034
2023-06-23 05:44:48,956 - distributed.scheduler - INFO - Remove client Client-05797499-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:48,956 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36034; closing.
2023-06-23 05:44:48,956 - distributed.scheduler - INFO - Remove client Client-05797499-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:48,957 - distributed.scheduler - INFO - Close client connection: Client-05797499-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:48,958 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:44:48,958 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:48,958 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:48,960 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:44:48,961 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-06-23 05:44:51,298 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:51,303 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38633 instead
  warnings.warn(
2023-06-23 05:44:51,307 - distributed.scheduler - INFO - State start
2023-06-23 05:44:51,335 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:51,336 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:44:51,337 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38633/status
2023-06-23 05:44:51,362 - distributed.scheduler - INFO - Receive client connection: Client-0da6a0a4-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:51,375 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36836
2023-06-23 05:44:51,384 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40437', status: init, memory: 0, processing: 0>
2023-06-23 05:44:51,385 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40437
2023-06-23 05:44:51,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36830
2023-06-23 05:44:51,400 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35963', status: init, memory: 0, processing: 0>
2023-06-23 05:44:51,401 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35963
2023-06-23 05:44:51,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36874
2023-06-23 05:44:51,406 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33413', status: init, memory: 0, processing: 0>
2023-06-23 05:44:51,407 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33413
2023-06-23 05:44:51,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36862
2023-06-23 05:44:51,409 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45839', status: init, memory: 0, processing: 0>
2023-06-23 05:44:51,409 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45839
2023-06-23 05:44:51,409 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36878
2023-06-23 05:44:51,422 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41875', status: init, memory: 0, processing: 0>
2023-06-23 05:44:51,423 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41875
2023-06-23 05:44:51,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36890
2023-06-23 05:44:51,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41051'
2023-06-23 05:44:51,491 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36215', status: init, memory: 0, processing: 0>
2023-06-23 05:44:51,492 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36215
2023-06-23 05:44:51,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36906
2023-06-23 05:44:51,695 - distributed.core - INFO - Connection to tcp://127.0.0.1:36874 has been closed.
2023-06-23 05:44:51,695 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35963', status: running, memory: 0, processing: 0>
2023-06-23 05:44:51,695 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35963
2023-06-23 05:44:51,697 - distributed.core - INFO - Connection to tcp://127.0.0.1:36890 has been closed.
2023-06-23 05:44:51,697 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41875', status: running, memory: 0, processing: 0>
2023-06-23 05:44:51,697 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41875
2023-06-23 05:44:51,698 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36890>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36890>: Stream is closed
2023-06-23 05:44:51,701 - distributed.core - INFO - Connection to tcp://127.0.0.1:36878 has been closed.
2023-06-23 05:44:51,701 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45839', status: running, memory: 0, processing: 0>
2023-06-23 05:44:51,701 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45839
2023-06-23 05:44:51,701 - distributed.core - INFO - Connection to tcp://127.0.0.1:36906 has been closed.
2023-06-23 05:44:51,702 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36215', status: running, memory: 0, processing: 0>
2023-06-23 05:44:51,702 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36215
2023-06-23 05:44:51,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:36862 has been closed.
2023-06-23 05:44:51,702 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33413', status: running, memory: 0, processing: 0>
2023-06-23 05:44:51,702 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33413
2023-06-23 05:44:51,703 - distributed.core - INFO - Connection to tcp://127.0.0.1:36830 has been closed.
2023-06-23 05:44:51,703 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40437', status: running, memory: 0, processing: 0>
2023-06-23 05:44:51,703 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40437
2023-06-23 05:44:51,703 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:52,091 - distributed.scheduler - INFO - Receive client connection: Client-115acd10-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:52,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36924
2023-06-23 05:44:53,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-km1d6_xv', purging
2023-06-23 05:44:53,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-43bw10b6', purging
2023-06-23 05:44:53,244 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rn8gz3u2', purging
2023-06-23 05:44:53,244 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f_ro_n04', purging
2023-06-23 05:44:53,245 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wshy4ev_', purging
2023-06-23 05:44:53,245 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9lgpwilq', purging
2023-06-23 05:44:53,245 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-njf9ao7k', purging
2023-06-23 05:44:53,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:53,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:53,272 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-23 05:44:53,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39199
2023-06-23 05:44:53,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39199
2023-06-23 05:44:53,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46083
2023-06-23 05:44:53,764 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:53,764 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:53,764 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:53,764 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:44:53,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ql88t43o
2023-06-23 05:44:53,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d2cd262-1695-412b-88d6-2ead1a929320
2023-06-23 05:44:53,764 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74165a69-411f-488c-b55b-609522e3f619
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-23 05:44:54,693 - distributed.worker - INFO - Starting Worker plugin PreImport-b677d87f-8484-4422-bb5f-1e085fff6bfb
2023-06-23 05:44:54,693 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39199. Reason: worker-close
2023-06-23 05:44:54,693 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:44:54,696 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:44:54,726 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:44:54,730 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41051'. Reason: nanny-instantiate-failed
2023-06-23 05:44:54,730 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:44:54,749 - distributed.nanny - INFO - Worker process 54285 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:44:59,311 - distributed.scheduler - INFO - Remove client Client-0da6a0a4-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:59,311 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36836; closing.
2023-06-23 05:44:59,311 - distributed.scheduler - INFO - Remove client Client-0da6a0a4-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:59,312 - distributed.scheduler - INFO - Close client connection: Client-0da6a0a4-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:59,483 - distributed.scheduler - INFO - Receive client connection: Client-1755e5ab-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:59,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36950
2023-06-23 05:45:02,187 - distributed.scheduler - INFO - Remove client Client-115acd10-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:45:02,188 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36924; closing.
2023-06-23 05:45:02,188 - distributed.scheduler - INFO - Remove client Client-115acd10-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:45:02,189 - distributed.scheduler - INFO - Close client connection: Client-115acd10-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:45:02,189 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:45:02,190 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:45:02,191 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:45:02,192 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:45:02,193 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-06-23 05:45:04,544 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:04,548 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36613 instead
  warnings.warn(
2023-06-23 05:45:04,553 - distributed.scheduler - INFO - State start
2023-06-23 05:45:04,574 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:04,575 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:45:04,576 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36613/status
2023-06-23 05:45:04,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38869', status: init, memory: 0, processing: 0>
2023-06-23 05:45:04,708 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38869
2023-06-23 05:45:04,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48060
2023-06-23 05:45:04,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40205'
2023-06-23 05:45:05,224 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:45:05,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:45:05,337 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48060; closing.
2023-06-23 05:45:05,338 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38869', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:05,338 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38869
2023-06-23 05:45:05,338 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:45:05,703 - distributed.scheduler - INFO - Receive client connection: Client-194277cd-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:45:05,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48078
2023-06-23 05:45:06,602 - distributed.scheduler - INFO - Receive client connection: Client-1b944583-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:45:06,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48094
2023-06-23 05:45:06,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:06,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:06,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:07,761 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45379
2023-06-23 05:45:07,761 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45379
2023-06-23 05:45:07,761 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41725
2023-06-23 05:45:07,761 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:07,761 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:07,762 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:07,762 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:45:07,762 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pxy_701b
2023-06-23 05:45:07,762 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8610706-61d3-4f17-9cd3-ad5642d755b0
2023-06-23 05:45:07,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d6ac794-780e-4329-b40c-e6d8c6cf9d8d
2023-06-23 05:45:07,892 - distributed.worker - INFO - Starting Worker plugin PreImport-0f783fe9-f7c0-4c01-afb7-d34c7cdf48f9
2023-06-23 05:45:07,892 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:07,919 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45379', status: init, memory: 0, processing: 0>
2023-06-23 05:45:07,920 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45379
2023-06-23 05:45:07,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48106
2023-06-23 05:45:07,921 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:07,921 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:07,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:07,937 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-06-23 05:45:07,942 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:07,945 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:45:07,947 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:45:07,949 - distributed.scheduler - INFO - Remove client Client-1b944583-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:45:07,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48094; closing.
2023-06-23 05:45:07,949 - distributed.scheduler - INFO - Remove client Client-1b944583-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:45:07,950 - distributed.scheduler - INFO - Close client connection: Client-1b944583-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:45:07,953 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-06-23 05:45:07,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:07,962 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:45:07,963 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:45:07,965 - distributed.scheduler - INFO - Remove client Client-194277cd-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:45:07,965 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48078; closing.
2023-06-23 05:45:07,966 - distributed.scheduler - INFO - Remove client Client-194277cd-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:45:07,966 - distributed.scheduler - INFO - Close client connection: Client-194277cd-1189-11ee-88ff-d8c49764f6bb
2023-06-23 05:45:07,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40205'. Reason: nanny-close
2023-06-23 05:45:07,967 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:07,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45379. Reason: nanny-close
2023-06-23 05:45:07,970 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48106; closing.
2023-06-23 05:45:07,970 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:07,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45379', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:07,970 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45379
2023-06-23 05:45:07,971 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:45:07,971 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:09,334 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:45:09,335 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:45:09,335 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:45:09,336 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:45:09,336 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36189 instead
  warnings.warn(
2023-06-23 05:45:20,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:20,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:20,808 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:20,808 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:20,812 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:20,812 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:20,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:20,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:20,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:20,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:20,886 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:20,886 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:20,899 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:20,899 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:20,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:20,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44469 instead
  warnings.warn(
2023-06-23 05:45:39,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39341 instead
  warnings.warn(
2023-06-23 05:45:57,443 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,443 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,468 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,468 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,496 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,496 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,498 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,498 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,535 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,535 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39545 instead
  warnings.warn(
2023-06-23 05:46:16,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:16,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:16,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:16,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:16,619 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:16,619 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:16,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:16,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:16,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:16,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:16,674 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:16,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:16,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:16,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:16,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:16,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40961 instead
  warnings.warn(
2023-06-23 05:46:34,180 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:34,180 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:34,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:34,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:34,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:34,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:34,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:34,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:34,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:34,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:34,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:34,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:34,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:34,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:34,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:34,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33935 instead
  warnings.warn(
2023-06-23 05:46:50,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:50,392 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:50,410 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:50,410 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:50,417 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:50,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:50,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:50,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:50,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:50,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:50,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:50,492 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:50,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:50,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:50,537 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:50,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42239 instead
  warnings.warn(
2023-06-23 05:47:08,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:08,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:08,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:08,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:08,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:08,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:08,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:08,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:08,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:08,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:08,349 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:08,349 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:08,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:08,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:08,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:08,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33995 instead
  warnings.warn(
2023-06-23 05:47:28,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,652 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,652 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,671 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,760 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,760 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34491 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33231 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38585 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40193 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45969 instead
  warnings.warn(
2023-06-23 05:48:46,907 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:37760 remote=tcp://127.0.0.1:35071>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36947 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35737 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36151 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46227 instead
  warnings.warn(
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-23 05:50:13,333 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-23 05:50:13,344 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-3f830438-99f6-4986-9356-847672aad6a1
Function:  _run_coroutine_on_worker
args:      (8216704394514027994713442026949565737, <function shuffle_task at 0x7f2bbc7a31f0>, ('explicit-comms-shuffle-f3f0080723666d0e98f2f56f733557aa', {0: set(), 1: {"('from_pandas-4344f5ab5f7e4a38e40d9a5e19817a11', 0)"}, 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 1))
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

Process SpawnProcess-17:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 153, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 314, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 599, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3186, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2345, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 349, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 416, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 389, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2208, in _gather
    raise exception.with_traceback(traceback)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 102, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 446, in result
    return self.__get_result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 99, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 446, in result
    return self.__get_result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 379, in shuffle_task
    await send_recv_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 296, in send_recv_partitions
    await asyncio.gather(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 124, in recv
    await asyncio.gather(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 120, in read_msg
    msg: Dict[int, DataFrame] = nested_deserialize(await eps[rank].read())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40353 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43073 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38141 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39375 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37189 instead
  warnings.warn(
2023-06-23 05:51:35,352 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-4e48f81f-13ab-4920-94f6-c2a8b1683eee
Function:  _run_coroutine_on_worker
args:      (83695014187855011416519932332829455620, <function shuffle_task at 0x7f96a9070820>, ('explicit-comms-shuffle-0d1d63bcf36ceb180d659e88cfa52e52', {0: {"('from_pandas-6ab0785600a3c24d7d2fb35d851d20e7', 0)"}, 1: set()}, {0: {0}, 1: set()}, ['key'], 1, False, 1, 1))
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 12 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
