/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45469 instead
  warnings.warn(
[dgx13:93517:0:93517] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93517) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f5199e5107d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f5199e51274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f5199e5143a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f51d6b5e420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f5199ed06b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f5199ef9839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f5199e0b3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f5199e0e838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f5199e5a4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f5199e0d5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f5199ecd8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f5199f8606a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5578383273f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x557838321fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x557838333469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5578383234e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5578383d66d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f51c113a1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55783832b6ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x5578382e63ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55783832a723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x557838328929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x557838333712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5578383234e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x557838333712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5578383234e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x557838333712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5578383234e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x557838333712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5578383234e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x557838321fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x557838333469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x557838324042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x557838321fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x5578383408cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55783834104c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55783840480e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55783832b6ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5578383273f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x557838333712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x5578383409ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5578383273f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x557838333712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5578383234e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x557838321fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x557838333469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5578383234e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x557838333712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x557838323232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x557838321fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x557838333469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x557838324042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x557838321fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x557838321c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x557838321c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5578383cf2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x5578383fc6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x5578383f8a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5578383f087a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5578383f076c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5578383ef9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5578383c3107]
=================================
[dgx13:93538:0:93538] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93538) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fc96cfc507d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fc96cfc5274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fc96cfc543a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fc9a9cb4420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fc96d0446b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fc96d06d839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fc96cf7f3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fc96cf82838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fc96cfce4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fc96cf815dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fc96d0418da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fc96d0fa06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x56131adb63f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x56131adb0fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x56131adc2469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x56131adb24e6]
16  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x56131adc2712]
17  /opt/conda/envs/gdf/bin/python(+0x14ca83) [0x56131adcfa83]
18  /opt/conda/envs/gdf/bin/python(+0x25819c) [0x56131aedb19c]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x56131ad753ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x56131adb9723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x56131adb7929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x56131adc2712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x56131adb24e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x56131adc2712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x56131adb24e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x56131adc2712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x56131adb24e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x56131adc2712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x56131adb24e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x56131adb0fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x56131adc2469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x56131adb3042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x56131adb0fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x56131adcf8cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x56131add004c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x56131ae9380e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x56131adba6ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x56131adb63f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x56131adc2712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x56131adcf9ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x56131adb63f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x56131adc2712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x56131adb24e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x56131adb0fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x56131adc2469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x56131adb24e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x56131adc2712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x56131adb2232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x56131adb0fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x56131adc2469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x56131adb3042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x56131adb0fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x56131adb0c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x56131adb0c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x56131ae5e2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x56131ae8b6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x56131ae87a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x56131ae7f87a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x56131ae7f76c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x56131ae7e9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x56131ae52107]
=================================
[dgx13:93523:0:93523] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93523) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fcbecbbb07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fcbecbbb274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fcbecbbb43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fcc258c0420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fcbecc3a6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fcbecc63839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fcbecb753df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fcbecb78838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fcbecbc44a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fcbecb775dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fcbecc378da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fcbeccf006a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55bb259a53f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55bb2599ffb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55bb259b1469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55bb259a14e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55bb25a546d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fcc18ee31e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55bb259a96ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55bb259643ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55bb259a8723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55bb259a6929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55bb259b1712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55bb259a14e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55bb259b1712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55bb259a14e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55bb259b1712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55bb259a14e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55bb259b1712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55bb259a14e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55bb2599ffb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55bb259b1469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55bb259a2042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55bb2599ffb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55bb259be8cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55bb259bf04c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55bb25a8280e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55bb259a96ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55bb259a53f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55bb259b1712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55bb259be9ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55bb259a53f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55bb259b1712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55bb259a14e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55bb2599ffb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55bb259b1469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55bb259a14e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55bb259b1712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55bb259a1232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55bb2599ffb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55bb259b1469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55bb259a2042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55bb2599ffb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55bb2599fc88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55bb2599fc39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55bb25a4d2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x55bb25a7a6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x55bb25a76a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55bb25a6e87a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55bb25a6e76c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55bb25a6d9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55bb25a41107]
=================================
[dgx13:93513:0:93513] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93513) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f9e402ef07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f9e402ef274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f9e402ef43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f9e6afc7420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f9e4036e6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f9e40397839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f9e402a93df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f9e402ac838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f9e402f84a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f9e402ab5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f9e4036b8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f9e4042406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x556bdf3ff3f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bdf3f9fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556bdf40b469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bdf3fb4e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x556bdf4ae6d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f9e5559f1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x556bdf4036ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x556bdf3be3ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x556bdf402723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x556bdf400929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bdf40b712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bdf3fb4e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bdf40b712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bdf3fb4e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bdf40b712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bdf3fb4e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bdf40b712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bdf3fb4e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bdf3f9fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556bdf40b469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x556bdf3fc042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bdf3f9fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x556bdf4188cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x556bdf41904c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x556bdf4dc80e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x556bdf4036ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x556bdf3ff3f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bdf40b712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x556bdf4189ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x556bdf3ff3f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bdf40b712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bdf3fb4e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bdf3f9fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556bdf40b469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bdf3fb4e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bdf40b712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x556bdf3fb232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bdf3f9fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556bdf40b469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x556bdf3fc042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bdf3f9fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x556bdf3f9c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556bdf3f9c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x556bdf4a72cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x556bdf4d46ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x556bdf4d0a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x556bdf4c887a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x556bdf4c876c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x556bdf4c79a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x556bdf49b107]
=================================
[dgx13:93507:0:93507] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93507) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f2c50ec807d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f2c50ec8274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f2c50ec843a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f2c7bbae420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f2c50f476b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f2c50f70839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f2c50e823df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f2c50e85838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f2c50ed14a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f2c50e845dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f2c50f448da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f2c50ffd06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55d3baa883f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d3baa82fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d3baa94469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d3baa844e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55d3bab376d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f2c661841e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55d3baa8c6ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55d3baa473ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55d3baa8b723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55d3baa89929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d3baa94712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d3baa844e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d3baa94712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d3baa844e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d3baa94712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d3baa844e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d3baa94712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d3baa844e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d3baa82fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d3baa94469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55d3baa85042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d3baa82fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55d3baaa18cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55d3baaa204c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55d3bab6580e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55d3baa8c6ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55d3baa883f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d3baa94712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55d3baaa19ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55d3baa883f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d3baa94712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d3baa844e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d3baa82fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d3baa94469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55d3baa844e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55d3baa94712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55d3baa84232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d3baa82fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55d3baa94469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55d3baa85042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55d3baa82fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55d3baa82c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55d3baa82c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55d3bab302cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x55d3bab5d6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x55d3bab59a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55d3bab5187a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55d3bab5176c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55d3bab509a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55d3bab24107]
=================================
2024-02-05 07:09:12,983 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51779 -> ucx://127.0.0.1:35161
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f79911c8400, tag: 0x907d64e576618acb, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-05 07:09:12,983 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:41443 -> ucx://127.0.0.1:35161
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fb70c0a4400, tag: 0x521ff9e90fb4a701, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-05 07:09:13,954 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56523
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f79911c81c0, tag: 0xa4d58ef12066db46, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f79911c81c0, tag: 0xa4d58ef12066db46, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-05 07:09:13,954 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56523
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fb70c0a4240, tag: 0x71f5480af062a3df, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fb70c0a4240, tag: 0x71f5480af062a3df, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-05 07:09:13,954 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56523
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7fa2b91a4180, tag: 0x59c4a971e1208710, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7fa2b91a4180, tag: 0x59c4a971e1208710, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-02-05 07:09:14,025 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41931
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #010] ep: 0x7fa2b91a4100, tag: 0x7bfd6bd2b4c4cbe6, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #010] ep: 0x7fa2b91a4100, tag: 0x7bfd6bd2b4c4cbe6, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-02-05 07:09:14,026 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41931
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #011] ep: 0x7f79911c8100, tag: 0x24c6dbfc7f87b63b, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #011] ep: 0x7f79911c8100, tag: 0x24c6dbfc7f87b63b, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-05 07:09:14,036 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41931
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #011] ep: 0x7fb70c0a4140, tag: 0x57248af8a43eba1a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #011] ep: 0x7fb70c0a4140, tag: 0x57248af8a43eba1a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-05 07:09:14,242 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:50765 -> ucx://127.0.0.1:54615
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa2b91a4340, tag: 0x1e4695c48a5a0b06, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
Task exception was never retrieved
future: <Task finished name='Task-1321' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2024-02-05 07:09:14,243 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54615
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f79911c8180, tag: 0x3e3a8a01236f6de, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f79911c8180, tag: 0x3e3a8a01236f6de, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-05 07:09:14,243 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54615
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fa2b91a4240, tag: 0x1cf3fb69b5263292, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fa2b91a4240, tag: 0x1cf3fb69b5263292, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-05 07:09:14,243 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54615
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fb70c0a4180, tag: 0xbfe45b5aba76ab2a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fb70c0a4180, tag: 0xbfe45b5aba76ab2a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-05 07:09:14,323 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:50765 -> ucx://127.0.0.1:51819
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa2b91a43c0, tag: 0x8cef9a2093854475, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-05 07:09:14,324 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51819
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fa2b91a41c0, tag: 0x918c563654bc29c2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fa2b91a41c0, tag: 0x918c563654bc29c2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-05 07:09:14,324 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51779 -> ucx://127.0.0.1:51819
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f79911c83c0, tag: 0x4da99561edbf91c4, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-05 07:09:14,377 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51819
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f79911c8240, tag: 0x5aeb58a67163dcfb, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f79911c8240, tag: 0x5aeb58a67163dcfb, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-05 07:09:14,365 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51819
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7fb70c0a4280, tag: 0x46d67456e403d902, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7fb70c0a4280, tag: 0x46d67456e403d902, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-02-05 07:09:16,180 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-05 07:09:16,180 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-05 07:09:16,384 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-05 07:09:16,385 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-05 07:09:16,395 - distributed.worker - ERROR - tuple indices must be integers or slices, not str
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2869, in get_data_from_worker
    status = response["status"]
TypeError: tuple indices must be integers or slices, not str
2024-02-05 07:09:16,402 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51779
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 360, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #088] ep: 0x7fa2b91a4140, tag: 0x96fd236735e132ed, nbytes: 3336, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #088] ep: 0x7fa2b91a4140, tag: 0x96fd236735e132ed, nbytes: 3336, type: <class 'numpy.ndarray'>>: Message truncated")
2024-02-05 07:09:16,424 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-54efe4e1c1ff02b405ca9dd1c57e206a', 0)
Function:  shuffle_group
args:      (               key   payload  _partitions
shuffle                                  
0           195650  92887283            4
0           195656  52713647            3
0           195658   8971647            3
0           195660  62094578            7
0            31910  63069741            0
...            ...       ...          ...
0        799995569   1719044            2
0        799995573  31461447            1
0        799999555  85463247            1
0        799999570  27477633            1
0        799999579  31335435            3

[100000000 rows x 3 columns], '_partitions', 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-05 07:09:16,430 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-05 07:09:16,430 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-05 07:09:16,728 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 3)
Function:  _concat
args:      ([                key   payload  _partitions
102980     12479344  21546832            3
103000    100035110  94572799            3
103005    800070719  15573826            3
103006    510008964  83296288            3
2755      806376872  23934316            3
...             ...       ...          ...
99960580  850101809  48210854            3
99960587  838636136  29832089            3
99960591  807362894  23843272            3
99960595  208372759  42926606            3
99960599  709833997  27659063            3

[12504123 rows x 3 columns],                 key   payload  _partitions
134374    919835909  57140261            3
134378    934129136   5537654            3
134383    933645491  71299023            3
102915    522168978   7280454            3
134385     15934770  45088444            3
...             ...       ...          ...
99982298  905785654  64957006            3
99982299  219181708   8240895            3
99982249  618115613  28690055            3
99982255  315931562  9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-05 07:09:17,547 - distributed.worker - ERROR - 'data'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2075, in gather_dep
    data=response["data"],
KeyError: 'data'
2024-02-05 07:09:17,552 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41443
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #137] ep: 0x7f79911c8200, tag: 0x5fed55b3fc9f4bae, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #137] ep: 0x7f79911c8200, tag: 0x5fed55b3fc9f4bae, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated")
2024-02-05 07:09:17,558 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41443
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #151] ep: 0x7fa2b91a4200, tag: 0x55c87d436596f404, nbytes: 219, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #151] ep: 0x7fa2b91a4200, tag: 0x55c87d436596f404, nbytes: 219, type: <class 'numpy.ndarray'>>: Message truncated")
2024-02-05 07:09:17,765 - distributed.nanny - WARNING - Restarting worker
2024-02-05 07:09:17,894 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 2)
Function:  subgraph_callable-53d3f491-f8f3-4c66-9757-77f60b92
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0               641        2  58160703            2
1               659        2   4064170            2
2               661        2  59880801            2
3             11369        2  77654026            2
4             73358        2  47035854            2
...             ...      ...       ...          ...
99999995  799994575        2  58463941            2
99999996  799994579        2  27488375            2
99999997  799994585        2  91881865            2
99999998  799994587        2  69749250            2
99999999  799981263        2  56857139            2

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-05 07:09:17,899 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 6)
Function:  subgraph_callable-53d3f491-f8f3-4c66-9757-77f60b92
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0               640        6  89790164            6
1               658        6    623951            6
2               666        6  47029042            6
3             11361        6  22593201            6
4             73352        6  80840539            6
...             ...      ...       ...          ...
99999995  799981116        6  90015579            6
99999996  799981249        6  90191283            6
99999997  799981251        6  11282749            6
99999998  799981257        6  34163014            6
99999999  799981262        6  41761564            6

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-05 07:09:17,939 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 5)
Function:  subgraph_callable-53d3f491-f8f3-4c66-9757-77f60b92
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0               647        5  54155886            5
1               663        5  21310414            5
2               670        5  93078773            5
3             11372        5  59765530            5
4             73348        5  58445871            5
...             ...      ...       ...          ...
99999995  799981255        5  89013895            5
99999996  799981256        5  39859860            5
99999997  799981259        5  73872054            5
99999998  799981264        5  75214921            5
99999999  799981268        5     72387            5

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-05 07:09:26,783 - distributed.nanny - WARNING - Restarting worker
2024-02-05 07:09:27,208 - distributed.nanny - WARNING - Restarting worker
2024-02-05 07:09:27,622 - distributed.nanny - WARNING - Restarting worker
2024-02-05 07:09:27,625 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 24 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
