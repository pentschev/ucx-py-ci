============================= test session starts ==============================
platform linux -- Python 3.9.19, pytest-8.1.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.6
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 3, in <module>
    from dask import config, datasets
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/datasets.py", line 3, in <module>
    import random
  File "/opt/conda/envs/gdf/lib/python3.9/random.py", line 61, in <module>
    from _sha512 import sha512 as _sha512
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 5, in <module>
    from dask.base import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 29, in <module>
    from dask._compatibility import EMSCRIPTEN
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/_compatibility.py", line 6, in <module>
    from importlib_metadata import entry_points as _entry_points
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 22, in <module>
    from .compat import py39
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 982, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 925, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1423, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1395, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1522, in find_spec
  File "<frozen importlib._bootstrap_external>", line 142, in _path_stat
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-21 05:47:36,545 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:36,550 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45093 instead
  warnings.warn(
2024-03-21 05:47:36,553 - distributed.scheduler - INFO - State start
2024-03-21 05:47:36,574 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:36,575 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:47:36,576 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:47:36,577 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:47:36,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34299'
2024-03-21 05:47:36,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34061'
2024-03-21 05:47:36,715 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34545'
2024-03-21 05:47:36,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36511'
2024-03-21 05:47:36,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45685'
2024-03-21 05:47:36,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46389'
2024-03-21 05:47:36,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40433'
2024-03-21 05:47:36,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38261'
2024-03-21 05:47:38,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,617 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,618 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45291
2024-03-21 05:47:38,618 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45291
2024-03-21 05:47:38,618 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41791
2024-03-21 05:47:38,618 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,618 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,618 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,618 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,618 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kshrvvmw
2024-03-21 05:47:38,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,618 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c6f59b1-53f9-40be-ab55-26f3e8e3d42c
2024-03-21 05:47:38,623 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,624 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45609
2024-03-21 05:47:38,624 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45609
2024-03-21 05:47:38,624 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36299
2024-03-21 05:47:38,624 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,624 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,624 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,624 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,624 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q1fw2qhr
2024-03-21 05:47:38,624 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50aabfff-a026-450e-94e7-028871119baa
2024-03-21 05:47:38,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,687 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34605
2024-03-21 05:47:38,687 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34605
2024-03-21 05:47:38,687 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43955
2024-03-21 05:47:38,687 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,687 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,687 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,687 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,687 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bxdvh2uj
2024-03-21 05:47:38,688 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cdcea880-f250-45ed-9c3c-8379d5b2b1e7
2024-03-21 05:47:38,688 - distributed.worker - INFO - Starting Worker plugin PreImport-19588393-dd13-47ba-9b12-316dece29847
2024-03-21 05:47:38,688 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0543e54b-a256-4dc0-9d91-57db217d0f2e
2024-03-21 05:47:38,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,707 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45479
2024-03-21 05:47:38,708 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45479
2024-03-21 05:47:38,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33313
2024-03-21 05:47:38,708 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,708 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,708 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,708 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,708 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-emukh2ug
2024-03-21 05:47:38,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0dae114e-4f5b-412f-a412-47eb207c7422
2024-03-21 05:47:38,709 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea727f19-cf7c-44ae-a680-adadd1e44f95
2024-03-21 05:47:38,709 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41889
2024-03-21 05:47:38,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41889
2024-03-21 05:47:38,709 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39171
2024-03-21 05:47:38,709 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,709 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,709 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,709 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tyztfxq2
2024-03-21 05:47:38,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-030bb621-277f-49df-9cdf-60669abf9a75
2024-03-21 05:47:38,709 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35483
2024-03-21 05:47:38,710 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35483
2024-03-21 05:47:38,710 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36781
2024-03-21 05:47:38,710 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,710 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,710 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,710 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,710 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-23lef3_w
2024-03-21 05:47:38,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-668c0622-9b61-4ef5-be18-9577a4b72c91
2024-03-21 05:47:38,713 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,713 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,717 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,718 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41311
2024-03-21 05:47:38,718 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41311
2024-03-21 05:47:38,718 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34809
2024-03-21 05:47:38,718 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,718 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,718 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,718 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,718 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-61wgzkqm
2024-03-21 05:47:38,718 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50d5a830-3e32-424e-a74e-03ef18557fe6
2024-03-21 05:47:38,866 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,866 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,872 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,873 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39751
2024-03-21 05:47:38,874 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39751
2024-03-21 05:47:38,874 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45431
2024-03-21 05:47:38,874 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,874 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,874 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,874 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qwu4j5cz
2024-03-21 05:47:38,874 - distributed.worker - INFO - Starting Worker plugin PreImport-1c39042c-9e84-4717-9704-7e543afbfa15
2024-03-21 05:47:38,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc35d00d-5534-4a8e-a846-acaf7f5d4055
2024-03-21 05:47:38,874 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a90514b1-f1c1-435b-acbf-ee4cfd6cd715
2024-03-21 05:47:42,248 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61861042-9cc2-4277-b045-4ee472dc3753
2024-03-21 05:47:42,249 - distributed.worker - INFO - Starting Worker plugin PreImport-4133e952-f33a-4293-992a-214d0f477843
2024-03-21 05:47:42,250 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,267 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1cfd7aa-153f-4737-8c53-4270edbe80f3
2024-03-21 05:47:42,267 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,268 - distributed.worker - INFO - Starting Worker plugin PreImport-036506c9-b04e-4fd4-96d3-ffedc117ab48
2024-03-21 05:47:42,269 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,287 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,288 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,288 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,292 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,293 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,293 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,304 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,306 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,306 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,307 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,354 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13a9bcb0-819a-4e21-a60e-862cb9d0e053
2024-03-21 05:47:42,355 - distributed.worker - INFO - Starting Worker plugin PreImport-f456aa42-a287-4cd4-935c-ea04072acae3
2024-03-21 05:47:42,356 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,364 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea1cc842-7d35-4827-b276-5dbe1ac59fc9
2024-03-21 05:47:42,365 - distributed.worker - INFO - Starting Worker plugin PreImport-b0a069ab-3ee8-4716-a20c-1144d1832736
2024-03-21 05:47:42,366 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,372 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:42,372 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:42,372 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:42,374 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:42,374 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:42,374 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:42,377 - distributed.worker - INFO - Starting Worker plugin PreImport-f098272d-1805-4cfc-8bb0-d54260cece59
2024-03-21 05:47:42,378 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90ada54f-f3ce-4ef4-9edc-ce8b6613b56c
2024-03-21 05:47:42,379 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,386 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,386 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,386 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34545'. Reason: nanny-close
2024-03-21 05:47:42,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36511'. Reason: nanny-close
2024-03-21 05:47:42,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45685'. Reason: nanny-close
2024-03-21 05:47:42,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46389'. Reason: nanny-close
2024-03-21 05:47:42,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40433'. Reason: nanny-close
2024-03-21 05:47:42,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38261'. Reason: nanny-close
2024-03-21 05:47:42,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:42,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34299'. Reason: nanny-close
2024-03-21 05:47:42,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:42,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34061'. Reason: nanny-close
2024-03-21 05:47:42,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:42,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39751. Reason: nanny-close
2024-03-21 05:47:42,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45291. Reason: nanny-close
2024-03-21 05:47:42,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45609. Reason: nanny-close
2024-03-21 05:47:42,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:42,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:42,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:42,399 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:42,399 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:42,399 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:42,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,401 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,406 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:42,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:42,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:42,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41889. Reason: nanny-close
2024-03-21 05:47:42,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35483. Reason: nanny-close
2024-03-21 05:47:42,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41311. Reason: nanny-close
2024-03-21 05:47:42,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:42,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:42,414 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:42,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:42,415 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:42,416 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:42,445 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:47:42,447 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34605. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:47:42,447 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:47:42,450 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:47:42,461 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:47:42,663 - distributed.worker - INFO - Starting Worker plugin PreImport-f6fd30fa-3f06-43ba-8c51-e28689274201
2024-03-21 05:47:42,665 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,692 - distributed.nanny - INFO - Worker process 43024 was killed by signal 15
2024-03-21 05:47:42,703 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,704 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,704 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,714 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:42,715 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45479. Reason: nanny-close
2024-03-21 05:47:42,718 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:42,720 - distributed.nanny - INFO - Worker closed
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-21 05:48:14,845 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:14,849 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35635 instead
  warnings.warn(
2024-03-21 05:48:14,854 - distributed.scheduler - INFO - State start
2024-03-21 05:48:15,291 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:15,292 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:48:15,293 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:48:15,294 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:48:15,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44429'
2024-03-21 05:48:15,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43209'
2024-03-21 05:48:15,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46355'
2024-03-21 05:48:15,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40057'
2024-03-21 05:48:15,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43403'
2024-03-21 05:48:15,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36629'
2024-03-21 05:48:15,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45539'
2024-03-21 05:48:15,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44507'
2024-03-21 05:48:17,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:17,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:17,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:17,307 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42535
2024-03-21 05:48:17,307 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42535
2024-03-21 05:48:17,307 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44517
2024-03-21 05:48:17,307 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,307 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,307 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:17,307 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:17,307 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k12pkndn
2024-03-21 05:48:17,307 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d488ffb6-785a-4510-b6ce-258ca36d8ad7
2024-03-21 05:48:17,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:17,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:17,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:17,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36727
2024-03-21 05:48:17,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36727
2024-03-21 05:48:17,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38873
2024-03-21 05:48:17,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,535 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,535 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:17,535 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:17,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-328t8ldp
2024-03-21 05:48:17,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03620652-d675-4c5c-b3f1-09589d1096f1
2024-03-21 05:48:17,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0139bfc3-e607-4eec-9553-82181e35fcbe
2024-03-21 05:48:17,576 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:17,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:17,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:17,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:17,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:17,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:17,581 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:17,582 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42129
2024-03-21 05:48:17,582 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42129
2024-03-21 05:48:17,582 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46631
2024-03-21 05:48:17,582 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,582 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:17,582 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:17,582 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:17,582 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-45f6gf2s
2024-03-21 05:48:17,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:17,582 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10a0f179-513f-4405-ae02-adb924c56b09
2024-03-21 05:48:17,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36759
2024-03-21 05:48:17,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36759
2024-03-21 05:48:17,583 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33211
2024-03-21 05:48:17,583 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,583 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6223e57d-2bb7-4a82-af55-fab3d767dd4d
2024-03-21 05:48:17,583 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,583 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:17,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44099
2024-03-21 05:48:17,583 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:17,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4w_6qedu
2024-03-21 05:48:17,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44099
2024-03-21 05:48:17,583 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33693
2024-03-21 05:48:17,583 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,583 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0d21397-536e-4d05-9a1c-fac47c22e985
2024-03-21 05:48:17,583 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,583 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:17,583 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5e95f4c-b399-48d6-a086-df0527045e48
2024-03-21 05:48:17,583 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:17,584 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-frd_1a5o
2024-03-21 05:48:17,584 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a0850b3-859d-4b23-9f1a-5ff835daacde
2024-03-21 05:48:17,584 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:17,584 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:17,589 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:17,590 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36523
2024-03-21 05:48:17,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36523
2024-03-21 05:48:17,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37305
2024-03-21 05:48:17,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,590 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,590 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:17,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:17,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vuq9p9pw
2024-03-21 05:48:17,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97391768-dd10-457b-9894-a0855de15b73
2024-03-21 05:48:17,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:17,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:17,630 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1e53a9f-f020-4ab9-a83a-50d14057f6ba
2024-03-21 05:48:17,631 - distributed.worker - INFO - Starting Worker plugin PreImport-1e8a1901-a541-4b68-b63e-98b2169cbd83
2024-03-21 05:48:17,631 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,634 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:17,635 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41069
2024-03-21 05:48:17,635 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41069
2024-03-21 05:48:17,635 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41121
2024-03-21 05:48:17,635 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,635 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,635 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:17,635 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:17,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dn2mrys1
2024-03-21 05:48:17,636 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27b6dbe6-6597-4a8e-9ced-406d2959de84
2024-03-21 05:48:17,643 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:17,643 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:17,647 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:17,648 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41221
2024-03-21 05:48:17,648 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41221
2024-03-21 05:48:17,648 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37317
2024-03-21 05:48:17,648 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,648 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,648 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:17,648 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:17,648 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v5b60rl9
2024-03-21 05:48:17,648 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c6245c9-af15-4328-bc68-f4174ba0018e
2024-03-21 05:48:17,653 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:17,654 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:17,654 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:17,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:19,108 - distributed.worker - INFO - Starting Worker plugin PreImport-7e89ba76-036a-400b-93c3-0a3eaec90f8c
2024-03-21 05:48:19,110 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:19,143 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:19,143 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,145 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:19,158 - distributed.worker - INFO - Starting Worker plugin PreImport-70f13de3-6b8b-443f-bbd8-ee44d3aee9ae
2024-03-21 05:48:19,159 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:19,183 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:19,183 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:19,211 - distributed.worker - INFO - Starting Worker plugin PreImport-4d76583c-bc45-4b67-9bb3-337d2452847c
2024-03-21 05:48:19,213 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,222 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4cd8283c-090f-4bae-b636-a5a4803bcb41
2024-03-21 05:48:19,222 - distributed.worker - INFO - Starting Worker plugin PreImport-5c8f61f3-7fb9-48a7-ab9a-24f987305d32
2024-03-21 05:48:19,223 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:19,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:19,247 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:19,247 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,247 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:19,247 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:19,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:19,266 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fd6fc9a-20f5-4da2-91a8-697b4306d6c9
2024-03-21 05:48:19,267 - distributed.worker - INFO - Starting Worker plugin PreImport-7cc6f567-059d-4c00-860b-7828b27fad24
2024-03-21 05:48:19,267 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,279 - distributed.worker - INFO - Starting Worker plugin PreImport-98abe857-e85a-4acd-8490-aefddb3126cc
2024-03-21 05:48:19,280 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dda8728f-07b4-4fdd-a891-416f7ae5cf69
2024-03-21 05:48:19,282 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,285 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3b200de-9ffa-4b4d-8ba0-039898f0af2c
2024-03-21 05:48:19,286 - distributed.worker - INFO - Starting Worker plugin PreImport-8a425b03-cf8a-43da-aec8-0099b4f5b632
2024-03-21 05:48:19,287 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:19,290 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:19,290 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:19,307 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:19,308 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:19,308 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:19,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:19,319 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:19,319 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:19,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:19,340 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:48:19,340 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:48:19,340 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:48:19,340 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:48:19,340 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:48:19,341 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:48:19,341 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:48:19,341 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:48:19,347 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44429'. Reason: nanny-close
2024-03-21 05:48:19,348 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:19,348 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43209'. Reason: nanny-close
2024-03-21 05:48:19,348 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:19,349 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46355'. Reason: nanny-close
2024-03-21 05:48:19,349 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:19,349 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42129. Reason: nanny-close
2024-03-21 05:48:19,349 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40057'. Reason: nanny-close
2024-03-21 05:48:19,349 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36523. Reason: nanny-close
2024-03-21 05:48:19,349 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:19,350 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43403'. Reason: nanny-close
2024-03-21 05:48:19,350 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44099. Reason: nanny-close
2024-03-21 05:48:19,350 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:19,350 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36629'. Reason: nanny-close
2024-03-21 05:48:19,350 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36759. Reason: nanny-close
2024-03-21 05:48:19,350 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:19,351 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45539'. Reason: nanny-close
2024-03-21 05:48:19,351 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36727. Reason: nanny-close
2024-03-21 05:48:19,351 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:19,351 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44507'. Reason: nanny-close
2024-03-21 05:48:19,351 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:19,351 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:19,352 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:19,352 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41221. Reason: nanny-close
2024-03-21 05:48:19,352 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42535. Reason: nanny-close
2024-03-21 05:48:19,352 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:19,352 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:19,352 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41069. Reason: nanny-close
2024-03-21 05:48:19,353 - distributed.nanny - INFO - Worker closed
2024-03-21 05:48:19,353 - distributed.nanny - INFO - Worker closed
2024-03-21 05:48:19,353 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:19,353 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:19,354 - distributed.nanny - INFO - Worker closed
2024-03-21 05:48:19,354 - distributed.nanny - INFO - Worker closed
2024-03-21 05:48:19,354 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:19,355 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:19,355 - distributed.nanny - INFO - Worker closed
2024-03-21 05:48:19,355 - distributed.nanny - INFO - Worker closed
2024-03-21 05:48:19,356 - distributed.nanny - INFO - Worker closed
2024-03-21 05:48:19,356 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-21 05:48:21,953 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:21,958 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41043 instead
  warnings.warn(
2024-03-21 05:48:21,961 - distributed.scheduler - INFO - State start
2024-03-21 05:48:21,983 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:21,984 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:48:21,984 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:48:21,985 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:48:22,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33251'
2024-03-21 05:48:22,071 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38187'
2024-03-21 05:48:22,086 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38951'
2024-03-21 05:48:22,089 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41471'
2024-03-21 05:48:22,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45049'
2024-03-21 05:48:22,108 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33715'
2024-03-21 05:48:22,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37109'
2024-03-21 05:48:22,127 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42211'
2024-03-21 05:48:23,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:23,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:23,720 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:23,721 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43021
2024-03-21 05:48:23,721 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43021
2024-03-21 05:48:23,721 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44223
2024-03-21 05:48:23,721 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:23,721 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:23,721 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:23,721 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:23,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5cd6pl7x
2024-03-21 05:48:23,721 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eeb074aa-cd31-4389-9559-f0f36f2409c8
2024-03-21 05:48:23,951 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:23,951 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:23,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:23,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:23,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:23,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:23,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:23,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:23,956 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:23,957 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33245
2024-03-21 05:48:23,957 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33245
2024-03-21 05:48:23,957 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43421
2024-03-21 05:48:23,957 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:23,957 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:23,957 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:23,957 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:23,958 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7446aan6
2024-03-21 05:48:23,958 - distributed.worker - INFO - Starting Worker plugin PreImport-ee7aeddc-3aed-4427-9cd1-bc4dbfa2d010
2024-03-21 05:48:23,958 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b9508c5-d29a-4d2d-b483-32b4ef82ddbf
2024-03-21 05:48:23,958 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0368daf7-fb71-45ce-a00a-c789fa5de7fb
2024-03-21 05:48:23,959 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:23,960 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:23,960 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:23,960 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36187
2024-03-21 05:48:23,960 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36187
2024-03-21 05:48:23,960 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33115
2024-03-21 05:48:23,960 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:23,960 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:23,960 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:23,960 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46165
2024-03-21 05:48:23,961 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:23,961 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46165
2024-03-21 05:48:23,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hts6hidp
2024-03-21 05:48:23,961 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35335
2024-03-21 05:48:23,961 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:23,961 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:23,961 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:23,961 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83e6d1eb-f63f-44bb-884a-eee9b732daf2
2024-03-21 05:48:23,961 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:23,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_4hq10m3
2024-03-21 05:48:23,961 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46847
2024-03-21 05:48:23,961 - distributed.worker - INFO - Starting Worker plugin RMMSetup-425732c8-336d-41e9-bad7-0dbffecee5cf
2024-03-21 05:48:23,961 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46847
2024-03-21 05:48:23,961 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39921
2024-03-21 05:48:23,961 - distributed.worker - INFO - Starting Worker plugin PreImport-8066a599-b015-48f9-91bb-b8be2fd37063
2024-03-21 05:48:23,961 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:23,961 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:23,961 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f9f9631-99a4-4261-8883-018bec5c077b
2024-03-21 05:48:23,961 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:23,961 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:23,961 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffe493c5-5715-4a99-9ea7-d1c38e582b94
2024-03-21 05:48:23,961 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nu2a4aj0
2024-03-21 05:48:23,962 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1be3e05-037f-4ed4-a859-375cde0f623c
2024-03-21 05:48:23,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:23,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:23,966 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:23,967 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37963
2024-03-21 05:48:23,967 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37963
2024-03-21 05:48:23,967 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41603
2024-03-21 05:48:23,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:23,967 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:23,967 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:23,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:23,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-clce8h8t
2024-03-21 05:48:23,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5dcfd3a8-e4ff-49b4-9269-93beb140ad45
2024-03-21 05:48:23,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:23,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:23,977 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:23,978 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46687
2024-03-21 05:48:23,978 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46687
2024-03-21 05:48:23,978 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33525
2024-03-21 05:48:23,978 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:23,978 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:23,979 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:23,979 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:23,979 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wh88y85q
2024-03-21 05:48:23,979 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f513305b-6169-4769-bde9-a9c036b2bb69
2024-03-21 05:48:23,979 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b157772c-da41-4afc-b514-b1b141920504
2024-03-21 05:48:23,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:23,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:23,992 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:23,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42929
2024-03-21 05:48:23,993 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42929
2024-03-21 05:48:23,993 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43537
2024-03-21 05:48:23,993 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:23,993 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:23,993 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:23,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:23,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nihl9w49
2024-03-21 05:48:23,993 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e082983-b01e-4b51-8ef3-899f37a794b2
2024-03-21 05:48:24,216 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-379e871a-53c7-472f-8c59-612a4b495b91
2024-03-21 05:48:24,217 - distributed.worker - INFO - Starting Worker plugin PreImport-39771dca-de0d-46d4-93c5-178d70c2068f
2024-03-21 05:48:24,217 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:24,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:24,247 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:24,248 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:24,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:25,759 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-510f0c60-cc1a-46db-825e-659472f6426d
2024-03-21 05:48:25,760 - distributed.worker - INFO - Starting Worker plugin PreImport-b737d9bf-b48f-4a66-b78f-3bd731d6c255
2024-03-21 05:48:25,760 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,765 - distributed.worker - INFO - Starting Worker plugin PreImport-0baf75f3-b1ca-42c5-960a-88fb9788655d
2024-03-21 05:48:25,766 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,764 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-21 05:48:25,768 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46165. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:48:25,768 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:48:25,774 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:25,784 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:25,785 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:25,785 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:25,787 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-941ff48f-f09b-4bb5-97cf-17939822bc2b
2024-03-21 05:48:25,789 - distributed.worker - INFO - Starting Worker plugin PreImport-1c76a25c-4bf3-4a39-89a5-974362347a1d
2024-03-21 05:48:25,790 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:25,789 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,790 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,790 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:25,791 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,792 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:25,794 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-761e0145-28bf-4ba5-b914-a18c337fe652
2024-03-21 05:48:25,795 - distributed.worker - INFO - Starting Worker plugin PreImport-6c7be71a-c4f1-44e5-b221-80d33af58d95
2024-03-21 05:48:25,795 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,797 - distributed.worker - INFO - Starting Worker plugin PreImport-fa0a6d37-849a-49b8-9291-fff9001bee3c
2024-03-21 05:48:25,799 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,809 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:25,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33251'. Reason: nanny-instantiate-failed
2024-03-21 05:48:25,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:48:25,818 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:25,819 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:25,819 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:25,820 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:25,821 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:25,821 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:25,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:25,823 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:25,823 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:25,832 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:25,833 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:25,833 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:25,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:25,867 - distributed.nanny - INFO - Worker process 43611 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:48:25,870 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43642 parent=43443 started daemon>
2024-03-21 05:48:25,871 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43638 parent=43443 started daemon>
2024-03-21 05:48:25,871 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43634 parent=43443 started daemon>
2024-03-21 05:48:25,871 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43630 parent=43443 started daemon>
2024-03-21 05:48:25,871 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43625 parent=43443 started daemon>
2024-03-21 05:48:25,871 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43620 parent=43443 started daemon>
2024-03-21 05:48:25,871 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43615 parent=43443 started daemon>
2024-03-21 05:48:26,164 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43615 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-21 05:48:38,398 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:38,404 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:48:38,408 - distributed.scheduler - INFO - State start
2024-03-21 05:48:38,410 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-hts6hidp', purging
2024-03-21 05:48:38,411 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nihl9w49', purging
2024-03-21 05:48:38,411 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5cd6pl7x', purging
2024-03-21 05:48:38,412 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-clce8h8t', purging
2024-03-21 05:48:38,412 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7446aan6', purging
2024-03-21 05:48:38,412 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-wh88y85q', purging
2024-03-21 05:48:38,413 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nu2a4aj0', purging
2024-03-21 05:48:38,437 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:38,438 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:48:38,438 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:48:38,438 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:48:38,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45993'
2024-03-21 05:48:38,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43991'
2024-03-21 05:48:38,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35823'
2024-03-21 05:48:38,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38641'
2024-03-21 05:48:38,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38419'
2024-03-21 05:48:38,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42997'
2024-03-21 05:48:38,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43539'
2024-03-21 05:48:38,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41237'
2024-03-21 05:48:39,033 - distributed.scheduler - INFO - Receive client connection: Client-a98c7b6f-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:39,045 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53756
2024-03-21 05:48:39,062 - distributed.scheduler - INFO - Receive client connection: Client-a910904d-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:39,063 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53774
2024-03-21 05:48:40,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:40,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:40,189 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:40,190 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39745
2024-03-21 05:48:40,190 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39745
2024-03-21 05:48:40,190 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44983
2024-03-21 05:48:40,190 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:40,190 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:40,190 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:40,190 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:40,190 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-412ssuyj
2024-03-21 05:48:40,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0877234-ff9d-4880-8627-098b87597f06
2024-03-21 05:48:40,419 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:40,419 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:40,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:40,424 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41609
2024-03-21 05:48:40,424 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41609
2024-03-21 05:48:40,424 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41109
2024-03-21 05:48:40,424 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:40,424 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:40,424 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:40,425 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:40,425 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lst3wu_a
2024-03-21 05:48:40,425 - distributed.worker - INFO - Starting Worker plugin PreImport-821dc9a1-c269-4d91-83b1-149ea0a28e20
2024-03-21 05:48:40,425 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eeb2606a-4e1c-414c-9330-61cd2d21a48f
2024-03-21 05:48:40,427 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0425e06e-3c11-43e8-8e2f-f785741d38b1
2024-03-21 05:48:40,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:40,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:40,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:40,433 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42173
2024-03-21 05:48:40,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42173
2024-03-21 05:48:40,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38851
2024-03-21 05:48:40,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:40,433 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:40,433 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:40,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:40,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ppvs5as
2024-03-21 05:48:40,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77d80d66-c6e3-4b59-8555-1d8356d54bb3
2024-03-21 05:48:40,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:40,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:40,478 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:40,479 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38649
2024-03-21 05:48:40,479 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38649
2024-03-21 05:48:40,479 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46333
2024-03-21 05:48:40,479 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:40,479 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:40,479 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:40,479 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:40,479 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wvgqoefy
2024-03-21 05:48:40,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a5a38f7-b416-4233-b03a-b6a0243acadf
2024-03-21 05:48:40,480 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d7691fd3-24ce-4c6a-9237-446644d2ba09
2024-03-21 05:48:40,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:40,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:40,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:40,489 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42429
2024-03-21 05:48:40,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42429
2024-03-21 05:48:40,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36487
2024-03-21 05:48:40,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:40,489 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:40,489 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:40,489 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:40,489 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2bpxiib9
2024-03-21 05:48:40,489 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fc09c72-1bda-4958-971f-9fc146c96f8d
2024-03-21 05:48:40,491 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99057b2d-bee8-4c40-ab7c-54ee0321d59c
2024-03-21 05:48:40,513 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:40,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:40,518 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:40,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43513
2024-03-21 05:48:40,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43513
2024-03-21 05:48:40,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35281
2024-03-21 05:48:40,519 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:40,519 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:40,519 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:40,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:40,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r9pkcu47
2024-03-21 05:48:40,519 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ec79c9e-7ecb-48b3-aff0-34b0363337b5
2024-03-21 05:48:40,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:40,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:40,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:40,529 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39747
2024-03-21 05:48:40,529 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39747
2024-03-21 05:48:40,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35447
2024-03-21 05:48:40,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:40,530 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:40,530 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:40,530 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:40,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0__59voq
2024-03-21 05:48:40,530 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e95bbcf-12c1-4087-92e0-137901908284
2024-03-21 05:48:40,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:40,945 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:40,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:40,953 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41587
2024-03-21 05:48:40,953 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41587
2024-03-21 05:48:40,953 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45999
2024-03-21 05:48:40,953 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:40,953 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:40,953 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:40,953 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:40,953 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fs6uudod
2024-03-21 05:48:40,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8527c309-7e45-40d3-b8f8-de52a4def925
2024-03-21 05:48:41,052 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-21 05:48:41,054 - distributed.worker - INFO - Starting Worker plugin PreImport-2e21362e-6e72-4f46-8f40-4cf705a13feb
2024-03-21 05:48:41,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc605414-5fab-4361-ac39-56e92d349f7b
2024-03-21 05:48:41,054 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39745. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:48:41,055 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:48:41,059 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:41,107 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:41,110 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45993'. Reason: nanny-instantiate-failed
2024-03-21 05:48:41,110 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:48:41,753 - distributed.nanny - INFO - Worker process 43894 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:48:41,758 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43925 parent=43726 started daemon>
2024-03-21 05:48:41,758 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43921 parent=43726 started daemon>
2024-03-21 05:48:41,758 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43917 parent=43726 started daemon>
2024-03-21 05:48:41,754 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53698'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53698>: Stream is closed
2024-03-21 05:48:41,758 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43914 parent=43726 started daemon>
2024-03-21 05:48:41,758 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43908 parent=43726 started daemon>
2024-03-21 05:48:41,759 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43903 parent=43726 started daemon>
2024-03-21 05:48:41,759 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43898 parent=43726 started daemon>
2024-03-21 05:48:41,863 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43898 exit status was already read will report exitcode 255
2024-03-21 05:48:41,882 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43903 exit status was already read will report exitcode 255
2024-03-21 05:48:41,939 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43908 exit status was already read will report exitcode 255
2024-03-21 05:48:42,213 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53740'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53740>: Stream is closed
2024-03-21 05:48:42,213 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53738'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53738>: Stream is closed
2024-03-21 05:48:42,214 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53734'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53734>: Stream is closed
2024-03-21 05:48:42,214 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53724'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53724>: Stream is closed
2024-03-21 05:48:42,214 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53716'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53716>: Stream is closed
2024-03-21 05:48:42,215 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53708'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53708>: Stream is closed
2024-03-21 05:48:42,215 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53704'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53704>: Stream is closed
2024-03-21 05:48:43,783 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35843', status: init, memory: 0, processing: 0>
2024-03-21 05:48:43,784 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35843
2024-03-21 05:48:43,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51950
2024-03-21 05:48:43,792 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34711', status: init, memory: 0, processing: 0>
2024-03-21 05:48:43,793 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34711
2024-03-21 05:48:43,793 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51960
2024-03-21 05:48:43,811 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45213', status: init, memory: 0, processing: 0>
2024-03-21 05:48:43,811 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45213
2024-03-21 05:48:43,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51966
2024-03-21 05:48:43,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39619', status: init, memory: 0, processing: 0>
2024-03-21 05:48:43,830 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39619
2024-03-21 05:48:43,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51988
2024-03-21 05:48:43,840 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33795', status: init, memory: 0, processing: 0>
2024-03-21 05:48:43,840 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33795
2024-03-21 05:48:43,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51976
2024-03-21 05:48:43,843 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34857', status: init, memory: 0, processing: 0>
2024-03-21 05:48:43,844 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34857
2024-03-21 05:48:43,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51994
2024-03-21 05:48:43,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45673', status: init, memory: 0, processing: 0>
2024-03-21 05:48:43,861 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45673
2024-03-21 05:48:43,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52000
2024-03-21 05:48:43,927 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53786'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53786>: Stream is closed
2024-03-21 05:48:43,951 - distributed.core - INFO - Connection to tcp://127.0.0.1:51988 has been closed.
2024-03-21 05:48:43,952 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39619', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000123.9520638')
2024-03-21 05:48:43,953 - distributed.core - INFO - Connection to tcp://127.0.0.1:51994 has been closed.
2024-03-21 05:48:43,953 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34857', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000123.9535627')
2024-03-21 05:48:43,954 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51994>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51994>: Stream is closed
2024-03-21 05:48:43,956 - distributed.core - INFO - Connection to tcp://127.0.0.1:51950 has been closed.
2024-03-21 05:48:43,957 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35843', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000123.9571126')
2024-03-21 05:48:43,957 - distributed.core - INFO - Connection to tcp://127.0.0.1:51966 has been closed.
2024-03-21 05:48:43,958 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45213', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000123.9579868')
2024-03-21 05:48:43,958 - distributed.core - INFO - Connection to tcp://127.0.0.1:51976 has been closed.
2024-03-21 05:48:43,959 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33795', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000123.9590046')
2024-03-21 05:48:43,959 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51976>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51976>: Stream is closed
2024-03-21 05:48:43,959 - distributed.core - INFO - Connection to tcp://127.0.0.1:51960 has been closed.
2024-03-21 05:48:43,959 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34711', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000123.9598176')
2024-03-21 05:48:43,960 - distributed.core - INFO - Connection to tcp://127.0.0.1:52000 has been closed.
2024-03-21 05:48:43,960 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45673', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000123.9605134')
2024-03-21 05:48:43,960 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:48:52,602 - distributed.scheduler - INFO - Remove client Client-a910904d-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:52,602 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53774; closing.
2024-03-21 05:48:52,603 - distributed.scheduler - INFO - Remove client Client-a910904d-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:52,603 - distributed.scheduler - INFO - Close client connection: Client-a910904d-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:52,604 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:48:52,604 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:48:52,604 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:48:52,606 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:48:52,606 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-21 05:48:54,541 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:54,545 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:48:54,548 - distributed.scheduler - INFO - State start
2024-03-21 05:48:54,550 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2bpxiib9', purging
2024-03-21 05:48:54,550 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-r9pkcu47', purging
2024-03-21 05:48:54,551 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-lst3wu_a', purging
2024-03-21 05:48:54,551 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0__59voq', purging
2024-03-21 05:48:54,551 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4ppvs5as', purging
2024-03-21 05:48:54,552 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fs6uudod', purging
2024-03-21 05:48:54,552 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-wvgqoefy', purging
2024-03-21 05:48:54,571 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:54,572 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:48:54,572 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:48:54,573 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:48:54,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46375'
2024-03-21 05:48:54,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43391'
2024-03-21 05:48:54,715 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44273'
2024-03-21 05:48:54,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36277'
2024-03-21 05:48:54,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35407'
2024-03-21 05:48:54,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46197'
2024-03-21 05:48:54,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43567'
2024-03-21 05:48:54,759 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41005'
2024-03-21 05:48:54,834 - distributed.scheduler - INFO - Receive client connection: Client-a98c7b6f-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:54,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52392
2024-03-21 05:48:55,382 - distributed.scheduler - INFO - Remove client Client-a98c7b6f-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:55,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52392; closing.
2024-03-21 05:48:55,383 - distributed.scheduler - INFO - Remove client Client-a98c7b6f-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:55,384 - distributed.scheduler - INFO - Close client connection: Client-a98c7b6f-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:55,554 - distributed.scheduler - INFO - Receive client connection: Client-b467b162-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:55,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52426
2024-03-21 05:48:56,521 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:56,521 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:56,525 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:56,526 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38651
2024-03-21 05:48:56,526 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38651
2024-03-21 05:48:56,526 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43661
2024-03-21 05:48:56,526 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:56,526 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:56,526 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:56,526 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:56,526 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xh410_j5
2024-03-21 05:48:56,527 - distributed.worker - INFO - Starting Worker plugin PreImport-f5791375-1b12-400b-a5fd-2f80df186ea6
2024-03-21 05:48:56,527 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-316d31dc-b588-4ddf-a9e8-1b7ff4e1cac9
2024-03-21 05:48:56,527 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5fa94d21-2318-4cda-89d7-31426888c5eb
2024-03-21 05:48:56,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:56,740 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:56,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:56,745 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39925
2024-03-21 05:48:56,745 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39925
2024-03-21 05:48:56,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42407
2024-03-21 05:48:56,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:56,745 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:56,745 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:56,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:56,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6kcdm40c
2024-03-21 05:48:56,745 - distributed.worker - INFO - Starting Worker plugin PreImport-cbe14c72-709c-4ed4-a388-662337927fc3
2024-03-21 05:48:56,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6139d3c-a0ab-48dd-8252-4bfe9a7678b1
2024-03-21 05:48:56,747 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14f05e0e-b7e7-4004-881f-ac86780d7780
2024-03-21 05:48:56,756 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:56,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:56,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:56,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:56,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:56,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:56,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:56,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:56,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:56,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:56,759 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:56,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:56,761 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:56,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:56,762 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41923
2024-03-21 05:48:56,762 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41923
2024-03-21 05:48:56,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:56,762 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35703
2024-03-21 05:48:56,762 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:56,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:56,762 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:56,762 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:56,763 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:56,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b4cwm8w6
2024-03-21 05:48:56,763 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ecd891c6-f372-47cf-ab1f-3e95f386bfeb
2024-03-21 05:48:56,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40227
2024-03-21 05:48:56,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40227
2024-03-21 05:48:56,763 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42027
2024-03-21 05:48:56,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45127
2024-03-21 05:48:56,763 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:56,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45127
2024-03-21 05:48:56,763 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:56,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42065
2024-03-21 05:48:56,763 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45729
2024-03-21 05:48:56,763 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:56,763 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:56,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42065
2024-03-21 05:48:56,763 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:56,763 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:56,763 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41919
2024-03-21 05:48:56,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6125y25o
2024-03-21 05:48:56,763 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:56,763 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:56,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:56,763 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:56,763 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:56,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dzth5430
2024-03-21 05:48:56,763 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:56,764 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:56,764 - distributed.worker - INFO - Starting Worker plugin PreImport-16203096-648a-4ace-8bc9-b5f69ff71144
2024-03-21 05:48:56,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8adtzosp
2024-03-21 05:48:56,764 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:56,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-68ef2435-8667-49d6-8673-cf8f6fb401d6
2024-03-21 05:48:56,764 - distributed.worker - INFO - Starting Worker plugin RMMSetup-417ab21e-d1ae-4262-94d6-b004afa862f5
2024-03-21 05:48:56,764 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d7f0f88-05b7-4fc2-aad5-c333bfa713e5
2024-03-21 05:48:56,764 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37309
2024-03-21 05:48:56,764 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37309
2024-03-21 05:48:56,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34227
2024-03-21 05:48:56,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32833
2024-03-21 05:48:56,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:56,765 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:56,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32833
2024-03-21 05:48:56,765 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:56,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33963
2024-03-21 05:48:56,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:56,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:56,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rfbgnr99
2024-03-21 05:48:56,765 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:56,765 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:56,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb30171f-2818-40cd-8e14-830988a87b88
2024-03-21 05:48:56,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:56,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9izwdz_v
2024-03-21 05:48:56,765 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18800133-e228-4d9b-b284-ca22646c86d5
2024-03-21 05:48:56,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cbfc59d9-e806-41ec-a263-b03cc9ddfeef
2024-03-21 05:48:56,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0167ede1-23c6-4ee6-ba2d-00161e71c84f
2024-03-21 05:48:56,993 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:48:56,995 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38651. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:48:56,995 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:48:57,000 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:57,023 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:57,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46375'. Reason: nanny-instantiate-failed
2024-03-21 05:48:57,026 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:48:57,430 - distributed.nanny - INFO - Worker process 44156 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:48:57,431 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52326'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52326>: Stream is closed
2024-03-21 05:48:57,434 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44187 parent=43988 started daemon>
2024-03-21 05:48:57,435 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44183 parent=43988 started daemon>
2024-03-21 05:48:57,435 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44179 parent=43988 started daemon>
2024-03-21 05:48:57,435 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44176 parent=43988 started daemon>
2024-03-21 05:48:57,435 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44170 parent=43988 started daemon>
2024-03-21 05:48:57,435 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44165 parent=43988 started daemon>
2024-03-21 05:48:57,435 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44160 parent=43988 started daemon>
2024-03-21 05:48:57,464 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44176 exit status was already read will report exitcode 255
2024-03-21 05:48:57,509 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44160 exit status was already read will report exitcode 255
2024-03-21 05:48:57,824 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52388'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52388>: Stream is closed
2024-03-21 05:48:57,824 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52374'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52374>: Stream is closed
2024-03-21 05:48:57,825 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52368'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52368>: Stream is closed
2024-03-21 05:48:57,825 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52354'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52354>: Stream is closed
2024-03-21 05:48:57,826 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52344'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52344>: Stream is closed
2024-03-21 05:48:57,826 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52340'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52340>: Stream is closed
2024-03-21 05:48:57,827 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52336'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52336>: Stream is closed
2024-03-21 05:48:59,516 - distributed.scheduler - INFO - Receive client connection: Client-b2d78d94-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:59,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52456
2024-03-21 05:48:59,642 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45647', status: init, memory: 0, processing: 0>
2024-03-21 05:48:59,643 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45647
2024-03-21 05:48:59,643 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52474
2024-03-21 05:48:59,693 - distributed.scheduler - INFO - Remove client Client-b467b162-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:59,694 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52426; closing.
2024-03-21 05:48:59,694 - distributed.scheduler - INFO - Remove client Client-b467b162-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:59,694 - distributed.scheduler - INFO - Close client connection: Client-b467b162-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:59,699 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52474; closing.
2024-03-21 05:48:59,699 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45647', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000139.699366')
2024-03-21 05:48:59,699 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:49:02,257 - distributed.scheduler - INFO - Receive client connection: Client-b86680ba-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:02,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46364
2024-03-21 05:49:06,144 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41917', status: init, memory: 0, processing: 0>
2024-03-21 05:49:06,145 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41917
2024-03-21 05:49:06,145 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46380
2024-03-21 05:49:06,211 - distributed.scheduler - INFO - Remove client Client-b86680ba-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:06,211 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46364; closing.
2024-03-21 05:49:06,212 - distributed.scheduler - INFO - Remove client Client-b86680ba-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:06,212 - distributed.scheduler - INFO - Close client connection: Client-b86680ba-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:06,216 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46380; closing.
2024-03-21 05:49:06,216 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41917', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000146.2169328')
2024-03-21 05:49:06,217 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:49:10,892 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:46402'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46402>: Stream is closed
2024-03-21 05:49:15,396 - distributed.scheduler - INFO - Receive client connection: Client-c03b7e97-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:15,397 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36052
2024-03-21 05:49:15,520 - distributed.scheduler - INFO - Remove client Client-b2d78d94-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:15,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52456; closing.
2024-03-21 05:49:15,520 - distributed.scheduler - INFO - Remove client Client-b2d78d94-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:15,521 - distributed.scheduler - INFO - Close client connection: Client-b2d78d94-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:15,522 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:49:15,522 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:49:15,522 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:15,524 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:49:15,524 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-21 05:49:17,503 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:17,507 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42557 instead
  warnings.warn(
2024-03-21 05:49:17,511 - distributed.scheduler - INFO - State start
2024-03-21 05:49:17,512 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6125y25o', purging
2024-03-21 05:49:17,513 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-b4cwm8w6', purging
2024-03-21 05:49:17,513 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dzth5430', purging
2024-03-21 05:49:17,514 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6kcdm40c', purging
2024-03-21 05:49:17,514 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-9izwdz_v', purging
2024-03-21 05:49:17,514 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rfbgnr99', purging
2024-03-21 05:49:17,515 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8adtzosp', purging
2024-03-21 05:49:17,535 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:17,536 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:49:17,536 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:17,537 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:49:17,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36251'
2024-03-21 05:49:19,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:19,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:19,484 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36251'. Reason: nanny-close
2024-03-21 05:49:19,879 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:19,880 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46303
2024-03-21 05:49:19,880 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46303
2024-03-21 05:49:19,880 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-21 05:49:19,880 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,881 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,881 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:19,881 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:49:19,881 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_hikc919
2024-03-21 05:49:19,881 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ec4bd63-1b2b-4bcb-a574-eef227583a0e
2024-03-21 05:49:19,882 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20fa26e0-4e37-4b98-a8b4-74c40a25cb82
2024-03-21 05:49:19,882 - distributed.worker - INFO - Starting Worker plugin PreImport-ce9cd9f8-bcc1-45b3-8be0-bbbd31bf5d1b
2024-03-21 05:49:19,882 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:20,963 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:20,964 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:20,965 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:20,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:21,016 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:49:21,017 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46303. Reason: nanny-close
2024-03-21 05:49:21,020 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:49:21,021 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-21 05:49:25,140 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:25,144 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45455 instead
  warnings.warn(
2024-03-21 05:49:25,147 - distributed.scheduler - INFO - State start
2024-03-21 05:49:25,168 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:25,169 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:49:25,169 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:25,170 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:49:25,232 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41905'
2024-03-21 05:49:26,839 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:26,840 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:27,358 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:27,360 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44695
2024-03-21 05:49:27,360 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44695
2024-03-21 05:49:27,360 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45235
2024-03-21 05:49:27,360 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:27,360 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:27,360 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:27,360 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:49:27,360 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zdjizbw1
2024-03-21 05:49:27,360 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2eda16c1-a654-4fdb-9b24-a86797251703
2024-03-21 05:49:27,361 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc3316c1-2ad2-4c7d-b1e7-d55196c06750
2024-03-21 05:49:27,361 - distributed.worker - INFO - Starting Worker plugin PreImport-e27015c3-369a-4532-8b12-a09d7af6b9d5
2024-03-21 05:49:27,362 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:27,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:27,420 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:27,420 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:27,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:27,494 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:49:27,499 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41905'. Reason: nanny-close
2024-03-21 05:49:27,499 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:49:27,500 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44695. Reason: nanny-close
2024-03-21 05:49:27,502 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:49:27,504 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-21 05:49:29,845 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:29,849 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39479 instead
  warnings.warn(
2024-03-21 05:49:29,852 - distributed.scheduler - INFO - State start
2024-03-21 05:49:29,873 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:29,874 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:49:29,875 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:29,876 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-21 05:49:33,903 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:33,908 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44277 instead
  warnings.warn(
2024-03-21 05:49:33,911 - distributed.scheduler - INFO - State start
2024-03-21 05:49:33,931 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:33,932 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-21 05:49:33,933 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44277/status
2024-03-21 05:49:33,933 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:49:34,054 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34769'
2024-03-21 05:49:34,225 - distributed.scheduler - INFO - Receive client connection: Client-ca523795-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:34,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40254
2024-03-21 05:49:35,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:35,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:35,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:35,631 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33945
2024-03-21 05:49:35,631 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33945
2024-03-21 05:49:35,631 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45523
2024-03-21 05:49:35,632 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:49:35,632 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:35,632 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:35,632 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:49:35,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-v7ibtu3r
2024-03-21 05:49:35,632 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba415fab-c101-45ce-ab00-83130c717460
2024-03-21 05:49:35,632 - distributed.worker - INFO - Starting Worker plugin PreImport-97347c05-fbb2-4a59-b4c0-2fa6aa1c34ea
2024-03-21 05:49:35,632 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4db7d326-83b8-4462-a320-ae00c540dd7f
2024-03-21 05:49:35,632 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:35,677 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33945', status: init, memory: 0, processing: 0>
2024-03-21 05:49:35,678 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33945
2024-03-21 05:49:35,678 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40278
2024-03-21 05:49:35,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:35,680 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:49:35,680 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:35,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:49:35,761 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:49:35,763 - distributed.scheduler - INFO - Remove client Client-ca523795-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:35,764 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40254; closing.
2024-03-21 05:49:35,764 - distributed.scheduler - INFO - Remove client Client-ca523795-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:35,764 - distributed.scheduler - INFO - Close client connection: Client-ca523795-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:35,765 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34769'. Reason: nanny-close
2024-03-21 05:49:35,765 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:49:35,766 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33945. Reason: nanny-close
2024-03-21 05:49:35,768 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:49:35,768 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40278; closing.
2024-03-21 05:49:35,768 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33945', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000175.7688794')
2024-03-21 05:49:35,769 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:49:35,769 - distributed.nanny - INFO - Worker closed
2024-03-21 05:49:36,180 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:49:36,180 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:49:36,181 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:36,181 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-21 05:49:36,182 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-03-21 05:49:38,186 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:38,190 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33107 instead
  warnings.warn(
2024-03-21 05:49:38,194 - distributed.scheduler - INFO - State start
2024-03-21 05:49:38,214 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:38,215 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:49:38,216 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:38,216 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:49:38,365 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43335'
2024-03-21 05:49:38,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38733'
2024-03-21 05:49:38,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38149'
2024-03-21 05:49:38,391 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37979'
2024-03-21 05:49:38,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37381'
2024-03-21 05:49:38,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34411'
2024-03-21 05:49:38,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44747'
2024-03-21 05:49:38,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36697'
2024-03-21 05:49:40,277 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:40,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:40,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:40,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:40,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:40,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:40,281 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:40,282 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44373
2024-03-21 05:49:40,282 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44373
2024-03-21 05:49:40,282 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35695
2024-03-21 05:49:40,282 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:40,282 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:40,283 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:40,283 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:40,283 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qf76qwyp
2024-03-21 05:49:40,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:40,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:40,283 - distributed.worker - INFO - Starting Worker plugin PreImport-fe4474d1-d011-4d6a-bd91-cc8fb6083102
2024-03-21 05:49:40,283 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb7dd72c-0c71-4886-ac37-5aca95acf3bd
2024-03-21 05:49:40,283 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24421f31-545e-47e2-aa6e-e15b87aa3550
2024-03-21 05:49:40,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:40,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:40,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:40,285 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:40,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43125
2024-03-21 05:49:40,286 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43125
2024-03-21 05:49:40,286 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33037
2024-03-21 05:49:40,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41095
2024-03-21 05:49:40,286 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:40,286 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41095
2024-03-21 05:49:40,286 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:40,286 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:40,286 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35895
2024-03-21 05:49:40,286 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:40,286 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:40,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-00ky08r_
2024-03-21 05:49:40,286 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:40,286 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:40,286 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:40,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fed47mqe
2024-03-21 05:49:40,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76419dbe-bac2-4028-9c8a-2701605c7869
2024-03-21 05:49:40,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df5eaffd-fea7-43af-b61e-e11ccc8e83c3
2024-03-21 05:49:40,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:40,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34963
2024-03-21 05:49:40,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34963
2024-03-21 05:49:40,289 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39353
2024-03-21 05:49:40,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:40,289 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:40,289 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:40,289 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:40,289 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:40,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0gcl85k6
2024-03-21 05:49:40,290 - distributed.worker - INFO - Starting Worker plugin PreImport-827d5918-aa79-4044-8269-8fd948c00b55
2024-03-21 05:49:40,290 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96749022-1f46-4e55-8fed-d5551b48bbd9
2024-03-21 05:49:40,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ee2dbb0-e84d-4f01-aab8-a665ecee0a57
2024-03-21 05:49:40,290 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45741
2024-03-21 05:49:40,290 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45741
2024-03-21 05:49:40,290 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45583
2024-03-21 05:49:40,290 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:40,290 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:40,290 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:40,290 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:40,290 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dxs60fiv
2024-03-21 05:49:40,291 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce80b445-7ddf-4b29-9d2b-c05b5c91d151
2024-03-21 05:49:40,291 - distributed.worker - INFO - Starting Worker plugin RMMSetup-618ec53c-5e6c-4768-96ea-ae439edec7d6
2024-03-21 05:49:40,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:40,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:40,301 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:40,302 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41397
2024-03-21 05:49:40,302 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41397
2024-03-21 05:49:40,302 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41957
2024-03-21 05:49:40,302 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:40,302 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:40,302 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:40,302 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:40,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7_dwmtip
2024-03-21 05:49:40,302 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8d02a77-ecd7-42a1-99c2-f32deb091ac0
2024-03-21 05:49:40,316 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:40,316 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:40,318 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:40,318 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:40,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:40,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44003
2024-03-21 05:49:40,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44003
2024-03-21 05:49:40,322 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43701
2024-03-21 05:49:40,322 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:40,322 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:40,322 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:40,322 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:40,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8hdiswwr
2024-03-21 05:49:40,322 - distributed.worker - INFO - Starting Worker plugin PreImport-b69f4194-0443-4c44-9071-6b947175bc63
2024-03-21 05:49:40,322 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:40,323 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-883e2776-7a7f-48f5-a452-f2664509c66a
2024-03-21 05:49:40,323 - distributed.worker - INFO - Starting Worker plugin RMMSetup-69416df6-c269-4b03-8270-f30a19b4aa0b
2024-03-21 05:49:40,323 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37093
2024-03-21 05:49:40,323 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37093
2024-03-21 05:49:40,324 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32983
2024-03-21 05:49:40,324 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:40,324 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:40,324 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:40,324 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:40,324 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ruxayip
2024-03-21 05:49:40,324 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b00643d3-917f-4468-b360-13b85ef8f3c3
2024-03-21 05:49:42,110 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:42,145 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:42,146 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:42,146 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:42,147 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:42,160 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-21 05:49:42,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98ccb87a-2a15-4287-a8ac-55cbca7bf963
2024-03-21 05:49:42,164 - distributed.worker - INFO - Starting Worker plugin PreImport-a315eeca-bbd9-46a7-af67-71fb06da027a
2024-03-21 05:49:42,166 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:42,173 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44373. Reason: scheduler-close
2024-03-21 05:49:42,174 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53276 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53276 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-03-21 05:49:42,182 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-473fa68d-ee44-496a-9909-87e4b5b248d5
2024-03-21 05:49:42,183 - distributed.worker - INFO - Starting Worker plugin PreImport-2b06097f-8ee7-4ec9-ae8f-0c031cdf2c71
2024-03-21 05:49:42,183 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:42,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37979'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-21 05:49:42,190 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-21 05:49:42,197 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:42,202 - distributed.worker - INFO - Starting Worker plugin PreImport-079556ea-f199-45e0-95bf-8121e5811aac
2024-03-21 05:49:42,203 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:42,214 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4dc57d4-3fc5-4998-96b6-9918645114d0
2024-03-21 05:49:42,215 - distributed.worker - INFO - Starting Worker plugin PreImport-fcecf3fc-f5eb-4dac-8075-5727a6553495
2024-03-21 05:49:42,215 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:42,232 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4a52a9e-ab69-409c-973d-3ceaf3fc39ec
2024-03-21 05:49:42,232 - distributed.worker - INFO - Starting Worker plugin PreImport-b9119bba-7947-4363-85e0-310962c8ad44
2024-03-21 05:49:42,233 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:42,240 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:42,259 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:49:42,261 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34963. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:49:42,261 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:49:42,264 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:42,293 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:42,296 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43335'. Reason: nanny-instantiate-failed
2024-03-21 05:49:42,296 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:49:42,345 - distributed.nanny - INFO - Worker process 45226 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:49:42,349 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45257 parent=45058 started daemon>
2024-03-21 05:49:42,349 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45253 parent=45058 started daemon>
2024-03-21 05:49:42,350 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45250 parent=45058 started daemon>
2024-03-21 05:49:42,350 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45245 parent=45058 started daemon>
2024-03-21 05:49:42,350 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45240 parent=45058 started daemon>
2024-03-21 05:49:42,350 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45235 parent=45058 started daemon>
2024-03-21 05:49:42,350 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45230 parent=45058 started daemon>
2024-03-21 05:49:42,449 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45257 exit status was already read will report exitcode 255
2024-03-21 05:49:42,506 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45230 exit status was already read will report exitcode 255
2024-03-21 05:49:42,602 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45235 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-21 05:49:54,417 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:54,421 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:49:54,424 - distributed.scheduler - INFO - State start
2024-03-21 05:49:54,426 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8hdiswwr', purging
2024-03-21 05:49:54,427 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dxs60fiv', purging
2024-03-21 05:49:54,427 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fed47mqe', purging
2024-03-21 05:49:54,427 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-00ky08r_', purging
2024-03-21 05:49:54,428 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7_dwmtip', purging
2024-03-21 05:49:54,428 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6ruxayip', purging
2024-03-21 05:49:54,428 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qf76qwyp', purging
2024-03-21 05:49:54,449 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:54,450 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:49:54,451 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:49:54,451 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:49:54,471 - distributed.scheduler - INFO - Receive client connection: Client-d67b5565-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:54,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55058
2024-03-21 05:49:54,562 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34617'
2024-03-21 05:49:56,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:56,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:56,185 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:56,186 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35197
2024-03-21 05:49:56,186 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35197
2024-03-21 05:49:56,186 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42981
2024-03-21 05:49:56,186 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:56,186 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:56,187 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:56,187 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:49:56,187 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p1et3zlg
2024-03-21 05:49:56,187 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02683466-b9c7-4f4b-83ac-04720dbef9ff
2024-03-21 05:49:56,187 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fbb5bdb-e831-47d0-a736-41aa091e1414
2024-03-21 05:49:56,487 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:49:56,488 - distributed.worker - INFO - Starting Worker plugin PreImport-618ea635-4054-4e64-8821-b660df3f8433
2024-03-21 05:49:56,488 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35197. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:49:56,488 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:49:56,490 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:56,540 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:56,543 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34617'. Reason: nanny-instantiate-failed
2024-03-21 05:49:56,543 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:49:56,588 - distributed.nanny - INFO - Worker process 45494 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:49:56,590 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:55074'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55074>: Stream is closed
2024-03-21 05:50:02,750 - distributed.scheduler - INFO - Remove client Client-d67b5565-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:50:02,750 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55058; closing.
2024-03-21 05:50:02,750 - distributed.scheduler - INFO - Remove client Client-d67b5565-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:50:02,751 - distributed.scheduler - INFO - Close client connection: Client-d67b5565-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:50:02,752 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:50:02,752 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:50:02,753 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:50:02,754 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:50:02,754 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-21 05:50:05,021 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:05,025 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:50:05,028 - distributed.scheduler - INFO - State start
2024-03-21 05:50:05,050 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:05,051 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:50:05,052 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:50:05,052 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:50:05,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36363'
2024-03-21 05:50:05,474 - distributed.scheduler - INFO - Receive client connection: Client-dcbc82b5-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:50:05,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37644
2024-03-21 05:50:06,683 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:06,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:06,687 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:06,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34867
2024-03-21 05:50:06,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34867
2024-03-21 05:50:06,688 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44703
2024-03-21 05:50:06,688 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:06,688 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:06,688 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:06,688 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:50:06,688 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gjvjapt_
2024-03-21 05:50:06,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7b0e9d1-852f-4cb9-96b6-aae2819ffe0f
2024-03-21 05:50:06,689 - distributed.worker - INFO - Starting Worker plugin PreImport-e10d1c56-7684-4053-bfc7-ba261e82fd78
2024-03-21 05:50:06,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff3ae794-1049-4def-8568-8a9cabbff9a5
2024-03-21 05:50:06,987 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:50:06,987 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34867. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:50:06,988 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:50:06,989 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:07,014 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:07,016 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36363'. Reason: nanny-instantiate-failed
2024-03-21 05:50:07,017 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:50:07,061 - distributed.nanny - INFO - Worker process 45676 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:50:07,062 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:37632'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37632>: Stream is closed
2024-03-21 05:50:15,502 - distributed.scheduler - INFO - Remove client Client-dcbc82b5-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:50:15,502 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37644; closing.
2024-03-21 05:50:15,503 - distributed.scheduler - INFO - Remove client Client-dcbc82b5-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:50:15,503 - distributed.scheduler - INFO - Close client connection: Client-dcbc82b5-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:50:15,504 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:50:15,504 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:50:15,505 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:50:15,506 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:50:15,506 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40019 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] 2024-03-21 05:50:37,484 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:50:37,489 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:37,514 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:37,726 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7faff0d94820>>, <Task finished name='Task-33' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-522' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-534' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40779 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37019 instead
  warnings.warn(
2024-03-21 05:50:51,542 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-4:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42255 instead
  warnings.warn(
2024-03-21 05:51:00,947 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:51:00,952 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:00,988 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:01,615 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f32eada0520>>, <Task finished name='Task-49' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-565' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-577' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40761 instead
  warnings.warn(
2024-03-21 05:51:12,530 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:51:12,534 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:12,544 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:13,026 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f3d015124c0>>, <Task finished name='Task-40' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-624' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-636' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] 2024-03-21 05:51:24,744 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:51:24,749 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:24,758 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:25,255 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7348de64c0>>, <Task finished name='Task-39' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-576' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-588' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] [1711000290.413540] [dgx13:47254:0]            sock.c:481  UCX  ERROR bind(fd=171 addr=0.0.0.0:50097) failed: Address already in use
[1711000295.742698] [dgx13:47348:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:56258) failed: Address already in use
2024-03-21 05:51:37,144 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:51:37,148 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:37,208 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:38,056 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fcab01d02b0>>, <Task finished name='Task-41' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-1100' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-1111' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44383 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41665 instead
  warnings.warn(
2024-03-21 05:51:59,326 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-11:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 37, in _test_local_cluster
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41209 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35869 instead
  warnings.warn(
Process SpawnProcess-16:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40861 instead
  warnings.warn(
Process SpawnProcess-17:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] Process SpawnProcess-18:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41431 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40173 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] [1711000420.906396] [dgx13:50375:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:36930) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] Process SpawnProcess-22:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45409 instead
  warnings.warn(
Process SpawnProcess-23:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-583' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
Process SpawnProcess-24:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37655 instead
  warnings.warn(
2024-03-21 05:54:26,127 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33543 instead
  warnings.warn(
2024-03-21 05:54:28,378 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45909 instead
  warnings.warn(
2024-03-21 05:54:30,414 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] 2024-03-21 05:54:33,549 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] 2024-03-21 05:54:36,682 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34183 instead
  warnings.warn(
2024-03-21 05:54:39,829 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44287 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43771 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43169 instead
  warnings.warn(
Process SpawnProcess-34:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36481 instead
  warnings.warn(
2024-03-21 05:57:16,075 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 05:57:16,083 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:33437'.
2024-03-21 05:57:16,084 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:33437'. Shutting down.
2024-03-21 05:57:16,087 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f17ca162220>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 05:57:17,435 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 05:57:17,441 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:34279'.
2024-03-21 05:57:17,442 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:34279'. Shutting down.
2024-03-21 05:57:17,444 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fbbb7b72220>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 05:57:18,090 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-03-21 05:57:19,447 - distributed.nanny - ERROR - Worker process died unexpectedly
