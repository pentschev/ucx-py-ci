============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-07 05:37:55,840 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:37:55,845 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44351 instead
  warnings.warn(
2023-10-07 05:37:55,849 - distributed.scheduler - INFO - State start
2023-10-07 05:37:55,870 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:37:55,871 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-07 05:37:55,872 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44351/status
2023-10-07 05:37:55,872 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:37:56,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44925'
2023-10-07 05:37:56,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35003'
2023-10-07 05:37:56,030 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32891'
2023-10-07 05:37:56,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42383'
2023-10-07 05:37:57,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:37:57,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:37:57,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:37:57,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:37:57,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:37:57,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:37:57,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:37:57,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:37:57,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:37:57,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:37:57,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:37:57,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-07 05:37:57,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37341
2023-10-07 05:37:57,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37341
2023-10-07 05:37:57,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46769
2023-10-07 05:37:57,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-07 05:37:57,765 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:57,765 - distributed.worker - INFO -               Threads:                          4
2023-10-07 05:37:57,765 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-07 05:37:57,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-c3f6p3oi
2023-10-07 05:37:57,766 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4d2f3f7-47de-438c-a89b-3d2216152ef7
2023-10-07 05:37:57,766 - distributed.worker - INFO - Starting Worker plugin PreImport-60ae5185-483d-4182-852c-5c83bd6e4513
2023-10-07 05:37:57,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-620d5887-fa83-492d-b47a-9ab5a7825e76
2023-10-07 05:37:57,766 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:57,887 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37341', status: init, memory: 0, processing: 0>
2023-10-07 05:37:57,903 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37341
2023-10-07 05:37:57,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52418
2023-10-07 05:37:57,904 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:37:57,905 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-07 05:37:57,905 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:57,905 - distributed.scheduler - INFO - Receive client connection: Client-a9a26b61-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:37:57,906 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52420
2023-10-07 05:37:57,906 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-07 05:37:59,059 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40691
2023-10-07 05:37:59,059 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43733
2023-10-07 05:37:59,059 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43733
2023-10-07 05:37:59,059 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40691
2023-10-07 05:37:59,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41445
2023-10-07 05:37:59,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37267
2023-10-07 05:37:59,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-07 05:37:59,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-07 05:37:59,060 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,060 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,059 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41031
2023-10-07 05:37:59,060 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41031
2023-10-07 05:37:59,060 - distributed.worker - INFO -               Threads:                          4
2023-10-07 05:37:59,060 - distributed.worker - INFO -               Threads:                          4
2023-10-07 05:37:59,060 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-07 05:37:59,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39535
2023-10-07 05:37:59,060 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qh8vfbzl
2023-10-07 05:37:59,060 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-07 05:37:59,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-07 05:37:59,060 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,060 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-u9sk37lb
2023-10-07 05:37:59,060 - distributed.worker - INFO -               Threads:                          4
2023-10-07 05:37:59,061 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-07 05:37:59,061 - distributed.worker - INFO - Starting Worker plugin PreImport-a31e9ecd-e29e-4d93-ad5d-96cce5374e78
2023-10-07 05:37:59,061 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-tg7a6yeb
2023-10-07 05:37:59,061 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-76aafa6a-4941-41f4-99bd-b54009666ff8
2023-10-07 05:37:59,061 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc997121-d74f-4943-83a0-8382e5afdb43
2023-10-07 05:37:59,061 - distributed.worker - INFO - Starting Worker plugin PreImport-0a037f0c-6dbb-4045-8cb6-19273c39cf68
2023-10-07 05:37:59,061 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85938584-ef8c-4365-8e28-56057473c71e
2023-10-07 05:37:59,061 - distributed.worker - INFO - Starting Worker plugin PreImport-a456e3b6-6192-4085-aba1-32c519569fd6
2023-10-07 05:37:59,061 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfb8476d-48ac-4716-a5d4-0563e5fc41cb
2023-10-07 05:37:59,061 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-36f4f59c-da7a-4933-8ca0-1cf3aa656c50
2023-10-07 05:37:59,061 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,061 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,062 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4f96ef7-ac71-4cbd-ad7c-3ed73f42c915
2023-10-07 05:37:59,062 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,084 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40691', status: init, memory: 0, processing: 0>
2023-10-07 05:37:59,085 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40691
2023-10-07 05:37:59,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52454
2023-10-07 05:37:59,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:37:59,087 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-07 05:37:59,087 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,089 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-07 05:37:59,093 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43733', status: init, memory: 0, processing: 0>
2023-10-07 05:37:59,094 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43733
2023-10-07 05:37:59,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52464
2023-10-07 05:37:59,095 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:37:59,096 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-07 05:37:59,096 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,098 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-07 05:37:59,105 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41031', status: init, memory: 0, processing: 0>
2023-10-07 05:37:59,105 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41031
2023-10-07 05:37:59,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52476
2023-10-07 05:37:59,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:37:59,108 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-07 05:37:59,108 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:37:59,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-07 05:37:59,138 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-07 05:37:59,138 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-07 05:37:59,139 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-07 05:37:59,139 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-07 05:37:59,145 - distributed.scheduler - INFO - Remove client Client-a9a26b61-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:37:59,145 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52420; closing.
2023-10-07 05:37:59,145 - distributed.scheduler - INFO - Remove client Client-a9a26b61-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:37:59,146 - distributed.scheduler - INFO - Close client connection: Client-a9a26b61-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:37:59,147 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44925'. Reason: nanny-close
2023-10-07 05:37:59,147 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:37:59,148 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35003'. Reason: nanny-close
2023-10-07 05:37:59,149 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:37:59,149 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41031. Reason: nanny-close
2023-10-07 05:37:59,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32891'. Reason: nanny-close
2023-10-07 05:37:59,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:37:59,150 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40691. Reason: nanny-close
2023-10-07 05:37:59,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42383'. Reason: nanny-close
2023-10-07 05:37:59,151 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:37:59,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43733. Reason: nanny-close
2023-10-07 05:37:59,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37341. Reason: nanny-close
2023-10-07 05:37:59,152 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52454; closing.
2023-10-07 05:37:59,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-07 05:37:59,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40691', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657079.152356')
2023-10-07 05:37:59,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-07 05:37:59,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-07 05:37:59,153 - distributed.nanny - INFO - Worker closed
2023-10-07 05:37:59,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-07 05:37:59,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52464; closing.
2023-10-07 05:37:59,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52476; closing.
2023-10-07 05:37:59,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43733', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657079.1546226')
2023-10-07 05:37:59,154 - distributed.nanny - INFO - Worker closed
2023-10-07 05:37:59,154 - distributed.nanny - INFO - Worker closed
2023-10-07 05:37:59,155 - distributed.nanny - INFO - Worker closed
2023-10-07 05:37:59,155 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41031', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657079.155033')
2023-10-07 05:37:59,155 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52418; closing.
2023-10-07 05:37:59,155 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37341', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657079.155725')
2023-10-07 05:37:59,155 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:38:00,864 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:38:00,864 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:38:00,865 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:38:00,866 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-07 05:38:00,866 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-07 05:38:02,927 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:02,931 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35351 instead
  warnings.warn(
2023-10-07 05:38:02,935 - distributed.scheduler - INFO - State start
2023-10-07 05:38:02,955 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:02,956 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:38:02,956 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35351/status
2023-10-07 05:38:02,957 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:38:03,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37125'
2023-10-07 05:38:03,271 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43667'
2023-10-07 05:38:03,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35143'
2023-10-07 05:38:03,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43953'
2023-10-07 05:38:03,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38005'
2023-10-07 05:38:03,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46117'
2023-10-07 05:38:03,310 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32869'
2023-10-07 05:38:03,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33957'
2023-10-07 05:38:05,227 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:05,228 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:05,229 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:05,229 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:05,230 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:05,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:05,232 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:05,233 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:05,234 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:05,234 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:05,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:05,238 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:05,242 - distributed.scheduler - INFO - Receive client connection: Client-add8c256-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:05,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:05,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:05,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:05,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:05,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49460
2023-10-07 05:38:05,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:05,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:05,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:05,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:05,260 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:05,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:05,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:05,265 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:08,313 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36315
2023-10-07 05:38:08,314 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36315
2023-10-07 05:38:08,314 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41391
2023-10-07 05:38:08,314 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,314 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,314 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:08,315 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:08,315 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_7xfuko_
2023-10-07 05:38:08,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4ae4df0-587a-4ac5-9c34-d098dfec80b9
2023-10-07 05:38:08,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33357
2023-10-07 05:38:08,323 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33357
2023-10-07 05:38:08,323 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40763
2023-10-07 05:38:08,323 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,323 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,323 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:08,323 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:08,324 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-au9x4a27
2023-10-07 05:38:08,324 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0524c009-0ccb-4bc3-b53a-79fa05631669
2023-10-07 05:38:08,334 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42197
2023-10-07 05:38:08,334 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42197
2023-10-07 05:38:08,334 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39093
2023-10-07 05:38:08,335 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,335 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,335 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:08,335 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:08,335 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ahvc_wio
2023-10-07 05:38:08,335 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a02ddf2-eac7-4392-a053-27e10c603756
2023-10-07 05:38:08,335 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32979
2023-10-07 05:38:08,336 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32979
2023-10-07 05:38:08,336 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45359
2023-10-07 05:38:08,336 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,336 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,336 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:08,337 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:08,337 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j7l8i5rk
2023-10-07 05:38:08,337 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31d70b6c-9ac7-4142-b2dd-b9cc34083134
2023-10-07 05:38:08,368 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44105
2023-10-07 05:38:08,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44105
2023-10-07 05:38:08,369 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43477
2023-10-07 05:38:08,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,369 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,369 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:08,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:08,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jg7hbqcy
2023-10-07 05:38:08,370 - distributed.worker - INFO - Starting Worker plugin PreImport-c50d7dfe-69f2-4717-84e9-20cfece3c14d
2023-10-07 05:38:08,370 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24f9faa7-0ef9-4a3a-8c6f-080ac59c9f85
2023-10-07 05:38:08,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebe97796-41f4-4e4f-89cc-7f5080ecf888
2023-10-07 05:38:08,370 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38269
2023-10-07 05:38:08,371 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38269
2023-10-07 05:38:08,371 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45419
2023-10-07 05:38:08,371 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,371 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,371 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:08,371 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:08,371 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6qiwh8nh
2023-10-07 05:38:08,371 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38849
2023-10-07 05:38:08,372 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38849
2023-10-07 05:38:08,372 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34823
2023-10-07 05:38:08,372 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,372 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,372 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd4fd0fa-d41c-4c55-95f4-f858aa4c6757
2023-10-07 05:38:08,372 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:08,372 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:08,372 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-crxom4xm
2023-10-07 05:38:08,372 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67f97421-7f86-439b-badf-e2eed841639b
2023-10-07 05:38:08,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41121
2023-10-07 05:38:08,374 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41121
2023-10-07 05:38:08,374 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41779
2023-10-07 05:38:08,374 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,374 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,374 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:08,374 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:08,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zzpl7_mu
2023-10-07 05:38:08,375 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64a93a73-7f04-4a59-a5c4-94d03c8e9f6e
2023-10-07 05:38:08,511 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5240400f-fce6-408f-8a68-48e847133db1
2023-10-07 05:38:08,511 - distributed.worker - INFO - Starting Worker plugin PreImport-554d70d7-4da8-4c24-b023-76eeb188a001
2023-10-07 05:38:08,511 - distributed.worker - INFO - Starting Worker plugin PreImport-f64239f6-64c8-40d9-9b40-9d884717cc56
2023-10-07 05:38:08,511 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1575a9ae-e6cd-4248-87e7-f972d9af44da
2023-10-07 05:38:08,511 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,512 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,518 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0608a45e-dab4-421c-b28f-8e49d4ec5058
2023-10-07 05:38:08,518 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d21bd0a6-43d2-46ce-85a9-58db3ffd5fc7
2023-10-07 05:38:08,518 - distributed.worker - INFO - Starting Worker plugin PreImport-b7d43067-0528-4835-a3ed-176ef2013b53
2023-10-07 05:38:08,519 - distributed.worker - INFO - Starting Worker plugin PreImport-291f26f7-43d4-4ae4-be34-451df4244172
2023-10-07 05:38:08,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab78949f-ef61-416b-934e-d38ca8dc7c79
2023-10-07 05:38:08,519 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,519 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,519 - distributed.worker - INFO - Starting Worker plugin PreImport-1a4702be-8009-4abc-9fcc-bab2b7f08a60
2023-10-07 05:38:08,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b831f0c3-cd7c-46a7-9b90-f00643346261
2023-10-07 05:38:08,519 - distributed.worker - INFO - Starting Worker plugin PreImport-3af24c55-a7e7-4af2-ba0b-59b264ee63f1
2023-10-07 05:38:08,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ef3e4bd-d52e-4385-91ff-a5c152edce09
2023-10-07 05:38:08,519 - distributed.worker - INFO - Starting Worker plugin PreImport-26e3e3e6-a7ca-4ca0-9a72-bd359149445e
2023-10-07 05:38:08,519 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,519 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,520 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,524 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,540 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36315', status: init, memory: 0, processing: 0>
2023-10-07 05:38:08,541 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36315
2023-10-07 05:38:08,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49478
2023-10-07 05:38:08,542 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:08,543 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33357', status: init, memory: 0, processing: 0>
2023-10-07 05:38:08,543 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,543 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,543 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33357
2023-10-07 05:38:08,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49476
2023-10-07 05:38:08,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:08,544 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44105', status: init, memory: 0, processing: 0>
2023-10-07 05:38:08,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:08,545 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44105
2023-10-07 05:38:08,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49494
2023-10-07 05:38:08,545 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,545 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:08,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:08,547 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,547 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38269', status: init, memory: 0, processing: 0>
2023-10-07 05:38:08,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:08,549 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38269
2023-10-07 05:38:08,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49500
2023-10-07 05:38:08,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:08,550 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,551 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,552 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:08,555 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32979', status: init, memory: 0, processing: 0>
2023-10-07 05:38:08,556 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32979
2023-10-07 05:38:08,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49514
2023-10-07 05:38:08,556 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38849', status: init, memory: 0, processing: 0>
2023-10-07 05:38:08,557 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38849
2023-10-07 05:38:08,557 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49528
2023-10-07 05:38:08,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:08,558 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41121', status: init, memory: 0, processing: 0>
2023-10-07 05:38:08,558 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41121
2023-10-07 05:38:08,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49532
2023-10-07 05:38:08,558 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,558 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:08,559 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42197', status: init, memory: 0, processing: 0>
2023-10-07 05:38:08,559 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,560 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,560 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42197
2023-10-07 05:38:08,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49510
2023-10-07 05:38:08,560 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:08,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:08,561 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,561 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,561 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:08,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:08,562 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:08,562 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:08,563 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:08,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:08,638 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:08,638 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:08,638 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:08,638 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:08,639 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:08,639 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:08,639 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:08,639 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:08,643 - distributed.scheduler - INFO - Remove client Client-add8c256-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:08,644 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49460; closing.
2023-10-07 05:38:08,644 - distributed.scheduler - INFO - Remove client Client-add8c256-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:08,644 - distributed.scheduler - INFO - Close client connection: Client-add8c256-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:08,645 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37125'. Reason: nanny-close
2023-10-07 05:38:08,646 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:08,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43667'. Reason: nanny-close
2023-10-07 05:38:08,647 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:08,647 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41121. Reason: nanny-close
2023-10-07 05:38:08,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35143'. Reason: nanny-close
2023-10-07 05:38:08,647 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:08,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38849. Reason: nanny-close
2023-10-07 05:38:08,648 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43953'. Reason: nanny-close
2023-10-07 05:38:08,648 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:08,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33357. Reason: nanny-close
2023-10-07 05:38:08,649 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38005'. Reason: nanny-close
2023-10-07 05:38:08,649 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:08,649 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44105. Reason: nanny-close
2023-10-07 05:38:08,649 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46117'. Reason: nanny-close
2023-10-07 05:38:08,649 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:08,649 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:08,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49532; closing.
2023-10-07 05:38:08,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42197. Reason: nanny-close
2023-10-07 05:38:08,650 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32869'. Reason: nanny-close
2023-10-07 05:38:08,650 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657088.6502588')
2023-10-07 05:38:08,650 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:08,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:08,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:08,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32979. Reason: nanny-close
2023-10-07 05:38:08,650 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33957'. Reason: nanny-close
2023-10-07 05:38:08,650 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:08,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:08,651 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38269. Reason: nanny-close
2023-10-07 05:38:08,651 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:08,651 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36315. Reason: nanny-close
2023-10-07 05:38:08,651 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:08,652 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49494; closing.
2023-10-07 05:38:08,652 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49476; closing.
2023-10-07 05:38:08,652 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49528; closing.
2023-10-07 05:38:08,652 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:08,652 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:08,652 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:08,653 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44105', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657088.653138')
2023-10-07 05:38:08,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:08,653 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33357', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657088.6535277')
2023-10-07 05:38:08,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:08,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:08,653 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38849', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657088.6539264')
2023-10-07 05:38:08,654 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49510; closing.
2023-10-07 05:38:08,654 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:08,655 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:08,655 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657088.655386')
2023-10-07 05:38:08,655 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:08,655 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:08,655 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49514; closing.
2023-10-07 05:38:08,655 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49500; closing.
2023-10-07 05:38:08,656 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49478; closing.
2023-10-07 05:38:08,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32979', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657088.6564088')
2023-10-07 05:38:08,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38269', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657088.6567776')
2023-10-07 05:38:08,657 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36315', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657088.6571207')
2023-10-07 05:38:08,657 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:38:10,163 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:38:10,163 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:38:10,164 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:38:10,165 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:38:10,166 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-07 05:38:12,197 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:12,201 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-07 05:38:12,205 - distributed.scheduler - INFO - State start
2023-10-07 05:38:12,225 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:12,226 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:38:12,227 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-07 05:38:12,227 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:38:12,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41257'
2023-10-07 05:38:12,363 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41167'
2023-10-07 05:38:12,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36193'
2023-10-07 05:38:12,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37013'
2023-10-07 05:38:12,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39337'
2023-10-07 05:38:12,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39037'
2023-10-07 05:38:12,408 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44311'
2023-10-07 05:38:12,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38717'
2023-10-07 05:38:12,488 - distributed.scheduler - INFO - Receive client connection: Client-b364d915-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:12,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51776
2023-10-07 05:38:14,240 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:14,241 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:14,241 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:14,242 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:14,245 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:14,246 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:14,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:14,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:14,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:14,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:14,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:14,262 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:14,277 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:14,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:14,278 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:14,278 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:14,278 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:14,278 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:14,282 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:14,282 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:14,282 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:14,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:14,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:14,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:17,148 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44453
2023-10-07 05:38:17,149 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44453
2023-10-07 05:38:17,149 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39285
2023-10-07 05:38:17,149 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,149 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,149 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:17,149 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:17,149 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bjjn65ri
2023-10-07 05:38:17,150 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0362dc04-ed86-4431-842b-02c7cd7dec00
2023-10-07 05:38:17,161 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44175
2023-10-07 05:38:17,161 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44175
2023-10-07 05:38:17,162 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42263
2023-10-07 05:38:17,162 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,162 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,162 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:17,162 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:17,162 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e4xrmgwb
2023-10-07 05:38:17,162 - distributed.worker - INFO - Starting Worker plugin PreImport-df43d2d6-70ab-462d-b2b4-6d372daa4222
2023-10-07 05:38:17,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-263c8f11-b715-412f-9803-5b7390ab0736
2023-10-07 05:38:17,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5be401c-f2bf-4fe3-8a58-7a8af113cde1
2023-10-07 05:38:17,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43735
2023-10-07 05:38:17,172 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43735
2023-10-07 05:38:17,172 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32955
2023-10-07 05:38:17,172 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,172 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,172 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:17,172 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:17,172 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-309ouyak
2023-10-07 05:38:17,173 - distributed.worker - INFO - Starting Worker plugin PreImport-82fcab1c-8a85-45cb-83ef-0a6a8dd9bc84
2023-10-07 05:38:17,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31226eeb-f041-4e1d-86a8-9e622cc757c8
2023-10-07 05:38:17,173 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2ed3d0b-3e49-429b-be11-cf72babb9cdb
2023-10-07 05:38:17,298 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33023
2023-10-07 05:38:17,299 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33023
2023-10-07 05:38:17,299 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33573
2023-10-07 05:38:17,299 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,299 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,300 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:17,300 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:17,300 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0ccjy51u
2023-10-07 05:38:17,300 - distributed.worker - INFO - Starting Worker plugin RMMSetup-672d32bc-752b-4fa7-b5c0-3d0f186f4a5c
2023-10-07 05:38:17,301 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41207
2023-10-07 05:38:17,302 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41207
2023-10-07 05:38:17,301 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42821
2023-10-07 05:38:17,302 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45719
2023-10-07 05:38:17,302 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42821
2023-10-07 05:38:17,302 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,302 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,302 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39697
2023-10-07 05:38:17,302 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,302 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,302 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:17,302 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:17,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d3vhrman
2023-10-07 05:38:17,302 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:17,302 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:17,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3798m_zb
2023-10-07 05:38:17,303 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b26be7a2-0af6-4476-b08f-545444fef51d
2023-10-07 05:38:17,303 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebcd20d8-b396-443d-891a-60ee60812558
2023-10-07 05:38:17,304 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39059
2023-10-07 05:38:17,305 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39059
2023-10-07 05:38:17,305 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38043
2023-10-07 05:38:17,305 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,305 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,305 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:17,305 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:17,305 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_zs87i8g
2023-10-07 05:38:17,306 - distributed.worker - INFO - Starting Worker plugin PreImport-45fc53ab-ddcf-4cc2-a485-f3c52dc3b13c
2023-10-07 05:38:17,306 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a765ef8-ec91-48cf-9bd0-9f617f720c8f
2023-10-07 05:38:17,307 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3e766fa-2780-419e-ae6c-e0a4068890c8
2023-10-07 05:38:17,315 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41665
2023-10-07 05:38:17,316 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41665
2023-10-07 05:38:17,316 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43457
2023-10-07 05:38:17,316 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,317 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,317 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:17,317 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:17,317 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-59nxlq0s
2023-10-07 05:38:17,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a81a32f-892d-47ba-990a-075630eb90b5
2023-10-07 05:38:17,318 - distributed.worker - INFO - Starting Worker plugin PreImport-2a2549af-8707-4413-8d3f-c4bf8cbcca9e
2023-10-07 05:38:17,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b560c632-b857-4b28-a495-d7f829b188bf
2023-10-07 05:38:17,320 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8ffded1-54c8-4c16-9ca1-8aeade8b32c0
2023-10-07 05:38:17,321 - distributed.worker - INFO - Starting Worker plugin PreImport-bb3806f4-46ab-41d9-90ff-137e1f40f229
2023-10-07 05:38:17,322 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,324 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,328 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,338 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-979e9aaa-76d0-4454-a19e-ec38aed095c3
2023-10-07 05:38:17,338 - distributed.worker - INFO - Starting Worker plugin PreImport-d0ef564a-ac0d-489f-875a-fb6cd598c52c
2023-10-07 05:38:17,338 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,338 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2faab232-b301-433d-97a5-f2f3b7476c9e
2023-10-07 05:38:17,338 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c68c41bb-aefb-4605-a35e-08799bb15e79
2023-10-07 05:38:17,338 - distributed.worker - INFO - Starting Worker plugin PreImport-a00b8c86-2be4-417c-9bf9-f6d33feeea08
2023-10-07 05:38:17,339 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,339 - distributed.worker - INFO - Starting Worker plugin PreImport-2b7ae97d-b314-47e1-bbfb-6b62d8212e1a
2023-10-07 05:38:17,339 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,339 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,341 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,347 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44175', status: init, memory: 0, processing: 0>
2023-10-07 05:38:17,349 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44175
2023-10-07 05:38:17,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51814
2023-10-07 05:38:17,350 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:17,351 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,351 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:17,352 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44453', status: init, memory: 0, processing: 0>
2023-10-07 05:38:17,353 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44453
2023-10-07 05:38:17,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51800
2023-10-07 05:38:17,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:17,356 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,356 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:17,361 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33023', status: init, memory: 0, processing: 0>
2023-10-07 05:38:17,362 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33023
2023-10-07 05:38:17,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51838
2023-10-07 05:38:17,363 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42821', status: init, memory: 0, processing: 0>
2023-10-07 05:38:17,363 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:17,364 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42821
2023-10-07 05:38:17,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51840
2023-10-07 05:38:17,364 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,364 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,365 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41665', status: init, memory: 0, processing: 0>
2023-10-07 05:38:17,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:17,365 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41665
2023-10-07 05:38:17,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51864
2023-10-07 05:38:17,366 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,366 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:17,366 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43735', status: init, memory: 0, processing: 0>
2023-10-07 05:38:17,366 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43735
2023-10-07 05:38:17,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51824
2023-10-07 05:38:17,367 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:17,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:17,367 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,367 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:17,369 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:17,370 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,370 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,371 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41207', status: init, memory: 0, processing: 0>
2023-10-07 05:38:17,372 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:17,372 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41207
2023-10-07 05:38:17,372 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51862
2023-10-07 05:38:17,372 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39059', status: init, memory: 0, processing: 0>
2023-10-07 05:38:17,373 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39059
2023-10-07 05:38:17,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51848
2023-10-07 05:38:17,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:17,374 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,374 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:17,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:17,376 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:17,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:17,378 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:17,437 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:17,437 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:17,437 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:17,438 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:17,438 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:17,438 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:17,438 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:17,438 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:17,442 - distributed.scheduler - INFO - Remove client Client-b364d915-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:17,443 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51776; closing.
2023-10-07 05:38:17,443 - distributed.scheduler - INFO - Remove client Client-b364d915-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:17,443 - distributed.scheduler - INFO - Close client connection: Client-b364d915-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:17,444 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41257'. Reason: nanny-close
2023-10-07 05:38:17,444 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:17,445 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41167'. Reason: nanny-close
2023-10-07 05:38:17,446 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:17,446 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39059. Reason: nanny-close
2023-10-07 05:38:17,446 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36193'. Reason: nanny-close
2023-10-07 05:38:17,446 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:17,447 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37013'. Reason: nanny-close
2023-10-07 05:38:17,447 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41207. Reason: nanny-close
2023-10-07 05:38:17,447 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:17,447 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44175. Reason: nanny-close
2023-10-07 05:38:17,447 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39337'. Reason: nanny-close
2023-10-07 05:38:17,448 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:17,448 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33023. Reason: nanny-close
2023-10-07 05:38:17,448 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39037'. Reason: nanny-close
2023-10-07 05:38:17,448 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:17,448 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43735. Reason: nanny-close
2023-10-07 05:38:17,448 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44311'. Reason: nanny-close
2023-10-07 05:38:17,448 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:17,449 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:17,449 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51848; closing.
2023-10-07 05:38:17,449 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:17,449 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38717'. Reason: nanny-close
2023-10-07 05:38:17,449 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44453. Reason: nanny-close
2023-10-07 05:38:17,449 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39059', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657097.4495301')
2023-10-07 05:38:17,449 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:17,449 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:17,449 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42821. Reason: nanny-close
2023-10-07 05:38:17,449 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:17,450 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51838; closing.
2023-10-07 05:38:17,450 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41665. Reason: nanny-close
2023-10-07 05:38:17,450 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657097.4506052')
2023-10-07 05:38:17,450 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:17,451 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51814; closing.
2023-10-07 05:38:17,451 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:17,451 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:17,451 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:17,451 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:17,451 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:17,451 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:17,451 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44175', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657097.4519324')
2023-10-07 05:38:17,452 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51862; closing.
2023-10-07 05:38:17,452 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:17,453 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:17,453 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:17,453 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:17,454 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:17,452 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51838>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-07 05:38:17,454 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41207', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657097.4547124')
2023-10-07 05:38:17,455 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51824; closing.
2023-10-07 05:38:17,455 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43735', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657097.455784')
2023-10-07 05:38:17,456 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51800; closing.
2023-10-07 05:38:17,456 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51840; closing.
2023-10-07 05:38:17,456 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51864; closing.
2023-10-07 05:38:17,457 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657097.4569354')
2023-10-07 05:38:17,457 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42821', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657097.4573543')
2023-10-07 05:38:17,457 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41665', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657097.4577668')
2023-10-07 05:38:17,458 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:38:18,961 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:38:18,962 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:38:18,962 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:38:18,963 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:38:18,964 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-07 05:38:20,990 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:20,996 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-07 05:38:21,000 - distributed.scheduler - INFO - State start
2023-10-07 05:38:21,026 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:21,028 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:38:21,029 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-07 05:38:21,029 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:38:21,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33249'
2023-10-07 05:38:21,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41923'
2023-10-07 05:38:21,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35509'
2023-10-07 05:38:21,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36469'
2023-10-07 05:38:21,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46657'
2023-10-07 05:38:21,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35379'
2023-10-07 05:38:21,175 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32925'
2023-10-07 05:38:21,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41759'
2023-10-07 05:38:22,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:22,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:22,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:22,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:22,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:22,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:22,976 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:22,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:22,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:22,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:22,980 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:22,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:23,016 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:23,016 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:23,021 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:23,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:23,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:23,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:23,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:23,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:23,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:23,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:23,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:23,033 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:23,037 - distributed.scheduler - INFO - Receive client connection: Client-b89a7f7e-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:23,052 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49058
2023-10-07 05:38:26,011 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41463
2023-10-07 05:38:26,012 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41463
2023-10-07 05:38:26,012 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33889
2023-10-07 05:38:26,012 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,012 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,012 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:26,012 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:26,012 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6a4sft0t
2023-10-07 05:38:26,013 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02977bc5-1d39-42ad-a3f9-f0f7913e9119
2023-10-07 05:38:26,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34429
2023-10-07 05:38:26,041 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34429
2023-10-07 05:38:26,041 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40057
2023-10-07 05:38:26,041 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,041 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,041 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:26,042 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:26,042 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-934cms4x
2023-10-07 05:38:26,042 - distributed.worker - INFO - Starting Worker plugin RMMSetup-429e1301-d157-469e-b5b3-82f093e6c93e
2023-10-07 05:38:26,164 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39759
2023-10-07 05:38:26,165 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39759
2023-10-07 05:38:26,165 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44963
2023-10-07 05:38:26,165 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,165 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,165 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:26,166 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:26,166 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0cugogl8
2023-10-07 05:38:26,166 - distributed.worker - INFO - Starting Worker plugin RMMSetup-237ad914-33da-40e3-b621-11617d7e2f7e
2023-10-07 05:38:26,173 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46123
2023-10-07 05:38:26,174 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46123
2023-10-07 05:38:26,174 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33537
2023-10-07 05:38:26,174 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,174 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,175 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:26,175 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:26,175 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-umt0dv3e
2023-10-07 05:38:26,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58963fcb-f69e-456c-96e3-952ce0158bb9
2023-10-07 05:38:26,182 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39429
2023-10-07 05:38:26,183 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39429
2023-10-07 05:38:26,183 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33011
2023-10-07 05:38:26,183 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,183 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,183 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:26,184 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:26,184 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-is0j1sjw
2023-10-07 05:38:26,183 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40221
2023-10-07 05:38:26,184 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40221
2023-10-07 05:38:26,184 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40953
2023-10-07 05:38:26,184 - distributed.worker - INFO - Starting Worker plugin PreImport-08cc1063-53f0-4cbd-bef8-cd33ceccecbf
2023-10-07 05:38:26,184 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,184 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,184 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1631d9b6-65cc-4e38-86ff-21c95234f88a
2023-10-07 05:38:26,184 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:26,185 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:26,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c69c343-74f4-4456-8f7e-b2cf49a463c7
2023-10-07 05:38:26,185 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v1he20u4
2023-10-07 05:38:26,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae68f69e-fab6-4d04-8ff3-d3f2575e7f20
2023-10-07 05:38:26,184 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39347
2023-10-07 05:38:26,186 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39347
2023-10-07 05:38:26,186 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34391
2023-10-07 05:38:26,187 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,187 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,187 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:26,187 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:26,187 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zpgv1cdg
2023-10-07 05:38:26,188 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14a77fe3-66ee-40ea-ac06-4d7d18d97bbc
2023-10-07 05:38:26,190 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35619
2023-10-07 05:38:26,191 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35619
2023-10-07 05:38:26,191 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39085
2023-10-07 05:38:26,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,191 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,191 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:26,191 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:26,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ia3j1_4w
2023-10-07 05:38:26,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebe1b86d-a214-453a-9fe1-7f9bf2ac55f2
2023-10-07 05:38:26,294 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a2a43f7e-7e85-4a20-a2e8-d06c2e032a60
2023-10-07 05:38:26,294 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53c1b8f5-2968-4fcf-ae75-10b920c93cba
2023-10-07 05:38:26,294 - distributed.worker - INFO - Starting Worker plugin PreImport-ccb8f68c-cfdd-4b5a-9e52-c8b928eaf44c
2023-10-07 05:38:26,295 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,295 - distributed.worker - INFO - Starting Worker plugin PreImport-1a1e2209-96a4-492a-88c2-65fe671e4104
2023-10-07 05:38:26,295 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,329 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41463', status: init, memory: 0, processing: 0>
2023-10-07 05:38:26,331 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41463
2023-10-07 05:38:26,331 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49072
2023-10-07 05:38:26,332 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34429', status: init, memory: 0, processing: 0>
2023-10-07 05:38:26,332 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:26,332 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34429
2023-10-07 05:38:26,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49080
2023-10-07 05:38:26,332 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,332 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:26,334 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:26,335 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,335 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:26,380 - distributed.worker - INFO - Starting Worker plugin PreImport-8cc48b44-e301-4dc1-bf20-3075a22c6d12
2023-10-07 05:38:26,380 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5df3bd49-2f1e-41b7-9b77-d436465fd71d
2023-10-07 05:38:26,381 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,413 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39759', status: init, memory: 0, processing: 0>
2023-10-07 05:38:26,413 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39759
2023-10-07 05:38:26,414 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49084
2023-10-07 05:38:26,415 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:26,416 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,416 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:26,426 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,434 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11744898-9d60-4ba5-94e9-ce7e90c0b883
2023-10-07 05:38:26,434 - distributed.worker - INFO - Starting Worker plugin PreImport-190eb96e-ee7d-46a0-8a93-7fefd6fe9d2a
2023-10-07 05:38:26,435 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,436 - distributed.worker - INFO - Starting Worker plugin PreImport-c6a70974-710e-41eb-a8b5-4f6e28c81706
2023-10-07 05:38:26,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d346a6e-ec2b-496f-abe4-9b83d9b710b6
2023-10-07 05:38:26,436 - distributed.worker - INFO - Starting Worker plugin PreImport-59f85e86-7b6a-4c8d-b451-d974986f9ebc
2023-10-07 05:38:26,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc68bb11-b4a7-44df-a80f-3eb20591b1bb
2023-10-07 05:38:26,436 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,437 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ad8fd4b-b564-43ca-b73f-1f29fdfa187d
2023-10-07 05:38:26,437 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,437 - distributed.worker - INFO - Starting Worker plugin PreImport-8495c843-0ed0-41c4-b404-eb76b8b8d1c7
2023-10-07 05:38:26,438 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,449 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39429', status: init, memory: 0, processing: 0>
2023-10-07 05:38:26,450 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39429
2023-10-07 05:38:26,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49098
2023-10-07 05:38:26,451 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:26,452 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,452 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:26,461 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40221', status: init, memory: 0, processing: 0>
2023-10-07 05:38:26,462 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40221
2023-10-07 05:38:26,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49108
2023-10-07 05:38:26,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:26,464 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,464 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:26,466 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46123', status: init, memory: 0, processing: 0>
2023-10-07 05:38:26,467 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46123
2023-10-07 05:38:26,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49106
2023-10-07 05:38:26,468 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39347', status: init, memory: 0, processing: 0>
2023-10-07 05:38:26,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39347
2023-10-07 05:38:26,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49124
2023-10-07 05:38:26,468 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:26,469 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35619', status: init, memory: 0, processing: 0>
2023-10-07 05:38:26,469 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,469 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,469 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35619
2023-10-07 05:38:26,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49134
2023-10-07 05:38:26,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:26,470 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,470 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:26,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:26,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:26,472 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:26,472 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:26,474 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:26,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:26,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:26,509 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:26,510 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:26,510 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:26,510 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:26,510 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:26,510 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:26,520 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:26,520 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:26,520 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:26,520 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:26,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:26,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:26,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:26,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:26,527 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:26,528 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:26,532 - distributed.scheduler - INFO - Remove client Client-b89a7f7e-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:26,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49058; closing.
2023-10-07 05:38:26,532 - distributed.scheduler - INFO - Remove client Client-b89a7f7e-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:26,533 - distributed.scheduler - INFO - Close client connection: Client-b89a7f7e-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:26,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33249'. Reason: nanny-close
2023-10-07 05:38:26,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:26,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41923'. Reason: nanny-close
2023-10-07 05:38:26,535 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:26,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35509'. Reason: nanny-close
2023-10-07 05:38:26,536 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35619. Reason: nanny-close
2023-10-07 05:38:26,536 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:26,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36469'. Reason: nanny-close
2023-10-07 05:38:26,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39759. Reason: nanny-close
2023-10-07 05:38:26,537 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:26,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39347. Reason: nanny-close
2023-10-07 05:38:26,537 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46657'. Reason: nanny-close
2023-10-07 05:38:26,537 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:26,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39429. Reason: nanny-close
2023-10-07 05:38:26,538 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35379'. Reason: nanny-close
2023-10-07 05:38:26,538 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:26,538 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32925'. Reason: nanny-close
2023-10-07 05:38:26,538 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46123. Reason: nanny-close
2023-10-07 05:38:26,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:26,539 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:26,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:26,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49134; closing.
2023-10-07 05:38:26,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41759'. Reason: nanny-close
2023-10-07 05:38:26,539 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34429. Reason: nanny-close
2023-10-07 05:38:26,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657106.5394602')
2023-10-07 05:38:26,539 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:26,539 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41463. Reason: nanny-close
2023-10-07 05:38:26,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:26,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:26,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49124; closing.
2023-10-07 05:38:26,540 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40221. Reason: nanny-close
2023-10-07 05:38:26,540 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39347', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657106.5404458')
2023-10-07 05:38:26,540 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:26,540 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:26,541 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:26,541 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:26,541 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:26,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49098; closing.
2023-10-07 05:38:26,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49084; closing.
2023-10-07 05:38:26,541 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:26,542 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:26,542 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:26,542 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:26,543 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:26,542 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49124>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-07 05:38:26,544 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:26,544 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39429', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657106.5439923')
2023-10-07 05:38:26,544 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:26,544 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657106.544458')
2023-10-07 05:38:26,544 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49106; closing.
2023-10-07 05:38:26,545 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46123', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657106.5454922')
2023-10-07 05:38:26,545 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49072; closing.
2023-10-07 05:38:26,546 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49080; closing.
2023-10-07 05:38:26,546 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41463', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657106.5467842')
2023-10-07 05:38:26,547 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34429', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657106.5471315')
2023-10-07 05:38:26,547 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49108; closing.
2023-10-07 05:38:26,548 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657106.5479467')
2023-10-07 05:38:26,548 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:38:26,548 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49108>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-07 05:38:28,051 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:38:28,051 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:38:28,052 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:38:28,053 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:38:28,054 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-07 05:38:30,058 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:30,062 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-07 05:38:30,066 - distributed.scheduler - INFO - State start
2023-10-07 05:38:30,102 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:30,103 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:38:30,104 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-07 05:38:30,104 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:38:30,247 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38971'
2023-10-07 05:38:30,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39311'
2023-10-07 05:38:30,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42909'
2023-10-07 05:38:30,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40631'
2023-10-07 05:38:30,286 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41373'
2023-10-07 05:38:30,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45439'
2023-10-07 05:38:30,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35479'
2023-10-07 05:38:30,310 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44897'
2023-10-07 05:38:31,908 - distributed.scheduler - INFO - Receive client connection: Client-be080d8e-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:31,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44202
2023-10-07 05:38:32,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:32,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:32,166 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:32,170 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:32,170 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:32,170 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:32,171 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:32,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:32,174 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:32,174 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:32,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:32,174 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:32,175 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:32,177 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:32,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:32,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:32,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:32,202 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:32,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:32,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:32,208 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:32,216 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:32,216 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:32,220 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:37,362 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42325
2023-10-07 05:38:37,362 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42325
2023-10-07 05:38:37,363 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44229
2023-10-07 05:38:37,363 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,363 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,363 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:37,363 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:37,363 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mrkjuov3
2023-10-07 05:38:37,363 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d660e2a-9079-4639-a688-7ab3fd239300
2023-10-07 05:38:37,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37489
2023-10-07 05:38:37,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37489
2023-10-07 05:38:37,373 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42247
2023-10-07 05:38:37,373 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,374 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,374 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:37,374 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:37,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6szsb5m6
2023-10-07 05:38:37,374 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e88ed57b-47f8-4a3b-a501-ef6a93fd86dc
2023-10-07 05:38:37,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45325
2023-10-07 05:38:37,458 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45325
2023-10-07 05:38:37,458 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45727
2023-10-07 05:38:37,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,458 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,459 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:37,459 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:37,459 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ridm9dao
2023-10-07 05:38:37,459 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c044709-4e5d-442f-9f3c-2aa59cc465ae
2023-10-07 05:38:37,474 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45133
2023-10-07 05:38:37,475 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45133
2023-10-07 05:38:37,475 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45911
2023-10-07 05:38:37,475 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,475 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,475 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:37,475 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:37,475 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v0odcgwf
2023-10-07 05:38:37,476 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21494204-0967-4c89-a353-c44b2c9e47da
2023-10-07 05:38:37,480 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46717
2023-10-07 05:38:37,481 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46717
2023-10-07 05:38:37,481 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43317
2023-10-07 05:38:37,481 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,481 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,481 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:37,481 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:37,481 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dcb6f4gx
2023-10-07 05:38:37,482 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e22fcbda-e8f9-4fc7-b2e9-d1a4bec37d77
2023-10-07 05:38:37,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38137
2023-10-07 05:38:37,507 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38137
2023-10-07 05:38:37,507 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35795
2023-10-07 05:38:37,507 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,507 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,507 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:37,507 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:37,508 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5gel115p
2023-10-07 05:38:37,508 - distributed.worker - INFO - Starting Worker plugin PreImport-7e7f3a0e-fe6c-4231-bab4-fc66fa0029f3
2023-10-07 05:38:37,508 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a2f91738-b15a-41e2-820b-31a4b14f513d
2023-10-07 05:38:37,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ae056ec-fd78-45cc-aa67-ac7119d87f0f
2023-10-07 05:38:37,509 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41151
2023-10-07 05:38:37,510 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41151
2023-10-07 05:38:37,510 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39169
2023-10-07 05:38:37,510 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,510 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,510 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:37,511 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:37,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tepmekp4
2023-10-07 05:38:37,511 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e68cb7e-accc-4619-bac6-31e93561e6b7
2023-10-07 05:38:37,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38751
2023-10-07 05:38:37,513 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38751
2023-10-07 05:38:37,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40229
2023-10-07 05:38:37,514 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,514 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,514 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:37,514 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:37,514 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xcjnpjlf
2023-10-07 05:38:37,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c78fe31d-4b9c-423d-9b8d-73ffe91590ba
2023-10-07 05:38:37,876 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3b230bc-9567-499a-a90d-1bc626030480
2023-10-07 05:38:37,876 - distributed.worker - INFO - Starting Worker plugin PreImport-ee353f83-7408-42e8-9e0b-619a4d452ced
2023-10-07 05:38:37,876 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,890 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da9e66cf-937f-43ad-9768-4da78b9fcbb7
2023-10-07 05:38:37,890 - distributed.worker - INFO - Starting Worker plugin PreImport-8a88f0bb-2015-438b-9ba0-92eeb35f1c87
2023-10-07 05:38:37,891 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,900 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42325', status: init, memory: 0, processing: 0>
2023-10-07 05:38:37,901 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42325
2023-10-07 05:38:37,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44240
2023-10-07 05:38:37,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:37,903 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,903 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:37,915 - distributed.worker - INFO - Starting Worker plugin PreImport-b0e99954-532c-4cdc-97b4-b1e7639789cb
2023-10-07 05:38:37,915 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39ec7e13-b493-45cc-b6ec-7fd8e7976ea6
2023-10-07 05:38:37,915 - distributed.worker - INFO - Starting Worker plugin PreImport-b7ee5c25-5446-406e-941e-0aa4ee339893
2023-10-07 05:38:37,915 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a83a29c8-dcfb-4dc9-9d49-c4d9f8a8099c
2023-10-07 05:38:37,916 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,917 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,921 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37489', status: init, memory: 0, processing: 0>
2023-10-07 05:38:37,922 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37489
2023-10-07 05:38:37,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44250
2023-10-07 05:38:37,922 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-51a00a41-ee79-498a-91d5-d754f909f942
2023-10-07 05:38:37,922 - distributed.worker - INFO - Starting Worker plugin PreImport-13b21463-22d1-460a-88c5-9556538f7ece
2023-10-07 05:38:37,922 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:37,924 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,924 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,924 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1aafe6c8-7c49-4da8-87ed-6f6f525e6b92
2023-10-07 05:38:37,925 - distributed.worker - INFO - Starting Worker plugin PreImport-d6ab0955-fd14-46ac-8769-102d7b3fdae0
2023-10-07 05:38:37,925 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e32217dd-3452-471d-a33f-2073a3a31624
2023-10-07 05:38:37,925 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,925 - distributed.worker - INFO - Starting Worker plugin PreImport-f1dc7490-fc22-484f-bfca-cdc8dff91d36
2023-10-07 05:38:37,925 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,926 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:37,948 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45133', status: init, memory: 0, processing: 0>
2023-10-07 05:38:37,948 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45133
2023-10-07 05:38:37,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44276
2023-10-07 05:38:37,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:37,950 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38137', status: init, memory: 0, processing: 0>
2023-10-07 05:38:37,950 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,950 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,951 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38137
2023-10-07 05:38:37,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44296
2023-10-07 05:38:37,952 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38751', status: init, memory: 0, processing: 0>
2023-10-07 05:38:37,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:37,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:37,952 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38751
2023-10-07 05:38:37,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44292
2023-10-07 05:38:37,953 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,953 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:37,954 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,954 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:37,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:37,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46717', status: init, memory: 0, processing: 0>
2023-10-07 05:38:37,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46717
2023-10-07 05:38:37,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44254
2023-10-07 05:38:37,958 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45325', status: init, memory: 0, processing: 0>
2023-10-07 05:38:37,959 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45325
2023-10-07 05:38:37,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44264
2023-10-07 05:38:37,959 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:37,960 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,960 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,960 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41151', status: init, memory: 0, processing: 0>
2023-10-07 05:38:37,960 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41151
2023-10-07 05:38:37,960 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:37,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44310
2023-10-07 05:38:37,961 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,961 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,962 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:37,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:37,963 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:37,963 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:37,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:37,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:38,007 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,007 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,007 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,007 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,008 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,008 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,008 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,008 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,019 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:38,019 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:38,019 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:38,020 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:38,020 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:38,020 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:38,020 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:38,020 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:38:38,026 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,028 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:38,031 - distributed.scheduler - INFO - Remove client Client-be080d8e-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:38,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44202; closing.
2023-10-07 05:38:38,031 - distributed.scheduler - INFO - Remove client Client-be080d8e-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:38,031 - distributed.scheduler - INFO - Close client connection: Client-be080d8e-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:38,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38971'. Reason: nanny-close
2023-10-07 05:38:38,033 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:38,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39311'. Reason: nanny-close
2023-10-07 05:38:38,034 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:38,034 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46717. Reason: nanny-close
2023-10-07 05:38:38,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42909'. Reason: nanny-close
2023-10-07 05:38:38,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:38,035 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45325. Reason: nanny-close
2023-10-07 05:38:38,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40631'. Reason: nanny-close
2023-10-07 05:38:38,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:38,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45133. Reason: nanny-close
2023-10-07 05:38:38,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41373'. Reason: nanny-close
2023-10-07 05:38:38,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:38,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38137. Reason: nanny-close
2023-10-07 05:38:38,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45439'. Reason: nanny-close
2023-10-07 05:38:38,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:38,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:38,037 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44254; closing.
2023-10-07 05:38:38,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37489. Reason: nanny-close
2023-10-07 05:38:38,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35479'. Reason: nanny-close
2023-10-07 05:38:38,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46717', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657118.0376883')
2023-10-07 05:38:38,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:38,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:38,038 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41151. Reason: nanny-close
2023-10-07 05:38:38,038 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44897'. Reason: nanny-close
2023-10-07 05:38:38,038 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:38,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:38,038 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42325. Reason: nanny-close
2023-10-07 05:38:38,039 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:38,039 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38751. Reason: nanny-close
2023-10-07 05:38:38,039 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44264; closing.
2023-10-07 05:38:38,039 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:38,040 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:38,040 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44296; closing.
2023-10-07 05:38:38,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:38,040 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45325', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657118.0403326')
2023-10-07 05:38:38,040 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44276; closing.
2023-10-07 05:38:38,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:38,040 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:38,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:38,041 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:38,041 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:38,041 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657118.0411584')
2023-10-07 05:38:38,041 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45133', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657118.0415566')
2023-10-07 05:38:38,041 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44250; closing.
2023-10-07 05:38:38,042 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:38,042 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:38,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37489', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657118.0425165')
2023-10-07 05:38:38,042 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:38,042 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:38,042 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44310; closing.
2023-10-07 05:38:38,043 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44240; closing.
2023-10-07 05:38:38,043 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41151', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657118.0435102')
2023-10-07 05:38:38,044 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42325', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657118.0439465')
2023-10-07 05:38:38,044 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44292; closing.
2023-10-07 05:38:38,044 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38751', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657118.0446582')
2023-10-07 05:38:38,044 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:38:39,600 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:38:39,600 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:38:39,601 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:38:39,602 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:38:39,602 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-07 05:38:41,707 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:41,711 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46715 instead
  warnings.warn(
2023-10-07 05:38:41,715 - distributed.scheduler - INFO - State start
2023-10-07 05:38:41,838 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:41,839 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:38:41,840 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46715/status
2023-10-07 05:38:41,840 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:38:42,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44645'
2023-10-07 05:38:42,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40907'
2023-10-07 05:38:42,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43989'
2023-10-07 05:38:42,081 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44681'
2023-10-07 05:38:42,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34523'
2023-10-07 05:38:42,091 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34911'
2023-10-07 05:38:42,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45117'
2023-10-07 05:38:42,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45687'
2023-10-07 05:38:42,167 - distributed.scheduler - INFO - Receive client connection: Client-c4f1cfef-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:42,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35072
2023-10-07 05:38:44,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:44,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:44,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:44,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:44,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:44,061 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:44,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:44,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:44,062 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:44,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:44,064 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:44,064 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:44,065 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:44,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:44,068 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:44,068 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:44,068 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:44,071 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:44,071 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:44,072 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:44,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:44,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:44,075 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:44,079 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:47,508 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39901
2023-10-07 05:38:47,509 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39901
2023-10-07 05:38:47,509 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35575
2023-10-07 05:38:47,509 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,509 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,509 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:47,509 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:47,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4pxy544o
2023-10-07 05:38:47,510 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a818831c-a9ba-490e-92d9-1c1093c58eb1
2023-10-07 05:38:47,509 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39249
2023-10-07 05:38:47,510 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39249
2023-10-07 05:38:47,510 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37259
2023-10-07 05:38:47,510 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,510 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,510 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:47,510 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:47,510 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-amb1zfet
2023-10-07 05:38:47,511 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffbfe354-2432-4f18-a183-11443b56a2ff
2023-10-07 05:38:47,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39573
2023-10-07 05:38:47,520 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39573
2023-10-07 05:38:47,520 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34909
2023-10-07 05:38:47,520 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,520 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,520 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:47,521 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:47,521 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l_jrumkv
2023-10-07 05:38:47,521 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3028950-987a-4498-a156-af1389367c99
2023-10-07 05:38:47,529 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46769
2023-10-07 05:38:47,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46769
2023-10-07 05:38:47,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42441
2023-10-07 05:38:47,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,530 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,530 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:47,529 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39039
2023-10-07 05:38:47,530 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:47,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39039
2023-10-07 05:38:47,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-thfo8oy7
2023-10-07 05:38:47,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37859
2023-10-07 05:38:47,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,530 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,530 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:47,531 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad1a0b37-4f33-482f-8c08-82c251a5e63f
2023-10-07 05:38:47,531 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:47,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-it7nt44w
2023-10-07 05:38:47,531 - distributed.worker - INFO - Starting Worker plugin PreImport-1b0eaa97-7887-4a7d-8c89-bc0a72f4f039
2023-10-07 05:38:47,531 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f0169b6-f3f6-4870-8e10-3ad14dea16bb
2023-10-07 05:38:47,531 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d05284bc-3a06-4808-8303-379a052b6539
2023-10-07 05:38:47,536 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38685
2023-10-07 05:38:47,536 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38685
2023-10-07 05:38:47,536 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38613
2023-10-07 05:38:47,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,536 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45765
2023-10-07 05:38:47,537 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,537 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45765
2023-10-07 05:38:47,537 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40983
2023-10-07 05:38:47,537 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:47,537 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,536 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33991
2023-10-07 05:38:47,537 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:47,537 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,537 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-593vwmzq
2023-10-07 05:38:47,537 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33991
2023-10-07 05:38:47,537 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39377
2023-10-07 05:38:47,537 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:47,537 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,537 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:47,537 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,537 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gnkdzesq
2023-10-07 05:38:47,537 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:47,537 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:38:47,537 - distributed.worker - INFO - Starting Worker plugin PreImport-0ae023b4-31b1-4967-8801-de1039ac9be0
2023-10-07 05:38:47,537 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dvidddh5
2023-10-07 05:38:47,537 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d95d320-5466-4140-bc8a-9f449a2f5424
2023-10-07 05:38:47,537 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e838cbbc-496b-485c-a706-5c2ce6515317
2023-10-07 05:38:47,538 - distributed.worker - INFO - Starting Worker plugin RMMSetup-875aa0df-6853-4311-99c5-9fdbf693d6e1
2023-10-07 05:38:47,538 - distributed.worker - INFO - Starting Worker plugin RMMSetup-615f57e5-2051-40a6-bfd4-b1ce91f9fca5
2023-10-07 05:38:47,538 - distributed.worker - INFO - Starting Worker plugin PreImport-f5162de3-4ff6-4f28-80a8-1f66ae0cb5d6
2023-10-07 05:38:47,539 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59daf8d6-be52-45c3-b654-d0fa593421d8
2023-10-07 05:38:47,842 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,843 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,847 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15a0e230-5b9e-44b2-82d2-fedae9879fe4
2023-10-07 05:38:47,847 - distributed.worker - INFO - Starting Worker plugin PreImport-cbbaa536-a454-4fba-8d13-4bcd6207ff5c
2023-10-07 05:38:47,847 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,860 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-74a34f02-38f1-43dd-b4a4-0e0677230fe8
2023-10-07 05:38:47,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5fe85698-35ce-4c79-9c2a-feed64a0c424
2023-10-07 05:38:47,861 - distributed.worker - INFO - Starting Worker plugin PreImport-7b725c3c-ad89-4ffb-9ee9-42be9f0f200d
2023-10-07 05:38:47,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34b01d0c-73eb-47d3-b787-c8ae1d97aaf6
2023-10-07 05:38:47,861 - distributed.worker - INFO - Starting Worker plugin PreImport-d37bc52e-183a-4d01-881c-cce4265828ec
2023-10-07 05:38:47,861 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fa805aa-db51-409b-a75e-8c4493e6129c
2023-10-07 05:38:47,861 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,861 - distributed.worker - INFO - Starting Worker plugin PreImport-0920d3df-2e35-40a3-948e-b60ee2b02551
2023-10-07 05:38:47,861 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,863 - distributed.worker - INFO - Starting Worker plugin PreImport-75b9b834-46de-4b83-b633-640605de4faf
2023-10-07 05:38:47,864 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,871 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39573', status: init, memory: 0, processing: 0>
2023-10-07 05:38:47,873 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39573
2023-10-07 05:38:47,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35118
2023-10-07 05:38:47,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:47,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,875 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,875 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45765', status: init, memory: 0, processing: 0>
2023-10-07 05:38:47,876 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45765
2023-10-07 05:38:47,876 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35108
2023-10-07 05:38:47,877 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38685', status: init, memory: 0, processing: 0>
2023-10-07 05:38:47,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:47,877 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:47,877 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38685
2023-10-07 05:38:47,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35094
2023-10-07 05:38:47,878 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,878 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:47,880 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,880 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:47,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:47,885 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39039', status: init, memory: 0, processing: 0>
2023-10-07 05:38:47,886 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39039
2023-10-07 05:38:47,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35130
2023-10-07 05:38:47,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:47,888 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,888 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:47,892 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39901', status: init, memory: 0, processing: 0>
2023-10-07 05:38:47,893 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39901
2023-10-07 05:38:47,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35138
2023-10-07 05:38:47,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39249', status: init, memory: 0, processing: 0>
2023-10-07 05:38:47,894 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:47,895 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39249
2023-10-07 05:38:47,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35136
2023-10-07 05:38:47,895 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,895 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,896 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:47,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33991', status: init, memory: 0, processing: 0>
2023-10-07 05:38:47,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,897 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,897 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33991
2023-10-07 05:38:47,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35162
2023-10-07 05:38:47,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:47,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:47,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:47,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46769', status: init, memory: 0, processing: 0>
2023-10-07 05:38:47,899 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,899 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46769
2023-10-07 05:38:47,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35150
2023-10-07 05:38:47,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:47,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:47,902 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:47,902 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:47,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:48,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:48,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:48,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:48,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:48,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:48,010 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:48,011 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:48,011 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:38:48,015 - distributed.scheduler - INFO - Remove client Client-c4f1cfef-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:48,015 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35072; closing.
2023-10-07 05:38:48,016 - distributed.scheduler - INFO - Remove client Client-c4f1cfef-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:48,016 - distributed.scheduler - INFO - Close client connection: Client-c4f1cfef-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:48,017 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44645'. Reason: nanny-close
2023-10-07 05:38:48,017 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:48,018 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40907'. Reason: nanny-close
2023-10-07 05:38:48,018 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:48,019 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45765. Reason: nanny-close
2023-10-07 05:38:48,019 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43989'. Reason: nanny-close
2023-10-07 05:38:48,019 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:48,019 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33991. Reason: nanny-close
2023-10-07 05:38:48,019 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44681'. Reason: nanny-close
2023-10-07 05:38:48,020 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:48,020 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39901. Reason: nanny-close
2023-10-07 05:38:48,020 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34523'. Reason: nanny-close
2023-10-07 05:38:48,020 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:48,020 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39249. Reason: nanny-close
2023-10-07 05:38:48,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34911'. Reason: nanny-close
2023-10-07 05:38:48,021 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:48,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:48,021 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35108; closing.
2023-10-07 05:38:48,021 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46769. Reason: nanny-close
2023-10-07 05:38:48,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45117'. Reason: nanny-close
2023-10-07 05:38:48,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:48,021 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:48,021 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45765', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657128.0218735')
2023-10-07 05:38:48,022 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38685. Reason: nanny-close
2023-10-07 05:38:48,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45687'. Reason: nanny-close
2023-10-07 05:38:48,022 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:48,022 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39039. Reason: nanny-close
2023-10-07 05:38:48,022 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:48,022 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:48,023 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:48,023 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39573. Reason: nanny-close
2023-10-07 05:38:48,023 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35138; closing.
2023-10-07 05:38:48,023 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:48,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:48,024 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:48,024 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:48,024 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:48,024 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:48,025 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:48,025 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:48,025 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39901', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657128.0252936')
2023-10-07 05:38:48,025 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35136; closing.
2023-10-07 05:38:48,026 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:48,026 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35162; closing.
2023-10-07 05:38:48,026 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:48,026 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:48,027 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35138>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35138>: Stream is closed
2023-10-07 05:38:48,029 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39249', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657128.029212')
2023-10-07 05:38:48,029 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657128.0296063')
2023-10-07 05:38:48,030 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35150; closing.
2023-10-07 05:38:48,031 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46769', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657128.0309894')
2023-10-07 05:38:48,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35094; closing.
2023-10-07 05:38:48,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35130; closing.
2023-10-07 05:38:48,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35118; closing.
2023-10-07 05:38:48,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38685', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657128.0322993')
2023-10-07 05:38:48,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657128.0327072')
2023-10-07 05:38:48,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39573', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657128.0332115')
2023-10-07 05:38:48,033 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:38:49,635 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:38:49,635 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:38:49,636 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:38:49,637 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:38:49,637 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-07 05:38:51,766 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:51,770 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37303 instead
  warnings.warn(
2023-10-07 05:38:51,775 - distributed.scheduler - INFO - State start
2023-10-07 05:38:51,797 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:38:51,798 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:38:51,799 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37303/status
2023-10-07 05:38:51,799 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:38:51,924 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37533'
2023-10-07 05:38:52,420 - distributed.scheduler - INFO - Receive client connection: Client-caf1102b-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:52,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53514
2023-10-07 05:38:53,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:38:53,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:38:53,970 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:38:55,017 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44703
2023-10-07 05:38:55,018 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44703
2023-10-07 05:38:55,018 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-07 05:38:55,018 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:38:55,018 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:55,018 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:38:55,018 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-07 05:38:55,018 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-svmmwa6v
2023-10-07 05:38:55,018 - distributed.worker - INFO - Starting Worker plugin PreImport-313475a8-7690-48ad-948d-33d396665a9d
2023-10-07 05:38:55,019 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57f93896-e581-47ef-a1ce-a7e4d77c25d5
2023-10-07 05:38:55,019 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f5107ac-73f5-43db-8951-f857f2d1867c
2023-10-07 05:38:55,026 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:55,062 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44703', status: init, memory: 0, processing: 0>
2023-10-07 05:38:55,063 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44703
2023-10-07 05:38:55,063 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53532
2023-10-07 05:38:55,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:38:55,065 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:38:55,065 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:38:55,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:38:55,072 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:38:55,075 - distributed.scheduler - INFO - Remove client Client-caf1102b-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:55,075 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53514; closing.
2023-10-07 05:38:55,075 - distributed.scheduler - INFO - Remove client Client-caf1102b-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:55,075 - distributed.scheduler - INFO - Close client connection: Client-caf1102b-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:38:55,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37533'. Reason: nanny-close
2023-10-07 05:38:55,105 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:38:55,106 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44703. Reason: nanny-close
2023-10-07 05:38:55,108 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53532; closing.
2023-10-07 05:38:55,108 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:38:55,109 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44703', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657135.1091385')
2023-10-07 05:38:55,109 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:38:55,110 - distributed.nanny - INFO - Worker closed
2023-10-07 05:38:56,193 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:38:56,193 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:38:56,193 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:38:56,194 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:38:56,195 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-07 05:39:00,068 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:00,073 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-07 05:39:00,076 - distributed.scheduler - INFO - State start
2023-10-07 05:39:00,097 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:00,098 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:39:00,099 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-07 05:39:00,099 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:39:00,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41179'
2023-10-07 05:39:01,044 - distributed.scheduler - INFO - Receive client connection: Client-cfef1159-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:01,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34750
2023-10-07 05:39:01,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:01,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:02,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:03,749 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42241
2023-10-07 05:39:03,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42241
2023-10-07 05:39:03,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46397
2023-10-07 05:39:03,750 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:03,750 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:03,750 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:03,750 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-07 05:39:03,750 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6pdmtbyn
2023-10-07 05:39:03,751 - distributed.worker - INFO - Starting Worker plugin PreImport-e6ada2cd-c346-4c73-bb12-dd2be8862168
2023-10-07 05:39:03,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb504b16-bd2e-4b11-a9a6-9f459712b105
2023-10-07 05:39:03,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85f16d32-28a6-469b-9d07-7c14b8eb2759
2023-10-07 05:39:03,753 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:03,796 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42241', status: init, memory: 0, processing: 0>
2023-10-07 05:39:03,797 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42241
2023-10-07 05:39:03,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34764
2023-10-07 05:39:03,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:03,801 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:03,801 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:03,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:03,860 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:03,863 - distributed.scheduler - INFO - Remove client Client-cfef1159-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:03,863 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34750; closing.
2023-10-07 05:39:03,863 - distributed.scheduler - INFO - Remove client Client-cfef1159-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:03,864 - distributed.scheduler - INFO - Close client connection: Client-cfef1159-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:03,865 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41179'. Reason: nanny-close
2023-10-07 05:39:03,865 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:03,867 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42241. Reason: nanny-close
2023-10-07 05:39:03,869 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34764; closing.
2023-10-07 05:39:03,869 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:03,870 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657143.8699927')
2023-10-07 05:39:03,870 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:39:03,871 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:05,031 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:39:05,031 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:39:05,032 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:39:05,033 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:39:05,033 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-07 05:39:07,142 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:07,147 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-07 05:39:07,150 - distributed.scheduler - INFO - State start
2023-10-07 05:39:07,182 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:07,184 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:39:07,184 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-07 05:39:07,185 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:39:11,163 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:34772'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34772>: Stream is closed
2023-10-07 05:39:11,400 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:39:11,400 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:39:11,401 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:39:11,402 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:39:11,402 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-07 05:39:13,554 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:13,559 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34055 instead
  warnings.warn(
2023-10-07 05:39:13,562 - distributed.scheduler - INFO - State start
2023-10-07 05:39:13,584 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:13,585 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-07 05:39:13,586 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34055/status
2023-10-07 05:39:13,586 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:39:13,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41063'
2023-10-07 05:39:13,962 - distributed.scheduler - INFO - Receive client connection: Client-d7ed0d85-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:13,973 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57990
2023-10-07 05:39:15,225 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:15,225 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:15,229 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:18,257 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41109
2023-10-07 05:39:18,258 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41109
2023-10-07 05:39:18,258 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37373
2023-10-07 05:39:18,259 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-07 05:39:18,259 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:18,259 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:18,259 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-07 05:39:18,259 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-z7yp6fcv
2023-10-07 05:39:18,260 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8cfef5fa-86d6-4a7a-9c05-54cc8dde373f
2023-10-07 05:39:18,260 - distributed.worker - INFO - Starting Worker plugin PreImport-6d2fcedd-2dc4-472e-827e-10feae701617
2023-10-07 05:39:18,260 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-469d2120-3c59-4f6d-9211-3daff16618d2
2023-10-07 05:39:18,261 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:18,296 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41109', status: init, memory: 0, processing: 0>
2023-10-07 05:39:18,297 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41109
2023-10-07 05:39:18,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58022
2023-10-07 05:39:18,299 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:18,300 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-07 05:39:18,300 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:18,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-07 05:39:18,360 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:18,363 - distributed.scheduler - INFO - Remove client Client-d7ed0d85-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:18,363 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57990; closing.
2023-10-07 05:39:18,363 - distributed.scheduler - INFO - Remove client Client-d7ed0d85-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:18,364 - distributed.scheduler - INFO - Close client connection: Client-d7ed0d85-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:18,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41063'. Reason: nanny-close
2023-10-07 05:39:18,365 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:18,366 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41109. Reason: nanny-close
2023-10-07 05:39:18,369 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58022; closing.
2023-10-07 05:39:18,369 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-07 05:39:18,369 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41109', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657158.3696094')
2023-10-07 05:39:18,370 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:39:18,371 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:19,481 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:39:19,482 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:39:19,482 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:39:19,483 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-07 05:39:19,484 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-07 05:39:21,839 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:21,843 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33683 instead
  warnings.warn(
2023-10-07 05:39:21,848 - distributed.scheduler - INFO - State start
2023-10-07 05:39:21,872 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:21,873 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:39:21,874 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33683/status
2023-10-07 05:39:21,874 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:39:21,885 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37001'
2023-10-07 05:39:21,910 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39783'
2023-10-07 05:39:21,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34931'
2023-10-07 05:39:21,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35665'
2023-10-07 05:39:21,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43477'
2023-10-07 05:39:21,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37191'
2023-10-07 05:39:21,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40319'
2023-10-07 05:39:21,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38833'
2023-10-07 05:39:23,523 - distributed.scheduler - INFO - Receive client connection: Client-dcbe0eff-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:23,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59752
2023-10-07 05:39:23,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:23,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:23,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:23,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:23,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:23,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:23,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:23,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:23,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:23,819 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:23,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:23,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:23,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:23,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:23,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:23,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:23,902 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:23,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:23,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:23,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:23,907 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:23,907 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:23,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:23,912 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:27,078 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36485
2023-10-07 05:39:27,078 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36485
2023-10-07 05:39:27,078 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38187
2023-10-07 05:39:27,079 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,079 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,079 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:27,079 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:39:27,079 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oga_831_
2023-10-07 05:39:27,079 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a0dde24-764f-45e8-b4e2-d7c0f5698ca3
2023-10-07 05:39:27,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44107
2023-10-07 05:39:27,107 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44107
2023-10-07 05:39:27,107 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39003
2023-10-07 05:39:27,107 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,107 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,107 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:27,107 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:39:27,107 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v6toc6ow
2023-10-07 05:39:27,108 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4760b713-604e-4e47-a4ac-a02afd399340
2023-10-07 05:39:27,348 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53ca077d-0dab-4079-bbdd-0810c5980c63
2023-10-07 05:39:27,348 - distributed.worker - INFO - Starting Worker plugin PreImport-ab4d26f3-70f6-4fa6-a76d-5c78d398524f
2023-10-07 05:39:27,349 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,364 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63783bf2-8f24-4098-a64c-505b33ba7d9f
2023-10-07 05:39:27,364 - distributed.worker - INFO - Starting Worker plugin PreImport-120569c4-5ffa-4dc6-9a73-922781ebb38b
2023-10-07 05:39:27,365 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,380 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41659
2023-10-07 05:39:27,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41659
2023-10-07 05:39:27,381 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35149
2023-10-07 05:39:27,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,381 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,381 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:27,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:39:27,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b81iqll0
2023-10-07 05:39:27,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f11839c7-360c-4f06-8dc4-e924b428c1d2
2023-10-07 05:39:27,384 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36999
2023-10-07 05:39:27,385 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36999
2023-10-07 05:39:27,385 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34263
2023-10-07 05:39:27,385 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,385 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,385 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:27,386 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:39:27,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ybje2oug
2023-10-07 05:39:27,386 - distributed.worker - INFO - Starting Worker plugin PreImport-9e315f20-d4b9-4105-8949-1e01ec09df80
2023-10-07 05:39:27,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0315b4e5-ad40-4ac7-86ce-5fa55d873634
2023-10-07 05:39:27,386 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e08af527-e934-44bf-9c35-fe1a3040c182
2023-10-07 05:39:27,387 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36485', status: init, memory: 0, processing: 0>
2023-10-07 05:39:27,388 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36485
2023-10-07 05:39:27,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59770
2023-10-07 05:39:27,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:27,390 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,390 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:27,394 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34541
2023-10-07 05:39:27,395 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34541
2023-10-07 05:39:27,395 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42097
2023-10-07 05:39:27,395 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,395 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,395 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:27,395 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:39:27,395 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6k7dj12w
2023-10-07 05:39:27,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6f4b181-576d-4eb3-a785-664f50658f95
2023-10-07 05:39:27,395 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33465
2023-10-07 05:39:27,397 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33465
2023-10-07 05:39:27,397 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39693
2023-10-07 05:39:27,397 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,397 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,397 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:27,398 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:39:27,398 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zrcn09uk
2023-10-07 05:39:27,399 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57c23b1b-3b46-483a-8ebe-d45129c2be5b
2023-10-07 05:39:27,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46361
2023-10-07 05:39:27,399 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46361
2023-10-07 05:39:27,400 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40811
2023-10-07 05:39:27,400 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,400 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,400 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:27,400 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:39:27,400 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dqto4ulr
2023-10-07 05:39:27,400 - distributed.worker - INFO - Starting Worker plugin RMMSetup-189f72f8-b238-4011-8edd-e56da7a80983
2023-10-07 05:39:27,400 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46807
2023-10-07 05:39:27,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46807
2023-10-07 05:39:27,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34779
2023-10-07 05:39:27,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,401 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,401 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:27,401 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-07 05:39:27,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f7155u1w
2023-10-07 05:39:27,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d116ba35-3ad7-4fcf-9206-38ba8a98250b
2023-10-07 05:39:27,402 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44107', status: init, memory: 0, processing: 0>
2023-10-07 05:39:27,403 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44107
2023-10-07 05:39:27,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59776
2023-10-07 05:39:27,404 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:27,405 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,405 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:27,564 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee80b75d-cedf-4fce-ba72-76479eb166ab
2023-10-07 05:39:27,564 - distributed.worker - INFO - Starting Worker plugin PreImport-c495d4f2-2bdd-4c9c-96a2-1fa712a00cee
2023-10-07 05:39:27,565 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,573 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b759c7ef-3166-42db-af10-b3ec0f55a984
2023-10-07 05:39:27,574 - distributed.worker - INFO - Starting Worker plugin PreImport-e48226a4-bfa8-432a-989c-dac5f19c975e
2023-10-07 05:39:27,574 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,574 - distributed.worker - INFO - Starting Worker plugin PreImport-53ef4aa8-c180-430c-a577-814f59b9b284
2023-10-07 05:39:27,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17e2fb1f-99f0-41fe-93d1-a3230ce6dd2e
2023-10-07 05:39:27,574 - distributed.worker - INFO - Starting Worker plugin PreImport-4fa06fb6-ebdb-451f-b095-75ffcae7cf0d
2023-10-07 05:39:27,574 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,574 - distributed.worker - INFO - Starting Worker plugin PreImport-862eacfa-1e82-4ff5-ad15-32b17fd7d4b6
2023-10-07 05:39:27,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da3aa394-dab1-4528-a328-b0023c20961e
2023-10-07 05:39:27,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26ed1bd6-7bdc-4b28-8b68-22d887bcc521
2023-10-07 05:39:27,574 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,574 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,575 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34541', status: init, memory: 0, processing: 0>
2023-10-07 05:39:27,587 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34541
2023-10-07 05:39:27,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59790
2023-10-07 05:39:27,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:27,588 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,588 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:27,595 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36999', status: init, memory: 0, processing: 0>
2023-10-07 05:39:27,595 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36999
2023-10-07 05:39:27,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59794
2023-10-07 05:39:27,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:27,597 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,597 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46361', status: init, memory: 0, processing: 0>
2023-10-07 05:39:27,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46361
2023-10-07 05:39:27,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59810
2023-10-07 05:39:27,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:27,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:27,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,600 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:27,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33465', status: init, memory: 0, processing: 0>
2023-10-07 05:39:27,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33465
2023-10-07 05:39:27,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59816
2023-10-07 05:39:27,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46807', status: init, memory: 0, processing: 0>
2023-10-07 05:39:27,606 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46807
2023-10-07 05:39:27,606 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59830
2023-10-07 05:39:27,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:27,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,607 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,608 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:27,609 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,609 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:27,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:27,615 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41659', status: init, memory: 0, processing: 0>
2023-10-07 05:39:27,615 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41659
2023-10-07 05:39:27,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59840
2023-10-07 05:39:27,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:27,618 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:27,619 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:27,621 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:27,702 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:39:27,702 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:39:27,702 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:39:27,702 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:39:27,702 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:39:27,702 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:39:27,703 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:39:27,703 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-07 05:39:27,715 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:27,715 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:27,715 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:27,715 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:27,716 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:27,716 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:27,716 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:27,716 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:27,720 - distributed.scheduler - INFO - Remove client Client-dcbe0eff-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:27,720 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59752; closing.
2023-10-07 05:39:27,720 - distributed.scheduler - INFO - Remove client Client-dcbe0eff-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:27,721 - distributed.scheduler - INFO - Close client connection: Client-dcbe0eff-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:27,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34931'. Reason: nanny-close
2023-10-07 05:39:27,722 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:27,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38833'. Reason: nanny-close
2023-10-07 05:39:27,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:27,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46807. Reason: nanny-close
2023-10-07 05:39:27,724 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43477'. Reason: nanny-close
2023-10-07 05:39:27,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:27,725 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39783'. Reason: nanny-close
2023-10-07 05:39:27,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41659. Reason: nanny-close
2023-10-07 05:39:27,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:27,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34541. Reason: nanny-close
2023-10-07 05:39:27,725 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37001'. Reason: nanny-close
2023-10-07 05:39:27,725 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:27,726 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36999. Reason: nanny-close
2023-10-07 05:39:27,726 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37191'. Reason: nanny-close
2023-10-07 05:39:27,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:27,726 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:27,726 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59830; closing.
2023-10-07 05:39:27,726 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44107. Reason: nanny-close
2023-10-07 05:39:27,726 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35665'. Reason: nanny-close
2023-10-07 05:39:27,727 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:27,727 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657167.7272618')
2023-10-07 05:39:27,727 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:27,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33465. Reason: nanny-close
2023-10-07 05:39:27,727 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40319'. Reason: nanny-close
2023-10-07 05:39:27,727 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:27,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36485. Reason: nanny-close
2023-10-07 05:39:27,728 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:27,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46361. Reason: nanny-close
2023-10-07 05:39:27,728 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:27,728 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:27,729 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59840; closing.
2023-10-07 05:39:27,729 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:27,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:27,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:27,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:27,730 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:27,730 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:27,730 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657167.7305636')
2023-10-07 05:39:27,730 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:27,731 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:27,731 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59794; closing.
2023-10-07 05:39:27,731 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:27,731 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59790; closing.
2023-10-07 05:39:27,731 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:27,731 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:27,732 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59840>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59840>: Stream is closed
2023-10-07 05:39:27,734 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657167.7348099')
2023-10-07 05:39:27,735 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34541', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657167.7353408')
2023-10-07 05:39:27,735 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59776; closing.
2023-10-07 05:39:27,737 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44107', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657167.736981')
2023-10-07 05:39:27,737 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59770; closing.
2023-10-07 05:39:27,737 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59816; closing.
2023-10-07 05:39:27,737 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59810; closing.
2023-10-07 05:39:27,738 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657167.7382052')
2023-10-07 05:39:27,738 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33465', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657167.738628')
2023-10-07 05:39:27,739 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657167.7391648')
2023-10-07 05:39:27,739 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:39:29,389 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:39:29,390 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:39:29,390 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:39:29,391 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:39:29,392 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-07 05:39:31,607 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:31,611 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41975 instead
  warnings.warn(
2023-10-07 05:39:31,615 - distributed.scheduler - INFO - State start
2023-10-07 05:39:31,639 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:31,641 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:39:31,642 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41975/status
2023-10-07 05:39:31,642 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:39:32,102 - distributed.scheduler - INFO - Receive client connection: Client-e2abc424-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:32,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49724
2023-10-07 05:39:32,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43069'
2023-10-07 05:39:33,799 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:33,799 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:33,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:34,650 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45549
2023-10-07 05:39:34,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45549
2023-10-07 05:39:34,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39625
2023-10-07 05:39:34,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:34,650 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:34,650 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:34,651 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-07 05:39:34,651 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cu11j_wz
2023-10-07 05:39:34,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f19b6482-104b-4695-8ee0-bfdd13df6c63
2023-10-07 05:39:34,745 - distributed.worker - INFO - Starting Worker plugin PreImport-75ff0b09-e7de-4d75-8578-4da9fc2fba03
2023-10-07 05:39:34,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d2d4088c-8053-4db4-829c-b14cccd58675
2023-10-07 05:39:34,746 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:34,782 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45549', status: init, memory: 0, processing: 0>
2023-10-07 05:39:34,783 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45549
2023-10-07 05:39:34,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49748
2023-10-07 05:39:34,785 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:34,786 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:34,786 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:34,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:34,869 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:39:34,874 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:34,876 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:34,878 - distributed.scheduler - INFO - Remove client Client-e2abc424-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:34,878 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49724; closing.
2023-10-07 05:39:34,878 - distributed.scheduler - INFO - Remove client Client-e2abc424-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:34,879 - distributed.scheduler - INFO - Close client connection: Client-e2abc424-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:34,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43069'. Reason: nanny-close
2023-10-07 05:39:34,880 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:34,881 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45549. Reason: nanny-close
2023-10-07 05:39:34,884 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49748; closing.
2023-10-07 05:39:34,884 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:34,885 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45549', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657174.8849854')
2023-10-07 05:39:34,885 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:39:34,886 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:36,196 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:39:36,197 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:39:36,198 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:39:36,199 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:39:36,200 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-07 05:39:38,674 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:38,680 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42261 instead
  warnings.warn(
2023-10-07 05:39:38,684 - distributed.scheduler - INFO - State start
2023-10-07 05:39:38,715 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-07 05:39:38,716 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-07 05:39:38,717 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42261/status
2023-10-07 05:39:38,717 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-07 05:39:38,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44255'
2023-10-07 05:39:39,048 - distributed.scheduler - INFO - Receive client connection: Client-e6bed4ab-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:39,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49900
2023-10-07 05:39:40,468 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-07 05:39:40,468 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-07 05:39:40,472 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-07 05:39:41,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45257
2023-10-07 05:39:41,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45257
2023-10-07 05:39:41,382 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45407
2023-10-07 05:39:41,382 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-07 05:39:41,382 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:41,382 - distributed.worker - INFO -               Threads:                          1
2023-10-07 05:39:41,382 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-07 05:39:41,382 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oh0y3at9
2023-10-07 05:39:41,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6a50fc9-a34d-487f-910c-031f71ddf142
2023-10-07 05:39:41,480 - distributed.worker - INFO - Starting Worker plugin PreImport-fcfaf4e1-c5b8-43f5-82bf-7354df9a7146
2023-10-07 05:39:41,481 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65c86f80-2ce0-4e7e-99e9-b61f1a8f80e2
2023-10-07 05:39:41,481 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:41,517 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45257', status: init, memory: 0, processing: 0>
2023-10-07 05:39:41,518 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45257
2023-10-07 05:39:41,519 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59634
2023-10-07 05:39:41,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-07 05:39:41,521 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-07 05:39:41,521 - distributed.worker - INFO - -------------------------------------------------
2023-10-07 05:39:41,523 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-07 05:39:41,615 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-10-07 05:39:41,620 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-07 05:39:41,625 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:41,626 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-07 05:39:41,629 - distributed.scheduler - INFO - Remove client Client-e6bed4ab-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:41,629 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49900; closing.
2023-10-07 05:39:41,629 - distributed.scheduler - INFO - Remove client Client-e6bed4ab-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:41,629 - distributed.scheduler - INFO - Close client connection: Client-e6bed4ab-64d3-11ee-afbc-d8c49764f6bb
2023-10-07 05:39:41,630 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44255'. Reason: nanny-close
2023-10-07 05:39:41,631 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-07 05:39:41,632 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45257. Reason: nanny-close
2023-10-07 05:39:41,634 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59634; closing.
2023-10-07 05:39:41,634 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-07 05:39:41,635 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45257', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1696657181.6350448')
2023-10-07 05:39:41,635 - distributed.scheduler - INFO - Lost all workers
2023-10-07 05:39:41,637 - distributed.nanny - INFO - Worker closed
2023-10-07 05:39:42,747 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-07 05:39:42,747 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-07 05:39:42,748 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-07 05:39:42,749 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-07 05:39:42,749 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34855 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33479 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33469 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44941 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41649 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38831 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43487 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38843 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36969 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44583 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43343 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39343 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36421 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32851 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32999 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46147 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33303 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44167 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43983 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38521 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42533 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36943 instead
  warnings.warn(
[1696657533.723096] [dgx13:71165:0]            sock.c:470  UCX  ERROR bind(fd=166 addr=0.0.0.0:45343) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39785 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38853 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35753 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37595 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46881 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36535 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45373 instead
  warnings.warn(
[1696657696.757249] [dgx13:73912:0]            sock.c:470  UCX  ERROR bind(fd=127 addr=0.0.0.0:52732) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45013 instead
  warnings.warn(
[1696657711.056362] [dgx13:74120:0]            sock.c:470  UCX  ERROR bind(fd=135 addr=0.0.0.0:41581) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36899 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36161 instead
  warnings.warn(
2023-10-07 05:49:52,385 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-07 05:49:52,605 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-07 05:49:52,632 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:58947'.
2023-10-07 05:49:52,646 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:58947'. Shutting down.
2023-10-07 05:49:52,659 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ff7ed8037c0>>, <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-10-07 05:49:54,662 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
