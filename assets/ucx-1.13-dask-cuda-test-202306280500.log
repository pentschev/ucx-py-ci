============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-06-28 05:38:54,331 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:54,335 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37363 instead
  warnings.warn(
2023-06-28 05:38:54,338 - distributed.scheduler - INFO - State start
2023-06-28 05:38:54,357 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:38:54,358 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-28 05:38:54,359 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37363/status
2023-06-28 05:38:54,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44173'
2023-06-28 05:38:54,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36409'
2023-06-28 05:38:54,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35735'
2023-06-28 05:38:54,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46561'
2023-06-28 05:38:55,724 - distributed.scheduler - INFO - Receive client connection: Client-10ae4f25-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:38:55,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41434
2023-06-28 05:38:55,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:55,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:55,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:55,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:55,978 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:55,984 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-06-28 05:38:55,998 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35669
2023-06-28 05:38:55,998 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35669
2023-06-28 05:38:55,998 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37527
2023-06-28 05:38:55,998 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:38:55,999 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:55,999 - distributed.worker - INFO -               Threads:                          4
2023-06-28 05:38:55,999 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-28 05:38:55,999 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qybreecv
2023-06-28 05:38:55,999 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf6224f7-5e38-475f-a58e-486befdf6587
2023-06-28 05:38:55,999 - distributed.worker - INFO - Starting Worker plugin PreImport-a202c040-ac0f-41b0-ac25-b3420358814f
2023-06-28 05:38:55,999 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dbb0174d-7e60-4e4e-bb46-67db9d66d0fa
2023-06-28 05:38:56,000 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:56,012 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:56,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:56,017 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35669', status: init, memory: 0, processing: 0>
2023-06-28 05:38:56,019 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35669
2023-06-28 05:38:56,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41444
2023-06-28 05:38:56,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:56,019 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:38:56,019 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:56,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:38:56,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:38:56,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:38:56,055 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:38:57,051 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36769
2023-06-28 05:38:57,051 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36769
2023-06-28 05:38:57,051 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36961
2023-06-28 05:38:57,051 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:38:57,051 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,051 - distributed.worker - INFO -               Threads:                          4
2023-06-28 05:38:57,051 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-28 05:38:57,051 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ml8ppzpn
2023-06-28 05:38:57,052 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f361be25-ef5e-4487-9295-bae8e32d9872
2023-06-28 05:38:57,052 - distributed.worker - INFO - Starting Worker plugin PreImport-070858d3-51b4-4a51-ac4b-ba9052dc06e3
2023-06-28 05:38:57,052 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5749dbc1-47c2-4f71-921a-19dc0085389a
2023-06-28 05:38:57,053 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,077 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36769', status: init, memory: 0, processing: 0>
2023-06-28 05:38:57,078 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36769
2023-06-28 05:38:57,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41464
2023-06-28 05:38:57,078 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:38:57,079 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:38:57,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33267
2023-06-28 05:38:57,349 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33267
2023-06-28 05:38:57,349 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35215
2023-06-28 05:38:57,349 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:38:57,349 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,349 - distributed.worker - INFO -               Threads:                          4
2023-06-28 05:38:57,349 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-28 05:38:57,349 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nllll04_
2023-06-28 05:38:57,349 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ba39116-9e0c-4019-bc9a-3c4d2c5ee7c4
2023-06-28 05:38:57,349 - distributed.worker - INFO - Starting Worker plugin PreImport-db8d50c7-927e-4d03-9f57-1675b2e81d11
2023-06-28 05:38:57,350 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ddefd45-bfe2-406f-b195-62c6eeddc303
2023-06-28 05:38:57,350 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,352 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39619
2023-06-28 05:38:57,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39619
2023-06-28 05:38:57,353 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33821
2023-06-28 05:38:57,353 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:38:57,353 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,353 - distributed.worker - INFO -               Threads:                          4
2023-06-28 05:38:57,353 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-28 05:38:57,353 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w3201ua3
2023-06-28 05:38:57,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d6c5dbd-138f-46b5-9617-6c388520726e
2023-06-28 05:38:57,353 - distributed.worker - INFO - Starting Worker plugin PreImport-a7ec29bf-3230-4f7d-b52b-f026a1533766
2023-06-28 05:38:57,353 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a17ac2e1-5048-48aa-9405-f67075922093
2023-06-28 05:38:57,354 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,379 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33267', status: init, memory: 0, processing: 0>
2023-06-28 05:38:57,380 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33267
2023-06-28 05:38:57,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41480
2023-06-28 05:38:57,380 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:38:57,380 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,381 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39619', status: init, memory: 0, processing: 0>
2023-06-28 05:38:57,382 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39619
2023-06-28 05:38:57,382 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41484
2023-06-28 05:38:57,382 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:38:57,383 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:38:57,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:38:57,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:38:57,464 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-28 05:38:57,466 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-28 05:38:57,466 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-28 05:38:57,467 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-28 05:38:57,471 - distributed.scheduler - INFO - Remove client Client-10ae4f25-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:38:57,471 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41434; closing.
2023-06-28 05:38:57,471 - distributed.scheduler - INFO - Remove client Client-10ae4f25-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:38:57,471 - distributed.scheduler - INFO - Close client connection: Client-10ae4f25-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:38:57,472 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46561'. Reason: nanny-close
2023-06-28 05:38:57,473 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:57,473 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36409'. Reason: nanny-close
2023-06-28 05:38:57,474 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:57,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35735'. Reason: nanny-close
2023-06-28 05:38:57,474 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39619. Reason: nanny-close
2023-06-28 05:38:57,474 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:57,475 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36769. Reason: nanny-close
2023-06-28 05:38:57,475 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44173'. Reason: nanny-close
2023-06-28 05:38:57,475 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:38:57,475 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33267. Reason: nanny-close
2023-06-28 05:38:57,476 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41484; closing.
2023-06-28 05:38:57,476 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:38:57,476 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35669. Reason: nanny-close
2023-06-28 05:38:57,476 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39619', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:57,477 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:38:57,477 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39619
2023-06-28 05:38:57,477 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:38:57,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41464; closing.
2023-06-28 05:38:57,478 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:57,478 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:57,478 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:38:57,478 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36769', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:57,478 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36769
2023-06-28 05:38:57,478 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41480; closing.
2023-06-28 05:38:57,479 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:57,479 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33267', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:57,479 - distributed.nanny - INFO - Worker closed
2023-06-28 05:38:57,479 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33267
2023-06-28 05:38:57,480 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41444; closing.
2023-06-28 05:38:57,480 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35669', status: closing, memory: 0, processing: 0>
2023-06-28 05:38:57,480 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35669
2023-06-28 05:38:57,480 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:38:58,689 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:38:58,690 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:38:58,690 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:38:58,691 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-28 05:38:58,692 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-06-28 05:39:00,777 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:00,781 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-28 05:39:00,785 - distributed.scheduler - INFO - State start
2023-06-28 05:39:00,805 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:00,806 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:39:00,806 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-28 05:39:01,225 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37399'
2023-06-28 05:39:01,237 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41513'
2023-06-28 05:39:01,250 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36985'
2023-06-28 05:39:01,253 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38253'
2023-06-28 05:39:01,276 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46843'
2023-06-28 05:39:01,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40421'
2023-06-28 05:39:01,288 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36393'
2023-06-28 05:39:01,295 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33159'
2023-06-28 05:39:01,675 - distributed.scheduler - INFO - Receive client connection: Client-14a2a5d2-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:01,692 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47272
2023-06-28 05:39:02,857 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,857 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:02,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,908 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:02,918 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:02,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:02,935 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:02,942 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:02,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:02,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:02,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:02,985 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,985 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:02,989 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,989 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:02,998 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:02,998 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:03,013 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:03,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:03,031 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:03,039 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:05,745 - distributed.scheduler - INFO - Receive client connection: Client-188e854b-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:05,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47278
2023-06-28 05:39:05,857 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45799
2023-06-28 05:39:05,858 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45799
2023-06-28 05:39:05,858 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42553
2023-06-28 05:39:05,858 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:05,858 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:05,858 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:05,858 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:05,858 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dxzke3zw
2023-06-28 05:39:05,859 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3568f28-6f0d-4582-8c2f-c8a1370d07c6
2023-06-28 05:39:05,860 - distributed.worker - INFO - Starting Worker plugin PreImport-781d7b99-27a5-4a00-bc5f-d49aa50867d1
2023-06-28 05:39:05,860 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d829fccb-85b7-45cc-8fa4-876c487ca4bb
2023-06-28 05:39:06,046 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37925
2023-06-28 05:39:06,046 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37925
2023-06-28 05:39:06,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38313
2023-06-28 05:39:06,046 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,046 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,046 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:06,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:06,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ggida949
2023-06-28 05:39:06,047 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d528cb59-1471-435a-b1e5-74cb5f3cca25
2023-06-28 05:39:06,048 - distributed.worker - INFO - Starting Worker plugin PreImport-3e534d39-3a0f-4b12-b331-70210bcfe372
2023-06-28 05:39:06,048 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77648500-1aea-4328-afdf-31d5bca6eb49
2023-06-28 05:39:06,052 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37077
2023-06-28 05:39:06,053 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37077
2023-06-28 05:39:06,053 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39131
2023-06-28 05:39:06,053 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,053 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,053 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:06,053 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:06,053 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g79k474n
2023-06-28 05:39:06,053 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2bffe472-1448-40af-b526-20f5ed761fb9
2023-06-28 05:39:06,055 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35465
2023-06-28 05:39:06,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35465
2023-06-28 05:39:06,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36875
2023-06-28 05:39:06,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,055 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,055 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:06,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:06,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zqum84du
2023-06-28 05:39:06,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-095534b4-e700-44bc-98bb-4cd553795fc0
2023-06-28 05:39:06,060 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,060 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43037
2023-06-28 05:39:06,061 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43037
2023-06-28 05:39:06,061 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34985
2023-06-28 05:39:06,061 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,062 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:06,062 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:06,062 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hacthn8f
2023-06-28 05:39:06,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7a9e3e1-a15e-4662-930b-ae89353cae3c
2023-06-28 05:39:06,089 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45799', status: init, memory: 0, processing: 0>
2023-06-28 05:39:06,091 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45799
2023-06-28 05:39:06,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47292
2023-06-28 05:39:06,091 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,092 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:06,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39387
2023-06-28 05:39:06,106 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39387
2023-06-28 05:39:06,106 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43875
2023-06-28 05:39:06,106 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,106 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,106 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:06,106 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:06,106 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-muee8_lt
2023-06-28 05:39:06,107 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b1f7d17-74be-442a-b62a-a820927b02b1
2023-06-28 05:39:06,108 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42887
2023-06-28 05:39:06,109 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42887
2023-06-28 05:39:06,109 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44023
2023-06-28 05:39:06,109 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,109 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,109 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:06,109 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:06,109 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-41o0i_9n
2023-06-28 05:39:06,109 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dccc753e-fd70-4034-b61b-6706c7c662d4
2023-06-28 05:39:06,117 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39103
2023-06-28 05:39:06,117 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39103
2023-06-28 05:39:06,117 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37143
2023-06-28 05:39:06,118 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,118 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,118 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:06,118 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:06,118 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5yzcrtwq
2023-06-28 05:39:06,119 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65db6c5c-8cf4-4811-a5cb-5a0136c9666e
2023-06-28 05:39:06,119 - distributed.worker - INFO - Starting Worker plugin PreImport-fb5cc872-e8aa-4e62-9a62-f2de3eb606ec
2023-06-28 05:39:06,119 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4720259-ddb8-454f-b463-f9e5c407b41b
2023-06-28 05:39:06,251 - distributed.worker - INFO - Starting Worker plugin PreImport-9fdabd7e-8b40-47ab-9ac4-43c27b04aa55
2023-06-28 05:39:06,251 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1995497b-2f16-4304-850e-4bfb61135be5
2023-06-28 05:39:06,252 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,264 - distributed.worker - INFO - Starting Worker plugin PreImport-99ce7df6-73e0-401a-aabf-5bc31ea6b571
2023-06-28 05:39:06,265 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,265 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ec04cb5-d5fd-4682-9a29-da4944673fc3
2023-06-28 05:39:06,266 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,281 - distributed.worker - INFO - Starting Worker plugin PreImport-df3a966c-cbb6-46e8-9b46-01434523ac53
2023-06-28 05:39:06,281 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ff83570-f334-47e9-ab81-7b0ffffd108f
2023-06-28 05:39:06,282 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,296 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39103', status: init, memory: 0, processing: 0>
2023-06-28 05:39:06,296 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39103
2023-06-28 05:39:06,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47310
2023-06-28 05:39:06,297 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,297 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:06,302 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39387', status: init, memory: 0, processing: 0>
2023-06-28 05:39:06,303 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39387
2023-06-28 05:39:06,303 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47314
2023-06-28 05:39:06,304 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,304 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,305 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35465', status: init, memory: 0, processing: 0>
2023-06-28 05:39:06,306 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35465
2023-06-28 05:39:06,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47296
2023-06-28 05:39:06,307 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,307 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:06,307 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:06,313 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-669db505-49aa-4192-9c80-16a1c0444d20
2023-06-28 05:39:06,314 - distributed.worker - INFO - Starting Worker plugin PreImport-8ca0dc85-f718-47ca-afde-433215d54c00
2023-06-28 05:39:06,314 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,320 - distributed.worker - INFO - Starting Worker plugin PreImport-6b2af411-2d6e-428d-8e4a-d535db00c10b
2023-06-28 05:39:06,320 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a277d3f6-6b58-4f24-962c-1485ed7d781a
2023-06-28 05:39:06,320 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,320 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,322 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43037', status: init, memory: 0, processing: 0>
2023-06-28 05:39:06,322 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43037
2023-06-28 05:39:06,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47330
2023-06-28 05:39:06,323 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,323 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:06,342 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37077', status: init, memory: 0, processing: 0>
2023-06-28 05:39:06,343 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37077
2023-06-28 05:39:06,343 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47340
2023-06-28 05:39:06,344 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:06,353 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42887', status: init, memory: 0, processing: 0>
2023-06-28 05:39:06,353 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42887
2023-06-28 05:39:06,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47370
2023-06-28 05:39:06,354 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,354 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:06,361 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37925', status: init, memory: 0, processing: 0>
2023-06-28 05:39:06,362 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37925
2023-06-28 05:39:06,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47354
2023-06-28 05:39:06,363 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:06,363 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:06,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:06,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:06,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:06,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:06,413 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:06,413 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:06,413 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:06,413 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:06,413 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:06,418 - distributed.scheduler - INFO - Remove client Client-14a2a5d2-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:06,418 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47272; closing.
2023-06-28 05:39:06,419 - distributed.scheduler - INFO - Remove client Client-14a2a5d2-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:06,419 - distributed.scheduler - INFO - Close client connection: Client-14a2a5d2-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:06,420 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37399'. Reason: nanny-close
2023-06-28 05:39:06,421 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:06,421 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41513'. Reason: nanny-close
2023-06-28 05:39:06,422 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:06,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38253'. Reason: nanny-close
2023-06-28 05:39:06,423 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37077. Reason: nanny-close
2023-06-28 05:39:06,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:06,423 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46843'. Reason: nanny-close
2023-06-28 05:39:06,423 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43037. Reason: nanny-close
2023-06-28 05:39:06,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:06,424 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36985'. Reason: nanny-close
2023-06-28 05:39:06,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39387. Reason: nanny-close
2023-06-28 05:39:06,424 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:06,424 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40421'. Reason: nanny-close
2023-06-28 05:39:06,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42887. Reason: nanny-close
2023-06-28 05:39:06,424 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47340; closing.
2023-06-28 05:39:06,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:06,424 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:06,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37077', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:06,425 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37077
2023-06-28 05:39:06,425 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39103. Reason: nanny-close
2023-06-28 05:39:06,425 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36393'. Reason: nanny-close
2023-06-28 05:39:06,425 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:06,425 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:06,425 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33159'. Reason: nanny-close
2023-06-28 05:39:06,425 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45799. Reason: nanny-close
2023-06-28 05:39:06,425 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:06,425 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:06,426 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:06,426 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35465. Reason: nanny-close
2023-06-28 05:39:06,426 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:06,426 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37077
2023-06-28 05:39:06,426 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:06,426 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37077
2023-06-28 05:39:06,427 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47330; closing.
2023-06-28 05:39:06,427 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:06,427 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:06,428 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37925. Reason: nanny-close
2023-06-28 05:39:06,428 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:06,428 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43037', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:06,428 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37077
2023-06-28 05:39:06,428 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43037
2023-06-28 05:39:06,428 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37077
2023-06-28 05:39:06,428 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:06,428 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47314; closing.
2023-06-28 05:39:06,429 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:06,429 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:06,429 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39387', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:06,429 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39387
2023-06-28 05:39:06,430 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47370; closing.
2023-06-28 05:39:06,430 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47310; closing.
2023-06-28 05:39:06,430 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:06,431 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:06,431 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:06,431 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42887', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:06,431 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42887
2023-06-28 05:39:06,431 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39103', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:06,432 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39103
2023-06-28 05:39:06,432 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47292; closing.
2023-06-28 05:39:06,432 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47296; closing.
2023-06-28 05:39:06,433 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:06,433 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45799', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:06,433 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45799
2023-06-28 05:39:06,434 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35465', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:06,434 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35465
2023-06-28 05:39:06,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47354; closing.
2023-06-28 05:39:06,435 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37925', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:06,435 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37925
2023-06-28 05:39:06,436 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:39:06,436 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47354>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-28 05:39:07,838 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:39:07,838 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:39:07,838 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:39:07,842 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:39:07,842 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-06-28 05:39:10,053 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:10,057 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-28 05:39:10,061 - distributed.scheduler - INFO - State start
2023-06-28 05:39:10,511 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:10,512 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:39:10,513 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-28 05:39:11,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40721'
2023-06-28 05:39:11,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37621'
2023-06-28 05:39:11,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45889'
2023-06-28 05:39:11,168 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42793'
2023-06-28 05:39:11,171 - distributed.scheduler - INFO - Receive client connection: Client-188e854b-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:11,180 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32989'
2023-06-28 05:39:11,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60320
2023-06-28 05:39:11,190 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43161'
2023-06-28 05:39:11,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36705'
2023-06-28 05:39:11,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34331'
2023-06-28 05:39:11,355 - distributed.scheduler - INFO - Receive client connection: Client-1a0f0d2c-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:11,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60348
2023-06-28 05:39:12,877 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:12,877 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:12,879 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:12,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:12,880 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:12,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:12,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:12,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:12,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:12,923 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:12,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:12,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:12,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:12,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:12,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:12,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:13,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:13,164 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:13,181 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:13,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:13,189 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:13,192 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:13,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:13,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:15,785 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46817', status: init, memory: 0, processing: 0>
2023-06-28 05:39:15,786 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46817
2023-06-28 05:39:15,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60372
2023-06-28 05:39:15,798 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36119', status: init, memory: 0, processing: 0>
2023-06-28 05:39:15,798 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36119
2023-06-28 05:39:15,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60362
2023-06-28 05:39:15,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38091', status: init, memory: 0, processing: 0>
2023-06-28 05:39:15,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38091
2023-06-28 05:39:15,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60388
2023-06-28 05:39:15,815 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33517', status: init, memory: 0, processing: 0>
2023-06-28 05:39:15,815 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33517
2023-06-28 05:39:15,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60378
2023-06-28 05:39:15,825 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43079', status: init, memory: 0, processing: 0>
2023-06-28 05:39:15,826 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43079
2023-06-28 05:39:15,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60394
2023-06-28 05:39:15,837 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33401', status: init, memory: 0, processing: 0>
2023-06-28 05:39:15,837 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33401
2023-06-28 05:39:15,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60410
2023-06-28 05:39:15,840 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45913', status: init, memory: 0, processing: 0>
2023-06-28 05:39:15,840 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45913
2023-06-28 05:39:15,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60412
2023-06-28 05:39:15,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36711', status: init, memory: 0, processing: 0>
2023-06-28 05:39:15,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36711
2023-06-28 05:39:15,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60418
2023-06-28 05:39:15,898 - distributed.scheduler - INFO - Remove client Client-1a0f0d2c-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:15,898 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60348; closing.
2023-06-28 05:39:15,898 - distributed.scheduler - INFO - Remove client Client-1a0f0d2c-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:15,899 - distributed.scheduler - INFO - Close client connection: Client-1a0f0d2c-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:39:15,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42793'. Reason: nanny-close
2023-06-28 05:39:15,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40721'. Reason: nanny-close
2023-06-28 05:39:15,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37621'. Reason: nanny-close
2023-06-28 05:39:15,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45889'. Reason: nanny-close
2023-06-28 05:39:15,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32989'. Reason: nanny-close
2023-06-28 05:39:15,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43161'. Reason: nanny-close
2023-06-28 05:39:15,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36705'. Reason: nanny-close
2023-06-28 05:39:15,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34331'. Reason: nanny-close
2023-06-28 05:39:15,910 - distributed.scheduler - INFO - Remove client Client-188e854b-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:15,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60320; closing.
2023-06-28 05:39:15,911 - distributed.scheduler - INFO - Remove client Client-188e854b-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:15,911 - distributed.scheduler - INFO - Close client connection: Client-188e854b-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:15,917 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60418; closing.
2023-06-28 05:39:15,917 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36711', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:15,917 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36711
2023-06-28 05:39:15,919 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60388; closing.
2023-06-28 05:39:15,919 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60378; closing.
2023-06-28 05:39:15,919 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60362; closing.
2023-06-28 05:39:15,920 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38091', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:15,920 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38091
2023-06-28 05:39:15,920 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33517', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:15,920 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33517
2023-06-28 05:39:15,921 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36119', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:15,921 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36119
2023-06-28 05:39:15,922 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60394; closing.
2023-06-28 05:39:15,922 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60372; closing.
2023-06-28 05:39:15,922 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60410; closing.
2023-06-28 05:39:15,922 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43079', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:15,923 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43079
2023-06-28 05:39:15,923 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46817', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:15,923 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46817
2023-06-28 05:39:15,923 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33401', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:15,923 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33401
2023-06-28 05:39:15,924 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60412; closing.
2023-06-28 05:39:15,924 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45913', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:15,925 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45913
2023-06-28 05:39:15,925 - distributed.scheduler - INFO - Lost all workers
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-28 05:39:16,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37385
2023-06-28 05:39:16,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37385
2023-06-28 05:39:16,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42117
2023-06-28 05:39:16,598 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:16,598 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:16,598 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:16,598 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:16,598 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6ln80yk1
2023-06-28 05:39:16,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58f51c17-c11a-4cf8-9981-d8bcae64b8ec
2023-06-28 05:39:17,361 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32917
2023-06-28 05:39:17,361 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34353
2023-06-28 05:39:17,362 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32917
2023-06-28 05:39:17,362 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34353
2023-06-28 05:39:17,362 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41237
2023-06-28 05:39:17,362 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40739
2023-06-28 05:39:17,362 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,362 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,362 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,362 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,362 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:17,362 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:17,362 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:17,362 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:17,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xxcasvzm
2023-06-28 05:39:17,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h8pokzut
2023-06-28 05:39:17,362 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5e29226-b805-4fbf-bde3-bf16eb5d5030
2023-06-28 05:39:17,362 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16053f6f-7e91-4918-b9c1-a0c6c0049933
2023-06-28 05:39:17,363 - distributed.worker - INFO - Starting Worker plugin PreImport-faa627ed-8d08-4572-ab8d-a2efaa7defcf
2023-06-28 05:39:17,363 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c4e2b8a-2c16-401f-ad0f-2b3593d374e4
2023-06-28 05:39:17,363 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd302ba8-cc07-4cc0-b6cd-c1837f2b84b0
2023-06-28 05:39:17,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45415
2023-06-28 05:39:17,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45415
2023-06-28 05:39:17,373 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33331
2023-06-28 05:39:17,373 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,373 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,374 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:17,374 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:17,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-09g0ssgz
2023-06-28 05:39:17,374 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-761d0db0-2538-4d42-b78c-faf7f283ab82
2023-06-28 05:39:17,375 - distributed.worker - INFO - Starting Worker plugin RMMSetup-501cd1ca-9d03-4fb5-8282-f6a8ba024792
2023-06-28 05:39:17,380 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35447
2023-06-28 05:39:17,380 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35447
2023-06-28 05:39:17,380 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44603
2023-06-28 05:39:17,380 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,380 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,380 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:17,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:17,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vkxi9oub
2023-06-28 05:39:17,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43969
2023-06-28 05:39:17,381 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94431472-7aeb-41bf-9a2d-b9cb8cdb266f
2023-06-28 05:39:17,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43969
2023-06-28 05:39:17,382 - distributed.worker - INFO - Starting Worker plugin PreImport-20be4f6f-8fbe-4548-8c73-bfd0ab990745
2023-06-28 05:39:17,382 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33437
2023-06-28 05:39:17,382 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f840e668-44fc-41f1-bdbd-378c3fa6fd29
2023-06-28 05:39:17,382 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,382 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:17,382 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:17,382 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-brz26cp1
2023-06-28 05:39:17,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbc44b82-90e8-4b56-980f-d7561e88b383
2023-06-28 05:39:17,383 - distributed.worker - INFO - Starting Worker plugin PreImport-a6339c4a-555a-4e4e-b84b-ec138e82d31d
2023-06-28 05:39:17,383 - distributed.worker - INFO - Starting Worker plugin RMMSetup-00ecbe82-9bc1-49e3-8f5f-d4ae1febfeaf
2023-06-28 05:39:17,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba80e8cc-8328-4e78-9067-5416bdfcb661
2023-06-28 05:39:17,388 - distributed.worker - INFO - Starting Worker plugin PreImport-33de45de-5ca7-49cf-b86a-a351229678e1
2023-06-28 05:39:17,389 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,405 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,406 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,408 - distributed.worker - INFO - Starting Worker plugin PreImport-267eae43-959f-4b02-a0e9-075f83f5acec
2023-06-28 05:39:17,408 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,409 - distributed.worker - INFO - Starting Worker plugin PreImport-7548fc29-6164-4379-bf33-1199ca2415a4
2023-06-28 05:39:17,410 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,429 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37385', status: init, memory: 0, processing: 0>
2023-06-28 05:39:17,429 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37385
2023-06-28 05:39:17,430 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60420
2023-06-28 05:39:17,430 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35447', status: init, memory: 0, processing: 0>
2023-06-28 05:39:17,430 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,431 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,431 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35447
2023-06-28 05:39:17,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60430
2023-06-28 05:39:17,431 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,432 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,432 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32917', status: init, memory: 0, processing: 0>
2023-06-28 05:39:17,432 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32917
2023-06-28 05:39:17,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60446
2023-06-28 05:39:17,433 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,433 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,433 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43969', status: init, memory: 0, processing: 0>
2023-06-28 05:39:17,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:17,434 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43969
2023-06-28 05:39:17,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60432
2023-06-28 05:39:17,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:17,434 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,434 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,435 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:17,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:17,439 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45415', status: init, memory: 0, processing: 0>
2023-06-28 05:39:17,440 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45415
2023-06-28 05:39:17,440 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60454
2023-06-28 05:39:17,440 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,440 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34353', status: init, memory: 0, processing: 0>
2023-06-28 05:39:17,441 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34353
2023-06-28 05:39:17,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60468
2023-06-28 05:39:17,442 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,442 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,443 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:17,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:17,472 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:17,472 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:17,473 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35447. Reason: nanny-close
2023-06-28 05:39:17,474 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37385. Reason: nanny-close
2023-06-28 05:39:17,475 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:17,475 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60430; closing.
2023-06-28 05:39:17,475 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35447', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:17,476 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35447
2023-06-28 05:39:17,476 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:17,476 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:17,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:17,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:17,477 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35447
2023-06-28 05:39:17,477 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35447
2023-06-28 05:39:17,477 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35447
2023-06-28 05:39:17,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:17,477 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45415. Reason: nanny-close
2023-06-28 05:39:17,478 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60420; closing.
2023-06-28 05:39:17,478 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35447
2023-06-28 05:39:17,478 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37385', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:17,478 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37385
2023-06-28 05:39:17,478 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32917. Reason: nanny-close
2023-06-28 05:39:17,478 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:17,478 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43969. Reason: nanny-close
2023-06-28 05:39:17,478 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34353. Reason: nanny-close
2023-06-28 05:39:17,480 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60446; closing.
2023-06-28 05:39:17,480 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:17,480 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35447
2023-06-28 05:39:17,480 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:17,480 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32917', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:17,480 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32917
2023-06-28 05:39:17,480 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:17,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60432; closing.
2023-06-28 05:39:17,481 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:17,481 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:17,481 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43969', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:17,481 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43969
2023-06-28 05:39:17,481 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:17,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60454; closing.
2023-06-28 05:39:17,482 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:17,482 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45415', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:17,482 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45415
2023-06-28 05:39:17,482 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60468; closing.
2023-06-28 05:39:17,482 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:17,483 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:17,483 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34353', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:17,483 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34353
2023-06-28 05:39:17,483 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:39:17,660 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43837
2023-06-28 05:39:17,660 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43837
2023-06-28 05:39:17,660 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42959
2023-06-28 05:39:17,660 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,660 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,660 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:17,660 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:17,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a4otqnau
2023-06-28 05:39:17,660 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02fea586-6eb6-4783-90df-775c7dc99eac
2023-06-28 05:39:17,661 - distributed.worker - INFO - Starting Worker plugin PreImport-23166732-1cbc-45c8-b686-b9d0f1013421
2023-06-28 05:39:17,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a44e419f-fb22-40cf-b940-5ba34a389f59
2023-06-28 05:39:17,662 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42291
2023-06-28 05:39:17,663 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42291
2023-06-28 05:39:17,663 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39583
2023-06-28 05:39:17,663 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,663 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,663 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:17,663 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:17,663 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r8ciziyg
2023-06-28 05:39:17,663 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-578529d6-c45e-4cf3-bf7b-f6a07a4637b7
2023-06-28 05:39:17,663 - distributed.worker - INFO - Starting Worker plugin PreImport-6e0f1035-3236-40a6-802c-2edfa8fdd21f
2023-06-28 05:39:17,664 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f71146d2-f428-445c-a880-a24911ece4ec
2023-06-28 05:39:17,669 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,671 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,692 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42291', status: init, memory: 0, processing: 0>
2023-06-28 05:39:17,693 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42291
2023-06-28 05:39:17,693 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60484
2023-06-28 05:39:17,693 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,694 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:17,701 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43837', status: init, memory: 0, processing: 0>
2023-06-28 05:39:17,702 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43837
2023-06-28 05:39:17,702 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60470
2023-06-28 05:39:17,703 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:17,703 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:17,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:17,727 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:17,727 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:17,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43837. Reason: nanny-close
2023-06-28 05:39:17,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42291. Reason: nanny-close
2023-06-28 05:39:17,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:17,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60484; closing.
2023-06-28 05:39:17,730 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42291', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:17,730 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42291
2023-06-28 05:39:17,730 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:17,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:17,731 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60470; closing.
2023-06-28 05:39:17,731 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43837', status: closing, memory: 0, processing: 0>
2023-06-28 05:39:17,731 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43837
2023-06-28 05:39:17,732 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:39:17,732 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:17,782 - distributed.scheduler - INFO - Receive client connection: Client-1fbb5162-1576-11ee-8780-d8c49764f6bb
2023-06-28 05:39:17,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60494
2023-06-28 05:39:19,073 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:39:19,073 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:39:19,074 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:39:19,076 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:39:19,076 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-06-28 05:39:21,211 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:21,215 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44183 instead
  warnings.warn(
2023-06-28 05:39:21,219 - distributed.scheduler - INFO - State start
2023-06-28 05:39:21,372 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:21,373 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:39:21,373 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:39:21,374 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-28 05:39:21,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38729'
2023-06-28 05:39:21,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35755'
2023-06-28 05:39:21,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43949'
2023-06-28 05:39:21,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38395'
2023-06-28 05:39:21,599 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43189'
2023-06-28 05:39:21,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40323'
2023-06-28 05:39:21,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45997'
2023-06-28 05:39:21,624 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38647'
2023-06-28 05:39:23,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:23,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:23,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:23,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:23,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:23,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:23,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:23,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:23,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:23,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:23,328 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:23,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:23,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:23,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:23,346 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:23,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:23,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:23,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:23,427 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:23,430 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:23,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:23,435 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:23,436 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:23,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:24,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33141
2023-06-28 05:39:24,468 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33141
2023-06-28 05:39:24,468 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40367
2023-06-28 05:39:24,469 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:24,469 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:24,469 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:24,469 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:24,469 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vr29wk93
2023-06-28 05:39:24,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1960a084-d6ad-4bd7-8f4f-4a18c362915d
2023-06-28 05:39:24,470 - distributed.worker - INFO - Starting Worker plugin PreImport-194a8fa6-1cd0-4930-979f-b5dfa27430a1
2023-06-28 05:39:24,470 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a64d5ee4-d4ca-4210-9ff7-7dbd3701a286
2023-06-28 05:39:25,281 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,792 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:25,792 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:25,841 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40749
2023-06-28 05:39:25,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40749
2023-06-28 05:39:25,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37643
2023-06-28 05:39:25,841 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:25,841 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,841 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:25,841 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:25,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-picjirvv
2023-06-28 05:39:25,842 - distributed.worker - INFO - Starting Worker plugin PreImport-486b0cfd-fb0c-4b73-bbdc-4fda68fbdf42
2023-06-28 05:39:25,843 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a28afa5-c57b-4919-b5f6-0e1d6d4f251a
2023-06-28 05:39:25,887 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42971
2023-06-28 05:39:25,888 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42971
2023-06-28 05:39:25,888 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44923
2023-06-28 05:39:25,888 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:25,888 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,888 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:25,888 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:25,888 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iogx7rp_
2023-06-28 05:39:25,888 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-beb46b69-17ff-4161-b774-bf80be56a0df
2023-06-28 05:39:25,889 - distributed.worker - INFO - Starting Worker plugin PreImport-b305d771-31d2-42fa-863a-75d44c87c4c4
2023-06-28 05:39:25,889 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4063e70-269c-419b-8edf-5510a3fab785
2023-06-28 05:39:25,889 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45005
2023-06-28 05:39:25,889 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45005
2023-06-28 05:39:25,889 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35589
2023-06-28 05:39:25,889 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:25,889 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,890 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:25,890 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:25,890 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ywfmh0kp
2023-06-28 05:39:25,890 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2594529-39b9-4cc4-9b2e-c7c8549e0f83
2023-06-28 05:39:25,974 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41495
2023-06-28 05:39:25,975 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41495
2023-06-28 05:39:25,975 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38355
2023-06-28 05:39:25,975 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:25,975 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,975 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:25,975 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:25,975 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xs0ssswo
2023-06-28 05:39:25,976 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56623285-9b88-44ba-8375-3e38825b9718
2023-06-28 05:39:25,976 - distributed.worker - INFO - Starting Worker plugin PreImport-73e94025-d780-4c96-89af-88e8ba7068ba
2023-06-28 05:39:25,976 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d527bfe-bf10-4ede-9478-fc445af1d248
2023-06-28 05:39:25,978 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42241
2023-06-28 05:39:25,978 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42241
2023-06-28 05:39:25,978 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40345
2023-06-28 05:39:25,978 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:25,978 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,978 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:25,978 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:25,978 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-11x2m3ok
2023-06-28 05:39:25,979 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dab02047-91a1-496c-b2fb-d647ca26a57b
2023-06-28 05:39:25,994 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41497
2023-06-28 05:39:25,994 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41497
2023-06-28 05:39:25,994 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40999
2023-06-28 05:39:25,994 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:25,994 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,994 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:25,994 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:25,994 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c94zoai3
2023-06-28 05:39:25,995 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c6c4108-1f36-458b-9824-5560af38337b
2023-06-28 05:39:25,997 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39365
2023-06-28 05:39:25,997 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39365
2023-06-28 05:39:25,997 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44761
2023-06-28 05:39:25,997 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:25,997 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:25,997 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:25,997 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:25,997 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wamopj8m
2023-06-28 05:39:25,998 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e16caef-fe18-43eb-9e56-6d8c8af8b89c
2023-06-28 05:39:26,019 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84cceb4d-2801-4f5b-b8be-a9ba8c92372d
2023-06-28 05:39:26,020 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,062 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:26,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,064 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:26,071 - distributed.worker - INFO - Starting Worker plugin PreImport-3d033a12-74ca-4b22-8f45-2589477baa83
2023-06-28 05:39:26,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13899007-15ba-4f45-9a3c-43fe81e94cda
2023-06-28 05:39:26,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,072 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,113 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:26,113 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:26,122 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:26,122 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:26,141 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,149 - distributed.worker - INFO - Starting Worker plugin PreImport-a7316081-5574-4bb1-94d6-b088df9f7f9d
2023-06-28 05:39:26,150 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1629a16a-c555-4c36-847b-86ad26398e7c
2023-06-28 05:39:26,150 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,161 - distributed.worker - INFO - Starting Worker plugin PreImport-d9837955-bffd-4fec-a6e0-7ecac4771e88
2023-06-28 05:39:26,162 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d7c7acc-741d-4fa2-900e-6efbf0edc4ab
2023-06-28 05:39:26,162 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,163 - distributed.worker - INFO - Starting Worker plugin PreImport-7ee43775-24a6-43f0-a8a6-53416e2db5a2
2023-06-28 05:39:26,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0995b60-04a6-4647-91e8-3a8fe5f789ac
2023-06-28 05:39:26,163 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,169 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:26,170 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:26,177 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:26,177 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:26,193 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:26,193 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:26,194 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:26,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:26,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:26,248 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:26,248 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:26,248 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:26,249 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:26,249 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:26,249 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:26,249 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:26,249 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:39:26,260 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:26,260 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:26,260 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:26,261 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:26,261 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:26,261 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:26,261 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:26,261 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:26,373 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7f40e2f38bb0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1318, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1348, in _ensure_connected
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-06-28 05:39:26,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43949'. Reason: nanny-close
2023-06-28 05:39:26,375 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:26,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38395'. Reason: nanny-close
2023-06-28 05:39:26,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:26,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45997'. Reason: nanny-close
2023-06-28 05:39:26,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40749. Reason: nanny-close
2023-06-28 05:39:26,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:26,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38729'. Reason: nanny-close
2023-06-28 05:39:26,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42241. Reason: nanny-close
2023-06-28 05:39:26,378 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:26,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41497. Reason: nanny-close
2023-06-28 05:39:26,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35755'. Reason: nanny-close
2023-06-28 05:39:26,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:26,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39365. Reason: nanny-close
2023-06-28 05:39:26,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43189'. Reason: nanny-close
2023-06-28 05:39:26,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:26,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:26,380 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:26,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40323'. Reason: nanny-close
2023-06-28 05:39:26,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41495. Reason: nanny-close
2023-06-28 05:39:26,380 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:26,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38647'. Reason: nanny-close
2023-06-28 05:39:26,381 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:26,381 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:39:26,381 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:26,381 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33141. Reason: nanny-close
2023-06-28 05:39:26,381 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:26,381 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45005. Reason: nanny-close
2023-06-28 05:39:26,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:26,382 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40749
2023-06-28 05:39:26,382 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:26,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:26,383 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40749
2023-06-28 05:39:26,383 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:26,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42971. Reason: nanny-close
2023-06-28 05:39:26,384 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40749
2023-06-28 05:39:26,384 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:26,384 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:26,384 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40749
2023-06-28 05:39:26,385 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:26,386 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:39:26,386 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:26,387 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:26,387 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-06-28 05:39:29,397 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:29,401 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45755 instead
  warnings.warn(
2023-06-28 05:39:29,405 - distributed.scheduler - INFO - State start
2023-06-28 05:39:29,423 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:39:29,424 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:39:29,424 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:39:29,425 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-28 05:39:29,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33413'
2023-06-28 05:39:29,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43483'
2023-06-28 05:39:29,815 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37635'
2023-06-28 05:39:29,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33265'
2023-06-28 05:39:29,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35123'
2023-06-28 05:39:29,834 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45353'
2023-06-28 05:39:29,843 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34877'
2023-06-28 05:39:29,852 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41527'
2023-06-28 05:39:31,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:31,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:31,407 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:31,478 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:31,478 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:31,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:31,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:31,521 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:31,521 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:31,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:31,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:31,528 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:31,528 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:31,528 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:31,528 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:31,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:39:31,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:39:31,646 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:31,651 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:31,671 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:31,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:31,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:31,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:31,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:39:32,956 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33749
2023-06-28 05:39:32,957 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33749
2023-06-28 05:39:32,957 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34541
2023-06-28 05:39:32,957 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:32,957 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:32,957 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:32,957 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:32,957 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-glmxz5kd
2023-06-28 05:39:32,957 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95c03296-2c3e-4681-80fc-67fee15829a4
2023-06-28 05:39:32,958 - distributed.worker - INFO - Starting Worker plugin PreImport-cdd033a0-40dc-4763-8ca9-0af5dbff4bc8
2023-06-28 05:39:32,958 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19b236cd-6d84-46e5-8830-5bea5241652c
2023-06-28 05:39:33,298 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:33,329 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:39:33,329 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:33,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:39:33,385 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-06-28 05:39:33,391 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:39:33,403 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33749. Reason: scheduler-close
2023-06-28 05:39:33,403 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:9369; closing.
2023-06-28 05:39:33,404 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:43334 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-28 05:39:33,409 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://127.0.0.1:41527'. Reason: scheduler-close
2023-06-28 05:39:33,413 - distributed.nanny - INFO - Worker closed
2023-06-28 05:39:34,250 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43749
2023-06-28 05:39:34,250 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43749
2023-06-28 05:39:34,250 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45513
2023-06-28 05:39:34,250 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:34,250 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,251 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:34,251 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:34,251 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6qduieop
2023-06-28 05:39:34,251 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9342b72e-3e21-4fd6-9e14-d16dcdfbf4d0
2023-06-28 05:39:34,252 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44459
2023-06-28 05:39:34,252 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44459
2023-06-28 05:39:34,252 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38313
2023-06-28 05:39:34,252 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:34,252 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,252 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:34,252 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:34,252 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_wga89ru
2023-06-28 05:39:34,253 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50eb726c-211f-4906-b1db-80ac68356602
2023-06-28 05:39:34,254 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41325
2023-06-28 05:39:34,254 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41325
2023-06-28 05:39:34,254 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35753
2023-06-28 05:39:34,254 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:34,254 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,254 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:34,255 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:34,255 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3e5e2i14
2023-06-28 05:39:34,255 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4852c349-ad8c-4b53-9614-0b56974cc361
2023-06-28 05:39:34,255 - distributed.worker - INFO - Starting Worker plugin PreImport-43b03ee8-58b9-48eb-acb6-e9d4d16120a2
2023-06-28 05:39:34,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-307a4bea-a66e-4cd3-bc82-050f8dadad2d
2023-06-28 05:39:34,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36399
2023-06-28 05:39:34,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36399
2023-06-28 05:39:34,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42777
2023-06-28 05:39:34,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42777
2023-06-28 05:39:34,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43391
2023-06-28 05:39:34,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39911
2023-06-28 05:39:34,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:34,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43391
2023-06-28 05:39:34,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40959
2023-06-28 05:39:34,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38481
2023-06-28 05:39:34,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35697
2023-06-28 05:39:34,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:34,257 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:34,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:34,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38481
2023-06-28 05:39:34,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:34,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,257 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:34,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38495
2023-06-28 05:39:34,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ganh2bog
2023-06-28 05:39:34,257 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:34,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:34,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:39:34,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:34,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-je6ezzxy
2023-06-28 05:39:34,257 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rnnkd3zv
2023-06-28 05:39:34,257 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:39:34,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:39:34,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vj60nv0x
2023-06-28 05:39:34,257 - distributed.worker - INFO - Starting Worker plugin PreImport-3fbcc7e9-621e-436d-a524-93153923980a
2023-06-28 05:39:34,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9287f571-166e-4e8a-81c1-4a67e1a62c41
2023-06-28 05:39:34,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09d75d59-5092-4312-ae1d-675bcf8da9da
2023-06-28 05:39:34,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4e42e5d0-7b12-4318-af42-4ad5174308c8
2023-06-28 05:39:34,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8d54a3a-5c01-4c91-9cb2-03ad367aae4d
2023-06-28 05:39:34,258 - distributed.worker - INFO - Starting Worker plugin PreImport-0fd349af-ed3a-4d0a-9be1-b71b43a9102c
2023-06-28 05:39:34,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2c3300c-16ea-4ecf-a827-096037bf2834
2023-06-28 05:39:34,387 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,390 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,406 - distributed.worker - INFO - Starting Worker plugin PreImport-5be4afed-e85a-4015-b4bd-4586659b2f94
2023-06-28 05:39:34,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0ab78b9-0af6-40d0-8cba-cdf6ef61fed3
2023-06-28 05:39:34,407 - distributed.worker - INFO - Starting Worker plugin PreImport-b466bef8-14f2-4a81-b222-d830fb21bfff
2023-06-28 05:39:34,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1858a711-8857-466f-9f94-eb7bf491b208
2023-06-28 05:39:34,407 - distributed.worker - INFO - Starting Worker plugin PreImport-5015c66a-3d19-4de3-9bf1-78db9557aeb4
2023-06-28 05:39:34,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1113cdb6-e9a1-4cdd-b1d0-8a4a547bc572
2023-06-28 05:39:34,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b809dc4-a1b6-4421-ba60-031ae4f7513b
2023-06-28 05:39:34,407 - distributed.worker - INFO - Starting Worker plugin PreImport-19eec9fd-fb62-4ae9-9646-54c69dc7d65e
2023-06-28 05:39:34,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9efb551-9e07-43e6-a19a-2b8f4f848b23
2023-06-28 05:39:34,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:34,410 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:39:35,416 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-06-28 05:39:36,095 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41527'. Reason: nanny-close-gracefully
2023-06-28 05:40:03,505 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
2023-06-28 05:40:03,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37635'. Reason: nanny-close
2023-06-28 05:40:03,513 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33265'. Reason: nanny-close
2023-06-28 05:40:03,513 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34877'. Reason: nanny-close
2023-06-28 05:40:03,513 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33413'. Reason: nanny-close
2023-06-28 05:40:03,513 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43483'. Reason: nanny-close
2023-06-28 05:40:03,513 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35123'. Reason: nanny-close
2023-06-28 05:40:03,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45353'. Reason: nanny-close
2023-06-28 05:40:04,388 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:40:04,390 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:40:04,408 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:40:04,408 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:40:04,408 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:40:04,409 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:40:04,411 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 42 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-06-28 05:40:35,516 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:40:35,520 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35031 instead
  warnings.warn(
2023-06-28 05:40:35,524 - distributed.scheduler - INFO - State start
2023-06-28 05:40:35,543 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:40:35,544 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:40:35,544 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35031/status
2023-06-28 05:40:35,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40225'
2023-06-28 05:40:36,990 - distributed.scheduler - INFO - Receive client connection: Client-4d1646ac-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:37,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44076
2023-06-28 05:40:37,169 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ganh2bog', purging
2023-06-28 05:40:37,170 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rnnkd3zv', purging
2023-06-28 05:40:37,170 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6qduieop', purging
2023-06-28 05:40:37,171 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vj60nv0x', purging
2023-06-28 05:40:37,171 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-je6ezzxy', purging
2023-06-28 05:40:37,171 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3e5e2i14', purging
2023-06-28 05:40:37,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_wga89ru', purging
2023-06-28 05:40:37,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:37,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
2023-06-28 05:40:37,696 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:40:38,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36963
2023-06-28 05:40:38,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36963
2023-06-28 05:40:38,597 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-06-28 05:40:38,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:40:38,597 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:38,597 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:40:38,597 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:40:38,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xj7kngfj
2023-06-28 05:40:38,597 - distributed.worker - INFO - Starting Worker plugin PreImport-68e448db-6bd1-4457-91ea-ac3d15d1cf6f
2023-06-28 05:40:38,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-219b0753-6c7f-4c31-b2a6-30b87152cdc1
2023-06-28 05:40:38,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9173adc4-b83e-4a3f-aef9-0dd3d8ef83a2
2023-06-28 05:40:38,598 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:38,622 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36963', status: init, memory: 0, processing: 0>
2023-06-28 05:40:38,623 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36963
2023-06-28 05:40:38,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44082
2023-06-28 05:40:38,624 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:40:38,624 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:38,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:40:38,633 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:40:38,636 - distributed.scheduler - INFO - Remove client Client-4d1646ac-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:38,636 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44076; closing.
2023-06-28 05:40:38,637 - distributed.scheduler - INFO - Remove client Client-4d1646ac-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:38,637 - distributed.scheduler - INFO - Close client connection: Client-4d1646ac-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:38,638 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40225'. Reason: nanny-close
2023-06-28 05:40:38,653 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:40:38,654 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36963. Reason: nanny-close
2023-06-28 05:40:38,656 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:40:38,656 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44082; closing.
2023-06-28 05:40:38,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36963', status: closing, memory: 0, processing: 0>
2023-06-28 05:40:38,656 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36963
2023-06-28 05:40:38,657 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:40:38,657 - distributed.nanny - INFO - Worker closed
2023-06-28 05:40:39,654 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:40:39,655 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:40:39,655 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:40:39,656 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:40:39,656 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-06-28 05:40:43,242 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:40:43,247 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33101 instead
  warnings.warn(
2023-06-28 05:40:43,251 - distributed.scheduler - INFO - State start
2023-06-28 05:40:43,270 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:40:43,271 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:40:43,271 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33101/status
2023-06-28 05:40:43,331 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44377'
2023-06-28 05:40:43,452 - distributed.scheduler - INFO - Receive client connection: Client-51bdac64-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:43,464 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58264
2023-06-28 05:40:44,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:44,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
2023-06-28 05:40:45,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:40:46,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37435
2023-06-28 05:40:46,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37435
2023-06-28 05:40:46,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33681
2023-06-28 05:40:46,272 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:40:46,272 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:46,272 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:40:46,272 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:40:46,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hzpgzwm1
2023-06-28 05:40:46,273 - distributed.worker - INFO - Starting Worker plugin PreImport-dc1e7224-35d2-4866-8722-bc28053952fb
2023-06-28 05:40:46,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7523dd88-4e5a-4349-a775-c869ced8716d
2023-06-28 05:40:46,274 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1a20a14-85ee-48ce-a5bb-852a55928ed9
2023-06-28 05:40:46,274 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:46,300 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37435', status: init, memory: 0, processing: 0>
2023-06-28 05:40:46,301 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37435
2023-06-28 05:40:46,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58268
2023-06-28 05:40:46,302 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:40:46,302 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:46,304 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:40:46,351 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:40:46,354 - distributed.scheduler - INFO - Remove client Client-51bdac64-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:46,354 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58264; closing.
2023-06-28 05:40:46,354 - distributed.scheduler - INFO - Remove client Client-51bdac64-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:46,355 - distributed.scheduler - INFO - Close client connection: Client-51bdac64-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:46,355 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44377'. Reason: nanny-close
2023-06-28 05:40:46,356 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:40:46,357 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37435. Reason: nanny-close
2023-06-28 05:40:46,359 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:40:46,359 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58268; closing.
2023-06-28 05:40:46,359 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37435', status: closing, memory: 0, processing: 0>
2023-06-28 05:40:46,359 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37435
2023-06-28 05:40:46,359 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:40:46,360 - distributed.nanny - INFO - Worker closed
2023-06-28 05:40:47,472 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:40:47,472 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:40:47,473 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:40:47,473 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:40:47,474 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-06-28 05:40:49,419 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:40:49,423 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37305 instead
  warnings.warn(
2023-06-28 05:40:49,427 - distributed.scheduler - INFO - State start
2023-06-28 05:40:49,447 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:40:49,448 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:40:49,448 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37305/status
2023-06-28 05:40:53,147 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:40:53,147 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:40:53,148 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:40:53,148 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:40:53,149 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-06-28 05:40:55,045 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:40:55,049 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40823 instead
  warnings.warn(
2023-06-28 05:40:55,052 - distributed.scheduler - INFO - State start
2023-06-28 05:40:55,072 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:40:55,073 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-28 05:40:55,073 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40823/status
2023-06-28 05:40:55,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37361'
2023-06-28 05:40:55,885 - distributed.scheduler - INFO - Receive client connection: Client-58c20d59-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:55,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53100
2023-06-28 05:40:56,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:40:56,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:40:56,709 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:40:57,469 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43617
2023-06-28 05:40:57,469 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43617
2023-06-28 05:40:57,469 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35775
2023-06-28 05:40:57,469 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-28 05:40:57,469 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:57,469 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:40:57,470 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:40:57,470 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-242xee02
2023-06-28 05:40:57,470 - distributed.worker - INFO - Starting Worker plugin PreImport-6f9817bd-fc1a-452a-95dc-5a16adf30c0c
2023-06-28 05:40:57,470 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-030157bc-9b07-4232-b9c8-15e936651e55
2023-06-28 05:40:57,470 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4bf14ae7-f1da-409b-bed4-4b29b019352f
2023-06-28 05:40:57,471 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:57,524 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43617', status: init, memory: 0, processing: 0>
2023-06-28 05:40:57,525 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43617
2023-06-28 05:40:57,525 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53112
2023-06-28 05:40:57,526 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-28 05:40:57,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:40:57,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-28 05:40:57,531 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:40:57,534 - distributed.scheduler - INFO - Remove client Client-58c20d59-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:57,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53100; closing.
2023-06-28 05:40:57,534 - distributed.scheduler - INFO - Remove client Client-58c20d59-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:57,534 - distributed.scheduler - INFO - Close client connection: Client-58c20d59-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:40:57,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37361'. Reason: nanny-close
2023-06-28 05:40:57,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:40:57,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43617. Reason: nanny-close
2023-06-28 05:40:57,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-28 05:40:57,579 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53112; closing.
2023-06-28 05:40:57,579 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43617', status: closing, memory: 0, processing: 0>
2023-06-28 05:40:57,579 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43617
2023-06-28 05:40:57,580 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:40:57,580 - distributed.nanny - INFO - Worker closed
2023-06-28 05:40:58,501 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:40:58,502 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:40:58,502 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:40:58,503 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-28 05:40:58,503 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-06-28 05:41:00,451 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:41:00,455 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43067 instead
  warnings.warn(
2023-06-28 05:41:00,459 - distributed.scheduler - INFO - State start
2023-06-28 05:41:00,479 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:41:00,480 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:41:00,481 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43067/status
2023-06-28 05:41:00,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38659'
2023-06-28 05:41:00,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41135'
2023-06-28 05:41:00,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33715'
2023-06-28 05:41:00,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38643'
2023-06-28 05:41:00,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41139'
2023-06-28 05:41:00,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40435'
2023-06-28 05:41:00,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46283'
2023-06-28 05:41:00,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44095'
2023-06-28 05:41:02,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:02,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:02,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:02,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:02,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:02,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:02,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:02,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:02,424 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:02,439 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:02,440 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:02,442 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:02,442 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:02,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:02,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:02,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:02,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:02,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:02,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:02,647 - distributed.scheduler - INFO - Receive client connection: Client-5bf1f90d-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:02,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:02,653 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:02,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49026
2023-06-28 05:41:02,684 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:02,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:02,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:03,926 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43987
2023-06-28 05:41:03,926 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43987
2023-06-28 05:41:03,927 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39733
2023-06-28 05:41:03,927 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:03,927 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:03,927 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:03,927 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:41:03,927 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s9q3av09
2023-06-28 05:41:03,927 - distributed.worker - INFO - Starting Worker plugin RMMSetup-83b90c0a-1f9a-4398-b950-845841813763
2023-06-28 05:41:04,456 - distributed.worker - INFO - Starting Worker plugin PreImport-29daaa91-c42c-4f33-8d96-10251f1eae83
2023-06-28 05:41:04,457 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4c13955-4352-4ed7-9ce9-9ebc660ef96f
2023-06-28 05:41:04,458 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:04,496 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43987', status: init, memory: 0, processing: 0>
2023-06-28 05:41:04,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43987
2023-06-28 05:41:04,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49056
2023-06-28 05:41:04,499 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:04,499 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:04,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:05,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41865
2023-06-28 05:41:05,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41865
2023-06-28 05:41:05,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36025
2023-06-28 05:41:05,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,634 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,634 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:05,634 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:41:05,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1uwmgs46
2023-06-28 05:41:05,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-979565e3-8918-4a5c-847c-a89ef33d43af
2023-06-28 05:41:05,641 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40153
2023-06-28 05:41:05,641 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40153
2023-06-28 05:41:05,641 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39813
2023-06-28 05:41:05,641 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,641 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,641 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:05,641 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37085
2023-06-28 05:41:05,641 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:41:05,641 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1186lumr
2023-06-28 05:41:05,641 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37085
2023-06-28 05:41:05,641 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42933
2023-06-28 05:41:05,641 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,641 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,642 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:05,642 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ac8e700-fc3f-4d20-a880-51f7c5504587
2023-06-28 05:41:05,642 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:41:05,642 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5614i7lq
2023-06-28 05:41:05,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ccf9d815-9c14-404d-ad28-7510f7473f9a
2023-06-28 05:41:05,643 - distributed.worker - INFO - Starting Worker plugin PreImport-c4760b5d-6e1e-408a-80ec-f499c190c29b
2023-06-28 05:41:05,643 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59caed98-3978-4899-966c-0a8138c7b2b4
2023-06-28 05:41:05,650 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40583
2023-06-28 05:41:05,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40583
2023-06-28 05:41:05,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42605
2023-06-28 05:41:05,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,650 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,650 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:05,650 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:41:05,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-22dcbuei
2023-06-28 05:41:05,651 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb0a8042-22be-42a5-87db-212bf77a3fad
2023-06-28 05:41:05,651 - distributed.worker - INFO - Starting Worker plugin PreImport-fcb185d7-2c41-455f-92a9-43f58655c67e
2023-06-28 05:41:05,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e52720d-a2c8-4a5a-822c-f6761d8c984a
2023-06-28 05:41:05,652 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36649
2023-06-28 05:41:05,652 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36649
2023-06-28 05:41:05,652 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33921
2023-06-28 05:41:05,652 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,652 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,652 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:05,652 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:41:05,652 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q79wth3i
2023-06-28 05:41:05,653 - distributed.worker - INFO - Starting Worker plugin PreImport-3898ed57-9d57-496d-a5d0-df79bce8ed74
2023-06-28 05:41:05,653 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35603820-d9a7-47fd-a083-957b4dabf1b6
2023-06-28 05:41:05,716 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45241
2023-06-28 05:41:05,716 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45241
2023-06-28 05:41:05,716 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43679
2023-06-28 05:41:05,716 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,717 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,717 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:05,717 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:41:05,717 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-698qk2lh
2023-06-28 05:41:05,717 - distributed.worker - INFO - Starting Worker plugin RMMSetup-414f2d83-e6a1-41f8-ae1f-e0881c7c8cfd
2023-06-28 05:41:05,719 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42819
2023-06-28 05:41:05,719 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42819
2023-06-28 05:41:05,719 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37157
2023-06-28 05:41:05,719 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,719 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,719 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:05,719 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-28 05:41:05,719 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u1y1hpkv
2023-06-28 05:41:05,719 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e639934-3273-4fb7-8fb3-1dc952e453ec
2023-06-28 05:41:05,720 - distributed.worker - INFO - Starting Worker plugin PreImport-1a53b174-4b13-4b5a-8267-8789fae80cc5
2023-06-28 05:41:05,720 - distributed.worker - INFO - Starting Worker plugin RMMSetup-058af094-15c6-4edb-aaf6-83a37daa687d
2023-06-28 05:41:05,850 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,850 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,865 - distributed.worker - INFO - Starting Worker plugin PreImport-c7f7735b-0903-402f-a075-89c922b908a0
2023-06-28 05:41:05,865 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac28992d-150d-440c-a99e-5f493423f0fd
2023-06-28 05:41:05,865 - distributed.worker - INFO - Starting Worker plugin PreImport-24e7ca16-fc50-4663-8d88-42348ed3006c
2023-06-28 05:41:05,865 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cde7900f-d2c3-4044-af3c-e463cc37c4f5
2023-06-28 05:41:05,866 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a421b0e9-9ba2-4b84-81ff-a9a83228c563
2023-06-28 05:41:05,866 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,866 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,866 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,871 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,871 - distributed.worker - INFO - Starting Worker plugin PreImport-58482ef2-93d3-43d9-92c0-78a871a58a76
2023-06-28 05:41:05,872 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34379654-d92a-4122-8f34-5a62a229cca2
2023-06-28 05:41:05,872 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,881 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37085', status: init, memory: 0, processing: 0>
2023-06-28 05:41:05,881 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37085
2023-06-28 05:41:05,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49090
2023-06-28 05:41:05,882 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,882 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,882 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40583', status: init, memory: 0, processing: 0>
2023-06-28 05:41:05,883 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40583
2023-06-28 05:41:05,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49086
2023-06-28 05:41:05,884 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,884 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:05,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:05,892 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41865', status: init, memory: 0, processing: 0>
2023-06-28 05:41:05,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41865
2023-06-28 05:41:05,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49092
2023-06-28 05:41:05,893 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,893 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40153', status: init, memory: 0, processing: 0>
2023-06-28 05:41:05,893 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40153
2023-06-28 05:41:05,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49100
2023-06-28 05:41:05,894 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,894 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:05,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:05,898 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42819', status: init, memory: 0, processing: 0>
2023-06-28 05:41:05,898 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42819
2023-06-28 05:41:05,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49114
2023-06-28 05:41:05,899 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,899 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,900 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36649', status: init, memory: 0, processing: 0>
2023-06-28 05:41:05,900 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36649
2023-06-28 05:41:05,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49110
2023-06-28 05:41:05,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:05,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,901 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:05,906 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45241', status: init, memory: 0, processing: 0>
2023-06-28 05:41:05,907 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45241
2023-06-28 05:41:05,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49120
2023-06-28 05:41:05,908 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:05,908 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:05,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:05,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:41:05,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:41:05,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:41:05,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:41:05,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:41:05,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:41:05,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:41:05,972 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-28 05:41:05,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:05,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:05,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:05,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:05,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:05,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:05,987 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:05,987 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:05,990 - distributed.scheduler - INFO - Remove client Client-5bf1f90d-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:05,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49026; closing.
2023-06-28 05:41:05,991 - distributed.scheduler - INFO - Remove client Client-5bf1f90d-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:05,991 - distributed.scheduler - INFO - Close client connection: Client-5bf1f90d-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:05,992 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41135'. Reason: nanny-close
2023-06-28 05:41:05,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:05,993 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41139'. Reason: nanny-close
2023-06-28 05:41:05,993 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:05,994 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38659'. Reason: nanny-close
2023-06-28 05:41:05,994 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36649. Reason: nanny-close
2023-06-28 05:41:05,994 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:05,994 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45241. Reason: nanny-close
2023-06-28 05:41:05,994 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33715'. Reason: nanny-close
2023-06-28 05:41:05,995 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:05,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38643'. Reason: nanny-close
2023-06-28 05:41:05,995 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43987. Reason: nanny-close
2023-06-28 05:41:05,996 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:05,996 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40153. Reason: nanny-close
2023-06-28 05:41:05,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40435'. Reason: nanny-close
2023-06-28 05:41:05,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49110; closing.
2023-06-28 05:41:05,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:05,996 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:05,996 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36649', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:05,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46283'. Reason: nanny-close
2023-06-28 05:41:05,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:05,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36649
2023-06-28 05:41:05,997 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40583. Reason: nanny-close
2023-06-28 05:41:05,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:05,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44095'. Reason: nanny-close
2023-06-28 05:41:05,997 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37085. Reason: nanny-close
2023-06-28 05:41:05,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:05,997 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:05,997 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49120; closing.
2023-06-28 05:41:05,997 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:05,998 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41865. Reason: nanny-close
2023-06-28 05:41:05,998 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:05,998 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:05,998 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36649
2023-06-28 05:41:05,998 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45241', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:05,998 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45241
2023-06-28 05:41:05,999 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:05,999 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36649
2023-06-28 05:41:05,999 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42819. Reason: nanny-close
2023-06-28 05:41:05,999 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36649
2023-06-28 05:41:05,999 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:05,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49100; closing.
2023-06-28 05:41:05,999 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36649
2023-06-28 05:41:05,999 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:05,999 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:05,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49056; closing.
2023-06-28 05:41:06,000 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40153', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:06,000 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:06,000 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40153
2023-06-28 05:41:06,000 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:06,000 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43987', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:06,001 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43987
2023-06-28 05:41:06,001 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:06,001 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49086; closing.
2023-06-28 05:41:06,001 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:06,002 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40583', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:06,002 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:06,002 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40583
2023-06-28 05:41:06,002 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:06,002 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49092; closing.
2023-06-28 05:41:06,002 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49090; closing.
2023-06-28 05:41:06,003 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41865', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:06,003 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41865
2023-06-28 05:41:06,003 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37085', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:06,003 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37085
2023-06-28 05:41:06,004 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49114; closing.
2023-06-28 05:41:06,004 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42819', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:06,004 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42819
2023-06-28 05:41:06,005 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:41:07,309 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:41:07,309 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:41:07,310 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:41:07,311 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:41:07,311 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-06-28 05:41:09,204 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:41:09,208 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38011 instead
  warnings.warn(
2023-06-28 05:41:09,212 - distributed.scheduler - INFO - State start
2023-06-28 05:41:09,230 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:41:09,231 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:41:09,232 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38011/status
2023-06-28 05:41:09,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42929'
2023-06-28 05:41:09,525 - distributed.scheduler - INFO - Receive client connection: Client-61321d95-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:09,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49230
2023-06-28 05:41:10,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:10,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:10,786 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:11,668 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45395
2023-06-28 05:41:11,668 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45395
2023-06-28 05:41:11,668 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41031
2023-06-28 05:41:11,668 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:11,668 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:11,669 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:11,669 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:41:11,669 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-onvzcbkx
2023-06-28 05:41:11,669 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc4a892d-1455-45ef-be4c-47653db4b13c
2023-06-28 05:41:11,669 - distributed.worker - INFO - Starting Worker plugin PreImport-71d7f4b3-0d27-48fb-9354-d05ee9d65fca
2023-06-28 05:41:11,670 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c830ad7-9331-4d5a-81d9-c47ebe12bd57
2023-06-28 05:41:11,780 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:11,808 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45395', status: init, memory: 0, processing: 0>
2023-06-28 05:41:11,809 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45395
2023-06-28 05:41:11,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39768
2023-06-28 05:41:11,810 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-28 05:41:11,810 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:11,815 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-28 05:41:11,876 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-28 05:41:11,879 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:11,881 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-28 05:41:11,883 - distributed.scheduler - INFO - Remove client Client-61321d95-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:11,883 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49230; closing.
2023-06-28 05:41:11,884 - distributed.scheduler - INFO - Remove client Client-61321d95-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:11,884 - distributed.scheduler - INFO - Close client connection: Client-61321d95-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:11,885 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42929'. Reason: nanny-close
2023-06-28 05:41:11,885 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-28 05:41:11,886 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45395. Reason: nanny-close
2023-06-28 05:41:11,888 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39768; closing.
2023-06-28 05:41:11,888 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-28 05:41:11,888 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45395', status: closing, memory: 0, processing: 0>
2023-06-28 05:41:11,888 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45395
2023-06-28 05:41:11,889 - distributed.scheduler - INFO - Lost all workers
2023-06-28 05:41:11,889 - distributed.nanny - INFO - Worker closed
2023-06-28 05:41:12,851 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:41:12,851 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:41:12,852 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:41:12,852 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:41:12,853 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-06-28 05:41:14,748 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:41:14,753 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35793 instead
  warnings.warn(
2023-06-28 05:41:14,757 - distributed.scheduler - INFO - State start
2023-06-28 05:41:14,776 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-28 05:41:14,777 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-28 05:41:14,778 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35793/status
2023-06-28 05:41:14,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43507'
2023-06-28 05:41:15,039 - distributed.scheduler - INFO - Receive client connection: Client-648109ff-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:15,052 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39882
2023-06-28 05:41:16,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:16,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:16,335 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-28 05:41:17,214 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42969
2023-06-28 05:41:17,215 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42969
2023-06-28 05:41:17,215 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41731
2023-06-28 05:41:17,215 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-28 05:41:17,215 - distributed.worker - INFO - -------------------------------------------------
2023-06-28 05:41:17,215 - distributed.worker - INFO -               Threads:                          1
2023-06-28 05:41:17,215 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-28 05:41:17,215 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c1u_1vuo
2023-06-28 05:41:17,215 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a1c641b-5557-43bc-982f-6f8533532151
2023-06-28 05:41:17,215 - distributed.worker - INFO - Starting Worker plugin PreImport-d47eaeb8-7b27-4866-b493-fe4a35134a04
2023-06-28 05:41:17,216 - distributed.worker - INFO - Starting Worker plugin RMMSetup-981ffe24-e655-4c20-8c9b-37507f672cd0
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-28 05:41:17,501 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42969. Reason: worker-close
2023-06-28 05:41:17,501 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-28 05:41:17,504 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-28 05:41:17,529 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-28 05:41:17,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43507'. Reason: nanny-instantiate-failed
2023-06-28 05:41:17,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-28 05:41:17,963 - distributed.nanny - INFO - Worker process 54487 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-28 05:41:25,076 - distributed.scheduler - INFO - Remove client Client-648109ff-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:25,077 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39882; closing.
2023-06-28 05:41:25,077 - distributed.scheduler - INFO - Remove client Client-648109ff-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:25,078 - distributed.scheduler - INFO - Close client connection: Client-648109ff-1576-11ee-88ee-d8c49764f6bb
2023-06-28 05:41:25,078 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-28 05:41:25,079 - distributed.scheduler - INFO - Scheduler closing...
2023-06-28 05:41:25,079 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-28 05:41:25,080 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-28 05:41:25,080 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
2023-06-28 05:41:34,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:34,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:34,360 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:34,360 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:34,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:34,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:34,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:34,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:34,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:34,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:34,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:34,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:34,384 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:34,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:34,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:34,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40547 instead
  warnings.warn(
2023-06-28 05:41:44,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:44,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:44,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:44,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:44,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:44,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:44,683 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:44,683 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:44,688 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:44,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:44,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:44,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:44,695 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:44,695 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:44,696 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:44,696 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41359 instead
  warnings.warn(
2023-06-28 05:41:55,599 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:55,599 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:55,601 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:55,601 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:55,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:55,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:55,653 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:55,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:55,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:55,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:55,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:55,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:55,692 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:55,692 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:41:55,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:41:55,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46215 instead
  warnings.warn(
2023-06-28 05:42:07,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:07,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:07,552 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:07,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:07,552 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:07,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:07,567 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:07,567 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:07,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:07,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:07,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:07,575 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:07,587 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:07,587 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:07,619 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:07,619 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36377 instead
  warnings.warn(
2023-06-28 05:42:20,441 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:20,441 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:20,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:20,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:20,449 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:20,449 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:20,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:20,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:20,487 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:20,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:20,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:20,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:20,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:20,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:20,559 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:20,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34459 instead
  warnings.warn(
2023-06-28 05:42:33,998 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:33,998 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:33,998 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:33,998 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:34,001 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:34,001 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:34,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:34,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:34,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:34,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:34,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:34,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:34,124 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:34,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:34,226 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:34,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45803 instead
  warnings.warn(
2023-06-28 05:42:48,439 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:48,440 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:48,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:48,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:48,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:48,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:48,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:48,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:48,477 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:48,477 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:48,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:48,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:48,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:48,501 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:42:48,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:42:48,553 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.0.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39113 instead
  warnings.warn(
2023-06-28 05:43:01,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:43:01,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:43:01,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:43:01,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:43:01,815 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:43:01,816 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:43:01,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:43:01,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:43:01,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:43:01,838 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:43:01,850 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:43:01,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:43:01,855 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:43:01,855 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-28 05:43:01,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-28 05:43:01,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37745 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40435 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39883 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37761 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37991 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40239 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46137 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35875 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46013 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43877 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39625 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33241 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39057 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34915 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40899 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38175 instead
  warnings.warn(
2023-06-28 05:48:49,643 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-28 05:48:49,650 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ff1c7d27850>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-28 05:48:49,732 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-28 05:48:49,742 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fa9f0719760>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-28 05:48:51,747 - distributed.nanny - ERROR - Worker process died unexpectedly
