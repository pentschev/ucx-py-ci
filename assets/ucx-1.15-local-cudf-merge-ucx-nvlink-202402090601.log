[dgx13:93825:0:93825] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93825) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f21f709c07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f21f709c274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f21f709c43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f2233d8a420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f21f711b6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f21f7144839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f21f70563df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f21f7059838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f21f70a54a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f21f70585dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f21f71188da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f21f71d106a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55ee0fad53f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55ee0facffb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55ee0fae1469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55ee0fad14e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55ee0fb846d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f221e3641e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55ee0fad96ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55ee0fa943ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55ee0fad8723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55ee0fad6929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55ee0fae1712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55ee0fad14e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55ee0fae1712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55ee0fad14e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55ee0fae1712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55ee0fad14e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55ee0fae1712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55ee0fad14e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55ee0facffb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55ee0fae1469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55ee0fad2042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55ee0facffb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55ee0faee8cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55ee0faef04c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55ee0fbb280e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55ee0fad96ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55ee0fad53f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55ee0fae1712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55ee0faee9ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55ee0fad53f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55ee0fae1712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55ee0fad14e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55ee0facffb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55ee0fae1469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55ee0fad14e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55ee0fae1712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55ee0fad1232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55ee0facffb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55ee0fae1469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55ee0fad2042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55ee0facffb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55ee0facfc88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55ee0facfc39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55ee0fb7d2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x55ee0fbaa6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x55ee0fba6a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55ee0fb9e87a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55ee0fb9e76c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55ee0fb9d9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55ee0fb71107]
=================================
[dgx13:93835:0:93835] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93835) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7eff01e1d07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7eff01e1d274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7eff01e1d43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7eff3eb21420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7eff01e9c6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7eff01ec5839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7eff01dd73df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7eff01dda838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7eff01e264a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7eff01dd95dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7eff01e998da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7eff01f5206a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55be0f84f3f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55be0f849fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55be0f85b469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55be0f84b4e6]
16  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55be0f85b712]
17  /opt/conda/envs/gdf/bin/python(+0x14ca83) [0x55be0f868a83]
18  /opt/conda/envs/gdf/bin/python(+0x25819c) [0x55be0f97419c]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55be0f80e3ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55be0f852723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55be0f850929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55be0f85b712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55be0f84b4e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55be0f85b712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55be0f84b4e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55be0f85b712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55be0f84b4e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55be0f85b712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55be0f84b4e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55be0f849fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55be0f85b469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55be0f84c042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55be0f849fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55be0f8688cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55be0f86904c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55be0f92c80e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55be0f8536ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55be0f84f3f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55be0f85b712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55be0f8689ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55be0f84f3f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55be0f85b712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55be0f84b4e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55be0f849fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55be0f85b469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55be0f84b4e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55be0f85b712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55be0f84b232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55be0f849fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55be0f85b469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55be0f84c042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55be0f849fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55be0f849c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55be0f849c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55be0f8f72cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x55be0f9246ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x55be0f920a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55be0f91887a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55be0f91876c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55be0f9179a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55be0f8eb107]
=================================
[dgx13:93841:0:93841] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93841) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fe6e0f1107d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fe6e0f11274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fe6e0f1143a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fe719bed420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fe6e0f906b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fe6e0fb9839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fe6e0ecb3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fe6e0ece838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fe6e0f1a4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fe6e0ecd5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fe6e0f8d8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fe6e104606a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55f6b7d553f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f6b7d4ffb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f6b7d61469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f6b7d514e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55f6b7e046d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fe70d2131e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55f6b7d596ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55f6b7d143ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55f6b7d58723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55f6b7d56929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f6b7d61712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f6b7d514e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f6b7d61712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f6b7d514e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f6b7d61712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f6b7d514e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f6b7d61712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f6b7d514e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f6b7d4ffb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f6b7d61469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55f6b7d52042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f6b7d4ffb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55f6b7d6e8cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55f6b7d6f04c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55f6b7e3280e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55f6b7d596ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55f6b7d553f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f6b7d61712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55f6b7d6e9ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55f6b7d553f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f6b7d61712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f6b7d514e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f6b7d4ffb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f6b7d61469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f6b7d514e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f6b7d61712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55f6b7d51232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f6b7d4ffb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f6b7d61469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55f6b7d52042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f6b7d4ffb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55f6b7d4fc88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55f6b7d4fc39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55f6b7dfd2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x55f6b7e2a6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x55f6b7e26a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55f6b7e1e87a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55f6b7e1e76c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55f6b7e1d9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55f6b7df1107]
=================================
[dgx13:93830:0:93830] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  93830) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f1f704cf07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f1f704cf274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f1f704cf43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f1f9b1a0420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f1f7054e6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f1f70577839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f1f704893df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f1f7048c838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f1f704d84a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f1f7048b5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f1f7054b8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f1f7060406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5576dba4a3f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5576dba44fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5576dba56469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5576dba464e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x5576dbaf96d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f1f8577e1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5576dba4e6ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x5576dba093ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x5576dba4d723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x5576dba4b929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5576dba56712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5576dba464e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5576dba56712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5576dba464e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5576dba56712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5576dba464e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5576dba56712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5576dba464e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5576dba44fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5576dba56469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x5576dba47042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5576dba44fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x5576dba638cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5576dba6404c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x5576dbb2780e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5576dba4e6ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5576dba4a3f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5576dba56712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x5576dba639ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x5576dba4a3f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5576dba56712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5576dba464e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5576dba44fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5576dba56469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5576dba464e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x5576dba56712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x5576dba46232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5576dba44fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5576dba56469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x5576dba47042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x5576dba44fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x5576dba44c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5576dba44c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5576dbaf22cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x5576dbb1f6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x5576dbb1ba63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5576dbb1387a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5576dbb1376c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5576dbb129a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5576dbae6107]
=================================
Task exception was never retrieved
future: <Task finished name='Task-1059' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2024-02-09 07:13:49,225 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34767
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f72ac18a280, tag: 0x87d23ba085b92e82, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2857, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f72ac18a280, tag: 0x87d23ba085b92e82, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-09 07:13:49,227 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:41051 -> ucx://127.0.0.1:34767
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f72ac18a380, tag: 0x86bf60e3d4a7247d, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-09 07:13:49,230 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34767
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 467, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1016, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 328, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 60, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXNotConnected: <stream_recv>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 469, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2857, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-02-09 07:13:49,230 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34767
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 467, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1016, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 328, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 60, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 469, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2857, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-02-09 07:13:49,231 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34767
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 467, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1016, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 328, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 60, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 469, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2857, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-02-09 07:13:49,282 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34761
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f6afc42c240, tag: 0x51560be4d87eed52, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f6afc42c240, tag: 0x51560be4d87eed52, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-09 07:13:49,282 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34761
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f69cc9ad240, tag: 0xb0d98dfdd47101b1, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f69cc9ad240, tag: 0xb0d98dfdd47101b1, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-09 07:13:49,282 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34761
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fec55fd5140, tag: 0xca99c010aa2ac4e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fec55fd5140, tag: 0xca99c010aa2ac4e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-09 07:13:49,283 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:48057 -> ucx://127.0.0.1:34761
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f69cc9ad400, tag: 0xc6d3a854e78288dc, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-09 07:13:49,283 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:53767 -> ucx://127.0.0.1:34761
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fec55fd5300, tag: 0x9ba9e0fb573ea6dd, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-09 07:13:49,288 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49849
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7f6afc42c280, tag: 0x74dad14d5fdf3ba6, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7f6afc42c280, tag: 0x74dad14d5fdf3ba6, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
Task exception was never retrieved
future: <Task finished name='Task-1113' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2024-02-09 07:13:49,357 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49849
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f72ac18a200, tag: 0x7b82775e4f5c4110, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f72ac18a200, tag: 0x7b82775e4f5c4110, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-09 07:13:49,357 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34761
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f72ac18a180, tag: 0x9cf25e31be19f338, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f72ac18a180, tag: 0x9cf25e31be19f338, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-09 07:13:49,313 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49849
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fec55fd5200, tag: 0xd3e4327d83971a99, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fec55fd5200, tag: 0xd3e4327d83971a99, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-09 07:13:49,339 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:48057 -> ucx://127.0.0.1:49849
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f69cc9ad300, tag: 0xc501511913587b5, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-09 07:13:49,361 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49849
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f69cc9ad280, tag: 0xa86475274405560c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f69cc9ad280, tag: 0xa86475274405560c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-09 07:13:50,193 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56699
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f6afc42c100, tag: 0xc97ab58b58b4729a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f6afc42c100, tag: 0xc97ab58b58b4729a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-09 07:13:50,193 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56699
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f72ac18a1c0, tag: 0x99db9b69413ec0b4, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f72ac18a1c0, tag: 0x99db9b69413ec0b4, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-09 07:13:50,193 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:53767 -> ucx://127.0.0.1:56699
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fec55fd5340, tag: 0xd6c576542d8eae32, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-09 07:13:50,193 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56699
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f69cc9ad180, tag: 0xcbec2d86bb47da54, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f69cc9ad180, tag: 0xcbec2d86bb47da54, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-02-09 07:13:50,194 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:41051 -> ucx://127.0.0.1:56699
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f72ac18a300, tag: 0xb04bcee96ce85589, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-09 07:13:50,194 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56699
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fec55fd5240, tag: 0xe59361479387ad54, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fec55fd5240, tag: 0xe59361479387ad54, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-09 07:13:50,194 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52601 -> ucx://127.0.0.1:56699
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f6afc42c2c0, tag: 0x491d07f34968fddd, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-09 07:13:51,588 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-09 07:13:51,588 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-09 07:13:51,853 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-54efe4e1c1ff02b405ca9dd1c57e206a', 3)
Function:  _concat
args:      ([               key   payload  _partitions
shuffle                                  
0           111603  96952799            3
0           302675  61731109            3
0           323781  70618780            3
0               37    364829            3
0           326882  14871920            3
...            ...       ...          ...
0        799994681  35555080            3
0        799956543  77009882            3
0        799866913  36526382            3
0        799801055  26877559            3
0        799993166   3819991            3

[12499485 rows x 3 columns],                key   payload  _partitions
shuffle                                  
1           551202  15450106            3
1           675377  17135354            3
1           630083  39142237            3
1           545045  70783363            3
1           714829   8828546            3
...            ...       ...          ...
1        799986746  49202769            3
1        799993255  80183296            3
1 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-09 07:13:51,862 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 7)
Function:  _concat
args:      ([                key   payload  _partitions
123399    804083262  43121962            7
123400    859973895  64081390            7
123403    819165046  14102153            7
20992     858528615  17654447            7
123407    856805845  29394821            7
...             ...       ...          ...
99996984  836687002  85061878            7
99996989  809985918  59390126            7
99997000  849223238  62734459            7
99997064  819998431  14430049            7
99997085  807408618  20805090            7

[12503560 rows x 3 columns],                 key   payload  _partitions
36937     925463924  43582270            7
36940     943594838  49465239            7
36955     933011011  94484762            7
36957     957335768  53719224            7
19195     719625237  35095718            7
...             ...       ...          ...
99996127  925067624  66476866            7
99996134  948349056  95436839            7
99996141  905123752  78709395            7
99996146  916586862  3
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-09 07:13:51,888 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-54efe4e1c1ff02b405ca9dd1c57e206a', 6)
Function:  _concat
args:      ([               key   payload  _partitions
shuffle                                  
0           296030  93730066            6
0            43807  73456943            6
0            25420  39877176            6
0            16570  18906712            6
0           286791  40279654            6
...            ...       ...          ...
0        799942264  66939924            6
0        799920180  59123389            6
0        799814368  14012369            6
0        799882390  95294688            6
0        799930507   8855671            6

[12499059 rows x 3 columns],                key   payload  _partitions
shuffle                                  
1           552787  86064439            6
1           554804  16763252            6
1           536985  79245265            6
1           517066   4556557            6
1           548734  67022340            6
...            ...       ...          ...
1        799961128  93283379            6
1        799850733  80324082            6
1 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-09 07:13:51,895 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 2)
Function:  _concat
args:      ([                key   payload  _partitions
123392    834491532  97468966            2
123398    814433676  23946513            2
123406    806131782  93826677            2
20994      10483064  98221626            2
123409    402202444  36702127            2
...             ...       ...          ...
99996987  815458122  95381789            2
99996994  835571652  51109070            2
99997013  825427263  32010948            2
99997020  849376281  94946035            2
99997075  838842962   9741608            2

[12500460 rows x 3 columns],                 key   payload  _partitions
36928     958936645   9128218            2
36930     968716632  55112812            2
36934     613097052  28818016            2
36950     322468778  72082014            2
36951     723269736  39701701            2
...             ...       ...          ...
99996097  938977552  67712066            2
99996104  716581557  13076527            2
99996105  917381186  12573426            2
99996111  952133255  2
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-09 07:13:51,988 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-54efe4e1c1ff02b405ca9dd1c57e206a', 0)
Function:  _concat
args:      ([               key   payload  _partitions
shuffle                                  
0           303344  48840305            0
0           175951  81932321            0
0            65980  91405797            0
0            74520  52309193            0
0            75731  85283051            0
...            ...       ...          ...
0        799931459  66583495            0
0        799956978  89175041            0
0        799820340  88385382            0
0        799936515   6270146            0
0        799943408  74352937            0

[12497200 rows x 3 columns],                key   payload  _partitions
shuffle                                  
1           578952  83381854            0
1           574947  41967079            0
1           686178  22805352            0
1           675390  10654891            0
1           630087  70518748            0
...            ...       ...          ...
1        799999817  83988827            0
1        799873728  70453778            0
1 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-09 07:13:52,000 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 5)
Function:  _concat
args:      ([                key   payload  _partitions
123393    823046644  48754626            5
123395    852415656  97726698            5
123402    836499983  63867205            5
21004     308660951  85515726            5
123410    839645845   8766326            5
...             ...       ...          ...
99997009  866053351  61242550            5
99997011  200466570  26660558            5
99997021  709066036  91244558            5
99997062  209264782  95392955            5
99997087  865579183  36666496            5

[12501172 rows x 3 columns],                 key   payload  _partitions
36939     957419677  28772919            5
36942     319505468  38135180            5
36943     955949672   9634071            5
36944     953053432  71440351            5
36946     219577906  63752794            5
...             ...       ...          ...
99996119  714657488  64828230            5
99996129  966002685  52107298            5
99996131  955941140  60057224            5
99996135  959105558  7
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-09 07:13:52,033 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-54efe4e1c1ff02b405ca9dd1c57e206a', 5)
Function:  _concat
args:      ([               key   payload  _partitions
shuffle                                  
0            44460  72271237            5
0            45330  91219073            5
0            61766  28000477            5
0            83431  86222724            5
0            46420  47504797            5
...            ...       ...          ...
0        799831242  33325030            5
0        799861991  92155134            5
0        799956528  99976116            5
0        799988614  31537217            5
0        799908045  48204308            5

[12507714 rows x 3 columns],                key   payload  _partitions
shuffle                                  
1           557835  70875098            5
1           548338  59189755            5
1           573234  75781863            5
1           503244  29219748            5
1           624627  78594896            5
...            ...       ...          ...
1        799953464  35691907            5
1        799893625  92804072            5
1 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-09 07:13:52,136 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-54efe4e1c1ff02b405ca9dd1c57e206a', 1)
Function:  _concat
args:      ([               key   payload  _partitions
shuffle                                  
0            84739  69168053            1
0            46994  98647104            1
0            93558  64282464            1
0           318121  24943967            1
0           454731  62793393            1
...            ...       ...          ...
0        799891489  72312451            1
0        799896815  23599796            1
0        799842359  31291923            1
0        799924946  84400289            1
0        799881837  52412642            1

[12502439 rows x 3 columns],                key   payload  _partitions
shuffle                                  
1           682140  80224838            1
1           504747  13426283            1
1           559974  57153943            1
1           542986  29418504            1
1           686623  29297428            1
...            ...       ...          ...
1        799990587  11150634            1
1        799858259  88087254            1
1 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-09 07:13:52,233 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-09 07:13:52,233 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-09 07:14:01,252 - distributed.nanny - WARNING - Restarting worker
2024-02-09 07:14:01,869 - distributed.nanny - WARNING - Restarting worker
2024-02-09 07:14:01,871 - distributed.nanny - WARNING - Restarting worker
2024-02-09 07:14:02,139 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 36 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
