2023-05-17 06:57:20,229 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:20,229 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:20,231 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:20,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:20,249 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:20,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:20,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:20,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:20,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:20,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:20,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:20,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:20,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:20,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:20,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:20,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1684306648.381816] [dgx13:81457:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_3: LRU push returned Unsupported operation
[dgx13:81457:0:81457]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  81457) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7fa2d4464dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7fa2d4461d28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7fa2d4461e44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x83605) [0x7fa2d42b4605]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7fa2d4289069]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7fa2d42cf1a0]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7fa2d42d40be]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7fa2d42d3872]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7fa2d43695d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x55c3386a6dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x55c3386a51a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55c33868bd36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c33868527a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c338696c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55c3386873cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c33868527a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c338696c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55c3386873cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c3386ab70e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c33868c923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c3386ab70e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c33868c923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c3386ab70e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c33868c923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c3386ab70e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c33868c923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c3386ab70e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55c33868c923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55c3386ab70e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fa2f2fc02fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7fa2f2fc0b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55c33868f2bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55c338642817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55c33868df83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55c33868bd36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c338696ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c33868681b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c338696ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c33868681b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c338696ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c33868681b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c338696ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c33868681b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c33868527a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c338696c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55c33868afa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c33868527a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55c3386a4935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55c3386a5104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55c33876bfc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55c33868f2bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55c33868a1bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c338696ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55c3386a4c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55c33868a1bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c338696ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c33868681b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c33868527a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c338696c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c33868681b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c338696ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55c338686568]
=================================
2023-05-17 06:57:28,578 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:36726
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7f0e10ce2240, tag: 0x2b19650cc00ff6c, nbytes: 800000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7f0e10ce2240, tag: 0x2b19650cc00ff6c, nbytes: 800000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
[1684306648.699914] [dgx13:81475:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_0: LRU push returned Unsupported operation
[dgx13:81475:0:81475]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  81475) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f27c987edec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7f27c987bd28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7f27c987be44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x83605) [0x7f27c993a605]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7f27c990f069]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7f27c99551a0]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7f27c995a0be]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7f27c9959872]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7f27c99ef5d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x563f2e5a5dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x563f2e5a41a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x563f2e58ad36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x563f2e58427a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x563f2e595c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x563f2e5863cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x563f2e58427a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x563f2e595c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x563f2e5863cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x563f2e5aa70e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x563f2e58b923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x563f2e5aa70e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x563f2e58b923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x563f2e5aa70e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x563f2e58b923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x563f2e5aa70e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x563f2e58b923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x563f2e5aa70e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x563f2e58b923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x563f2e5aa70e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f28600582fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7f2860058b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x563f2e58e2bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x563f2e541817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x563f2e58cf83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x563f2e58ad36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x563f2e595ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563f2e58581b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x563f2e595ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563f2e58581b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x563f2e595ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563f2e58581b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x563f2e595ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563f2e58581b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x563f2e58427a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x563f2e595c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x563f2e589fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x563f2e58427a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x563f2e5a3935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x563f2e5a4104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x563f2e66afc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x563f2e58e2bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x563f2e5891bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x563f2e595ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x563f2e5a3c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x563f2e5891bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x563f2e595ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563f2e58581b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x563f2e58427a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x563f2e595c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x563f2e58581b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x563f2e595ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x563f2e585568]
=================================
2023-05-17 06:57:29,092 - distributed.nanny - WARNING - Restarting worker
2023-05-17 06:57:29,234 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43423
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7f0e10ce2100, tag: 0xa1293d3373d2843e, nbytes: 800000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7f0e10ce2100, tag: 0xa1293d3373d2843e, nbytes: 800000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-17 06:57:29,346 - distributed.nanny - WARNING - Restarting worker
[1684306650.029688] [dgx13:81481:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_2: LRU push returned Unsupported operation
[dgx13:81481:0:81481]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  81481) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7ff2ccd27dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7ff2ccd24d28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7ff2ccd24e44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x83605) [0x7ff2ccde3605]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7ff2ccdb8069]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7ff2ccdfe1a0]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7ff2cce030be]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7ff2cce02872]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7ff2cce985d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x55b51a6f8dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x55b51a6f71a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55b51a6ddd36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b51a6d727a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55b51a6e8c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55b51a6d93cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b51a6d727a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55b51a6e8c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55b51a6d93cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55b51a6fd70e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55b51a6de923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55b51a6fd70e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55b51a6de923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55b51a6fd70e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55b51a6de923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55b51a6fd70e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55b51a6de923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55b51a6fd70e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55b51a6de923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55b51a6fd70e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7ff2eba6c2fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7ff2eba6cb4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55b51a6e12bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55b51a694817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55b51a6dff83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55b51a6ddd36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b51a6e8ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b51a6d881b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b51a6e8ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b51a6d881b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b51a6e8ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b51a6d881b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b51a6e8ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b51a6d881b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b51a6d727a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55b51a6e8c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55b51a6dcfa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b51a6d727a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55b51a6f6935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55b51a6f7104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55b51a7bdfc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55b51a6e12bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55b51a6dc1bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b51a6e8ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55b51a6f6c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55b51a6dc1bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b51a6e8ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b51a6d881b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b51a6d727a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55b51a6e8c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b51a6d881b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b51a6e8ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55b51a6d8568]
=================================
2023-05-17 06:57:30,224 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56904
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7feb60689180, tag: 0x772ccfc355586abe, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7feb60689180, tag: 0x772ccfc355586abe, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:56904 after 30 s
2023-05-17 06:57:30,225 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56904
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f0e10ce2100, tag: 0xd265016c2f2db51e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f0e10ce2100, tag: 0xd265016c2f2db51e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-17 06:57:30,227 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56904
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #007] ep: 0x7f8d800011c0, tag: 0x50cf0b3f47024154, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #007] ep: 0x7f8d800011c0, tag: 0x50cf0b3f47024154, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-17 06:57:30,645 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zuprcpec', purging
2023-05-17 06:57:30,646 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:30,646 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:30,659 - distributed.nanny - WARNING - Restarting worker
2023-05-17 06:57:30,856 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:30,856 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
terminate called after throwing an instance of 'rmm::out_of_memory'
  what():  std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-17 06:57:31,173 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54123
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #033] ep: 0x7fae201a0140, tag: 0xccdbba801c8e33fd, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #033] ep: 0x7fae201a0140, tag: 0xccdbba801c8e33fd, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-17 06:57:31,173 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54123
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #026] ep: 0x7f3a10855140, tag: 0x20e452060c6472ef, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #026] ep: 0x7f3a10855140, tag: 0x20e452060c6472ef, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-17 06:57:31,174 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:57227 -> ucx://127.0.0.1:54123
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #012] ep: 0x7fae201a0100, tag: 0x1837b90a99be6598, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1793, in get_data
    response = await comm.read(deserializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #012] ep: 0x7fae201a0100, tag: 0x1837b90a99be6598, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-17 06:57:31,175 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54123
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #048] ep: 0x7feb60689140, tag: 0x236711d6a37d07aa, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #048] ep: 0x7feb60689140, tag: 0x236711d6a37d07aa, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-17 06:57:31,175 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54123
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #059] ep: 0x7f8d80001180, tag: 0x8f9ff8aa24d53ac7, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #059] ep: 0x7f8d80001180, tag: 0x8f9ff8aa24d53ac7, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-17 06:57:31,635 - distributed.nanny - WARNING - Restarting worker
2023-05-17 06:57:32,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:32,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:33,160 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-17 06:57:33,160 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-17 06:57:59,924 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56904
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 318, in connect
    raise OSError(
OSError: Timed out trying to connect to ucx://127.0.0.1:56904 after 30 s
2023-05-17 06:57:59,930 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56904
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXNotConnected: <stream_recv>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 468, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 318, in connect
    raise OSError(
OSError: Timed out trying to connect to ucx://127.0.0.1:56904 after 30 s
2023-05-17 06:58:03,762 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7f83ac
args:      ([                key   payload
123568    600483593  10247191
123579    814903498  64358750
123581    841167305  10709460
61993     827572658  26344055
123583    815309370  92173477
...             ...       ...
99986724  824662534   5539593
99986729  854843646  32773503
99986738  834717247   7053327
99986740  600719787  91093760
99986748  857390880  81296052

[12498923 rows x 2 columns],                 key   payload
61984     962518746  21737795
104459    966168020  34201654
61987     616090734  49950532
104474    413056489  43857967
61991     514771484  27532155
...             ...       ...
99985184  960052492  20832750
99985207  960250811  86083194
99985319  908798282  24468202
99985320  930444967  54626444
99985341  421618957  28832541

[12501128 rows x 2 columns],                  key   payload
63557     1021973531   6275292
55341     1045631969  39456310
63574     1060417584  18984924
55346      531273914  73678680
60107      334516947  88015196
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-05-17 06:58:03,778 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-e80f89dc6a1b2bfbaa0737a1e420e266', 1)
Function:  <dask.layers.CallableLazyImport object at 0x7f881c
args:      ([               key   payload
shuffle                     
0          1030208  28830536
0           644495  25775833
0          1030209   3023271
0           187036  91076308
0           912804  96972438
...            ...       ...
0        799822247  63907761
0        799888192  57558465
0        799756952  76387038
0        799854067  38714256
0        799819680  79222670

[12497508 rows x 2 columns],                key   payload
shuffle                     
1           339104  91477500
1           400525  65033476
1           381155  18973483
1           370755  26047128
1            27929  56425798
...            ...       ...
1        799978739   7199632
1        799991971  92049894
1        799978633    485575
1        799978647  37921095
1        799991437   8192647

[12503907 rows x 2 columns],                key   payload
shuffle                     
2            58383  36599065
2           219789   2113620
2           126046  16207661
2          1037370    357525
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-17 06:58:03,893 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-e80f89dc6a1b2bfbaa0737a1e420e266', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7f881c
args:      ([               key   payload
shuffle                     
0           994183  10613766
0           725689  57293826
0           917307  80685564
0           201954  23744228
0          1045539   8453845
...            ...       ...
0        799885487  13437749
0        799883796   8343839
0        799998349  25637548
0        799873883  78093481
0        799696585  82699703

[12502296 rows x 2 columns],                key   payload
shuffle                     
1           393792  49501995
1           359518  27283163
1           390866  85210391
1           382054  64002033
1           402667  76455022
...            ...       ...
1        799960105  48873186
1        799991440  52693133
1        799927310  13469865
1        799989462  29246484
1        799961125  50464740

[12499115 rows x 2 columns],                key   payload
shuffle                     
2           139814  24805023
2           192616   7204801
2           209948   1937895
2           227580  82273883
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-17 06:58:03,920 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-17 06:58:03,920 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-17 06:58:04,023 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-5f0abfa572e2fe9a259f7b95babe2310', 2)
Function:  subgraph_callable-f770a7a8-cf01-4660-bc8d-2602f1b4
args:      (               key   payload
shuffle                     
0          1055303  40957758
0           642504  67250024
0           122940  51772818
0           630101  22805978
0           704738  63877855
...            ...       ...
7        799901783  51849320
7        799922667  78002118
7        799838733  44809047
7        799839422  73456057
7        799825598  78815285

[99996471 rows x 2 columns],                  key   payload
123556        400079  83057077
123558     849609979  31525029
123559     838757397  34464890
61984      862518746  21737795
123567     411030235  51208788
...              ...       ...
99996002   599287355  49230190
99996007  1535381755   7581229
99996016  1517392630   9844505
99996020  1563845219  15146017
99996022  1510701113  32274182

[100013945 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
