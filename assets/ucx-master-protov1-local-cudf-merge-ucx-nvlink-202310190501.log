/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42955 instead
  warnings.warn(
[dgx13:73219:0:73219] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  73219) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f94b4035eed]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c0e4) [0x7f94b40360e4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c2aa) [0x7f94b40362aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f9541c1f420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f94a0b5b637]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7f94a0b83edd]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2086f) [0x7f94a0af486f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23dc8) [0x7f94a0af7dc8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f94b403f639]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f94a0af6b5d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f94a0b5853a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f94a0c1706a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560808396fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556080835094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556080846519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560808365c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560808467c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x556080853e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x55608095eb2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x5560807f0d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55608083d7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55608083b929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560808467c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560808365c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560808467c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560808365c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560808467c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560808365c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560808467c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560808365c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556080835094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556080846519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x556080837128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556080835094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x556080853ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55608085444c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55608091710e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55608083e77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560808396fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560808467c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x556080853dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560808396fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560808467c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560808365c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556080835094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556080846519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560808365c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560808467c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x556080836312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556080835094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556080846519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x556080837128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x556080835094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x556080834d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556080834d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5560808e207b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55608090efca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55608090b353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55608090316a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55608090305c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x556080902297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5560808d5f07]
=================================
[dgx13:73225:0:73225] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
[dgx13:73204:0:73204] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  73225) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fc718866eed]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c0e4) [0x7fc7188670e4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c2aa) [0x7fc7188672aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fc7b994d420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fc7188e8637]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7fc718910edd]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2086f) [0x7fc71881f86f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23dc8) [0x7fc718822dc8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fc718870639]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fc718821b5d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fc7188e553a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fc7189a406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55c10ec4c6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55c10ec48094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55c10ec59519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10ec495c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55c10ec597c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x55c10ec66e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x55c10ed71b2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55c10ec03d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55c10ec507f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55c10ec4e929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55c10ec597c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10ec495c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55c10ec597c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10ec495c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55c10ec597c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10ec495c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55c10ec597c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10ec495c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55c10ec48094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55c10ec59519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55c10ec4a128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55c10ec48094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55c10ec66ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55c10ec6744c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55c10ed2a10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55c10ec5177c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55c10ec4c6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55c10ec597c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55c10ec66dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55c10ec4c6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55c10ec597c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10ec495c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55c10ec48094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55c10ec59519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55c10ec495c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55c10ec597c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55c10ec49312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55c10ec48094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55c10ec59519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55c10ec4a128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55c10ec48094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55c10ec47d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55c10ec47d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55c10ecf507b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55c10ed21fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55c10ed1e353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55c10ed1616a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55c10ed1605c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55c10ed15297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55c10ece8f07]
=================================
==== backtrace (tid:  73204) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f2f5d869eed]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c0e4) [0x7f2f5d86a0e4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c2aa) [0x7f2f5d86a2aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f30029e2420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f2f5d8eb637]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7f2f5d913edd]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2086f) [0x7f2f5d82286f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23dc8) [0x7f2f5d825dc8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f2f5d873639]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f2f5d824b5d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f2f5d8e853a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f2f5d9a706a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5609293396fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x560929335094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x560929346519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5609293365c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5609293467c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x560929353e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x56092945eb2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x5609292f0d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x56092933d7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x56092933b929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5609293467c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5609293365c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5609293467c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5609293365c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5609293467c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5609293365c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5609293467c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5609293365c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x560929335094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x560929346519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x560929337128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x560929335094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x560929353ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x56092935444c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x56092941710e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x56092933e77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5609293396fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5609293467c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x560929353dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5609293396fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5609293467c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5609293365c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x560929335094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x560929346519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5609293365c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5609293467c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x560929336312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x560929335094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x560929346519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x560929337128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x560929335094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x560929334d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x560929334d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5609293e207b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x56092940efca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x56092940b353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x56092940316a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x56092940305c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x560929402297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5609293d5f07]
=================================
[dgx13:73207:0:73207] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  73207) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f5d34d90eed]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c0e4) [0x7f5d34d910e4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c2aa) [0x7f5d34d912aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f5dc7e88420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f5d34e12637]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7f5d34e3aedd]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2086f) [0x7f5d34d4986f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23dc8) [0x7f5d34d4cdc8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f5d34d9a639]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f5d34d4bb5d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f5d34e0f53a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f5d34ece06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55616a2bc6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55616a2b8094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55616a2c9519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55616a2b95c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55616a2c97c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x55616a2d6e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x55616a3e1b2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x55616a273d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x55616a2c07f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x55616a2be929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55616a2c97c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55616a2b95c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55616a2c97c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55616a2b95c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55616a2c97c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55616a2b95c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55616a2c97c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55616a2b95c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55616a2b8094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55616a2c9519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55616a2ba128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55616a2b8094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55616a2d6ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55616a2d744c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x55616a39a10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55616a2c177c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55616a2bc6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55616a2c97c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55616a2d6dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x55616a2bc6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55616a2c97c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55616a2b95c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55616a2b8094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55616a2c9519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55616a2b95c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x55616a2c97c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55616a2b9312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55616a2b8094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55616a2c9519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x55616a2ba128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x55616a2b8094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55616a2b7d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55616a2b7d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55616a36507b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x55616a391fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x55616a38e353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55616a38616a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55616a38605c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55616a385297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55616a358f07]
=================================
[dgx13:73222:0:73222] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  73222) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f0b98219eed]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c0e4) [0x7f0b9821a0e4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2c2aa) [0x7f0b9821a2aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f0c2b304420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f0b9829b637]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7f0b982c3edd]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2086f) [0x7f0b981d286f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23dc8) [0x7f0b981d5dc8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f0b98223639]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f0b981d4b5d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f0b9829853a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f0b9835706a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561892e2f6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561892e2b094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561892e3c519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561892e2c5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561892e3c7c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x561892e49e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x561892f54b2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x561892de6d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x561892e337f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x561892e31929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561892e3c7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561892e2c5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561892e3c7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561892e2c5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561892e3c7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561892e2c5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561892e3c7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561892e2c5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561892e2b094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561892e3c519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x561892e2d128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561892e2b094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x561892e49ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x561892e4a44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x561892f0d10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x561892e3477c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561892e2f6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561892e3c7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x561892e49dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561892e2f6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561892e3c7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561892e2c5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561892e2b094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561892e3c519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561892e2c5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561892e3c7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x561892e2c312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561892e2b094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561892e3c519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x561892e2d128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561892e2b094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x561892e2ad68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x561892e2ad19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x561892ed807b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x561892f04fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x561892f01353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x561892ef916a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x561892ef905c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x561892ef8297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x561892ecbf07]
=================================
2023-10-19 05:58:57,950 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34057 -> ucx://127.0.0.1:36701
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f33559c1340, tag: 0xbf8228ff6eb1b3b4, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:57,952 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:36701
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f33559c1200, tag: 0x232a1e116ef966c8, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f33559c1200, tag: 0x232a1e116ef966c8, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-19 05:58:57,957 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:36701
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f89e5b57140, tag: 0x7cc5de9339f39202, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f89e5b57140, tag: 0x7cc5de9339f39202, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-19 05:58:57,968 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46601
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f89e5b57240, tag: 0xdabd36ea4dd85e72, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f89e5b57240, tag: 0xdabd36ea4dd85e72, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-19 05:58:57,958 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:36701
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-10-19 05:58:57,984 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46601
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f33559c1240, tag: 0x24bba88930e8286a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f33559c1240, tag: 0x24bba88930e8286a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-19 05:58:57,984 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39445
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f89e5b571c0, tag: 0xf27e8eeb0c4844e0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f89e5b571c0, tag: 0xf27e8eeb0c4844e0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-19 05:58:57,988 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39445
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f56c3a9a200, tag: 0xd1a7bf83bd6f85e7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f56c3a9a200, tag: 0xd1a7bf83bd6f85e7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-19 05:58:57,997 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34057 -> ucx://127.0.0.1:39445
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f33559c13c0, tag: 0x136cc17992af3574, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,002 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39445
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f33559c1140, tag: 0x8d75fd5eb7e643d0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f33559c1140, tag: 0x8d75fd5eb7e643d0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-19 05:58:58,004 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41367
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7f33559c1100, tag: 0xba601215982d5b33, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7f33559c1100, tag: 0xba601215982d5b33, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-10-19 05:58:58,008 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47491 -> ucx://127.0.0.1:46601
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f56c3a9a440, tag: 0x283c15a6c2d8a7e1, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,009 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47491 -> ucx://127.0.0.1:39445
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f56c3a9a400, tag: 0x8aa376551b58b750, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,008 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41367
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f89e5b57200, tag: 0x37743bc92357caa3, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f89e5b57200, tag: 0x37743bc92357caa3, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-19 05:58:58,034 - distributed.nanny - WARNING - Restarting worker
2023-10-19 05:58:58,036 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47491 -> ucx://127.0.0.1:41367
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f56c3a9a100, tag: 0x704a7470aebc58db, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,086 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46601
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f56c3a9a300, tag: 0xb1312cf74081a5c9, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f56c3a9a300, tag: 0xb1312cf74081a5c9, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-19 05:58:58,091 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41367
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f56c3a9a1c0, tag: 0x81b65b3f69969d4f, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f56c3a9a1c0, tag: 0x81b65b3f69969d4f, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-19 05:58:58,244 - distributed.nanny - WARNING - Restarting worker
2023-10-19 05:58:58,246 - distributed.nanny - WARNING - Restarting worker
2023-10-19 05:58:58,247 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34057 -> ucx://127.0.0.1:46601
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f33559c1400, tag: 0x7b373a08bd9d39bd, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,247 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:49885 -> ucx://127.0.0.1:54197
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f89e5b57300, tag: 0x1bee51a180046a37, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,248 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:49885 -> ucx://127.0.0.1:36701
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f89e5b57340, tag: 0x146d7a43ebf9f0fd, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,248 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34057 -> ucx://127.0.0.1:41367
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f33559c12c0, tag: 0x251c618cf9ad0bf0, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,249 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:49885 -> ucx://127.0.0.1:39445
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f89e5b57400, tag: 0x2d66ce3e7a3b8e93, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,249 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34057 -> ucx://127.0.0.1:54197
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #010] ep: 0x7f33559c1380, tag: 0x2a66882691e4322a, nbytes: 50000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,249 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:49885 -> ucx://127.0.0.1:41367
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f89e5b572c0, tag: 0xa0d0308590a1723a, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:58:58,250 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54197
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f89e5b57180, tag: 0x5efc3416e87f1c52, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f89e5b57180, tag: 0x5efc3416e87f1c52, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-10-19 05:58:58,250 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54197
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #010] ep: 0x7f33559c1180, tag: 0x6139807fcf07616c, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #010] ep: 0x7f33559c1180, tag: 0x6139807fcf07616c, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-10-19 05:58:58,285 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54197
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f56c3a9a240, tag: 0x5052565da95ccdf8, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f56c3a9a240, tag: 0x5052565da95ccdf8, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-10-19 05:58:58,484 - distributed.nanny - WARNING - Restarting worker
2023-10-19 05:58:58,538 - distributed.nanny - WARNING - Restarting worker
2023-10-19 05:59:00,279 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 2)
Function:  shuffle_group
args:      (               key   payload
shuffle                     
2            12493  57704049
2            12495  14230916
2           120558  16840699
2            12503  13298221
2            29869  24687651
...            ...       ...
2        799992521  80606393
2        799992526  26595965
2        799992535  40189036
2        799992539  23484285
2        799992542  49390464

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-19 05:59:00,407 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 2)
Function:  _concat
args:      ([                key   payload
31979     705158638  88254522
31988     843567112  82237711
31992     845758972   6972612
31997     833018957   8781169
72769     852162748   8032661
...             ...       ...
99996561  823552097  81912298
99996563  861118450  35976418
99952458  604978971  19480834
99952467  501285062  44869702
99952478  836780618  55084818

[12503879 rows x 2 columns],                 key   payload
61990     945593778  60051373
61994     921353739  46410377
62004     939364759  20559527
59910     324987074  25601047
59935     924284367  55819413
...             ...       ...
99996531  965585843  28575990
99996544  901809995  60781729
99996546  900540067  92895134
99996549  969815716  78621588
99996568  911113726  58061378

[12499576 rows x 2 columns],                  key   payload
43052     1015692073  34225196
43053     1043387882  89139922
43054     1055977384  28693160
43066     1053310440  35489508
43067      733516652  35345528
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-19 05:59:00,440 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 0)
Function:  _concat
args:      ([                key   payload
31975     855454091  89838610
31981     842398802  94640116
31993     863602944  56330063
31994     858475648   2796736
72768     828733897  14164474
...             ...       ...
99952456  510295665  95223881
99952461  855897549  72964988
99952463  864608006   8721142
99952474  853295409  52049905
99952477  840653017    813937

[12497168 rows x 2 columns],                 key   payload
61986     966441335   6255663
61988     321756261  66449205
62002     955195260  92763240
62007      20692416  89812076
62008     908023498  96383374
...             ...       ...
99996521  964131210  38513711
99987289  949608516  10941321
99996541  941143333  22238472
99987292  621288631  48831132
99996573  321234533  75855995

[12502889 rows x 2 columns],                  key   payload
43048     1019529850  76997771
2147       529021485  82228596
2151      1032456698  15065101
11329     1021395554   6173773
63498      725312266  19771099
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-19 05:59:00,451 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 1)
Function:  _concat
args:      ([                key   payload
31984     820758465  12067622
31989     412131415   7370839
31991     107193377  32225161
31998     815146569  80138374
72777     840097296  30530049
...             ...       ...
99952442  305161813   9286661
99952453  833853320  79443429
99952471  859484627  13547007
99952475  820402615  47501649
99952476  840023868  65508809

[12502120 rows x 2 columns],                 key   payload
61996      22062559  52565627
62003     949349406  18106440
59911     949708147  59508073
59922     720991636  68892252
59925     118344207  33625091
...             ...       ...
99996547  917821976  68559906
99996548  940634908  97832413
99996552  903854675  25056479
99996560  961882162   1134554
99996572  316644846  47213662

[12499414 rows x 2 columns],                  key   payload
43040      135317327  55899404
43051     1034154927  81032139
43057      136800389   7267516
43062     1044690085  22188267
43063      228664117   1841284
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Exception ignored in: 'cupy.cuda.thrust.cupy_malloc'
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 740, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:01,255 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-3393696d3757f92da87d0dddd0c6ae01', 7)
Function:  generate_chunk
args:      (7, 100000000, 8, 'other', 0.3, True)
kwargs:    {}
Exception: "RuntimeError('radix_sort: failed on 2nd step: cudaErrorInvalidValue: invalid argument')"

2023-10-19 05:59:01,880 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 1)
Function:  subgraph_callable-8cc835fb-e472-4d55-aa7b-6c2d6ca3
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0            113856        1  53843941            1
1            113867        1  20998787            1
2            104645        1  35241683            1
3            101026        1  18930046            1
4             43170        1  64773873            1
...             ...      ...       ...          ...
99999995  799999729        1  92788168            1
99999996  799999733        1  77530309            1
99999997  799999736        1  49913480            1
99999998  799999742        1  94740881            1
99999999  799999671        1  64397485            1

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-19 05:59:01,887 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:01,887 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:01,967 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:01,967 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:02,002 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 3)
Function:  subgraph_callable-8cc835fb-e472-4d55-aa7b-6c2d6ca3
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0            113858        3   8215365            3
1            113860        3  73564068            3
2            104641        3  81449864            3
3            101043        3  84504004            3
4            113861        3  47518922            3
...             ...      ...       ...          ...
99999995  799999698        3  82132605            3
99999996  799999596        3  21551906            3
99999997  799999732        3  52961653            3
99999998  799999665        3  55361539            3
99999999  799999676        3   3458424            3

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-19 05:59:02,107 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 2)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0           113869        2  47179723            2
1           113873        2  43975740            2
2           104647        2  61989140            2
3           101031        2  36191300            2
4           113874        2  32281329            2
...            ...      ...       ...          ...
12499995  99976166        2   3018343            2
12499996  99976168        2  70521005            2
12499997  99976171        2  79500697            2
12499998  99976182        2  45044355            2
12499999  99976188        2  90520856            2

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100052385        2  20977489            2
1         100052389        2  21873827            2
2         100052398        2  31917995            2
3         100102409        2  56678129            2
4         100052399        2  60984090            2
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-19 05:59:02,108 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47491
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 362, in read
    header_fmt = nframes * "?" + nframes * "Q"
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: MemoryError()
2023-10-19 05:59:02,112 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47491 -> ucx://127.0.0.1:49885
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #135] ep: 0x7f56c3a9a3c0, tag: 0x5ce8128fb890312b, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-10-19 05:59:02,290 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 6)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0           113866        6  89759299            6
1           113876        6  66716611            6
2           104646        6   7506439            6
3           101040        6  28133427            6
4            43183        6  66508933            6
...            ...      ...       ...          ...
12499995  99976179        6  14421924            6
12499996  99976181        6  53238247            6
12499997  99976183        6  66336360            6
12499998  99976190        6  51574003            6
12499999  99976191        6  22503239            6

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100052386        6  65315006            6
1         100052388        6  90057319            6
2         100052409        6  51000978            6
3         100102428        6  68168999            6
4         100102429        6  79919177            6
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-19 05:59:02,292 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 4)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0           113865        4  39830208            4
1           113872        4  15472874            4
2           104653        4   1484332            4
3           101038        4  17671166            4
4           113875        4  83384383            4
...            ...      ...       ...          ...
12499995  99976174        4  82895380            4
12499996  99976176        4  65184382            4
12499997  99976180        4  12388029            4
12499998  99976186        4  35937211            4
12499999  99976189        4  96473482            4

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100052401        4  26456481            4
1         100102400        4  46922641            4
2         100102403        4  33618129            4
3         100102405        4  26305359            4
4         100041169        4  31215505            4
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-10-19 05:59:02,732 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:02,733 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:02,744 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:02,744 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-10-19 05:59:02,927 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 7)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0           113859        7  40387994            7
1           113870        7  76507412            7
2           104642        7  83826250            7
3           101034        7  10969765            7
4           113883        7  37789951            7
...            ...      ...       ...          ...
12499995  99976082        7  43478968            7
12499996  99976165        7  27575465            7
12499997  99976169        7  67439410            7
12499998  99976184        7  79401301            7
12499999  99976187        7  19166116            7

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100052384        7  52950928            7
1         100052397        7   3423364            7
2         100052410        7  79501743            7
3         100102408        7  31989819            7
4         100102416        7  90954724            7
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
