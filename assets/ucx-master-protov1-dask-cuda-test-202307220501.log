============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-07-22 05:39:52,635 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:52,639 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32987 instead
  warnings.warn(
2023-07-22 05:39:52,642 - distributed.scheduler - INFO - State start
2023-07-22 05:39:52,661 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:52,662 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-22 05:39:52,662 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32987/status
2023-07-22 05:39:52,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39033'
2023-07-22 05:39:52,794 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33269'
2023-07-22 05:39:52,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46541'
2023-07-22 05:39:52,804 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46573'
2023-07-22 05:39:52,921 - distributed.scheduler - INFO - Receive client connection: Client-2d7a9185-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:39:52,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40186
2023-07-22 05:39:54,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:54,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:54,301 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:54,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:54,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-07-22 05:39:54,312 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45511
2023-07-22 05:39:54,312 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45511
2023-07-22 05:39:54,313 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36777
2023-07-22 05:39:54,313 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:39:54,313 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:54,313 - distributed.worker - INFO -               Threads:                          4
2023-07-22 05:39:54,313 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-22 05:39:54,313 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wtnzqvfs
2023-07-22 05:39:54,313 - distributed.worker - INFO - Starting Worker plugin PreImport-38181d34-f59c-40ca-85b6-9c01bbc9b7b8
2023-07-22 05:39:54,313 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1cb097ca-6236-4616-bd55-34ff3954ae8e
2023-07-22 05:39:54,313 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39d836b1-24cf-4b14-b276-ea7b7b94dbd6
2023-07-22 05:39:54,313 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:54,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:54,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:54,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:54,330 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45511', status: init, memory: 0, processing: 0>
2023-07-22 05:39:54,331 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45511
2023-07-22 05:39:54,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40202
2023-07-22 05:39:54,332 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:39:54,332 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:54,334 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:39:54,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:54,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:39:54,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:39:54,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:39:55,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38431
2023-07-22 05:39:55,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38431
2023-07-22 05:39:55,744 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35551
2023-07-22 05:39:55,744 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:39:55,744 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,744 - distributed.worker - INFO -               Threads:                          4
2023-07-22 05:39:55,744 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-22 05:39:55,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sm6mdi4y
2023-07-22 05:39:55,745 - distributed.worker - INFO - Starting Worker plugin PreImport-d051ccfe-42ad-4908-9467-15714c7fc4ae
2023-07-22 05:39:55,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35153259-268d-4d97-a089-6e27dbdf4c94
2023-07-22 05:39:55,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-170116eb-1f10-4184-a6ef-dbb70e688b60
2023-07-22 05:39:55,745 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,769 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38431', status: init, memory: 0, processing: 0>
2023-07-22 05:39:55,769 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38431
2023-07-22 05:39:55,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40226
2023-07-22 05:39:55,770 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:39:55,770 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,772 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:39:55,839 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33225
2023-07-22 05:39:55,839 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33225
2023-07-22 05:39:55,839 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44625
2023-07-22 05:39:55,839 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:39:55,839 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,839 - distributed.worker - INFO -               Threads:                          4
2023-07-22 05:39:55,839 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-22 05:39:55,839 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ig8uthh8
2023-07-22 05:39:55,840 - distributed.worker - INFO - Starting Worker plugin PreImport-373ada7a-0ccf-4ab9-95e3-394aea0632d4
2023-07-22 05:39:55,840 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d04202a8-1270-4c98-a11f-85a3e4f48fb9
2023-07-22 05:39:55,840 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b402e012-b4a3-4250-94b1-ca096ffe304a
2023-07-22 05:39:55,840 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33225', status: init, memory: 0, processing: 0>
2023-07-22 05:39:55,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33225
2023-07-22 05:39:55,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40238
2023-07-22 05:39:55,861 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:39:55,861 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:39:55,874 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34141
2023-07-22 05:39:55,874 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34141
2023-07-22 05:39:55,874 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40653
2023-07-22 05:39:55,874 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:39:55,874 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,874 - distributed.worker - INFO -               Threads:                          4
2023-07-22 05:39:55,874 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-22 05:39:55,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-up6_3oki
2023-07-22 05:39:55,875 - distributed.worker - INFO - Starting Worker plugin PreImport-9b7acc3a-d6d8-4215-b57b-bb5b83ba5bfc
2023-07-22 05:39:55,875 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b25ff9f-e3d4-427a-9b69-558791f405fb
2023-07-22 05:39:55,875 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5582c0f-fb03-4c25-b348-4a9615755dc9
2023-07-22 05:39:55,875 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34141', status: init, memory: 0, processing: 0>
2023-07-22 05:39:55,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34141
2023-07-22 05:39:55,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40242
2023-07-22 05:39:55,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:39:55,897 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:39:55,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:39:55,902 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-22 05:39:55,903 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-22 05:39:55,903 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-22 05:39:55,904 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-07-22 05:39:55,908 - distributed.scheduler - INFO - Remove client Client-2d7a9185-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:39:55,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40186; closing.
2023-07-22 05:39:55,909 - distributed.scheduler - INFO - Remove client Client-2d7a9185-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:39:55,909 - distributed.scheduler - INFO - Close client connection: Client-2d7a9185-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:39:55,910 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39033'. Reason: nanny-close
2023-07-22 05:39:55,910 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:55,911 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33269'. Reason: nanny-close
2023-07-22 05:39:55,911 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:55,911 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34141. Reason: nanny-close
2023-07-22 05:39:55,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46541'. Reason: nanny-close
2023-07-22 05:39:55,912 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:55,912 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38431. Reason: nanny-close
2023-07-22 05:39:55,912 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46573'. Reason: nanny-close
2023-07-22 05:39:55,912 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:39:55,913 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33225. Reason: nanny-close
2023-07-22 05:39:55,913 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40242; closing.
2023-07-22 05:39:55,913 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:39:55,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34141', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:55,913 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34141
2023-07-22 05:39:55,913 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45511. Reason: nanny-close
2023-07-22 05:39:55,914 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:39:55,914 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:55,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40226; closing.
2023-07-22 05:39:55,914 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34141
2023-07-22 05:39:55,914 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:39:55,914 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38431', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:55,915 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38431
2023-07-22 05:39:55,915 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:55,915 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40238; closing.
2023-07-22 05:39:55,915 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34141
2023-07-22 05:39:55,915 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33225', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:55,915 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33225
2023-07-22 05:39:55,915 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:39:55,915 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:55,916 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40202; closing.
2023-07-22 05:39:55,916 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45511', status: closing, memory: 0, processing: 0>
2023-07-22 05:39:55,916 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45511
2023-07-22 05:39:55,916 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:39:55,917 - distributed.nanny - INFO - Worker closed
2023-07-22 05:39:56,927 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:39:56,927 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:39:56,927 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:39:56,928 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-22 05:39:56,929 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-07-22 05:39:58,731 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:58,736 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36137 instead
  warnings.warn(
2023-07-22 05:39:58,740 - distributed.scheduler - INFO - State start
2023-07-22 05:39:58,760 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:39:58,760 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:39:58,761 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36137/status
2023-07-22 05:39:58,825 - distributed.scheduler - INFO - Receive client connection: Client-312cacc9-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:39:58,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42692
2023-07-22 05:39:58,929 - distributed.scheduler - INFO - Receive client connection: Client-31a801b9-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:39:58,930 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42710
2023-07-22 05:39:59,052 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34891'
2023-07-22 05:39:59,068 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44857'
2023-07-22 05:39:59,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33285'
2023-07-22 05:39:59,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35803'
2023-07-22 05:39:59,090 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38695'
2023-07-22 05:39:59,098 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45601'
2023-07-22 05:39:59,107 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35895'
2023-07-22 05:39:59,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37021'
2023-07-22 05:40:00,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:00,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:00,723 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:00,724 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:00,730 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:00,731 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:00,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:00,731 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:00,734 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:00,734 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:00,743 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:00,748 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:00,759 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:00,759 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:00,761 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:00,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:00,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:00,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:00,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:00,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:00,792 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:00,815 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:00,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:00,829 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:03,795 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45725
2023-07-22 05:40:03,796 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45725
2023-07-22 05:40:03,796 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43739
2023-07-22 05:40:03,796 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,796 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,796 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,796 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:03,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i__c71a5
2023-07-22 05:40:03,796 - distributed.worker - INFO - Starting Worker plugin PreImport-ae06b058-76ff-4c03-b1d8-6f2e1e612b96
2023-07-22 05:40:03,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b995d4c9-8b67-436e-ab19-d3392b6c8b27
2023-07-22 05:40:03,797 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf1e3d2b-6041-4699-95d7-15feec935cd6
2023-07-22 05:40:03,801 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35557
2023-07-22 05:40:03,801 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40719
2023-07-22 05:40:03,801 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35557
2023-07-22 05:40:03,801 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40719
2023-07-22 05:40:03,801 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33835
2023-07-22 05:40:03,801 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37121
2023-07-22 05:40:03,801 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,801 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41463
2023-07-22 05:40:03,801 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37121
2023-07-22 05:40:03,801 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,801 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,801 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,801 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42811
2023-07-22 05:40:03,801 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,801 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,801 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:03,801 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,801 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-muwgsfb1
2023-07-22 05:40:03,801 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46353
2023-07-22 05:40:03,801 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:03,801 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,801 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46353
2023-07-22 05:40:03,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c1ktpask
2023-07-22 05:40:03,801 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:03,801 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39085
2023-07-22 05:40:03,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y7vq8p4q
2023-07-22 05:40:03,802 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,802 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,802 - distributed.worker - INFO - Starting Worker plugin PreImport-cc6b3491-cfe1-48ba-a8c8-e5f13a135a76
2023-07-22 05:40:03,802 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,802 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:03,802 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yw5186ld
2023-07-22 05:40:03,802 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb45b5e5-d2b1-4d40-9d4f-c9519f3e8e28
2023-07-22 05:40:03,802 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9f106cb-9833-4b77-ae3e-cdb5433c8d74
2023-07-22 05:40:03,802 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6343878c-e331-4897-ab9e-3b811617d3ab
2023-07-22 05:40:03,802 - distributed.worker - INFO - Starting Worker plugin PreImport-8ff17fb3-3d08-4952-8db6-02db2677d84d
2023-07-22 05:40:03,802 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f95a093-109e-4eaf-abe2-902d0f6ab91b
2023-07-22 05:40:03,803 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81ee85f6-e3ff-4bcb-b600-78fc292ee14d
2023-07-22 05:40:03,803 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0cad7066-1e53-41d7-a432-15d6f3375ce4
2023-07-22 05:40:03,804 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45367
2023-07-22 05:40:03,804 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45367
2023-07-22 05:40:03,804 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37419
2023-07-22 05:40:03,804 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,804 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,804 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,804 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:03,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-03j54h81
2023-07-22 05:40:03,805 - distributed.worker - INFO - Starting Worker plugin PreImport-7011cb1a-fd3e-4199-ba5b-8fa0236c18ec
2023-07-22 05:40:03,805 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e4c95f7-d6cf-474a-9e35-26be2e28f029
2023-07-22 05:40:03,805 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04a907cb-62f3-4c71-9372-0652339cd18e
2023-07-22 05:40:03,805 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36835
2023-07-22 05:40:03,805 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36835
2023-07-22 05:40:03,805 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36195
2023-07-22 05:40:03,805 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,805 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,805 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,805 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:03,805 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lz_sdk17
2023-07-22 05:40:03,806 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4af7d373-881d-4939-a061-cbfdd04df66d
2023-07-22 05:40:03,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38341
2023-07-22 05:40:03,808 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38341
2023-07-22 05:40:03,809 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40419
2023-07-22 05:40:03,809 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,809 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,809 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:03,809 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:03,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ctserofs
2023-07-22 05:40:03,809 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6b399d5-2919-47c5-b247-abf41d9469ad
2023-07-22 05:40:03,810 - distributed.worker - INFO - Starting Worker plugin PreImport-ad8f9dae-1aa9-4e36-92b7-6a685aeb247a
2023-07-22 05:40:03,810 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7239af72-e883-493e-8720-2612c06734aa
2023-07-22 05:40:03,839 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41371', status: init, memory: 0, processing: 0>
2023-07-22 05:40:03,840 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41371
2023-07-22 05:40:03,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60280
2023-07-22 05:40:03,886 - distributed.scheduler - INFO - Remove client Client-31a801b9-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:03,886 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42710; closing.
2023-07-22 05:40:03,887 - distributed.scheduler - INFO - Remove client Client-31a801b9-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:03,887 - distributed.scheduler - INFO - Close client connection: Client-31a801b9-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:03,891 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60280; closing.
2023-07-22 05:40:03,892 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41371', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:03,892 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41371
2023-07-22 05:40:03,892 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:40:03,948 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c29ddde4-a7b2-43bf-b04f-eed6331a352a
2023-07-22 05:40:03,948 - distributed.worker - INFO - Starting Worker plugin PreImport-e1cd1cb6-5cf0-4e81-9d6d-a9d8fbb45114
2023-07-22 05:40:03,949 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,952 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,975 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d1ab475-0aeb-4d5e-8f04-56b537f86441
2023-07-22 05:40:03,975 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,975 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,975 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,975 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0536084b-d731-4047-abd6-8cce4e151bd5
2023-07-22 05:40:03,976 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,978 - distributed.worker - INFO - Starting Worker plugin PreImport-2d6ae93b-ae37-4350-8393-0df038a16552
2023-07-22 05:40:03,978 - distributed.worker - INFO - Starting Worker plugin PreImport-e0277e17-cab8-4995-a1e5-e51f8991f1db
2023-07-22 05:40:03,978 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,979 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,980 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46353', status: init, memory: 0, processing: 0>
2023-07-22 05:40:03,981 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46353
2023-07-22 05:40:03,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60288
2023-07-22 05:40:03,981 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,981 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:03,985 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38341', status: init, memory: 0, processing: 0>
2023-07-22 05:40:03,985 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38341
2023-07-22 05:40:03,985 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60304
2023-07-22 05:40:03,986 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:03,986 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:03,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:04,005 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35557', status: init, memory: 0, processing: 0>
2023-07-22 05:40:04,006 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35557
2023-07-22 05:40:04,006 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60332
2023-07-22 05:40:04,006 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:04,006 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:04,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45367', status: init, memory: 0, processing: 0>
2023-07-22 05:40:04,007 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45367
2023-07-22 05:40:04,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60310
2023-07-22 05:40:04,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:04,008 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:04,008 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45725', status: init, memory: 0, processing: 0>
2023-07-22 05:40:04,008 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45725
2023-07-22 05:40:04,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60320
2023-07-22 05:40:04,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:04,009 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:04,009 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:04,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:04,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:04,014 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36835', status: init, memory: 0, processing: 0>
2023-07-22 05:40:04,015 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36835
2023-07-22 05:40:04,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60346
2023-07-22 05:40:04,015 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:04,015 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:04,016 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37121', status: init, memory: 0, processing: 0>
2023-07-22 05:40:04,016 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37121
2023-07-22 05:40:04,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60350
2023-07-22 05:40:04,017 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:04,017 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:04,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:04,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:04,022 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40719', status: init, memory: 0, processing: 0>
2023-07-22 05:40:04,022 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40719
2023-07-22 05:40:04,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60340
2023-07-22 05:40:04,023 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:04,023 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:04,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:04,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:04,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:04,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:04,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:04,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:04,094 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:04,094 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:04,094 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:04,098 - distributed.scheduler - INFO - Remove client Client-312cacc9-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:04,098 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42692; closing.
2023-07-22 05:40:04,098 - distributed.scheduler - INFO - Remove client Client-312cacc9-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:04,098 - distributed.scheduler - INFO - Close client connection: Client-312cacc9-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:04,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33285'. Reason: nanny-close
2023-07-22 05:40:04,100 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:04,101 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35803'. Reason: nanny-close
2023-07-22 05:40:04,101 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:04,102 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36835. Reason: nanny-close
2023-07-22 05:40:04,102 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35895'. Reason: nanny-close
2023-07-22 05:40:04,102 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:04,102 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34891'. Reason: nanny-close
2023-07-22 05:40:04,103 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46353. Reason: nanny-close
2023-07-22 05:40:04,103 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:04,103 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45725. Reason: nanny-close
2023-07-22 05:40:04,103 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44857'. Reason: nanny-close
2023-07-22 05:40:04,103 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:04,103 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45367. Reason: nanny-close
2023-07-22 05:40:04,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38695'. Reason: nanny-close
2023-07-22 05:40:04,104 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:04,104 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60346; closing.
2023-07-22 05:40:04,104 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:04,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45601'. Reason: nanny-close
2023-07-22 05:40:04,104 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36835', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:04,104 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37121. Reason: nanny-close
2023-07-22 05:40:04,104 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36835
2023-07-22 05:40:04,105 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:04,105 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:04,105 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:04,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37021'. Reason: nanny-close
2023-07-22 05:40:04,105 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38341. Reason: nanny-close
2023-07-22 05:40:04,105 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:04,105 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:04,105 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:04,105 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40719. Reason: nanny-close
2023-07-22 05:40:04,105 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36835
2023-07-22 05:40:04,106 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60320; closing.
2023-07-22 05:40:04,106 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:04,106 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35557. Reason: nanny-close
2023-07-22 05:40:04,106 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60288; closing.
2023-07-22 05:40:04,106 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:04,106 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:04,107 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45725', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:04,107 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45725
2023-07-22 05:40:04,107 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36835
2023-07-22 05:40:04,107 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46353', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:04,107 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46353
2023-07-22 05:40:04,108 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36835
2023-07-22 05:40:04,108 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:04,108 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60310; closing.
2023-07-22 05:40:04,108 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:04,108 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36835
2023-07-22 05:40:04,108 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45367', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:04,108 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45367
2023-07-22 05:40:04,108 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:04,109 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:04,109 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60350; closing.
2023-07-22 05:40:04,109 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:04,109 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60340; closing.
2023-07-22 05:40:04,109 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60332; closing.
2023-07-22 05:40:04,110 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60304; closing.
2023-07-22 05:40:04,110 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:04,110 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:04,110 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37121', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:04,110 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37121
2023-07-22 05:40:04,111 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40719', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:04,111 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40719
2023-07-22 05:40:04,111 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:04,111 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35557', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:04,111 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35557
2023-07-22 05:40:04,112 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38341', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:04,112 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38341
2023-07-22 05:40:04,112 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:40:05,668 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:40:05,668 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:05,668 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:05,669 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:40:05,670 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-07-22 05:40:07,593 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:07,597 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42647 instead
  warnings.warn(
2023-07-22 05:40:07,601 - distributed.scheduler - INFO - State start
2023-07-22 05:40:07,621 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:07,622 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:07,622 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:07,623 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-22 05:40:08,225 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43547'
2023-07-22 05:40:08,245 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38631'
2023-07-22 05:40:08,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37195'
2023-07-22 05:40:08,257 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36135'
2023-07-22 05:40:08,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44899'
2023-07-22 05:40:08,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40591'
2023-07-22 05:40:08,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38533'
2023-07-22 05:40:08,296 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41151'
2023-07-22 05:40:09,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:09,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:09,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:09,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:09,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:09,977 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:09,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:09,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:09,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:09,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:09,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:09,993 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:10,003 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:10,003 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:10,007 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:10,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:10,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:10,018 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:10,018 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:10,021 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:10,021 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:10,034 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:10,040 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:10,055 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:13,024 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43521
2023-07-22 05:40:13,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43521
2023-07-22 05:40:13,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37973
2023-07-22 05:40:13,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:13,024 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,024 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:13,025 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:13,025 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lbq1qmj6
2023-07-22 05:40:13,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf8ff837-d09c-42d0-9bf7-9497667e757e
2023-07-22 05:40:13,030 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8da3f029-a3aa-427d-88df-46c781bd10b0
2023-07-22 05:40:13,030 - distributed.worker - INFO - Starting Worker plugin PreImport-438a62f5-4ef2-4bbc-aa6b-523a32980197
2023-07-22 05:40:13,031 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,060 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42403
2023-07-22 05:40:13,060 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42403
2023-07-22 05:40:13,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36955
2023-07-22 05:40:13,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:13,060 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,060 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:13,061 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:13,061 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tccddbx0
2023-07-22 05:40:13,061 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d7fa5c0-5428-4d58-826b-a9849ac2605d
2023-07-22 05:40:13,066 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d51aa116-8a17-4eac-bfd9-28706488891a
2023-07-22 05:40:13,066 - distributed.worker - INFO - Starting Worker plugin PreImport-4fca4eb2-cf91-471e-84a9-f51aeabd8b0a
2023-07-22 05:40:13,066 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,213 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35967
2023-07-22 05:40:13,213 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35967
2023-07-22 05:40:13,213 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34775
2023-07-22 05:40:13,213 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:13,213 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,213 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:13,214 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:13,214 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gqqpzsnp
2023-07-22 05:40:13,214 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05099e87-eefb-4fa4-817c-059c84e027f6
2023-07-22 05:40:13,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43307
2023-07-22 05:40:13,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43307
2023-07-22 05:40:13,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39689
2023-07-22 05:40:13,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:13,219 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,220 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:13,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:13,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3qikh89f
2023-07-22 05:40:13,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e62e9c7e-7ffa-4811-b65a-5c12949847fc
2023-07-22 05:40:13,223 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45593
2023-07-22 05:40:13,223 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45593
2023-07-22 05:40:13,223 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33213
2023-07-22 05:40:13,223 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:13,223 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,223 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:13,223 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:13,223 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0lp8eh71
2023-07-22 05:40:13,224 - distributed.worker - INFO - Starting Worker plugin PreImport-7d842703-1525-4d57-b58d-eb0c2da236ef
2023-07-22 05:40:13,224 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a61cccbd-f4b9-46a2-9387-bae9f978d44f
2023-07-22 05:40:13,224 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2906374-c320-45ce-9e81-20fa4f2724a3
2023-07-22 05:40:13,226 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36529
2023-07-22 05:40:13,227 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36529
2023-07-22 05:40:13,227 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39753
2023-07-22 05:40:13,227 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:13,227 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,227 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:13,227 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:13,227 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_sd1oggo
2023-07-22 05:40:13,227 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62f5c1a8-b4fe-40b4-a8b3-48994a1a3e1c
2023-07-22 05:40:13,229 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34837
2023-07-22 05:40:13,230 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34837
2023-07-22 05:40:13,230 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37595
2023-07-22 05:40:13,230 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:13,230 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,230 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:13,230 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:13,230 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e_07rw8o
2023-07-22 05:40:13,230 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cdf16667-9ffb-459e-8788-64225a9c9dc4
2023-07-22 05:40:13,233 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43589
2023-07-22 05:40:13,233 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43589
2023-07-22 05:40:13,234 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43247
2023-07-22 05:40:13,234 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:13,234 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,234 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:13,234 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:13,234 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4cqnf6la
2023-07-22 05:40:13,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e6f91c4-31b5-4713-842d-6af780e0241a
2023-07-22 05:40:13,250 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd2f7bcf-076a-43b0-8ce8-ece9bc2be806
2023-07-22 05:40:13,250 - distributed.worker - INFO - Starting Worker plugin PreImport-f9d150d4-7e7d-49d3-959a-f0e3cdebb3ed
2023-07-22 05:40:13,250 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,252 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-edf08ba5-6f93-41e3-b8f9-854dc4a48b70
2023-07-22 05:40:13,252 - distributed.worker - INFO - Starting Worker plugin PreImport-0f6735c8-e4bc-4500-84b6-246e1cdaddad
2023-07-22 05:40:13,252 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1d331ade-33e6-4e06-84a9-993e5a8dd543
2023-07-22 05:40:13,253 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,253 - distributed.worker - INFO - Starting Worker plugin PreImport-19674977-0fce-4c77-8e1b-1dd524f01ca0
2023-07-22 05:40:13,253 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64fd3fa6-fd57-4a59-be05-e7a48bca66be
2023-07-22 05:40:13,254 - distributed.worker - INFO - Starting Worker plugin PreImport-3b62165c-0746-4eeb-a7bd-0b5ede01277a
2023-07-22 05:40:13,254 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-432191d2-7c05-4c14-9608-e6f21e72206b
2023-07-22 05:40:13,255 - distributed.worker - INFO - Starting Worker plugin PreImport-f3477bbe-4b86-4d24-8bf9-82df53a6d084
2023-07-22 05:40:13,255 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:13,255 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:19,832 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:19,832 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:19,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:19,838 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:19,838 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:19,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:20,155 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:20,155 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:20,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:21,289 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:21,290 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:21,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:23,398 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:23,398 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:23,400 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:24,831 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,831 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:24,980 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:24,980 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:24,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:27,053 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:27,053 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:27,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:27,433 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37195'. Reason: nanny-close
2023-07-22 05:40:27,434 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,434 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43547'. Reason: nanny-close
2023-07-22 05:40:27,435 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,435 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38631'. Reason: nanny-close
2023-07-22 05:40:27,435 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43307. Reason: nanny-close
2023-07-22 05:40:27,436 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,436 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36135'. Reason: nanny-close
2023-07-22 05:40:27,436 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36529. Reason: nanny-close
2023-07-22 05:40:27,436 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,436 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42403. Reason: nanny-close
2023-07-22 05:40:27,437 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44899'. Reason: nanny-close
2023-07-22 05:40:27,437 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,437 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40591'. Reason: nanny-close
2023-07-22 05:40:27,437 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34837. Reason: nanny-close
2023-07-22 05:40:27,438 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,438 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38533'. Reason: nanny-close
2023-07-22 05:40:27,438 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,438 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,438 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45593. Reason: nanny-close
2023-07-22 05:40:27,438 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,438 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,439 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41151'. Reason: nanny-close
2023-07-22 05:40:27,439 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:27,439 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43521. Reason: nanny-close
2023-07-22 05:40:27,439 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35967. Reason: nanny-close
2023-07-22 05:40:27,439 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,440 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,440 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,440 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43589. Reason: nanny-close
2023-07-22 05:40:27,440 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,441 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,441 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,441 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,442 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,442 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,442 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43307
2023-07-22 05:40:27,442 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,443 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,443 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:27,444 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,444 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:27,445 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-07-22 05:40:31,137 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:31,141 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41255 instead
  warnings.warn(
2023-07-22 05:40:31,145 - distributed.scheduler - INFO - State start
2023-07-22 05:40:31,164 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:31,165 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:40:31,166 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41255/status
2023-07-22 05:40:31,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42443'
2023-07-22 05:40:31,472 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40067'
2023-07-22 05:40:31,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43679'
2023-07-22 05:40:31,482 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38423'
2023-07-22 05:40:31,492 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39413'
2023-07-22 05:40:31,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37885'
2023-07-22 05:40:31,510 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35113'
2023-07-22 05:40:31,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39193'
2023-07-22 05:40:32,465 - distributed.scheduler - INFO - Receive client connection: Client-447c649c-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:32,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53278
2023-07-22 05:40:33,051 - distributed.scheduler - INFO - Receive client connection: Client-44742ece-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:33,051 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53304
2023-07-22 05:40:33,062 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:33,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:33,086 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:33,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:33,120 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:33,164 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:33,165 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:33,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:33,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:33,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:33,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:33,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:33,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:33,201 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:33,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:33,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:33,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:33,299 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:33,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:33,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:33,322 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:33,323 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:33,323 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:33,324 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:35,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43905', status: init, memory: 0, processing: 0>
2023-07-22 05:40:35,199 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43905
2023-07-22 05:40:35,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53326
2023-07-22 05:40:35,228 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:35,230 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:35,232 - distributed.scheduler - INFO - Remove client Client-447c649c-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:35,232 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53278; closing.
2023-07-22 05:40:35,233 - distributed.scheduler - INFO - Remove client Client-447c649c-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:35,233 - distributed.scheduler - INFO - Close client connection: Client-447c649c-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:35,238 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53326; closing.
2023-07-22 05:40:35,238 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43905', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:35,238 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43905
2023-07-22 05:40:35,238 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:40:36,088 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35821
2023-07-22 05:40:36,088 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35821
2023-07-22 05:40:36,088 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39023
2023-07-22 05:40:36,088 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,088 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,088 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:36,089 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:36,089 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d72n4128
2023-07-22 05:40:36,089 - distributed.worker - INFO - Starting Worker plugin RMMSetup-89f30125-508e-4ddd-af48-5d6e5529cddc
2023-07-22 05:40:36,311 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45609
2023-07-22 05:40:36,311 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45609
2023-07-22 05:40:36,311 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43471
2023-07-22 05:40:36,311 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,311 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,311 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:36,311 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:36,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xba8nl45
2023-07-22 05:40:36,312 - distributed.worker - INFO - Starting Worker plugin PreImport-77d5f563-fbdf-4019-b57b-1821b12549bf
2023-07-22 05:40:36,312 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0a7babf-aa9b-4bf2-8e55-9eeb3723a50d
2023-07-22 05:40:36,312 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dad4d612-4ba5-4528-95b6-45109e2d502e
2023-07-22 05:40:36,317 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41335
2023-07-22 05:40:36,317 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41335
2023-07-22 05:40:36,317 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46439
2023-07-22 05:40:36,317 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,317 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,317 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:36,318 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:36,318 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jhzvnbm_
2023-07-22 05:40:36,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa21e1b8-0f94-40a9-b665-b0d785d7b1a3
2023-07-22 05:40:36,319 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37895
2023-07-22 05:40:36,319 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37895
2023-07-22 05:40:36,319 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39153
2023-07-22 05:40:36,320 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,320 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,320 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:36,320 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:36,320 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-94xx7pb9
2023-07-22 05:40:36,320 - distributed.worker - INFO - Starting Worker plugin PreImport-9e34729e-3903-419d-81bf-45f240be7cb1
2023-07-22 05:40:36,320 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7711cdf-67a6-4c5d-801a-bc76b0940d3c
2023-07-22 05:40:36,320 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39737
2023-07-22 05:40:36,321 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39737
2023-07-22 05:40:36,321 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37959
2023-07-22 05:40:36,321 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,321 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,321 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:36,321 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:36,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bzru8eql
2023-07-22 05:40:36,321 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5667ff1a-a779-4c53-b0a5-b2af55a1944c
2023-07-22 05:40:36,321 - distributed.worker - INFO - Starting Worker plugin PreImport-390828bf-f888-41fb-8704-7e9fc2cdb576
2023-07-22 05:40:36,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da95b281-cd45-4076-8756-07e25ef142b9
2023-07-22 05:40:36,322 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81eb915c-a3a1-4e2e-8b1a-8bb7399d9672
2023-07-22 05:40:36,324 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44277
2023-07-22 05:40:36,324 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44277
2023-07-22 05:40:36,324 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39767
2023-07-22 05:40:36,324 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,324 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,324 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:36,324 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:36,324 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a_jgv995
2023-07-22 05:40:36,325 - distributed.worker - INFO - Starting Worker plugin PreImport-eb1b8725-5dc4-432b-8f72-e1ddbf74c467
2023-07-22 05:40:36,325 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5d4a591-4fee-43c4-87f9-295a616ff658
2023-07-22 05:40:36,325 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44665
2023-07-22 05:40:36,325 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44665
2023-07-22 05:40:36,325 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4079e5bf-b51e-422f-86e3-c546b05fb30a
2023-07-22 05:40:36,325 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44093
2023-07-22 05:40:36,325 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,325 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,325 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:36,325 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:36,325 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ko6t8ik4
2023-07-22 05:40:36,326 - distributed.worker - INFO - Starting Worker plugin PreImport-ed855a6e-f164-43a6-b21d-63886fe7d4a2
2023-07-22 05:40:36,326 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-563f9a99-2bcd-48a2-a5af-404485e2ae37
2023-07-22 05:40:36,327 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35911
2023-07-22 05:40:36,327 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35911
2023-07-22 05:40:36,327 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34215
2023-07-22 05:40:36,327 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,327 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ad62f20-840c-43a7-8400-192675356688
2023-07-22 05:40:36,327 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,327 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:36,328 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:36,328 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eaw33ak2
2023-07-22 05:40:36,328 - distributed.worker - INFO - Starting Worker plugin PreImport-4ed3812e-667f-46cc-aa65-99af72a1c1a3
2023-07-22 05:40:36,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d32434ac-0402-45c1-9f37-8b6b9b66336f
2023-07-22 05:40:36,328 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b94fd0e-00eb-471a-b90f-e431a1a77153
2023-07-22 05:40:36,341 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e0d8519-f188-407c-bd2a-e6bb5e11b52f
2023-07-22 05:40:36,341 - distributed.worker - INFO - Starting Worker plugin PreImport-7411ddfe-ca3e-43cf-9aa5-02083d7b2dc3
2023-07-22 05:40:36,342 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,389 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35821', status: init, memory: 0, processing: 0>
2023-07-22 05:40:36,390 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35821
2023-07-22 05:40:36,390 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53336
2023-07-22 05:40:36,390 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,391 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,393 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:36,462 - distributed.scheduler - INFO - Receive client connection: Client-488b0095-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:36,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53352
2023-07-22 05:40:36,468 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-07-22 05:40:36,474 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,477 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:36,478 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:36,481 - distributed.scheduler - INFO - Remove client Client-488b0095-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:36,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53352; closing.
2023-07-22 05:40:36,481 - distributed.scheduler - INFO - Remove client Client-488b0095-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:36,481 - distributed.scheduler - INFO - Close client connection: Client-488b0095-2852-11ee-88c9-d8c49764f6bb
2023-07-22 05:40:36,529 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69c88bc5-59f4-42de-a979-517136cd2472
2023-07-22 05:40:36,536 - distributed.worker - INFO - Starting Worker plugin PreImport-5dd8151e-c950-47e8-a9e1-8fa454df6707
2023-07-22 05:40:36,536 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,540 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,544 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,545 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,548 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,548 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,558 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45609', status: init, memory: 0, processing: 0>
2023-07-22 05:40:36,559 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45609
2023-07-22 05:40:36,559 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53370
2023-07-22 05:40:36,559 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,559 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:36,563 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41335', status: init, memory: 0, processing: 0>
2023-07-22 05:40:36,563 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41335
2023-07-22 05:40:36,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53376
2023-07-22 05:40:36,564 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,564 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:36,572 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39737', status: init, memory: 0, processing: 0>
2023-07-22 05:40:36,573 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39737
2023-07-22 05:40:36,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53390
2023-07-22 05:40:36,574 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,574 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,574 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35911', status: init, memory: 0, processing: 0>
2023-07-22 05:40:36,575 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35911
2023-07-22 05:40:36,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53394
2023-07-22 05:40:36,575 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,576 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:36,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:36,582 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37895', status: init, memory: 0, processing: 0>
2023-07-22 05:40:36,583 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37895
2023-07-22 05:40:36,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53406
2023-07-22 05:40:36,584 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,584 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,584 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44277', status: init, memory: 0, processing: 0>
2023-07-22 05:40:36,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44277
2023-07-22 05:40:36,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53402
2023-07-22 05:40:36,585 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,585 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,585 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44665', status: init, memory: 0, processing: 0>
2023-07-22 05:40:36,586 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44665
2023-07-22 05:40:36,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53404
2023-07-22 05:40:36,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:36,587 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:36,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:36,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:36,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:36,662 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:36,662 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:36,663 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:36,663 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:36,663 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:36,663 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:36,663 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:36,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:36,675 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,675 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,675 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,675 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,676 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,676 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,676 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,676 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:40:36,681 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:36,682 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:36,685 - distributed.scheduler - INFO - Remove client Client-44742ece-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:36,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53304; closing.
2023-07-22 05:40:36,685 - distributed.scheduler - INFO - Remove client Client-44742ece-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:36,685 - distributed.scheduler - INFO - Close client connection: Client-44742ece-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:36,686 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40067'. Reason: nanny-close
2023-07-22 05:40:36,687 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:36,687 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43679'. Reason: nanny-close
2023-07-22 05:40:36,688 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:36,688 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35911. Reason: nanny-close
2023-07-22 05:40:36,688 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42443'. Reason: nanny-close
2023-07-22 05:40:36,689 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:36,689 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41335. Reason: nanny-close
2023-07-22 05:40:36,689 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38423'. Reason: nanny-close
2023-07-22 05:40:36,689 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:36,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39413'. Reason: nanny-close
2023-07-22 05:40:36,690 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44665. Reason: nanny-close
2023-07-22 05:40:36,690 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:36,690 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53394; closing.
2023-07-22 05:40:36,690 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:36,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37885'. Reason: nanny-close
2023-07-22 05:40:36,690 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35911', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:36,690 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37895. Reason: nanny-close
2023-07-22 05:40:36,690 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35911
2023-07-22 05:40:36,690 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:36,691 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:36,691 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35113'. Reason: nanny-close
2023-07-22 05:40:36,691 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35821. Reason: nanny-close
2023-07-22 05:40:36,691 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:36,691 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39737. Reason: nanny-close
2023-07-22 05:40:36,691 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:36,691 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39193'. Reason: nanny-close
2023-07-22 05:40:36,691 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:36,692 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:36,692 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35911
2023-07-22 05:40:36,692 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53376; closing.
2023-07-22 05:40:36,692 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:36,692 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45609. Reason: nanny-close
2023-07-22 05:40:36,692 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35911
2023-07-22 05:40:36,692 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41335', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:36,692 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41335
2023-07-22 05:40:36,693 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35911
2023-07-22 05:40:36,693 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:36,693 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53404; closing.
2023-07-22 05:40:36,693 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44277. Reason: nanny-close
2023-07-22 05:40:36,693 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35911
2023-07-22 05:40:36,693 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:36,693 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:36,693 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44665', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:36,693 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44665
2023-07-22 05:40:36,693 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:36,694 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53406; closing.
2023-07-22 05:40:36,694 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35911
2023-07-22 05:40:36,694 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53390; closing.
2023-07-22 05:40:36,694 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:36,694 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37895', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:36,694 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37895
2023-07-22 05:40:36,694 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:36,695 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:36,695 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39737', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:36,695 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:36,695 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39737
2023-07-22 05:40:36,695 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:36,695 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53336; closing.
2023-07-22 05:40:36,696 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35821', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:36,696 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35821
2023-07-22 05:40:36,696 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:36,696 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:36,696 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53370; closing.
2023-07-22 05:40:36,696 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53402; closing.
2023-07-22 05:40:36,697 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45609', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:36,697 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45609
2023-07-22 05:40:36,697 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44277', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:36,697 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44277
2023-07-22 05:40:36,698 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:40:38,254 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:40:38,254 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:38,255 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:38,256 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:40:38,256 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-07-22 05:40:40,266 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:40,270 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36907 instead
  warnings.warn(
2023-07-22 05:40:40,274 - distributed.scheduler - INFO - State start
2023-07-22 05:40:40,293 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:40,294 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:40:40,295 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36907/status
2023-07-22 05:40:40,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33515'
2023-07-22 05:40:40,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43345'
2023-07-22 05:40:40,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39353'
2023-07-22 05:40:40,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40821'
2023-07-22 05:40:40,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33635'
2023-07-22 05:40:40,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41909'
2023-07-22 05:40:40,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35679'
2023-07-22 05:40:40,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40573'
2023-07-22 05:40:40,683 - distributed.scheduler - INFO - Receive client connection: Client-49dbafeb-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:40,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49690
2023-07-22 05:40:42,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:42,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:42,195 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:42,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:42,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:42,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:42,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:42,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:42,245 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:42,247 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:42,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:42,268 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:42,268 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:42,268 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:42,268 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:42,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:42,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:40:42,277 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:42,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:42,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:42,281 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:42,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:42,310 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:42,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:44,020 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38853
2023-07-22 05:40:44,020 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38853
2023-07-22 05:40:44,020 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34481
2023-07-22 05:40:44,020 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:44,021 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:44,021 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:44,021 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:44,021 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5npz2pq7
2023-07-22 05:40:44,021 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e6889e9-453a-49a5-bc2e-833857509058
2023-07-22 05:40:44,219 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-97ba4a6b-d8cf-4940-9deb-9f1d810cba45
2023-07-22 05:40:44,219 - distributed.worker - INFO - Starting Worker plugin PreImport-62f85536-f0e4-46ec-b1bd-e74711acd20d
2023-07-22 05:40:44,219 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:44,252 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38853', status: init, memory: 0, processing: 0>
2023-07-22 05:40:44,253 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38853
2023-07-22 05:40:44,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49716
2023-07-22 05:40:44,254 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:44,254 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:44,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:45,024 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36261
2023-07-22 05:40:45,025 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36261
2023-07-22 05:40:45,025 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35479
2023-07-22 05:40:45,025 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,025 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,025 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:45,025 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:45,025 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o6l597f1
2023-07-22 05:40:45,026 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc17892b-4642-4cce-8ead-4592b3486d1a
2023-07-22 05:40:45,145 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e64ba818-50d8-4c1e-ad5f-b2ec17b09377
2023-07-22 05:40:45,145 - distributed.worker - INFO - Starting Worker plugin PreImport-46cccbcf-ed0d-4bc8-98b0-88fdc4bd8394
2023-07-22 05:40:45,146 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,179 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36261', status: init, memory: 0, processing: 0>
2023-07-22 05:40:45,180 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36261
2023-07-22 05:40:45,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49724
2023-07-22 05:40:45,180 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,181 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,183 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:45,376 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34997
2023-07-22 05:40:45,376 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34997
2023-07-22 05:40:45,376 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42497
2023-07-22 05:40:45,376 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,376 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,376 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:45,376 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:45,376 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jq3_vwqx
2023-07-22 05:40:45,377 - distributed.worker - INFO - Starting Worker plugin PreImport-046ced27-ec4a-4004-88ef-e953e700cacf
2023-07-22 05:40:45,377 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7968951c-71a9-403e-9731-e187d880fac2
2023-07-22 05:40:45,377 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee2c377e-f506-4550-8431-aae59e1e7dbf
2023-07-22 05:40:45,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41923
2023-07-22 05:40:45,379 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41923
2023-07-22 05:40:45,379 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43449
2023-07-22 05:40:45,379 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,379 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,379 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:45,379 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:45,379 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dpa7n4d_
2023-07-22 05:40:45,379 - distributed.worker - INFO - Starting Worker plugin PreImport-ca13fb5e-750f-4bbf-9002-64e3966b8d89
2023-07-22 05:40:45,380 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2afae6cd-0565-402b-a420-90f7915e9980
2023-07-22 05:40:45,380 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f644d60-d796-4767-9172-a7555a614814
2023-07-22 05:40:45,380 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37549
2023-07-22 05:40:45,380 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37549
2023-07-22 05:40:45,380 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40607
2023-07-22 05:40:45,380 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40607
2023-07-22 05:40:45,380 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44677
2023-07-22 05:40:45,380 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,380 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33493
2023-07-22 05:40:45,380 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,381 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:45,381 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:45,381 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:45,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u0ksh72o
2023-07-22 05:40:45,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:45,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-40zzx13m
2023-07-22 05:40:45,381 - distributed.worker - INFO - Starting Worker plugin PreImport-844b8b88-06b2-415e-8977-399b5a6ee312
2023-07-22 05:40:45,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41865
2023-07-22 05:40:45,381 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3956002e-70aa-40a6-8c7c-af6c5a2a0d9f
2023-07-22 05:40:45,381 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46060091-a95f-4200-9a4b-ab00ab1232da
2023-07-22 05:40:45,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41865
2023-07-22 05:40:45,381 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0e974ab-5808-4c30-a583-ec47132eec06
2023-07-22 05:40:45,381 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34461
2023-07-22 05:40:45,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,381 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,381 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:45,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:45,382 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2g1s8szm
2023-07-22 05:40:45,381 - distributed.worker - INFO - Starting Worker plugin PreImport-6292e267-0318-43ce-8590-c364e9c51975
2023-07-22 05:40:45,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9887989d-75b6-4e23-905a-f4057dd1316e
2023-07-22 05:40:45,382 - distributed.worker - INFO - Starting Worker plugin PreImport-72a4c0c2-c992-4f76-9ab7-2390164d87f6
2023-07-22 05:40:45,382 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3382b662-ba4b-40bc-bac2-d719fae64abd
2023-07-22 05:40:45,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ba92d35-49ce-4f4d-9c91-c2fb406ce7d3
2023-07-22 05:40:45,387 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41487
2023-07-22 05:40:45,387 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41487
2023-07-22 05:40:45,387 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35135
2023-07-22 05:40:45,387 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,387 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,387 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:45,387 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:40:45,387 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r5ekg5cp
2023-07-22 05:40:45,388 - distributed.worker - INFO - Starting Worker plugin PreImport-cd80ae0b-a016-4eb6-b6ac-b2b12f9145f0
2023-07-22 05:40:45,388 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-215b586f-1837-4294-b01c-91e02a67ad26
2023-07-22 05:40:45,388 - distributed.worker - INFO - Starting Worker plugin RMMSetup-15a547b4-074a-4391-bdc5-50aed0002024
2023-07-22 05:40:45,526 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,526 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,526 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,527 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,527 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,527 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,557 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41865', status: init, memory: 0, processing: 0>
2023-07-22 05:40:45,558 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41865
2023-07-22 05:40:45,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49778
2023-07-22 05:40:45,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,559 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,560 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41923', status: init, memory: 0, processing: 0>
2023-07-22 05:40:45,560 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41923
2023-07-22 05:40:45,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49748
2023-07-22 05:40:45,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:45,561 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,561 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,561 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37549', status: init, memory: 0, processing: 0>
2023-07-22 05:40:45,562 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37549
2023-07-22 05:40:45,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49746
2023-07-22 05:40:45,563 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,563 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,563 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:45,563 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41487', status: init, memory: 0, processing: 0>
2023-07-22 05:40:45,564 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41487
2023-07-22 05:40:45,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49780
2023-07-22 05:40:45,565 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,565 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,565 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:45,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34997', status: init, memory: 0, processing: 0>
2023-07-22 05:40:45,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:45,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34997
2023-07-22 05:40:45,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49798
2023-07-22 05:40:45,567 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,568 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,569 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40607', status: init, memory: 0, processing: 0>
2023-07-22 05:40:45,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40607
2023-07-22 05:40:45,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49796
2023-07-22 05:40:45,570 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:45,570 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:45,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:45,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:45,600 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:45,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:45,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:45,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:45,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:45,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:45,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:45,601 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:40:45,607 - distributed.scheduler - INFO - Remove client Client-49dbafeb-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:45,607 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49690; closing.
2023-07-22 05:40:45,607 - distributed.scheduler - INFO - Remove client Client-49dbafeb-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:45,608 - distributed.scheduler - INFO - Close client connection: Client-49dbafeb-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:45,609 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43345'. Reason: nanny-close
2023-07-22 05:40:45,609 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33635'. Reason: nanny-close
2023-07-22 05:40:45,609 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:45,610 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33515'. Reason: nanny-close
2023-07-22 05:40:45,610 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39353'. Reason: nanny-close
2023-07-22 05:40:45,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40821'. Reason: nanny-close
2023-07-22 05:40:45,611 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:45,611 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38853. Reason: nanny-close
2023-07-22 05:40:45,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41909'. Reason: nanny-close
2023-07-22 05:40:45,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35679'. Reason: nanny-close
2023-07-22 05:40:45,612 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40573'. Reason: nanny-close
2023-07-22 05:40:45,612 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:45,612 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36261. Reason: nanny-close
2023-07-22 05:40:45,613 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41865. Reason: nanny-close
2023-07-22 05:40:45,613 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49716; closing.
2023-07-22 05:40:45,613 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:45,614 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:45,614 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:45,614 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38853', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:45,614 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:45,614 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38853
2023-07-22 05:40:45,614 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:45,614 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:45,614 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41923. Reason: nanny-close
2023-07-22 05:40:45,615 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:45,615 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34997. Reason: nanny-close
2023-07-22 05:40:45,615 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:45,615 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41487. Reason: nanny-close
2023-07-22 05:40:45,615 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:45,615 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:45,616 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:45,616 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40607. Reason: nanny-close
2023-07-22 05:40:45,616 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38853
2023-07-22 05:40:45,616 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:45,616 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49724; closing.
2023-07-22 05:40:45,616 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37549. Reason: nanny-close
2023-07-22 05:40:45,616 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49778; closing.
2023-07-22 05:40:45,617 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38853
2023-07-22 05:40:45,617 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38853
2023-07-22 05:40:45,617 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:45,617 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:45,617 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:45,617 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36261', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:45,618 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36261
2023-07-22 05:40:45,618 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:45,618 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38853
2023-07-22 05:40:45,618 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41865', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:45,618 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41865
2023-07-22 05:40:45,618 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:45,618 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:45,619 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:45,619 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:45,619 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49748; closing.
2023-07-22 05:40:45,620 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49780; closing.
2023-07-22 05:40:45,620 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:45,620 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41923', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:45,620 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41923
2023-07-22 05:40:45,621 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41487', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:45,621 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41487
2023-07-22 05:40:45,622 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49798; closing.
2023-07-22 05:40:45,622 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49746; closing.
2023-07-22 05:40:45,622 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49796; closing.
2023-07-22 05:40:45,623 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34997', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:45,623 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34997
2023-07-22 05:40:45,624 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37549', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:45,624 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37549
2023-07-22 05:40:45,624 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40607', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:45,624 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40607
2023-07-22 05:40:45,625 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:40:47,026 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:40:47,027 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:47,027 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:47,029 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:40:47,029 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-07-22 05:40:49,137 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:49,142 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41189 instead
  warnings.warn(
2023-07-22 05:40:49,146 - distributed.scheduler - INFO - State start
2023-07-22 05:40:49,574 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:49,575 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:40:49,576 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41189/status
2023-07-22 05:40:49,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44255'
2023-07-22 05:40:50,239 - distributed.scheduler - INFO - Receive client connection: Client-4f193ce7-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:50,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45636
2023-07-22 05:40:51,225 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:40:51,225 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-22 05:40:51,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:40:53,129 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38213
2023-07-22 05:40:53,129 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38213
2023-07-22 05:40:53,129 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-07-22 05:40:53,130 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:40:53,130 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:53,130 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:40:53,130 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:40:53,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lvr3dohx
2023-07-22 05:40:53,131 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2dbfe596-ec13-478d-821d-c93ab4c653f1
2023-07-22 05:40:53,131 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a50b9e5c-a56a-4519-91dc-8b731a7579a5
2023-07-22 05:40:53,131 - distributed.worker - INFO - Starting Worker plugin PreImport-f71f7ea3-d16b-450c-bc12-a294e985636f
2023-07-22 05:40:53,132 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:53,171 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38213', status: init, memory: 0, processing: 0>
2023-07-22 05:40:53,172 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38213
2023-07-22 05:40:53,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45658
2023-07-22 05:40:53,173 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:40:53,174 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:40:53,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:40:53,219 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:40:53,222 - distributed.scheduler - INFO - Remove client Client-4f193ce7-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:53,222 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45636; closing.
2023-07-22 05:40:53,223 - distributed.scheduler - INFO - Remove client Client-4f193ce7-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:53,223 - distributed.scheduler - INFO - Close client connection: Client-4f193ce7-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:53,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44255'. Reason: nanny-close
2023-07-22 05:40:53,225 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:40:53,226 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38213. Reason: nanny-close
2023-07-22 05:40:53,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45658; closing.
2023-07-22 05:40:53,228 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:40:53,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38213', status: closing, memory: 0, processing: 0>
2023-07-22 05:40:53,229 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38213
2023-07-22 05:40:53,229 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:40:53,230 - distributed.nanny - INFO - Worker closed
2023-07-22 05:40:54,291 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:40:54,291 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:40:54,292 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:40:54,292 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:40:54,293 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-07-22 05:40:58,199 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:58,204 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37507 instead
  warnings.warn(
2023-07-22 05:40:58,208 - distributed.scheduler - INFO - State start
2023-07-22 05:40:58,407 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:40:58,408 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:40:58,409 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37507/status
2023-07-22 05:40:58,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33437'
2023-07-22 05:40:59,171 - distributed.scheduler - INFO - Receive client connection: Client-5473218e-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:40:59,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45826
2023-07-22 05:41:00,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:00,122 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-22 05:41:00,700 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:01,882 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33879
2023-07-22 05:41:01,882 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33879
2023-07-22 05:41:01,882 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34627
2023-07-22 05:41:01,882 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:01,882 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:01,882 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:01,882 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:41:01,882 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nlfejcv7
2023-07-22 05:41:01,883 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e579a5ef-eaf5-47d8-adaa-61563f343091
2023-07-22 05:41:01,883 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77fa226c-52db-4e0d-9adf-843e5e7e0337
2023-07-22 05:41:01,883 - distributed.worker - INFO - Starting Worker plugin PreImport-ce719210-60ec-4059-8e79-c33deacb98fc
2023-07-22 05:41:01,885 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:01,913 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33879', status: init, memory: 0, processing: 0>
2023-07-22 05:41:01,915 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33879
2023-07-22 05:41:01,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42994
2023-07-22 05:41:01,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:01,916 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:01,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:01,996 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:01,999 - distributed.scheduler - INFO - Remove client Client-5473218e-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:01,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45826; closing.
2023-07-22 05:41:02,000 - distributed.scheduler - INFO - Remove client Client-5473218e-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:02,000 - distributed.scheduler - INFO - Close client connection: Client-5473218e-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:02,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33437'. Reason: nanny-close
2023-07-22 05:41:02,001 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:02,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33879. Reason: nanny-close
2023-07-22 05:41:02,004 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:02,004 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42994; closing.
2023-07-22 05:41:02,004 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33879', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:02,005 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33879
2023-07-22 05:41:02,005 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:41:02,005 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:03,117 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:41:03,118 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:41:03,118 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:41:03,119 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:41:03,119 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-07-22 05:41:05,141 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:05,146 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34059 instead
  warnings.warn(
2023-07-22 05:41:05,150 - distributed.scheduler - INFO - State start
2023-07-22 05:41:05,172 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:05,173 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:41:05,173 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34059/status
2023-07-22 05:41:10,649 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:41:10,649 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:41:10,650 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:41:10,650 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:41:10,651 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-07-22 05:41:12,653 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:12,657 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46043 instead
  warnings.warn(
2023-07-22 05:41:12,661 - distributed.scheduler - INFO - State start
2023-07-22 05:41:12,680 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:12,681 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-22 05:41:12,681 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46043/status
2023-07-22 05:41:12,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33901'
2023-07-22 05:41:13,829 - distributed.scheduler - INFO - Receive client connection: Client-5d2bef70-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:13,841 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59204
2023-07-22 05:41:14,367 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:14,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:14,375 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:15,314 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45421
2023-07-22 05:41:15,314 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45421
2023-07-22 05:41:15,314 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35389
2023-07-22 05:41:15,314 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-22 05:41:15,314 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:15,314 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:15,315 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:41:15,315 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-02zzr05a
2023-07-22 05:41:15,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0ca571c-dce6-4281-a579-11a26b64c650
2023-07-22 05:41:15,315 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17d78ab6-1f4c-4226-a015-ea83e26f7015
2023-07-22 05:41:15,315 - distributed.worker - INFO - Starting Worker plugin PreImport-7707581f-7eb4-44a8-a736-919b80872a38
2023-07-22 05:41:15,316 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:15,337 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45421', status: init, memory: 0, processing: 0>
2023-07-22 05:41:15,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45421
2023-07-22 05:41:15,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59222
2023-07-22 05:41:15,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-22 05:41:15,339 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:15,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-22 05:41:15,374 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:15,377 - distributed.scheduler - INFO - Remove client Client-5d2bef70-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:15,377 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59204; closing.
2023-07-22 05:41:15,377 - distributed.scheduler - INFO - Remove client Client-5d2bef70-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:15,378 - distributed.scheduler - INFO - Close client connection: Client-5d2bef70-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:15,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33901'. Reason: nanny-close
2023-07-22 05:41:15,379 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:15,380 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45421. Reason: nanny-close
2023-07-22 05:41:15,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-22 05:41:15,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59222; closing.
2023-07-22 05:41:15,383 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45421', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:15,383 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45421
2023-07-22 05:41:15,383 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:41:15,384 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:16,595 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:41:16,596 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:41:16,596 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:41:16,597 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-22 05:41:16,597 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-07-22 05:41:18,703 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:18,707 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45535 instead
  warnings.warn(
2023-07-22 05:41:18,711 - distributed.scheduler - INFO - State start
2023-07-22 05:41:18,731 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:18,734 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:41:18,734 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45535/status
2023-07-22 05:41:18,870 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34793'
2023-07-22 05:41:18,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43007'
2023-07-22 05:41:18,896 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36497'
2023-07-22 05:41:18,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40907'
2023-07-22 05:41:18,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45627'
2023-07-22 05:41:18,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38997'
2023-07-22 05:41:18,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37285'
2023-07-22 05:41:18,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38129'
2023-07-22 05:41:19,863 - distributed.scheduler - INFO - Receive client connection: Client-60bed9e9-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:19,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58824
2023-07-22 05:41:20,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:20,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:20,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:20,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:20,632 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:20,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:20,634 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:20,635 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:20,648 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:20,662 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:20,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:20,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:20,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:20,664 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:20,664 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:20,664 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:20,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:20,667 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:20,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:20,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:20,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:20,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:20,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:20,779 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:23,570 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33881
2023-07-22 05:41:23,570 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33881
2023-07-22 05:41:23,570 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42583
2023-07-22 05:41:23,570 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,570 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,570 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:23,570 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:41:23,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8cg98mvt
2023-07-22 05:41:23,571 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-474beac9-3f03-48c2-8180-d493853589d3
2023-07-22 05:41:23,571 - distributed.worker - INFO - Starting Worker plugin PreImport-a8454ad6-a2fb-4d20-a4c9-9bb1457d2c75
2023-07-22 05:41:23,571 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e98b3120-4037-438c-970f-13fd190e39b2
2023-07-22 05:41:23,575 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43855
2023-07-22 05:41:23,575 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43855
2023-07-22 05:41:23,575 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39107
2023-07-22 05:41:23,575 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,575 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,575 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:23,575 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:41:23,575 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1666ugp1
2023-07-22 05:41:23,576 - distributed.worker - INFO - Starting Worker plugin PreImport-c83d7dbb-a2b8-4a03-b39c-3fa8c35d5aa2
2023-07-22 05:41:23,576 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5d90a3a-1a18-4aa0-939d-fd0f6881ed48
2023-07-22 05:41:23,576 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1119d9e-40ad-4267-89d0-ff4ed8e9a31b
2023-07-22 05:41:23,577 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43509
2023-07-22 05:41:23,577 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43509
2023-07-22 05:41:23,577 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42679
2023-07-22 05:41:23,577 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,577 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,577 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:23,577 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:41:23,577 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c_ranqsa
2023-07-22 05:41:23,578 - distributed.worker - INFO - Starting Worker plugin RMMSetup-83ce05b5-d2ad-40d7-988f-15431f0cbe5a
2023-07-22 05:41:23,584 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45917
2023-07-22 05:41:23,584 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45917
2023-07-22 05:41:23,584 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33079
2023-07-22 05:41:23,584 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,585 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,585 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:23,585 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:41:23,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3iks3dij
2023-07-22 05:41:23,585 - distributed.worker - INFO - Starting Worker plugin PreImport-00707ae6-dc24-40e0-a0c3-e33e69206d09
2023-07-22 05:41:23,585 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69ddc82b-a1fa-4ccd-9a2e-d2aafed75372
2023-07-22 05:41:23,585 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dba905b7-1e5a-4884-b066-915ab27849a4
2023-07-22 05:41:23,610 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33965
2023-07-22 05:41:23,610 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33965
2023-07-22 05:41:23,610 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39735
2023-07-22 05:41:23,610 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,610 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,610 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:23,610 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:41:23,610 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4dhsb1i4
2023-07-22 05:41:23,611 - distributed.worker - INFO - Starting Worker plugin PreImport-df66c227-75b2-4a47-a363-d92d20771d31
2023-07-22 05:41:23,611 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09c1d183-357a-4cbe-b85c-be53c61c60f6
2023-07-22 05:41:23,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25296d7a-7a36-44ac-ba04-dd7f81f91327
2023-07-22 05:41:23,611 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39791
2023-07-22 05:41:23,612 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39791
2023-07-22 05:41:23,612 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34059
2023-07-22 05:41:23,612 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,612 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,612 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:23,612 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:41:23,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fg3e02eo
2023-07-22 05:41:23,612 - distributed.worker - INFO - Starting Worker plugin PreImport-c6dcd8cb-36a6-4e35-b403-9ca7b5892e44
2023-07-22 05:41:23,613 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0b116fb-94f0-4d89-8814-99af176b95bc
2023-07-22 05:41:23,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea36b047-d192-4940-bceb-fb581bf7b4e8
2023-07-22 05:41:23,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40199
2023-07-22 05:41:23,708 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40199
2023-07-22 05:41:23,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35417
2023-07-22 05:41:23,708 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,708 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,708 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:23,709 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:41:23,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z_5mhbvw
2023-07-22 05:41:23,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70ceb309-3bc5-4366-a128-6657f6daf0d2
2023-07-22 05:41:23,709 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36789
2023-07-22 05:41:23,710 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36789
2023-07-22 05:41:23,710 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44241
2023-07-22 05:41:23,710 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,710 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,710 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:23,710 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-22 05:41:23,710 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jux_w14b
2023-07-22 05:41:23,711 - distributed.worker - INFO - Starting Worker plugin PreImport-e95c3383-f848-45a1-ae30-8c3282a12de8
2023-07-22 05:41:23,711 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfbd9ee7-e751-40cf-b718-c1a8b80bea02
2023-07-22 05:41:23,711 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78a2bf9b-40b5-420a-8669-268355c995f8
2023-07-22 05:41:23,748 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6bb1808-0c58-4ca1-b7c3-5ecd9509f401
2023-07-22 05:41:23,748 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,748 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,748 - distributed.worker - INFO - Starting Worker plugin PreImport-a7180e19-2d70-41c5-82a7-63506ddee5b0
2023-07-22 05:41:23,748 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,749 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,749 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,749 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,781 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33965', status: init, memory: 0, processing: 0>
2023-07-22 05:41:23,784 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33965
2023-07-22 05:41:23,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58870
2023-07-22 05:41:23,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,784 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,785 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43855', status: init, memory: 0, processing: 0>
2023-07-22 05:41:23,786 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43855
2023-07-22 05:41:23,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:23,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58856
2023-07-22 05:41:23,787 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,787 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,788 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39791', status: init, memory: 0, processing: 0>
2023-07-22 05:41:23,789 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:23,789 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39791
2023-07-22 05:41:23,789 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58846
2023-07-22 05:41:23,790 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,790 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,790 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45917', status: init, memory: 0, processing: 0>
2023-07-22 05:41:23,791 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45917
2023-07-22 05:41:23,791 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58854
2023-07-22 05:41:23,792 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:23,792 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,792 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,792 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43509', status: init, memory: 0, processing: 0>
2023-07-22 05:41:23,793 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43509
2023-07-22 05:41:23,793 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58892
2023-07-22 05:41:23,794 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:23,794 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,795 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33881', status: init, memory: 0, processing: 0>
2023-07-22 05:41:23,796 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33881
2023-07-22 05:41:23,796 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58880
2023-07-22 05:41:23,797 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,797 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:23,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:23,833 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7479531-0afa-48e7-bb21-776910059549
2023-07-22 05:41:23,833 - distributed.worker - INFO - Starting Worker plugin PreImport-6c8cfb3e-746b-429c-a632-34e5cd49adc0
2023-07-22 05:41:23,834 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,845 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40199', status: init, memory: 0, processing: 0>
2023-07-22 05:41:23,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40199
2023-07-22 05:41:23,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58898
2023-07-22 05:41:23,868 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,868 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:23,874 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36789', status: init, memory: 0, processing: 0>
2023-07-22 05:41:23,874 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36789
2023-07-22 05:41:23,875 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58908
2023-07-22 05:41:23,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:23,875 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:23,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:23,942 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:41:23,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:41:23,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:41:23,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:41:23,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:41:23,943 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:41:23,944 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:41:23,944 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-22 05:41:23,956 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:23,957 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:23,957 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:23,957 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:23,957 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:23,957 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:23,958 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:23,958 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:23,962 - distributed.scheduler - INFO - Remove client Client-60bed9e9-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:23,962 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58824; closing.
2023-07-22 05:41:23,962 - distributed.scheduler - INFO - Remove client Client-60bed9e9-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:23,963 - distributed.scheduler - INFO - Close client connection: Client-60bed9e9-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:23,963 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34793'. Reason: nanny-close
2023-07-22 05:41:23,964 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:23,964 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43007'. Reason: nanny-close
2023-07-22 05:41:23,965 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:23,965 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36789. Reason: nanny-close
2023-07-22 05:41:23,965 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38997'. Reason: nanny-close
2023-07-22 05:41:23,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:23,966 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37285'. Reason: nanny-close
2023-07-22 05:41:23,966 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43509. Reason: nanny-close
2023-07-22 05:41:23,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:23,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38129'. Reason: nanny-close
2023-07-22 05:41:23,967 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45917. Reason: nanny-close
2023-07-22 05:41:23,967 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:23,967 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39791. Reason: nanny-close
2023-07-22 05:41:23,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36497'. Reason: nanny-close
2023-07-22 05:41:23,968 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:23,968 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58908; closing.
2023-07-22 05:41:23,968 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40907'. Reason: nanny-close
2023-07-22 05:41:23,968 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:23,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40199. Reason: nanny-close
2023-07-22 05:41:23,968 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:23,968 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36789', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:23,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:23,969 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36789
2023-07-22 05:41:23,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:23,969 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45627'. Reason: nanny-close
2023-07-22 05:41:23,969 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33881. Reason: nanny-close
2023-07-22 05:41:23,969 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:23,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:23,969 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43855. Reason: nanny-close
2023-07-22 05:41:23,970 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:23,970 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:23,970 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:23,970 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33965. Reason: nanny-close
2023-07-22 05:41:23,970 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:23,970 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:23,971 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36789
2023-07-22 05:41:23,971 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58854; closing.
2023-07-22 05:41:23,971 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36789
2023-07-22 05:41:23,971 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58892; closing.
2023-07-22 05:41:23,971 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:23,972 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36789
2023-07-22 05:41:23,972 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:23,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58846; closing.
2023-07-22 05:41:23,972 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:23,972 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:23,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45917', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:23,973 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45917
2023-07-22 05:41:23,973 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:23,973 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:23,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43509', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:23,974 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43509
2023-07-22 05:41:23,974 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:23,974 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39791', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:23,974 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39791
2023-07-22 05:41:23,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58898; closing.
2023-07-22 05:41:23,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58880; closing.
2023-07-22 05:41:23,976 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58856; closing.
2023-07-22 05:41:23,976 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40199', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:23,976 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40199
2023-07-22 05:41:23,977 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33881', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:23,977 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33881
2023-07-22 05:41:23,977 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43855', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:23,977 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43855
2023-07-22 05:41:23,978 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58870; closing.
2023-07-22 05:41:23,978 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33965', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:23,979 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33965
2023-07-22 05:41:23,979 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:41:23,979 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58870>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-22 05:41:25,381 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:41:25,381 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:41:25,382 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:41:25,383 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:41:25,383 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-07-22 05:41:27,383 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:27,388 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44919 instead
  warnings.warn(
2023-07-22 05:41:27,391 - distributed.scheduler - INFO - State start
2023-07-22 05:41:27,436 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:27,437 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:41:27,437 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44919/status
2023-07-22 05:41:27,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33227'
2023-07-22 05:41:29,040 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:29,040 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:29,200 - distributed.scheduler - INFO - Receive client connection: Client-65f32b59-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:29,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59014
2023-07-22 05:41:29,813 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:31,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38915
2023-07-22 05:41:31,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38915
2023-07-22 05:41:31,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38379
2023-07-22 05:41:31,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:31,219 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:31,219 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:31,219 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:41:31,219 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q2316s6_
2023-07-22 05:41:31,219 - distributed.worker - INFO - Starting Worker plugin PreImport-e29dc9c6-5a12-40fc-8406-ee42dc857c7a
2023-07-22 05:41:31,220 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95509e13-c33e-4d04-8cc2-a002260eb318
2023-07-22 05:41:31,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6f35514-564f-44b9-a302-9bd2bc5c84fc
2023-07-22 05:41:31,451 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:31,485 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38915', status: init, memory: 0, processing: 0>
2023-07-22 05:41:31,486 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38915
2023-07-22 05:41:31,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44956
2023-07-22 05:41:31,487 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:31,487 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:31,491 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:31,543 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:41:31,547 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:31,549 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:31,551 - distributed.scheduler - INFO - Remove client Client-65f32b59-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:31,551 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59014; closing.
2023-07-22 05:41:31,551 - distributed.scheduler - INFO - Remove client Client-65f32b59-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:31,552 - distributed.scheduler - INFO - Close client connection: Client-65f32b59-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:31,552 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33227'. Reason: nanny-close
2023-07-22 05:41:31,553 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:31,554 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38915. Reason: nanny-close
2023-07-22 05:41:31,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44956; closing.
2023-07-22 05:41:31,556 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:31,557 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38915', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:31,557 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38915
2023-07-22 05:41:31,557 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:41:31,558 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:32,619 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:41:32,619 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:41:32,620 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:41:32,620 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:41:32,621 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-07-22 05:41:34,790 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:34,796 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38327 instead
  warnings.warn(
2023-07-22 05:41:34,801 - distributed.scheduler - INFO - State start
2023-07-22 05:41:34,825 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-22 05:41:34,825 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-22 05:41:34,826 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38327/status
2023-07-22 05:41:34,939 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40697'
2023-07-22 05:41:35,575 - distributed.scheduler - INFO - Receive client connection: Client-6a55453a-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:35,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45066
2023-07-22 05:41:36,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:36,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:36,513 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-22 05:41:37,476 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32823
2023-07-22 05:41:37,476 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32823
2023-07-22 05:41:37,476 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40053
2023-07-22 05:41:37,476 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-22 05:41:37,476 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:37,476 - distributed.worker - INFO -               Threads:                          1
2023-07-22 05:41:37,476 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-22 05:41:37,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r2tkmti4
2023-07-22 05:41:37,477 - distributed.worker - INFO - Starting Worker plugin PreImport-88b3d2ce-5b38-49dd-9ccf-055d529c8c79
2023-07-22 05:41:37,477 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a937fe74-1425-42b5-b611-097490bb9886
2023-07-22 05:41:37,477 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20eefa21-e3d9-4b08-80c4-4ceb59056e8c
2023-07-22 05:41:37,579 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:37,603 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32823', status: init, memory: 0, processing: 0>
2023-07-22 05:41:37,604 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32823
2023-07-22 05:41:37,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45082
2023-07-22 05:41:37,605 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-22 05:41:37,605 - distributed.worker - INFO - -------------------------------------------------
2023-07-22 05:41:37,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-22 05:41:37,632 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-07-22 05:41:37,637 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-22 05:41:37,640 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:37,641 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-22 05:41:37,643 - distributed.scheduler - INFO - Remove client Client-6a55453a-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:37,643 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45066; closing.
2023-07-22 05:41:37,644 - distributed.scheduler - INFO - Remove client Client-6a55453a-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:37,644 - distributed.scheduler - INFO - Close client connection: Client-6a55453a-2852-11ee-8813-d8c49764f6bb
2023-07-22 05:41:37,645 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40697'. Reason: nanny-close
2023-07-22 05:41:37,646 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-22 05:41:37,647 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32823. Reason: nanny-close
2023-07-22 05:41:37,649 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45082; closing.
2023-07-22 05:41:37,649 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-22 05:41:37,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32823', status: closing, memory: 0, processing: 0>
2023-07-22 05:41:37,649 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32823
2023-07-22 05:41:37,649 - distributed.scheduler - INFO - Lost all workers
2023-07-22 05:41:37,650 - distributed.nanny - INFO - Worker closed
2023-07-22 05:41:38,762 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-22 05:41:38,762 - distributed.scheduler - INFO - Scheduler closing...
2023-07-22 05:41:38,763 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-22 05:41:38,763 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-22 05:41:38,764 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40063 instead
  warnings.warn(
2023-07-22 05:41:48,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:48,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:48,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:48,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:48,759 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:48,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:48,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:48,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:48,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:48,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:48,775 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:48,775 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:48,794 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:48,794 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:48,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:48,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-22 05:41:59,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:59,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:59,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:59,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:59,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:59,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:59,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:59,448 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:59,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:59,508 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:59,510 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:59,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:59,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:59,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:41:59,513 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:41:59,513 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44119 instead
  warnings.warn(
2023-07-22 05:42:09,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:09,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:09,780 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:09,780 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:09,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:09,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:09,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:09,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:09,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:09,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:09,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:09,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:09,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:09,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:09,914 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:09,914 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39587 instead
  warnings.warn(
2023-07-22 05:42:20,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:20,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:20,355 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:20,355 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:20,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:20,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:20,402 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:20,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:20,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:20,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:20,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:20,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:20,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:20,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:20,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:20,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42271 instead
  warnings.warn(
2023-07-22 05:42:34,747 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:34,747 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:34,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:34,772 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:34,779 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:34,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:34,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:34,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:34,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:34,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:34,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:34,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:34,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:34,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:34,897 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:34,897 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46499 instead
  warnings.warn(
2023-07-22 05:42:48,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:48,165 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:48,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:48,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:48,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:48,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:48,247 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:48,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:48,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:48,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:48,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:48,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:48,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:48,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:42:48,410 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:42:48,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37611 instead
  warnings.warn(
2023-07-22 05:43:01,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:01,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:01,688 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:01,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:01,705 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:01,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:01,705 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:01,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:01,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:01,749 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:01,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:01,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:01,772 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:01,772 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:01,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:01,773 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38319 instead
  warnings.warn(
2023-07-22 05:43:15,011 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:15,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:15,034 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:15,034 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:15,034 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:15,034 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:15,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:15,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:15,050 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:15,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:15,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:15,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:15,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:15,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-22 05:43:15,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-22 05:43:15,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32971 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40281 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43969 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43605 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43405 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43563 instead
  warnings.warn(
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-07-22 05:44:55,659 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-07-22 05:44:55,667 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7efc7fec2bb0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-07-22 05:44:57,671 - distributed.nanny - ERROR - Worker process died unexpectedly
