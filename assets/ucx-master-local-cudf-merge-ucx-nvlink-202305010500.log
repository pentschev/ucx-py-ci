2023-05-01 05:49:12,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:12,581 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:12,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:12,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:12,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:12,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:12,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:12,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:12,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:12,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:12,877 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:12,877 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:12,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:12,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:12,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:12,902 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:79825:0:79825] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  79825) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f6ca897ddec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x31fcf) [0x7f6ca897dfcf]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x32304) [0x7f6ca897e304]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f6d47db3420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x18) [0x7f6ca8a0d838]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f6ca8a420d8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23457) [0x7f6ca892d457]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23a38) [0x7f6ca892da38]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x260fc) [0x7f6ca89300fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7f6ca89894d9]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f6ca89301ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f6ca8a09caa]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7f6ca8acc6e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5591dc87bb08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5591dc86c112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5591dc86527a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5591dc876c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5591dc86681b]
18  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5591dc876ef3]
19  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x5591dc884a16]
20  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x5591dc9949b1]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5591dc822817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5591dc86df83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5591dc86bd36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5591dc876ef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5591dc86681b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5591dc876ef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5591dc86681b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5591dc876ef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5591dc86681b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5591dc876ef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5591dc86681b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5591dc86527a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5591dc876c05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5591dc86afa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5591dc86527a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5591dc884935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5591dc885104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5591dc94bfc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5591dc86f2bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5591dc86a1bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5591dc876ef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5591dc884c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5591dc86a1bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5591dc876ef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5591dc86681b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5591dc86527a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5591dc876c05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5591dc86681b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5591dc876ef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5591dc866568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5591dc86527a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5591dc876c05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5591dc8673cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5591dc86527a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5591dc864f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5591dc864eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5591dc9158bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5591dc943adc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5591dc93fc24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5591dc9377ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5591dc9376bd]
=================================
2023-05-01 05:49:20,879 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52341 -> ucx://127.0.0.1:34037
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f6f6dd74100, tag: 0x2e6ac532d3141ae1, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
Task exception was never retrieved
future: <Task finished name='Task-814' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-05-01 05:49:20,965 - distributed.nanny - WARNING - Restarting worker
[dgx13:79816:0:79816] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  79816) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7fad652bbdec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x31fcf) [0x7fad652bbfcf]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x32304) [0x7fad652bc304]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fae086fb420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x18) [0x7fad6534b838]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7fad653800d8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23457) [0x7fad6526b457]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23a38) [0x7fad6526ba38]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x260fc) [0x7fad6526e0fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7fad652c74d9]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7fad6526e1ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7fad65347caa]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7fad6540a6e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55b432b16b08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55b432b07112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b432b0027a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55b432b11c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b432b0181b]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55b432b2670e]
19  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fad87fc72fe]
20  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55b432b0a2bc]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55b432abd817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55b432b08f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55b432b06d36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b432b11ef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b432b0181b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b432b11ef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b432b0181b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b432b11ef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b432b0181b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b432b11ef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b432b0181b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b432b0027a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55b432b11c05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55b432b05fa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b432b0027a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55b432b1f935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55b432b20104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55b432be6fc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55b432b0a2bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55b432b051bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b432b11ef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55b432b1fc72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55b432b051bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b432b11ef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b432b0181b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b432b0027a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55b432b11c05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55b432b0181b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55b432b11ef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55b432b01568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b432b0027a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55b432b11c05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55b432b023cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55b432b0027a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55b432afff07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55b432affeb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55b432bb08bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55b432bdeadc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55b432bdac24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55b432bd27ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55b432bd26bd]
=================================
2023-05-01 05:49:21,266 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52341 -> ucx://127.0.0.1:51087
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f6f6dd74100, tag: 0xf14cb44174ad40b3, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:21,345 - distributed.nanny - WARNING - Restarting worker
[dgx13:79822:0:79822] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  79822) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7fadb71bedec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x31fcf) [0x7fadb71befcf]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x32304) [0x7fadb71bf304]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fae5a615420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x18) [0x7fadb724e838]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7fadb72830d8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23457) [0x7fadb716e457]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23a38) [0x7fadb716ea38]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x260fc) [0x7fadb71710fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7fadb71ca4d9]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7fadb71711ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7fadb724acaa]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7fadb730d6e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x555b2dd9fb08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x555b2dd90112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555b2dd8927a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x555b2dd9ac05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555b2dd8a81b]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x555b2ddaf70e]
19  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fadd9ee32fe]
20  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x555b2dd932bc]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x555b2dd46817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x555b2dd91f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x555b2dd8fd36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555b2dd9aef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555b2dd8a81b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555b2dd9aef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555b2dd8a81b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555b2dd9aef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555b2dd8a81b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555b2dd9aef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555b2dd8a81b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555b2dd8927a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x555b2dd9ac05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x555b2dd8efa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555b2dd8927a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x555b2dda8935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x555b2dda9104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x555b2de6ffc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x555b2dd932bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x555b2dd8e1bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555b2dd9aef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x555b2dda8c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x555b2dd8e1bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555b2dd9aef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555b2dd8a81b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555b2dd8927a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x555b2dd9ac05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x555b2dd8a81b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x555b2dd9aef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x555b2dd8a568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555b2dd8927a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x555b2dd9ac05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x555b2dd8b3cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x555b2dd8927a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x555b2dd88f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x555b2dd88eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x555b2de398bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x555b2de67adc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x555b2de63c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x555b2de5b7ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x555b2de5b6bd]
=================================
[dgx13:79820:0:79820] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  79820) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7fcf6118bdec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x31fcf) [0x7fcf6118bfcf]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x32304) [0x7fcf6118c304]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fd0045ee420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x18) [0x7fcf6121b838]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7fcf612500d8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23457) [0x7fcf6113b457]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23a38) [0x7fcf6113ba38]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x260fc) [0x7fcf6113e0fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7fcf611974d9]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7fcf6113e1ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7fcf61217caa]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7fcf612da6e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x556508fefb08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x556508fe0112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556508fd927a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556508feac05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556508fda81b]
18  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556508feaef3]
19  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x556508ff8a16]
20  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x5565091089b1]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x556508f96817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x556508fe1f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x556508fdfd36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556508feaef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556508fda81b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556508feaef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556508fda81b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556508feaef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556508fda81b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556508feaef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556508fda81b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556508fd927a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556508feac05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x556508fdefa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556508fd927a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x556508ff8935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x556508ff9104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5565090bffc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x556508fe32bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x556508fde1bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556508feaef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x556508ff8c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x556508fde1bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556508feaef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556508fda81b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556508fd927a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556508feac05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556508fda81b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556508feaef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x556508fda568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556508fd927a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556508feac05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x556508fdb3cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556508fd927a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x556508fd8f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556508fd8eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5565090898bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5565090b7adc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5565090b3c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5565090ab7ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5565090ab6bd]
=================================
2023-05-01 05:49:22,064 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49155
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f6f6dd74140, tag: 0xb8898d6f34eeffd5, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f6f6dd74140, tag: 0xb8898d6f34eeffd5, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
Task exception was never retrieved
future: <Task finished name='Task-995' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2023-05-01 05:49:22,065 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49155
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f0c799ec100, tag: 0x632a77d6161a2c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f0c799ec100, tag: 0x632a77d6161a2c, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-05-01 05:49:22,065 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:53807 -> ucx://127.0.0.1:49155
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f64685c6100, tag: 0xbe9d0ab9835bc566, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:22,066 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49155
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7faf742ae140, tag: 0x7581bb229650db38, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7faf742ae140, tag: 0x7581bb229650db38, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:49155 after 30 s
2023-05-01 05:49:22,100 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59281
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f0c799ec180, tag: 0xa2dfdf20f87ef9, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f0c799ec180, tag: 0xa2dfdf20f87ef9, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-05-01 05:49:22,101 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52611 -> ucx://127.0.0.1:59281
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f0c799ec280, tag: 0xc8312518a4edec56, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:22,070 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:49155
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 468, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-05-01 05:49:22,139 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52341 -> ucx://127.0.0.1:59281
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f6f6dd742c0, tag: 0x97ef769d5febbf8a, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:22,141 - distributed.nanny - WARNING - Restarting worker
2023-05-01 05:49:22,141 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59281
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f6f6dd741c0, tag: 0xbb06a39f133b54e0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f6f6dd741c0, tag: 0xbb06a39f133b54e0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-01 05:49:22,186 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:53807 -> ucx://127.0.0.1:59281
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f64685c6140, tag: 0xcd1f1120559f82a2, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:22,203 - distributed.nanny - WARNING - Restarting worker
2023-05-01 05:49:22,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:22,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:22,861 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:22,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:23,678 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:23,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:23,713 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:23,713 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:51,570 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59281
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 318, in connect
    raise OSError(
OSError: Timed out trying to connect to ucx://127.0.0.1:59281 after 30 s
[dgx13:80327:0:80327] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  80327) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f17f5b95dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x31fcf) [0x7f17f5b95fcf]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x32304) [0x7f17f5b96304]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f18aee79420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x18) [0x7f17f5c25838]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f17f5c5a0d8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23457) [0x7f17f5b45457]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23a38) [0x7f17f5b45a38]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x260fc) [0x7f17f5b480fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7f17f5ba14d9]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f17f5b481ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f17f5c21caa]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7f17f5ce46e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5627e695db08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5627e694e112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5627e694727a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5627e6958c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5627e694881b]
18  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5627e6958ef3]
19  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x5627e6966a16]
20  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x5627e6a769b1]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5627e6904817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5627e694ff83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5627e694dd36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5627e6958ef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5627e694881b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5627e6958ef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5627e694881b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5627e6958ef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5627e694881b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5627e6958ef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5627e694881b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5627e694727a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5627e6958c05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5627e694cfa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5627e694727a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5627e6966935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5627e6967104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5627e6a2dfc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5627e69512bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5627e694c1bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5627e6958ef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5627e6966c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5627e694c1bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5627e6958ef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5627e694881b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5627e694727a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5627e6958c05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5627e694881b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5627e6958ef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5627e6948568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5627e694727a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5627e6958c05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5627e69493cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5627e694727a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5627e6946f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5627e6946eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5627e69f78bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5627e6a25adc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5627e6a21c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5627e6a197ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5627e6a196bd]
=================================
2023-05-01 05:49:52,667 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52341 -> ucx://127.0.0.1:59971
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f6f6dd742c0, tag: 0x524efdb4a449b601, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:52,738 - distributed.nanny - WARNING - Restarting worker
[dgx13:80344:0:80344] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  80344) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f53a8303dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x31fcf) [0x7f53a8303fcf]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x32304) [0x7f53a8304304]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f54475cc420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x18) [0x7f53a8393838]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f53a83c80d8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23457) [0x7f53a82b3457]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23a38) [0x7f53a82b3a38]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x260fc) [0x7f53a82b60fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7f53a830f4d9]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f53a82b61ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f53a838fcaa]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7f53a84526e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x562f91cf7b08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x562f91ce8112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562f91ce127a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x562f91cf2c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562f91ce281b]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x562f91d0770e]
19  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f53c6e962fe]
20  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x562f91ceb2bc]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x562f91c9e817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x562f91ce9f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x562f91ce7d36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562f91cf2ef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562f91ce281b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562f91cf2ef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562f91ce281b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562f91cf2ef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562f91ce281b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562f91cf2ef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562f91ce281b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562f91ce127a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x562f91cf2c05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x562f91ce6fa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562f91ce127a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x562f91d00935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x562f91d01104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x562f91dc7fc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x562f91ceb2bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x562f91ce61bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562f91cf2ef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x562f91d00c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x562f91ce61bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562f91cf2ef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562f91ce281b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562f91ce127a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x562f91cf2c05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562f91ce281b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562f91cf2ef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x562f91ce2568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562f91ce127a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x562f91cf2c05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x562f91ce33cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562f91ce127a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x562f91ce0f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x562f91ce0eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x562f91d918bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x562f91dbfadc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x562f91dbbc24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x562f91db37ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x562f91db36bd]
=================================
[dgx13:80341:0:80341] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  80341) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f56ecb87dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x31fcf) [0x7f56ecb87fcf]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x32304) [0x7f56ecb88304]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f577de35420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x18) [0x7f56ecc17838]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f56ecc4c0d8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23457) [0x7f56ecb37457]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23a38) [0x7f56ecb37a38]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x260fc) [0x7f56ecb3a0fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7f56ecb934d9]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f56ecb3a1ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f56ecc13caa]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7f56eccd66e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x560751c93b08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x560751c84112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x560751c7d27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x560751c8ec05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x560751c7e81b]
18  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x560751c8eef3]
19  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x560751c9ca16]
20  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x560751dac9b1]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x560751c3a817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x560751c85f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x560751c83d36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x560751c8eef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x560751c7e81b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x560751c8eef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x560751c7e81b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x560751c8eef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x560751c7e81b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x560751c8eef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x560751c7e81b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x560751c7d27a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x560751c8ec05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x560751c82fa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x560751c7d27a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x560751c9c935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x560751c9d104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x560751d63fc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x560751c872bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x560751c821bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x560751c8eef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x560751c9cc72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x560751c821bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x560751c8eef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x560751c7e81b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x560751c7d27a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x560751c8ec05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x560751c7e81b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x560751c8eef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x560751c7e568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x560751c7d27a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x560751c8ec05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x560751c7f3cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x560751c7d27a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x560751c7cf07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x560751c7ceb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x560751d2d8bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x560751d5badc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x560751d57c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x560751d4f7ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x560751d4f6bd]
=================================
2023-05-01 05:49:52,934 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52341 -> ucx://127.0.0.1:56317
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f6f6dd74140, tag: 0x15230a88973bed26, nbytes: 100003520, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:52,934 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42679 -> ucx://127.0.0.1:56317
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #006] ep: 0x7faf742ae240, tag: 0xfa3083c8395553b8, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1793, in get_data
    response = await comm.read(deserializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #006] ep: 0x7faf742ae240, tag: 0xfa3083c8395553b8, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-01 05:49:52,935 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:53807 -> ucx://127.0.0.1:56317
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f64685c6100, tag: 0x3a63cf09a3a3cd27, nbytes: 100004464, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:52,953 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:53807 -> ucx://127.0.0.1:53825
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f64685c6380, tag: 0x8375aee5757b217d, nbytes: 99988080, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:52,953 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42679 -> ucx://127.0.0.1:53825
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7faf742ae340, tag: 0x41f586acf4303e9f, nbytes: 99935688, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:52,996 - distributed.nanny - WARNING - Restarting worker
2023-05-01 05:49:53,003 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,003 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,007 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52611 -> ucx://127.0.0.1:56317
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f0c799ec180, tag: 0xcc2e15a047a39267, nbytes: 99968864, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:53,054 - distributed.nanny - WARNING - Restarting worker
[dgx13:80332:0:80332] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  80332) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f9740adddec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x31fcf) [0x7f9740addfcf]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x32304) [0x7f9740ade304]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f97d1d90420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x18) [0x7f9740b6d838]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f9740ba20d8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23457) [0x7f9740a8d457]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23a38) [0x7f9740a8da38]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x260fc) [0x7f9740a900fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7f9740ae94d9]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f9740a901ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f9740b69caa]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7f9740c2c6e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55d3c316fb08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55d3c3160112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55d3c315927a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55d3c316ac05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55d3c315a81b]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55d3c317f70e]
19  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f975165e2fe]
20  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55d3c31632bc]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55d3c3116817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55d3c3161f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55d3c315fd36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55d3c316aef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55d3c315a81b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55d3c316aef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55d3c315a81b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55d3c316aef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55d3c315a81b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55d3c316aef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55d3c315a81b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55d3c315927a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55d3c316ac05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55d3c315efa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55d3c315927a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55d3c3178935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55d3c3179104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55d3c323ffc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55d3c31632bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55d3c315e1bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55d3c316aef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55d3c3178c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55d3c315e1bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55d3c316aef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55d3c315a81b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55d3c315927a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55d3c316ac05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55d3c315a81b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55d3c316aef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55d3c315a568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55d3c315927a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55d3c316ac05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55d3c315b3cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55d3c315927a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55d3c3158f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55d3c3158eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55d3c32098bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55d3c3237adc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55d3c3233c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55d3c322b7ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55d3c322b6bd]
=================================
2023-05-01 05:49:53,120 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-befd56feb5b44f4046c8d6896290c9a9', 0)
Function:  <dask.layers.CallableLazyImport object at 0x7f071c
args:      ([               key   payload
shuffle                     
0           598971  89439504
0           785429   3039871
0           737295  87449122
0           809617  43822033
0           647561  18710650
...            ...       ...
0        799891177  98013105
0        799930010  98330215
0        799993451  97031748
0        799982182   2609578
0        799989968  43360256

[12505522 rows x 2 columns],                key   payload
shuffle                     
1          1045482  52124750
1          1128600  35167400
1          1108626  66282622
1          1145351  64680751
1           295363  66042768
...            ...       ...
1        799816754  53126981
1        799917474   8565833
1        799895457  13200169
1        799768768   3221679
1        799931319  87907177

[12497568 rows x 2 columns],                key   payload
shuffle                     
2           826724  97200532
2            95454   2407252
2           988400  16192670
2           192056  22710721
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-01 05:49:53,121 - distributed.worker - ERROR - 'int' object is not subscriptable
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2902, in get_data_from_worker
    status = response["status"]
TypeError: 'int' object is not subscriptable
2023-05-01 05:49:53,130 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 830, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 987, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1794, in get_data
    assert response == "OK", response
AssertionError: {'op': 'get_data', 'keys': {"('split-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 2, 7)"}, 'who': 'ucx://127.0.0.1:52611', 'max_connections': None, 'reply': True}
2023-05-01 05:49:53,213 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42679 -> ucx://127.0.0.1:32825
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7faf742ae140, tag: 0xa723845166e6ddc1, nbytes: 99967120, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:53,250 - distributed.nanny - WARNING - Restarting worker
2023-05-01 05:49:53,332 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52341
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 359, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #060] ep: 0x7f0c799ec140, tag: 0xa7b0916358914ead, nbytes: 912, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #060] ep: 0x7f0c799ec140, tag: 0xa7b0916358914ead, nbytes: 912, type: <class 'numpy.ndarray'>>: Message truncated")
2023-05-01 05:49:53,333 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52341 -> ucx://127.0.0.1:32825
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f6f6dd742c0, tag: 0x6c206b0c4b5cddb6, nbytes: 100007144, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-01 05:49:53,334 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52341 -> ucx://127.0.0.1:52611
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 315, in write
    await self.ep.send(struct.pack("?Q", False, nframes))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 629, in send
    self._ep.raise_on_error()
  File "ucp/_libs/ucx_endpoint.pyx", line 353, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXConnectionReset: Endpoint 0x7f6f6dd74240 error: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
Task exception was never retrieved
future: <Task finished name='Task-8981' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-05-01 05:49:53,408 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-4d32fb6c640c69ea76934d2a181df14b', 1)
Function:  subgraph_callable-5636c7b5-1d70-480e-a955-78dbe98a
args:      (               key   payload
shuffle                     
0           769068  64731840
0           698284  23855235
0           738819  93944543
0           788733  29586698
0           662448  29037948
...            ...       ...
7        799977388  79020643
7        799970919  12441993
7        799929083  28193918
7        799926956  34099619
7        799970528  24784232

[100005187 rows x 2 columns],                  key   payload
116931     857118165  93004152
116936     810477296  68038258
116943     107165162  32354325
116944     204488700  41530072
116953     823183242   6764266
...              ...       ...
99980033  1548460555  85143957
99980038  1525980811   5809217
99980044  1550897900  49419230
99980050  1543266768  14352921
99980101   193669540  31201175

[99996328 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-01 05:49:53,449 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-befd56feb5b44f4046c8d6896290c9a9', 4)
Function:  <dask.layers.CallableLazyImport object at 0x7f071c
args:      ([               key   payload
shuffle                     
0           653607  26957669
0           718190   6153843
0           736832  88644388
0           900745  53680786
0           655254  52506877
...            ...       ...
0        799757359  39226369
0        799760017  42839794
0        799790798  89159293
0        799791634  64862265
0        799744167  12189111

[12503392 rows x 2 columns],                key   payload
shuffle                     
1          1029538  33815762
1          1149441   8121880
1          1123565  67264962
1          1168431  11985419
1          1028391  63573196
...            ...       ...
1        799925072   3119750
1        799791292  71925467
1        799878529  16827991
1        799835840  31475889
1        799792800  22131879

[12500389 rows x 2 columns],                key   payload
shuffle                     
2           940217   6366463
2           160478  39577095
2           978498  18220769
2           201749  75468317
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-01 05:49:53,486 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,486 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,513 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,513 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,586 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,587 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,678 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7f5f09
args:      ([                key   payload
116929    311067274  31186314
116934    401324542  34149300
116942    102626363  24603926
116956    508635911  53653638
11940     811823217  18752101
...             ...       ...
99993084  853285074  94598415
99992844  706129319  94373768
99992851  821917681  49570819
99992862  842740784   6553431
99992982  838556550  96878336

[12498923 rows x 2 columns],                 key   payload
84164     920095803  29581572
11340     953683789  46491149
84166     929650345  81153971
11347     958066256  66031875
84169     929626428  79291337
...             ...       ...
99997679  520254441  62525717
99997680  959828919    881121
99997686  620399963  62402143
99997687  918300642  87524808
99997694  416316066  20919932

[12501128 rows x 2 columns],                  key   payload
31754     1028261611  45376742
31756     1014330093  61944960
31757      326493387  67147294
31766     1025501185  77670255
62017     1033880481   3867679
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-01 05:49:53,698 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7f6a17
args:      ([                key   payload
116930    210022120  15784618
11937     849259566  50854102
182       512439025   9287220
11956     835682267  97602932
11466     844248299  84330162
...             ...       ...
99992976  836470685  52671800
99992978  858430697  40313575
99992980    1749966  11050951
99992983  603386561  29895783
99992984  869190469  29270732

[12498632 rows x 2 columns],                 key   payload
84161     969035181  15173494
11328     519442638  85833678
84163     930142600  52357155
11333     954834090  18856541
84168     923486765   5163788
...             ...       ...
99997684  324401839  12705498
99997689  944553536  85468445
99997690  907972802  42924399
99997691  953129856  16734150
99997695  905227189  69756680

[12502237 rows x 2 columns],                  key   payload
31749     1000691494  18667423
62030     1021208894   9336513
62035     1029355579  36046697
34967      735945406  31989100
62041     1034160056   8252898
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-01 05:49:53,740 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,741 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-01 05:49:53,755 - distributed.core - ERROR - not enough values to unpack (expected 2, got 0)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 376, in read
    cuda_recv_frames, recv_frames = zip(
ValueError: not enough values to unpack (expected 2, got 0)
2023-05-01 05:49:53,755 - distributed.worker - ERROR - not enough values to unpack (expected 2, got 0)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 376, in read
    cuda_recv_frames, recv_frames = zip(
ValueError: not enough values to unpack (expected 2, got 0)
2023-05-01 05:49:53,760 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52611
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 359, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #095] ep: 0x7f6f6dd74100, tag: 0xc9799d63bd0700d, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #095] ep: 0x7f6f6dd74100, tag: 0xc9799d63bd0700d, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated")
2023-05-01 05:49:53,903 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-befd56feb5b44f4046c8d6896290c9a9', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7f5f09
args:      ([               key   payload
shuffle                     
0           705173  27547860
0           781477  54045845
0           581946   8537000
0           864841  98617512
0           652149  58376848
...            ...       ...
0        799963227  63285417
0        799917138  51639452
0        799942666  86587818
0        799981234  54223371
0        799989961  25837424

[12498811 rows x 2 columns],                key   payload
shuffle                     
1           202684  27944618
1           150889  44875109
1           209758  78227655
1           167861   8414413
1            49507  94017366
...            ...       ...
1        799783647  97190171
1        799770742  40738921
1        799931275  40573497
1        799929769  39624431
1        799856250  80935245

[12498510 rows x 2 columns],                key   payload
shuffle                     
2           937709  56747093
2           106184  48273303
2           953067  55864726
2           202758  92040695
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-01 05:49:54,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:54,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:54,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:54,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:54,649 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:54,649 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-01 05:49:54,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-01 05:49:54,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
