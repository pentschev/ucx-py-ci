============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.2, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-06-23 05:42:43,149 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:42:43,154 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39449 instead
  warnings.warn(
2023-06-23 05:42:43,158 - distributed.scheduler - INFO - State start
2023-06-23 05:42:43,180 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:42:43,181 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:42:43,181 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:42:43,182 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:42:43,299 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35317'
2023-06-23 05:42:43,320 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40843'
2023-06-23 05:42:43,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33619'
2023-06-23 05:42:43,331 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42649'
2023-06-23 05:42:44,975 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:44,975 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:44,978 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:44,978 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:44,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:44,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:44,982 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:44,986 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:44,986 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:44,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:44,987 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:44,994 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-06-23 05:42:45,011 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37847
2023-06-23 05:42:45,011 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37847
2023-06-23 05:42:45,012 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34115
2023-06-23 05:42:45,012 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:42:45,012 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:45,012 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:42:45,012 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:42:45,012 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w_24wimx
2023-06-23 05:42:45,012 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08facb0b-db12-49ea-bec0-95d2898db5f7
2023-06-23 05:42:45,012 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac601881-9206-42e0-93f4-4a5807e1c605
2023-06-23 05:42:45,013 - distributed.worker - INFO - Starting Worker plugin PreImport-eae796a9-a404-4f26-bcc3-df9f971d5bcb
2023-06-23 05:42:45,013 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:45,029 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:42:45,029 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:45,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:42:46,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35563
2023-06-23 05:42:46,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35563
2023-06-23 05:42:46,585 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35697
2023-06-23 05:42:46,585 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:42:46,585 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,585 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:42:46,585 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:42:46,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_uue_sv2
2023-06-23 05:42:46,586 - distributed.worker - INFO - Starting Worker plugin PreImport-da692492-ab03-4cf0-8b1d-45e2917549bf
2023-06-23 05:42:46,586 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3673410-e3d2-4b0c-86da-bde0f541f8e9
2023-06-23 05:42:46,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1be60330-a44c-4c0c-8ee1-a7d8c2fdd1a3
2023-06-23 05:42:46,586 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,590 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40777
2023-06-23 05:42:46,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40777
2023-06-23 05:42:46,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41425
2023-06-23 05:42:46,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:42:46,590 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,590 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:42:46,591 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:42:46,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y5xwfekc
2023-06-23 05:42:46,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c062090-ca80-4dfa-908d-ddaaa97f7172
2023-06-23 05:42:46,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-844ef3d1-e307-4ca0-9c95-b963ad8b3d91
2023-06-23 05:42:46,591 - distributed.worker - INFO - Starting Worker plugin PreImport-6268bb32-add7-4400-b934-ea9c07c5e1fe
2023-06-23 05:42:46,591 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,606 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:42:46,606 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:42:46,610 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:42:46,610 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:42:46,617 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45921
2023-06-23 05:42:46,617 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45921
2023-06-23 05:42:46,617 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43537
2023-06-23 05:42:46,617 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:42:46,617 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,617 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:42:46,617 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:42:46,617 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tatr43g9
2023-06-23 05:42:46,617 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b030329f-be2c-4e67-b850-8dfd3899ea38
2023-06-23 05:42:46,618 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5509aadf-c5fc-4e61-8e66-05334dbd8261
2023-06-23 05:42:46,618 - distributed.worker - INFO - Starting Worker plugin PreImport-5b9492aa-b71c-4ea7-94d5-480d7c5de0d4
2023-06-23 05:42:46,622 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,647 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:42:46,648 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:42:46,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:42:51,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40843'. Reason: nanny-close
2023-06-23 05:42:51,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:42:51,532 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35317'. Reason: nanny-close
2023-06-23 05:42:51,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:42:51,533 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33619'. Reason: nanny-close
2023-06-23 05:42:51,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40777. Reason: nanny-close
2023-06-23 05:42:51,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:42:51,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42649'. Reason: nanny-close
2023-06-23 05:42:51,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:42:51,534 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45921. Reason: nanny-close
2023-06-23 05:42:51,534 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35563. Reason: nanny-close
2023-06-23 05:42:51,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37847. Reason: nanny-close
2023-06-23 05:42:51,536 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:42:51,536 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:42:51,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:42:51,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:42:51,537 - distributed.nanny - INFO - Worker closed
2023-06-23 05:42:51,537 - distributed.nanny - INFO - Worker closed
2023-06-23 05:42:51,539 - distributed.nanny - INFO - Worker closed
2023-06-23 05:42:51,539 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-06-23 05:42:54,669 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:42:54,673 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41053 instead
  warnings.warn(
2023-06-23 05:42:54,677 - distributed.scheduler - INFO - State start
2023-06-23 05:42:54,697 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:42:54,698 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:42:54,699 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41053/status
2023-06-23 05:42:54,751 - distributed.scheduler - INFO - Receive client connection: Client-cbf3d2dd-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:42:54,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47740
2023-06-23 05:42:54,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45363'
2023-06-23 05:42:54,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40859'
2023-06-23 05:42:54,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40543'
2023-06-23 05:42:54,968 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45303'
2023-06-23 05:42:54,977 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38775'
2023-06-23 05:42:54,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33607'
2023-06-23 05:42:54,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34023'
2023-06-23 05:42:55,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38019'
2023-06-23 05:42:56,411 - distributed.scheduler - INFO - Receive client connection: Client-cc003f67-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:42:56,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47916
2023-06-23 05:42:56,692 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,692 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,705 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,760 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,760 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,777 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,777 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:42:56,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:42:56,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,006 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,071 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,081 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,084 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,088 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:42:57,126 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:04,443 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42775
2023-06-23 05:43:04,443 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42775
2023-06-23 05:43:04,443 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43101
2023-06-23 05:43:04,443 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,443 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,443 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,443 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,443 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-quyjd03u
2023-06-23 05:43:04,444 - distributed.worker - INFO - Starting Worker plugin PreImport-033fd1bc-36a0-446f-89da-9972c9d503f3
2023-06-23 05:43:04,444 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3ecbf92-ab62-4270-a6c0-ef99c268627c
2023-06-23 05:43:04,444 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e13fdeee-3fbc-426c-b798-85b946797351
2023-06-23 05:43:04,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44697
2023-06-23 05:43:04,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44697
2023-06-23 05:43:04,506 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38423
2023-06-23 05:43:04,506 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,506 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,506 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,506 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,507 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gyohtt5j
2023-06-23 05:43:04,507 - distributed.worker - INFO - Starting Worker plugin PreImport-b2d0eb86-b7bd-41f1-b85f-d60a65c8d32b
2023-06-23 05:43:04,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a8afc6b-7a38-44f2-bb04-89729b5af1f7
2023-06-23 05:43:04,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42730f56-c079-41d2-85a3-26b877958f3a
2023-06-23 05:43:04,552 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39691
2023-06-23 05:43:04,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39691
2023-06-23 05:43:04,553 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36165
2023-06-23 05:43:04,553 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,553 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,553 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,553 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,553 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e_nm6gjb
2023-06-23 05:43:04,554 - distributed.worker - INFO - Starting Worker plugin PreImport-880cadf3-7712-42aa-9c25-963136d76729
2023-06-23 05:43:04,554 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c3496ae-ffc4-488d-923f-39fdd0447988
2023-06-23 05:43:04,558 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2abf8fd-9b9b-4aa7-acf5-02ff140d0c22
2023-06-23 05:43:04,584 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41331
2023-06-23 05:43:04,584 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41331
2023-06-23 05:43:04,584 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43235
2023-06-23 05:43:04,584 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,584 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,584 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,584 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,584 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1izlzle0
2023-06-23 05:43:04,585 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b539ed3e-9a4b-4151-a7cf-b1561c537d26
2023-06-23 05:43:04,585 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6659981-4d12-43b3-a290-2ba2e8b031f5
2023-06-23 05:43:04,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35139
2023-06-23 05:43:04,593 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35139
2023-06-23 05:43:04,593 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41121
2023-06-23 05:43:04,593 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,593 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,593 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,593 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uccnkdjl
2023-06-23 05:43:04,593 - distributed.worker - INFO - Starting Worker plugin PreImport-b835177e-845c-48b6-96ec-4acb487ba551
2023-06-23 05:43:04,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08a0131d-63d2-4034-9c19-6ff2717bbb2c
2023-06-23 05:43:04,595 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32837
2023-06-23 05:43:04,596 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32837
2023-06-23 05:43:04,596 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36467
2023-06-23 05:43:04,596 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,596 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,596 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,596 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1q_056mb
2023-06-23 05:43:04,597 - distributed.worker - INFO - Starting Worker plugin PreImport-6ba6f30e-0c32-4b97-9db7-bc94784d7851
2023-06-23 05:43:04,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-922f3d06-56b6-460b-9740-8363f56b4a17
2023-06-23 05:43:04,603 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42559
2023-06-23 05:43:04,603 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42559
2023-06-23 05:43:04,603 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33017
2023-06-23 05:43:04,603 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,603 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,603 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,603 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,604 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yky4r8u7
2023-06-23 05:43:04,604 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6d7df75-4d24-4707-b237-f45eb3893260
2023-06-23 05:43:04,605 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2f30518-4bde-457c-9810-5e407d366f11
2023-06-23 05:43:04,608 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43919
2023-06-23 05:43:04,609 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43919
2023-06-23 05:43:04,609 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39729
2023-06-23 05:43:04,609 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,609 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,609 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:04,609 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:04,609 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yhi3iknx
2023-06-23 05:43:04,610 - distributed.worker - INFO - Starting Worker plugin PreImport-61637d11-9779-487b-addf-9e265bb3ace3
2023-06-23 05:43:04,611 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46c56cfa-d415-445d-a837-f9d8be15094d
2023-06-23 05:43:04,614 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1146c08-a4f3-4143-91a0-267c947d5f0a
2023-06-23 05:43:04,794 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,810 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36075', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,811 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36075
2023-06-23 05:43:04,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46706
2023-06-23 05:43:04,827 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43469', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,828 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43469
2023-06-23 05:43:04,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46720
2023-06-23 05:43:04,833 - distributed.worker - INFO - Starting Worker plugin PreImport-4c947d84-16bc-4357-92f4-180453c0a9cc
2023-06-23 05:43:04,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,835 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42775', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,835 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42775
2023-06-23 05:43:04,835 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46712
2023-06-23 05:43:04,836 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,836 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,837 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46561', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,837 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46561
2023-06-23 05:43:04,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46730
2023-06-23 05:43:04,838 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-490f76cf-ea03-44c1-8f3b-79d20788bd63
2023-06-23 05:43:04,839 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,843 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,859 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c00117f3-713c-459f-84f0-c8acee5009e2
2023-06-23 05:43:04,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,861 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,866 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34727', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,866 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34727
2023-06-23 05:43:04,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46736
2023-06-23 05:43:04,868 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44697', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,869 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44697
2023-06-23 05:43:04,869 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46760
2023-06-23 05:43:04,870 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,870 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,870 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42559', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,871 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42559
2023-06-23 05:43:04,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46752
2023-06-23 05:43:04,871 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,872 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,872 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35139', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,872 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35139
2023-06-23 05:43:04,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46766
2023-06-23 05:43:04,873 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,873 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,876 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,886 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39691', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,887 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39691
2023-06-23 05:43:04,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46776
2023-06-23 05:43:04,888 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,888 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,888 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34357', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,888 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34357
2023-06-23 05:43:04,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46794
2023-06-23 05:43:04,892 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43919', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,893 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43919
2023-06-23 05:43:04,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46814
2023-06-23 05:43:04,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,893 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,894 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38861', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,897 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38861
2023-06-23 05:43:04,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46778
2023-06-23 05:43:04,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32837', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32837
2023-06-23 05:43:04,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46800
2023-06-23 05:43:04,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:04,900 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:04,900 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:04,901 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44633', status: init, memory: 0, processing: 0>
2023-06-23 05:43:04,902 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44633
2023-06-23 05:43:04,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46826
2023-06-23 05:43:04,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-23 05:43:05,039 - distributed.worker - INFO - Starting Worker plugin PreImport-6d73ae11-51a1-45a9-b37b-6107734b0e98
2023-06-23 05:43:05,040 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41331. Reason: worker-close
2023-06-23 05:43:05,040 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:43:05,043 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:05,072 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:05,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40859'. Reason: nanny-instantiate-failed
2023-06-23 05:43:05,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:43:05,495 - distributed.nanny - INFO - Worker process 52573 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:43:05,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52594 parent=52403 started daemon>
2023-06-23 05:43:05,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52591 parent=52403 started daemon>
2023-06-23 05:43:05,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52588 parent=52403 started daemon>
2023-06-23 05:43:05,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52585 parent=52403 started daemon>
2023-06-23 05:43:05,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52582 parent=52403 started daemon>
2023-06-23 05:43:05,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52577 parent=52403 started daemon>
2023-06-23 05:43:05,502 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52570 parent=52403 started daemon>
2023-06-23 05:43:05,508 - distributed.core - INFO - Connection to tcp://127.0.0.1:46720 has been closed.
2023-06-23 05:43:05,509 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43469', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,509 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43469
2023-06-23 05:43:05,518 - distributed.core - INFO - Connection to tcp://127.0.0.1:46778 has been closed.
2023-06-23 05:43:05,518 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38861', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,518 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38861
2023-06-23 05:43:05,518 - distributed.core - INFO - Connection to tcp://127.0.0.1:46736 has been closed.
2023-06-23 05:43:05,519 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34727', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,519 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34727
2023-06-23 05:43:05,520 - distributed.core - INFO - Connection to tcp://127.0.0.1:46730 has been closed.
2023-06-23 05:43:05,520 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46561', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,521 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46561
2023-06-23 05:43:05,521 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46730>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46730>: Stream is closed
2023-06-23 05:43:05,524 - distributed.core - INFO - Connection to tcp://127.0.0.1:46706 has been closed.
2023-06-23 05:43:05,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36075', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,524 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36075
2023-06-23 05:43:05,525 - distributed.core - INFO - Connection to tcp://127.0.0.1:46826 has been closed.
2023-06-23 05:43:05,525 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44633', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,525 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44633
2023-06-23 05:43:05,525 - distributed.core - INFO - Connection to tcp://127.0.0.1:46794 has been closed.
2023-06-23 05:43:05,526 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34357', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,526 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34357
2023-06-23 05:43:05,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:46766 has been closed.
2023-06-23 05:43:05,561 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35139', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,561 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35139
2023-06-23 05:43:05,562 - distributed.core - INFO - Connection to tcp://127.0.0.1:46752 has been closed.
2023-06-23 05:43:05,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42559', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,563 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42559
2023-06-23 05:43:05,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:46800 has been closed.
2023-06-23 05:43:05,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32837', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,563 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32837
2023-06-23 05:43:05,565 - distributed.core - INFO - Connection to tcp://127.0.0.1:46814 has been closed.
2023-06-23 05:43:05,565 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43919', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,565 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43919
2023-06-23 05:43:05,566 - distributed.core - INFO - Connection to tcp://127.0.0.1:46712 has been closed.
2023-06-23 05:43:05,566 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42775', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,566 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42775
2023-06-23 05:43:05,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:46760 has been closed.
2023-06-23 05:43:05,567 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44697', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,567 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44697
2023-06-23 05:43:05,568 - distributed.core - INFO - Connection to tcp://127.0.0.1:46776 has been closed.
2023-06-23 05:43:05,568 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39691', status: running, memory: 0, processing: 0>
2023-06-23 05:43:05,568 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39691
2023-06-23 05:43:05,568 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:43:06,486 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52570 exit status was already read will report exitcode 255
2023-06-23 05:43:06,550 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52585 exit status was already read will report exitcode 255
2023-06-23 05:43:06,575 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52591 exit status was already read will report exitcode 255
2023-06-23 05:43:06,609 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 52594 exit status was already read will report exitcode 255
2023-06-23 05:43:10,801 - distributed.scheduler - INFO - Remove client Client-cbf3d2dd-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:10,801 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47740; closing.
2023-06-23 05:43:10,802 - distributed.scheduler - INFO - Remove client Client-cbf3d2dd-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:10,802 - distributed.scheduler - INFO - Close client connection: Client-cbf3d2dd-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:10,803 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:43:10,803 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:43:10,804 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:43:10,806 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:43:10,806 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-06-23 05:43:12,971 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:12,976 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45721 instead
  warnings.warn(
2023-06-23 05:43:12,980 - distributed.scheduler - INFO - State start
2023-06-23 05:43:13,001 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:13,002 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:43:13,002 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45721/status
2023-06-23 05:43:13,255 - distributed.scheduler - INFO - Receive client connection: Client-d6cc36bb-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:13,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54908
2023-06-23 05:43:13,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45733'
2023-06-23 05:43:13,295 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45897'
2023-06-23 05:43:13,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46271'
2023-06-23 05:43:13,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44595'
2023-06-23 05:43:13,329 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41967'
2023-06-23 05:43:13,334 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39925'
2023-06-23 05:43:13,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38857'
2023-06-23 05:43:13,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33903'
2023-06-23 05:43:15,060 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e_nm6gjb', purging
2023-06-23 05:43:15,060 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yky4r8u7', purging
2023-06-23 05:43:15,060 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-quyjd03u', purging
2023-06-23 05:43:15,061 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yhi3iknx', purging
2023-06-23 05:43:15,061 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1q_056mb', purging
2023-06-23 05:43:15,061 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gyohtt5j', purging
2023-06-23 05:43:15,062 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uccnkdjl', purging
2023-06-23 05:43:15,062 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:15,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:15,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:15,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:15,089 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:15,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:15,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:15,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:15,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:15,109 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:15,109 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:15,112 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:15,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:15,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:15,151 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:15,153 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:15,153 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:15,153 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:15,159 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:15,160 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:15,170 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:15,171 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:15,236 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:15,236 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:16,301 - distributed.scheduler - INFO - Receive client connection: Client-d9d5babf-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:16,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55008
2023-06-23 05:43:18,633 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33663
2023-06-23 05:43:18,633 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33663
2023-06-23 05:43:18,633 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37393
2023-06-23 05:43:18,633 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:18,633 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:18,633 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:18,634 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:18,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xttdmcnx
2023-06-23 05:43:18,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5621570f-2e7a-4c4d-a61b-17eb9a6be75c
2023-06-23 05:43:18,634 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bfc6554d-e7c2-47d7-9e6e-e8599640f927
2023-06-23 05:43:18,640 - distributed.worker - INFO - Starting Worker plugin PreImport-9b96b0a3-87d3-443e-9118-46bf0b55ff9c
2023-06-23 05:43:18,640 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:18,670 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33663', status: init, memory: 0, processing: 0>
2023-06-23 05:43:18,672 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33663
2023-06-23 05:43:18,672 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55094
2023-06-23 05:43:18,673 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:18,673 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:18,675 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:19,364 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33255
2023-06-23 05:43:19,365 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33255
2023-06-23 05:43:19,365 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41727
2023-06-23 05:43:19,365 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,365 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,365 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:19,365 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:19,365 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tdpuc_ck
2023-06-23 05:43:19,365 - distributed.worker - INFO - Starting Worker plugin PreImport-d7fb1d29-0e7b-4dab-b4b7-6ffb79243661
2023-06-23 05:43:19,365 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc5cca5e-2b89-4107-bf24-58749a347980
2023-06-23 05:43:19,365 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39409
2023-06-23 05:43:19,366 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39409
2023-06-23 05:43:19,366 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35767
2023-06-23 05:43:19,366 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,366 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,366 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:19,366 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:19,366 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qi85rsvx
2023-06-23 05:43:19,366 - distributed.worker - INFO - Starting Worker plugin PreImport-a1dfca4e-6dc4-41ae-bb28-9dd307f258e6
2023-06-23 05:43:19,367 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4748974a-dfd6-405e-a391-9b63f526c841
2023-06-23 05:43:19,388 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43337
2023-06-23 05:43:19,388 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43337
2023-06-23 05:43:19,388 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46241
2023-06-23 05:43:19,388 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40225
2023-06-23 05:43:19,388 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,388 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,388 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40225
2023-06-23 05:43:19,388 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:19,388 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:19,388 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36437
2023-06-23 05:43:19,388 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7y8qzo37
2023-06-23 05:43:19,388 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,388 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,388 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:19,389 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:19,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bxfn6hkq
2023-06-23 05:43:19,389 - distributed.worker - INFO - Starting Worker plugin PreImport-47f8d891-e1ed-4e58-b317-1b79fc411e17
2023-06-23 05:43:19,389 - distributed.worker - INFO - Starting Worker plugin RMMSetup-716c6d98-2f82-4315-ac01-f5c0349090b5
2023-06-23 05:43:19,389 - distributed.worker - INFO - Starting Worker plugin PreImport-1568e42e-fc44-4e02-97a2-339cd2558cf5
2023-06-23 05:43:19,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-41c35fb9-9ca5-419f-9150-188495106770
2023-06-23 05:43:19,415 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41049
2023-06-23 05:43:19,415 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41049
2023-06-23 05:43:19,415 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36569
2023-06-23 05:43:19,415 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36847
2023-06-23 05:43:19,415 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36569
2023-06-23 05:43:19,415 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,415 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,415 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43773
2023-06-23 05:43:19,415 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:19,415 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,415 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,415 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:19,415 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:19,415 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k8zxulmq
2023-06-23 05:43:19,415 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:19,415 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ntnk_9q3
2023-06-23 05:43:19,416 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-927b5cbd-b5b3-43f5-9778-a198c6b3b7a2
2023-06-23 05:43:19,416 - distributed.worker - INFO - Starting Worker plugin PreImport-a8c35881-2629-4f1b-a6c0-047149a5552c
2023-06-23 05:43:19,416 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e37eb0f-9ba4-45e3-9c27-a96e13c3debb
2023-06-23 05:43:19,416 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30182e01-d538-4b08-9832-9fb8fafafce2
2023-06-23 05:43:19,416 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33313
2023-06-23 05:43:19,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33313
2023-06-23 05:43:19,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35473
2023-06-23 05:43:19,417 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,417 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,417 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:19,417 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:19,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p7zmn95k
2023-06-23 05:43:19,418 - distributed.worker - INFO - Starting Worker plugin PreImport-1811c393-dc8f-4dc9-b9aa-4345bf9c9054
2023-06-23 05:43:19,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a84a5193-3bf8-4587-a58f-f45a2dc03525
2023-06-23 05:43:19,445 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c379f70-1ad8-4c47-95cd-e25e17a3ff2f
2023-06-23 05:43:19,445 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a934871-ae44-4a21-9b69-7dd599e7f376
2023-06-23 05:43:19,445 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,445 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,445 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14a46e67-5ee3-4f34-9b37-9e343c1d1680
2023-06-23 05:43:19,447 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb465c84-5afd-4dc6-839b-50d640609433
2023-06-23 05:43:19,447 - distributed.worker - INFO - Starting Worker plugin PreImport-25d3b61d-0b13-46e9-bff2-7d711ef75b2d
2023-06-23 05:43:19,447 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-123ad95e-87fe-4903-bb41-b69bbb6ee0bf
2023-06-23 05:43:19,447 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,447 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,448 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,448 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cae8ab13-b219-474a-a0b8-c67d13c85df9
2023-06-23 05:43:19,448 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,446 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,478 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43337', status: init, memory: 0, processing: 0>
2023-06-23 05:43:19,479 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43337
2023-06-23 05:43:19,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55110
2023-06-23 05:43:19,480 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,480 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39409', status: init, memory: 0, processing: 0>
2023-06-23 05:43:19,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39409
2023-06-23 05:43:19,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:19,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55118
2023-06-23 05:43:19,482 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,483 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:19,485 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33313', status: init, memory: 0, processing: 0>
2023-06-23 05:43:19,486 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33313
2023-06-23 05:43:19,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55132
2023-06-23 05:43:19,486 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,486 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,487 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41049', status: init, memory: 0, processing: 0>
2023-06-23 05:43:19,487 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41049
2023-06-23 05:43:19,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55120
2023-06-23 05:43:19,488 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40225', status: init, memory: 0, processing: 0>
2023-06-23 05:43:19,488 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,488 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,489 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40225
2023-06-23 05:43:19,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55136
2023-06-23 05:43:19,489 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,490 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,490 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33255', status: init, memory: 0, processing: 0>
2023-06-23 05:43:19,490 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:19,491 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33255
2023-06-23 05:43:19,491 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55116
2023-06-23 05:43:19,491 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,492 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:19,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:19,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:19,500 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36569', status: init, memory: 0, processing: 0>
2023-06-23 05:43:19,500 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36569
2023-06-23 05:43:19,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55134
2023-06-23 05:43:19,501 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:19,501 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:19,505 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:19,611 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,611 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,611 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,611 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,613 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,614 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,615 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,615 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,615 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,617 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,618 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:43:19,621 - distributed.scheduler - INFO - Remove client Client-d9d5babf-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:19,621 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55008; closing.
2023-06-23 05:43:19,621 - distributed.scheduler - INFO - Remove client Client-d9d5babf-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:19,622 - distributed.scheduler - INFO - Close client connection: Client-d9d5babf-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:19,622 - distributed.scheduler - INFO - Remove client Client-d6cc36bb-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:19,622 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54908; closing.
2023-06-23 05:43:19,622 - distributed.scheduler - INFO - Remove client Client-d6cc36bb-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:19,623 - distributed.scheduler - INFO - Close client connection: Client-d6cc36bb-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:19,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45733'. Reason: nanny-close
2023-06-23 05:43:19,624 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:19,625 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45897'. Reason: nanny-close
2023-06-23 05:43:19,625 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:19,625 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44595'. Reason: nanny-close
2023-06-23 05:43:19,625 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33663. Reason: nanny-close
2023-06-23 05:43:19,626 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:19,626 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41967'. Reason: nanny-close
2023-06-23 05:43:19,626 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40225. Reason: nanny-close
2023-06-23 05:43:19,626 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:19,627 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39925'. Reason: nanny-close
2023-06-23 05:43:19,627 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:19,627 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41049. Reason: nanny-close
2023-06-23 05:43:19,627 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46271'. Reason: nanny-close
2023-06-23 05:43:19,627 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33313. Reason: nanny-close
2023-06-23 05:43:19,627 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:19,628 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38857'. Reason: nanny-close
2023-06-23 05:43:19,628 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39409. Reason: nanny-close
2023-06-23 05:43:19,628 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:19,628 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55094; closing.
2023-06-23 05:43:19,628 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:19,628 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33903'. Reason: nanny-close
2023-06-23 05:43:19,628 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33663', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:19,628 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43337. Reason: nanny-close
2023-06-23 05:43:19,629 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33663
2023-06-23 05:43:19,629 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:19,629 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:19,629 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36569. Reason: nanny-close
2023-06-23 05:43:19,629 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:19,630 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:19,630 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:19,630 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:19,630 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55136; closing.
2023-06-23 05:43:19,630 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:19,630 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33663
2023-06-23 05:43:19,630 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33663
2023-06-23 05:43:19,631 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:19,631 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40225', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:19,631 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:19,631 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:19,631 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:19,631 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40225
2023-06-23 05:43:19,631 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33255. Reason: nanny-close
2023-06-23 05:43:19,631 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55120; closing.
2023-06-23 05:43:19,631 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33663
2023-06-23 05:43:19,632 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55132; closing.
2023-06-23 05:43:19,632 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:19,632 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41049', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:19,632 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:19,632 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41049
2023-06-23 05:43:19,632 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33313', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:19,632 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33313
2023-06-23 05:43:19,633 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55118; closing.
2023-06-23 05:43:19,633 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55110; closing.
2023-06-23 05:43:19,633 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:19,633 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:19,633 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39409', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:19,634 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39409
2023-06-23 05:43:19,634 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43337', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:19,634 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43337
2023-06-23 05:43:19,634 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55134; closing.
2023-06-23 05:43:19,635 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:19,635 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36569', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:19,635 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36569
2023-06-23 05:43:19,635 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55116; closing.
2023-06-23 05:43:19,636 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33255', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:19,636 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33255
2023-06-23 05:43:19,636 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:43:21,593 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:43:21,594 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:43:21,594 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:43:21,596 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:43:21,597 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-06-23 05:43:23,900 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:23,905 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43873 instead
  warnings.warn(
2023-06-23 05:43:23,909 - distributed.scheduler - INFO - State start
2023-06-23 05:43:24,063 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:24,064 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:43:24,065 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43873/status
2023-06-23 05:43:24,075 - distributed.scheduler - INFO - Receive client connection: Client-dd49db29-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:24,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56826
2023-06-23 05:43:24,286 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33591'
2023-06-23 05:43:24,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41569'
2023-06-23 05:43:24,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45557'
2023-06-23 05:43:24,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39693'
2023-06-23 05:43:24,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40829'
2023-06-23 05:43:24,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38487'
2023-06-23 05:43:24,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40297'
2023-06-23 05:43:24,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33427'
2023-06-23 05:43:25,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42455', status: init, memory: 0, processing: 0>
2023-06-23 05:43:25,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42455
2023-06-23 05:43:25,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56908
2023-06-23 05:43:25,274 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45491', status: init, memory: 0, processing: 0>
2023-06-23 05:43:25,274 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45491
2023-06-23 05:43:25,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56924
2023-06-23 05:43:25,279 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42565', status: init, memory: 0, processing: 0>
2023-06-23 05:43:25,279 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42565
2023-06-23 05:43:25,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56940
2023-06-23 05:43:25,280 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33011', status: init, memory: 0, processing: 0>
2023-06-23 05:43:25,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33011
2023-06-23 05:43:25,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56950
2023-06-23 05:43:25,283 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41995', status: init, memory: 0, processing: 0>
2023-06-23 05:43:25,284 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41995
2023-06-23 05:43:25,284 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56936
2023-06-23 05:43:25,287 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42719', status: init, memory: 0, processing: 0>
2023-06-23 05:43:25,288 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42719
2023-06-23 05:43:25,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56980
2023-06-23 05:43:25,290 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36311', status: init, memory: 0, processing: 0>
2023-06-23 05:43:25,290 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36311
2023-06-23 05:43:25,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56968
2023-06-23 05:43:25,291 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38303', status: init, memory: 0, processing: 0>
2023-06-23 05:43:25,292 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38303
2023-06-23 05:43:25,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56958
2023-06-23 05:43:25,326 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56908; closing.
2023-06-23 05:43:25,328 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42455', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:25,328 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42455
2023-06-23 05:43:25,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56980; closing.
2023-06-23 05:43:25,330 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56924; closing.
2023-06-23 05:43:25,330 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56968; closing.
2023-06-23 05:43:25,332 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42719', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:25,332 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42719
2023-06-23 05:43:25,332 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45491', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:25,332 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45491
2023-06-23 05:43:25,333 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36311', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:25,333 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36311
2023-06-23 05:43:25,333 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56936; closing.
2023-06-23 05:43:25,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56950; closing.
2023-06-23 05:43:25,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56958; closing.
2023-06-23 05:43:25,334 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:42455 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:47726 remote=tcp://127.0.0.1:42455>: Stream is closed
2023-06-23 05:43:25,336 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41995', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:25,336 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41995
2023-06-23 05:43:25,336 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33011', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:25,336 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33011
2023-06-23 05:43:25,336 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38303', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:25,337 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38303
2023-06-23 05:43:25,337 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:33011 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:34032 remote=tcp://127.0.0.1:33011>: Stream is closed
2023-06-23 05:43:25,337 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:36311 failed: CommClosedError: 
2023-06-23 05:43:25,337 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:38303 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:43558 remote=tcp://127.0.0.1:38303>: Stream is closed
2023-06-23 05:43:25,337 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:41995 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:33792 remote=tcp://127.0.0.1:41995>: Stream is closed
2023-06-23 05:43:25,338 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:42565 failed: CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://127.0.0.1:50812 remote=tcp://127.0.0.1:42565>: Stream is closed
2023-06-23 05:43:25,338 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:42719 failed: CommClosedError: 
2023-06-23 05:43:25,339 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56940; closing.
2023-06-23 05:43:25,339 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42565', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:25,340 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42565
2023-06-23 05:43:25,340 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:43:25,340 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:45491 failed: OSError: Timed out during handshake while connecting to tcp://127.0.0.1:45491 after 30 s
2023-06-23 05:43:25,344 - distributed.scheduler - INFO - Remove client Client-dd49db29-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:25,344 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56826; closing.
2023-06-23 05:43:25,344 - distributed.scheduler - INFO - Remove client Client-dd49db29-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:25,344 - distributed.scheduler - INFO - Close client connection: Client-dd49db29-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:25,346 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39693'. Reason: nanny-close
2023-06-23 05:43:25,347 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40829'. Reason: nanny-close
2023-06-23 05:43:25,347 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33591'. Reason: nanny-close
2023-06-23 05:43:25,347 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41569'. Reason: nanny-close
2023-06-23 05:43:25,347 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45557'. Reason: nanny-close
2023-06-23 05:43:25,347 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38487'. Reason: nanny-close
2023-06-23 05:43:25,347 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40297'. Reason: nanny-close
2023-06-23 05:43:25,348 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33427'. Reason: nanny-close
2023-06-23 05:43:26,057 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:56988 closed before handshake completed
2023-06-23 05:43:26,138 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:26,138 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:26,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:26,165 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:26,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:26,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:26,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:26,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:26,194 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:26,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:26,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:26,218 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:26,218 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:26,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:26,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:26,240 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:43:26,240 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:43:26,261 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:26,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:26,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:26,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:26,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:26,345 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:26,345 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:43:26,393 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:57004 closed before handshake completed
2023-06-23 05:43:26,393 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:57016 closed before handshake completed
2023-06-23 05:43:26,393 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:57026 closed before handshake completed
2023-06-23 05:43:26,393 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:57032 closed before handshake completed
2023-06-23 05:43:26,394 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:57040 closed before handshake completed
2023-06-23 05:43:26,460 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:57046 closed before handshake completed
2023-06-23 05:43:26,576 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:57048 closed before handshake completed
2023-06-23 05:43:27,212 - distributed.scheduler - INFO - Receive client connection: Client-e05677cb-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:27,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57058
2023-06-23 05:43:30,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42409
2023-06-23 05:43:30,407 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42409
2023-06-23 05:43:30,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33795
2023-06-23 05:43:30,407 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,407 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,407 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:30,407 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:30,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1w6um3gh
2023-06-23 05:43:30,408 - distributed.worker - INFO - Starting Worker plugin PreImport-037fd330-1fe8-40a6-98a5-321386dbd1c6
2023-06-23 05:43:30,408 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99a7a7ba-865f-4c92-93ae-8264f8193512
2023-06-23 05:43:30,423 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33601
2023-06-23 05:43:30,423 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33601
2023-06-23 05:43:30,423 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38311
2023-06-23 05:43:30,423 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,423 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,423 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:30,423 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:30,423 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7xw_vh63
2023-06-23 05:43:30,424 - distributed.worker - INFO - Starting Worker plugin PreImport-ed0c46d4-f67b-4ade-a3a6-945f35a7dc2c
2023-06-23 05:43:30,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3898e7fe-1fb2-42cd-9a7e-009e2ece6015
2023-06-23 05:43:30,424 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7aed4e9-4627-42dc-9601-95b674b769be
2023-06-23 05:43:30,439 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35039
2023-06-23 05:43:30,439 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35039
2023-06-23 05:43:30,439 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42289
2023-06-23 05:43:30,439 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,439 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,439 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:30,439 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:30,439 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ezic7oqw
2023-06-23 05:43:30,440 - distributed.worker - INFO - Starting Worker plugin PreImport-e3f32a61-e9ae-4497-a63c-f7259fe77ff9
2023-06-23 05:43:30,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-001889e7-221a-42be-b351-aeecb290f02f
2023-06-23 05:43:30,440 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55bc403e-e8a8-451f-a073-dfb2ff9815ca
2023-06-23 05:43:30,461 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41163
2023-06-23 05:43:30,461 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41163
2023-06-23 05:43:30,461 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40307
2023-06-23 05:43:30,461 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,461 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,462 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:30,462 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:30,462 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q5jzia0q
2023-06-23 05:43:30,462 - distributed.worker - INFO - Starting Worker plugin PreImport-064ee28c-defb-4c3d-b6f1-be52cecf9847
2023-06-23 05:43:30,462 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3fb3cdf6-15e8-48c8-a762-0c32cbc22bb3
2023-06-23 05:43:30,463 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51881709-0ae7-4340-aa4b-378b87586998
2023-06-23 05:43:30,563 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38037
2023-06-23 05:43:30,563 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38037
2023-06-23 05:43:30,563 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40179
2023-06-23 05:43:30,564 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,564 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,564 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:30,564 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:30,564 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e5v0syau
2023-06-23 05:43:30,564 - distributed.worker - INFO - Starting Worker plugin PreImport-cc94a050-ef38-44b9-906b-ab98037f6fe4
2023-06-23 05:43:30,564 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ccda7e11-92e3-4049-abd2-d756cbd742da
2023-06-23 05:43:30,582 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46391
2023-06-23 05:43:30,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46391
2023-06-23 05:43:30,583 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37911
2023-06-23 05:43:30,583 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,583 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,583 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:30,583 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:30,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vidjd8e4
2023-06-23 05:43:30,584 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb337007-b5ae-4c98-97e0-02630a47e49a
2023-06-23 05:43:30,584 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c3535fe-ea46-46b2-9bd0-07239c587e11
2023-06-23 05:43:30,595 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46437
2023-06-23 05:43:30,595 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46437
2023-06-23 05:43:30,595 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38609
2023-06-23 05:43:30,595 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,595 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,595 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:30,596 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:30,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4mq4i84v
2023-06-23 05:43:30,596 - distributed.worker - INFO - Starting Worker plugin PreImport-61d669e4-ceee-419e-b581-eec3709d2cfd
2023-06-23 05:43:30,597 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4dc58fd4-12f3-4196-ae18-154e6707dbdd
2023-06-23 05:43:30,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2b5c0e6-229d-4894-b000-a3fb2e247b2e
2023-06-23 05:43:30,599 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33041
2023-06-23 05:43:30,599 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33041
2023-06-23 05:43:30,599 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39735
2023-06-23 05:43:30,600 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,600 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,600 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:43:30,600 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:43:30,600 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f_4j8rjl
2023-06-23 05:43:30,600 - distributed.worker - INFO - Starting Worker plugin PreImport-72f3ab25-e609-4377-afd0-aa4c23ce8756
2023-06-23 05:43:30,600 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62236448-938f-4b60-b77f-a8bc6ec879a2
2023-06-23 05:43:30,781 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd36e3ea-41f9-4632-a011-750802f6197e
2023-06-23 05:43:30,782 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,805 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,810 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,814 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42409', status: init, memory: 0, processing: 0>
2023-06-23 05:43:30,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42409
2023-06-23 05:43:30,815 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38614
2023-06-23 05:43:30,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,815 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:30,824 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,828 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,828 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:30,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f744661-68e3-471c-a27d-fb57adb86964
2023-06-23 05:43:30,829 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,829 - distributed.worker - INFO - Starting Worker plugin PreImport-e627a565-eb47-440a-b60a-8a04336e6f8f
2023-06-23 05:43:30,829 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,829 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42409. Reason: nanny-close
2023-06-23 05:43:30,831 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:30,832 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38614; closing.
2023-06-23 05:43:30,833 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:30,833 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42409', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:30,833 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42409
2023-06-23 05:43:30,833 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:43:30,841 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41163', status: init, memory: 0, processing: 0>
2023-06-23 05:43:30,842 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41163
2023-06-23 05:43:30,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38636
2023-06-23 05:43:30,842 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,843 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,843 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35039', status: init, memory: 0, processing: 0>
2023-06-23 05:43:30,844 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35039
2023-06-23 05:43:30,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38620
2023-06-23 05:43:30,844 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,845 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:30,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:30,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46391', status: init, memory: 0, processing: 0>
2023-06-23 05:43:30,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46391
2023-06-23 05:43:30,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38656
2023-06-23 05:43:30,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33041', status: init, memory: 0, processing: 0>
2023-06-23 05:43:30,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33041
2023-06-23 05:43:30,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38672
2023-06-23 05:43:30,861 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,861 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46437', status: init, memory: 0, processing: 0>
2023-06-23 05:43:30,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:30,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46437
2023-06-23 05:43:30,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38646
2023-06-23 05:43:30,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:30,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:30,866 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33601', status: init, memory: 0, processing: 0>
2023-06-23 05:43:30,866 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33601
2023-06-23 05:43:30,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38644
2023-06-23 05:43:30,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:43:30,867 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:43:30,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:43:30,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:30,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:30,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:30,871 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33041. Reason: nanny-close
2023-06-23 05:43:30,871 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:30,872 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46437. Reason: nanny-close
2023-06-23 05:43:30,872 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:30,872 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41163. Reason: nanny-close
2023-06-23 05:43:30,872 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:43:30,873 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33601. Reason: nanny-close
2023-06-23 05:43:30,873 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35039. Reason: nanny-close
2023-06-23 05:43:30,873 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38672; closing.
2023-06-23 05:43:30,873 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:30,873 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46391. Reason: nanny-close
2023-06-23 05:43:30,873 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33041', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:30,874 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33041
2023-06-23 05:43:30,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:30,874 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:30,875 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:30,875 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:30,875 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:30,875 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:30,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38646; closing.
2023-06-23 05:43:30,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38636; closing.
2023-06-23 05:43:30,876 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33041
2023-06-23 05:43:30,876 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:30,876 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:30,876 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:43:30,876 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:30,876 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46437', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:30,877 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46437
2023-06-23 05:43:30,877 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41163', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:30,877 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41163
2023-06-23 05:43:30,878 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38644; closing.
2023-06-23 05:43:30,878 - distributed.nanny - INFO - Worker closed
2023-06-23 05:43:30,878 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38656; closing.
2023-06-23 05:43:30,879 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38620; closing.
2023-06-23 05:43:30,879 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33601', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:30,879 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33601
2023-06-23 05:43:30,879 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46391', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:30,879 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46391
2023-06-23 05:43:30,880 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35039', status: closing, memory: 0, processing: 0>
2023-06-23 05:43:30,880 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35039
2023-06-23 05:43:30,880 - distributed.scheduler - INFO - Lost all workers
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 93, in setup
    rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 305, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory
2023-06-23 05:43:30,960 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6568045-1788-456e-a366-b1ac703af423
2023-06-23 05:43:30,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38037. Reason: worker-close
2023-06-23 05:43:30,961 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:43:30,975 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 93, in setup
    rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 305, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:31,024 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 93, in setup
    rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 305, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:121: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:43:31,475 - distributed.nanny - INFO - Worker process 53155 was killed by signal 15
2023-06-23 05:43:36,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40325', status: init, memory: 0, processing: 0>
2023-06-23 05:43:36,218 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40325
2023-06-23 05:43:36,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38688
2023-06-23 05:43:36,241 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40619', status: init, memory: 0, processing: 0>
2023-06-23 05:43:36,241 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40619
2023-06-23 05:43:36,241 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38696
2023-06-23 05:43:36,319 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43399', status: init, memory: 0, processing: 0>
2023-06-23 05:43:36,319 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43399
2023-06-23 05:43:36,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38702
2023-06-23 05:43:36,350 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39875', status: init, memory: 0, processing: 0>
2023-06-23 05:43:36,350 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39875
2023-06-23 05:43:36,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38708
2023-06-23 05:43:36,385 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32865', status: init, memory: 0, processing: 0>
2023-06-23 05:43:36,385 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32865
2023-06-23 05:43:36,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38734
2023-06-23 05:43:36,386 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43471', status: init, memory: 0, processing: 0>
2023-06-23 05:43:36,388 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43471
2023-06-23 05:43:36,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38718
2023-06-23 05:43:36,389 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43497', status: init, memory: 0, processing: 0>
2023-06-23 05:43:36,389 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43497
2023-06-23 05:43:36,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38748
2023-06-23 05:43:36,987 - distributed.core - INFO - Connection to tcp://127.0.0.1:38708 has been closed.
2023-06-23 05:43:36,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39875', status: running, memory: 0, processing: 0>
2023-06-23 05:43:36,987 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39875
2023-06-23 05:43:36,989 - distributed.core - INFO - Connection to tcp://127.0.0.1:38702 has been closed.
2023-06-23 05:43:36,989 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43399', status: running, memory: 0, processing: 0>
2023-06-23 05:43:36,989 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43399
2023-06-23 05:43:36,990 - distributed.core - INFO - Connection to tcp://127.0.0.1:38734 has been closed.
2023-06-23 05:43:36,990 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32865', status: running, memory: 0, processing: 0>
2023-06-23 05:43:36,990 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32865
2023-06-23 05:43:36,990 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38734>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38734>: Stream is closed
2023-06-23 05:43:36,993 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38702>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38702>: Stream is closed
2023-06-23 05:43:36,994 - distributed.core - INFO - Connection to tcp://127.0.0.1:38718 has been closed.
2023-06-23 05:43:36,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43471', status: running, memory: 0, processing: 0>
2023-06-23 05:43:36,995 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43471
2023-06-23 05:43:36,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:38696 has been closed.
2023-06-23 05:43:36,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40619', status: running, memory: 0, processing: 0>
2023-06-23 05:43:36,995 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40619
2023-06-23 05:43:36,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:38688 has been closed.
2023-06-23 05:43:36,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40325', status: running, memory: 0, processing: 0>
2023-06-23 05:43:36,996 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40325
2023-06-23 05:43:36,997 - distributed.core - INFO - Connection to tcp://127.0.0.1:38748 has been closed.
2023-06-23 05:43:36,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43497', status: running, memory: 0, processing: 0>
2023-06-23 05:43:36,997 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43497
2023-06-23 05:43:36,997 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:43:43,283 - distributed.scheduler - INFO - Remove client Client-e05677cb-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:43,284 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57058; closing.
2023-06-23 05:43:43,284 - distributed.scheduler - INFO - Remove client Client-e05677cb-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:43,284 - distributed.scheduler - INFO - Close client connection: Client-e05677cb-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:43,491 - distributed.scheduler - INFO - Receive client connection: Client-ea0a8a4c-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:43,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47586
2023-06-23 05:43:51,639 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44063', status: init, memory: 0, processing: 0>
2023-06-23 05:43:51,640 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44063
2023-06-23 05:43:51,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34098
2023-06-23 05:43:51,647 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35467', status: init, memory: 0, processing: 0>
2023-06-23 05:43:51,647 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35467
2023-06-23 05:43:51,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34084
2023-06-23 05:43:51,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37081', status: init, memory: 0, processing: 0>
2023-06-23 05:43:51,656 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37081
2023-06-23 05:43:51,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34100
2023-06-23 05:43:51,660 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32815', status: init, memory: 0, processing: 0>
2023-06-23 05:43:51,660 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32815
2023-06-23 05:43:51,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34102
2023-06-23 05:43:51,665 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40263', status: init, memory: 0, processing: 0>
2023-06-23 05:43:51,665 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40263
2023-06-23 05:43:51,665 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34110
2023-06-23 05:43:51,675 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40085', status: init, memory: 0, processing: 0>
2023-06-23 05:43:51,676 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40085
2023-06-23 05:43:51,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34124
2023-06-23 05:43:51,679 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44743', status: init, memory: 0, processing: 0>
2023-06-23 05:43:51,680 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44743
2023-06-23 05:43:51,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34132
2023-06-23 05:43:52,407 - distributed.core - INFO - Connection to tcp://127.0.0.1:34124 has been closed.
2023-06-23 05:43:52,407 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40085', status: running, memory: 0, processing: 0>
2023-06-23 05:43:52,408 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40085
2023-06-23 05:43:52,409 - distributed.core - INFO - Connection to tcp://127.0.0.1:34098 has been closed.
2023-06-23 05:43:52,410 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44063', status: running, memory: 0, processing: 0>
2023-06-23 05:43:52,410 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44063
2023-06-23 05:43:52,410 - distributed.core - INFO - Connection to tcp://127.0.0.1:34100 has been closed.
2023-06-23 05:43:52,410 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37081', status: running, memory: 0, processing: 0>
2023-06-23 05:43:52,410 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37081
2023-06-23 05:43:52,411 - distributed.core - INFO - Connection to tcp://127.0.0.1:34132 has been closed.
2023-06-23 05:43:52,411 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44743', status: running, memory: 0, processing: 0>
2023-06-23 05:43:52,411 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44743
2023-06-23 05:43:52,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:34110 has been closed.
2023-06-23 05:43:52,414 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40263', status: running, memory: 0, processing: 0>
2023-06-23 05:43:52,415 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40263
2023-06-23 05:43:52,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:34102 has been closed.
2023-06-23 05:43:52,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32815', status: running, memory: 0, processing: 0>
2023-06-23 05:43:52,415 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32815
2023-06-23 05:43:52,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:34084 has been closed.
2023-06-23 05:43:52,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35467', status: running, memory: 0, processing: 0>
2023-06-23 05:43:52,416 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35467
2023-06-23 05:43:52,416 - distributed.scheduler - INFO - Lost all workers
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-06-23 05:43:55,378 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:43:55,378 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:43:55,379 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:43:55,381 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:43:55,382 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-06-23 05:43:58,356 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:58,361 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44551 instead
  warnings.warn(
2023-06-23 05:43:58,365 - distributed.scheduler - INFO - State start
2023-06-23 05:43:58,385 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:43:58,386 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:43:58,387 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44551/status
2023-06-23 05:43:58,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40067'
2023-06-23 05:43:58,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32859'
2023-06-23 05:43:58,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43245'
2023-06-23 05:43:58,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35597'
2023-06-23 05:43:58,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38399'
2023-06-23 05:43:58,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45471'
2023-06-23 05:43:58,649 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43203'
2023-06-23 05:43:58,659 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34709'
2023-06-23 05:43:58,907 - distributed.scheduler - INFO - Receive client connection: Client-ea0a8a4c-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:43:58,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34614
2023-06-23 05:43:59,109 - distributed.scheduler - INFO - Receive client connection: Client-f1e050c8-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:43:59,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34616
2023-06-23 05:44:00,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:00,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:00,363 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:00,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:00,375 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:00,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:00,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:00,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:00,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:00,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:00,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:00,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:00,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:00,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:00,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:00,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:00,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:00,426 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:00,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:00,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:00,430 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:00,472 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:00,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:00,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:02,748 - distributed.scheduler - INFO - Remove client Client-ea0a8a4c-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:02,748 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34614; closing.
2023-06-23 05:44:02,748 - distributed.scheduler - INFO - Remove client Client-ea0a8a4c-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:02,749 - distributed.scheduler - INFO - Close client connection: Client-ea0a8a4c-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:02,902 - distributed.scheduler - INFO - Receive client connection: Client-f59c7fd0-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:02,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60478
2023-06-23 05:44:04,107 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45225
2023-06-23 05:44:04,107 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45225
2023-06-23 05:44:04,107 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33475
2023-06-23 05:44:04,107 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46607
2023-06-23 05:44:04,107 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,107 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33475
2023-06-23 05:44:04,107 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,107 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44437
2023-06-23 05:44:04,108 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:04,108 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,108 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:04,108 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,108 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jshlcvbp
2023-06-23 05:44:04,108 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:04,108 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:04,108 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9kziom4c
2023-06-23 05:44:04,108 - distributed.worker - INFO - Starting Worker plugin PreImport-f7d63432-f4a7-4ea2-9304-5110c2338a21
2023-06-23 05:44:04,108 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca1b9880-2067-4c0a-bfda-a89552eeca83
2023-06-23 05:44:04,108 - distributed.worker - INFO - Starting Worker plugin PreImport-ca3d1630-dba7-413d-abbc-9606c6340dfc
2023-06-23 05:44:04,108 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fddca3b0-5f8f-46f7-94f1-db2e330dbe00
2023-06-23 05:44:04,108 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d36e0211-58a1-47aa-a738-a19e970bf826
2023-06-23 05:44:04,121 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35603
2023-06-23 05:44:04,121 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35603
2023-06-23 05:44:04,121 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43141
2023-06-23 05:44:04,121 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,121 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,121 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:04,121 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:04,122 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-49ol84gn
2023-06-23 05:44:04,122 - distributed.worker - INFO - Starting Worker plugin PreImport-9184b7a8-7cb0-491f-b4a6-0b3cc7dda0a1
2023-06-23 05:44:04,122 - distributed.worker - INFO - Starting Worker plugin RMMSetup-16514409-b1ac-4ddc-b362-60bac4af4344
2023-06-23 05:44:04,154 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35183
2023-06-23 05:44:04,154 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35183
2023-06-23 05:44:04,154 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40261
2023-06-23 05:44:04,154 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,154 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,154 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:04,154 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:04,154 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-04t1ztlp
2023-06-23 05:44:04,155 - distributed.worker - INFO - Starting Worker plugin PreImport-91774fa8-263f-4447-b456-a385e59c9bc8
2023-06-23 05:44:04,155 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a003ed73-fa7b-4eee-a12c-bba82e1d4a79
2023-06-23 05:44:04,273 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36261
2023-06-23 05:44:04,273 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36261
2023-06-23 05:44:04,273 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34957
2023-06-23 05:44:04,273 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,273 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39619
2023-06-23 05:44:04,273 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,273 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39619
2023-06-23 05:44:04,273 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:04,273 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:04,273 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32857
2023-06-23 05:44:04,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pd5r2l1u
2023-06-23 05:44:04,273 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,273 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,273 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:04,273 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:04,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-brli4v58
2023-06-23 05:44:04,274 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed110d95-ccde-4d4a-a586-ea31478f4522
2023-06-23 05:44:04,274 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62c61a6f-9544-4a1c-a20e-e358bc59dc10
2023-06-23 05:44:04,274 - distributed.worker - INFO - Starting Worker plugin PreImport-02c7ecbd-ea04-44bd-9aad-43e8793190c9
2023-06-23 05:44:04,274 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a7bfc96-f01c-40df-a1b2-8ca24e2085b9
2023-06-23 05:44:04,277 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41645
2023-06-23 05:44:04,278 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41645
2023-06-23 05:44:04,278 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42431
2023-06-23 05:44:04,278 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,278 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,278 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:04,278 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:04,278 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-68b5i9z5
2023-06-23 05:44:04,278 - distributed.worker - INFO - Starting Worker plugin PreImport-eb33c3f8-5f65-4020-bf1b-e8b3c3aa0388
2023-06-23 05:44:04,279 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ef97b90-8006-4923-a049-e6e1a479da81
2023-06-23 05:44:04,283 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44927
2023-06-23 05:44:04,284 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44927
2023-06-23 05:44:04,284 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33083
2023-06-23 05:44:04,284 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,284 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,284 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:04,284 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:04,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_66igopv
2023-06-23 05:44:04,285 - distributed.worker - INFO - Starting Worker plugin PreImport-c572f038-37d0-4b7f-be1d-f230b9ebd54c
2023-06-23 05:44:04,285 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2b8e6c8-b91c-43ff-8f12-ec4d83e6abb6
2023-06-23 05:44:04,423 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04bca950-b4fc-4370-979f-f56f931b858e
2023-06-23 05:44:04,423 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf24d84c-d32a-4c32-b2a7-11e221131c45
2023-06-23 05:44:04,424 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,424 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,427 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54800ba7-f303-4662-8a40-43cdd1348e44
2023-06-23 05:44:04,428 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,435 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af0d5be5-d58e-431e-ba11-eac0f2a05f0f
2023-06-23 05:44:04,435 - distributed.worker - INFO - Starting Worker plugin PreImport-ed2f2980-c07c-41c6-b47d-4ed65b221524
2023-06-23 05:44:04,435 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,436 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9256ec8f-eeeb-476d-b4ea-2e5a0022aaac
2023-06-23 05:44:04,441 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,460 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33475', status: init, memory: 0, processing: 0>
2023-06-23 05:44:04,461 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33475
2023-06-23 05:44:04,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60506
2023-06-23 05:44:04,461 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,462 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,462 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36261', status: init, memory: 0, processing: 0>
2023-06-23 05:44:04,463 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36261
2023-06-23 05:44:04,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60516
2023-06-23 05:44:04,463 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5cd25910-514f-4f1a-a7a1-bbbec00de364
2023-06-23 05:44:04,463 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,464 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,464 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35183', status: init, memory: 0, processing: 0>
2023-06-23 05:44:04,464 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,464 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35183
2023-06-23 05:44:04,464 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60514
2023-06-23 05:44:04,465 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,465 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:04,465 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35603', status: init, memory: 0, processing: 0>
2023-06-23 05:44:04,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:04,466 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35603
2023-06-23 05:44:04,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60492
2023-06-23 05:44:04,467 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,467 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,468 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44927', status: init, memory: 0, processing: 0>
2023-06-23 05:44:04,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:04,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44927
2023-06-23 05:44:04,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60520
2023-06-23 05:44:04,469 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,469 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41645', status: init, memory: 0, processing: 0>
2023-06-23 05:44:04,469 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:04,470 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41645
2023-06-23 05:44:04,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60526
2023-06-23 05:44:04,470 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,470 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:04,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:04,502 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39619', status: init, memory: 0, processing: 0>
2023-06-23 05:44:04,502 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39619
2023-06-23 05:44:04,503 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60530
2023-06-23 05:44:04,503 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:04,503 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:04,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 903, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-23 05:44:04,639 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45225. Reason: worker-close
2023-06-23 05:44:04,639 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:44:04,644 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 903, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:44:04,677 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 903, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:44:04,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43245'. Reason: nanny-instantiate-failed
2023-06-23 05:44:04,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:44:05,104 - distributed.nanny - INFO - Worker process 53445 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 903, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:44:05,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53462 parent=53272 started daemon>
2023-06-23 05:44:05,109 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53459 parent=53272 started daemon>
2023-06-23 05:44:05,110 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53456 parent=53272 started daemon>
2023-06-23 05:44:05,110 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53453 parent=53272 started daemon>
2023-06-23 05:44:05,110 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53449 parent=53272 started daemon>
2023-06-23 05:44:05,110 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53441 parent=53272 started daemon>
2023-06-23 05:44:05,111 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53438 parent=53272 started daemon>
2023-06-23 05:44:05,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:60526 has been closed.
2023-06-23 05:44:05,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41645', status: running, memory: 0, processing: 0>
2023-06-23 05:44:05,154 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41645
2023-06-23 05:44:05,155 - distributed.core - INFO - Connection to tcp://127.0.0.1:60516 has been closed.
2023-06-23 05:44:05,155 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36261', status: running, memory: 0, processing: 0>
2023-06-23 05:44:05,156 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36261
2023-06-23 05:44:05,156 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60516>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60516>: Stream is closed
2023-06-23 05:44:05,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:60492 has been closed.
2023-06-23 05:44:05,158 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35603', status: running, memory: 0, processing: 0>
2023-06-23 05:44:05,158 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35603
2023-06-23 05:44:05,159 - distributed.core - INFO - Connection to tcp://127.0.0.1:60520 has been closed.
2023-06-23 05:44:05,159 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44927', status: running, memory: 0, processing: 0>
2023-06-23 05:44:05,159 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44927
2023-06-23 05:44:05,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:60530 has been closed.
2023-06-23 05:44:05,160 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39619', status: running, memory: 0, processing: 0>
2023-06-23 05:44:05,160 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39619
2023-06-23 05:44:05,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:60514 has been closed.
2023-06-23 05:44:05,161 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35183', status: running, memory: 0, processing: 0>
2023-06-23 05:44:05,161 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35183
2023-06-23 05:44:05,161 - distributed.core - INFO - Connection to tcp://127.0.0.1:60506 has been closed.
2023-06-23 05:44:05,161 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33475', status: running, memory: 0, processing: 0>
2023-06-23 05:44:05,161 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33475
2023-06-23 05:44:05,161 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:05,779 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 53449 exit status was already read will report exitcode 255
2023-06-23 05:44:05,953 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 53456 exit status was already read will report exitcode 255
2023-06-23 05:44:08,573 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39921', status: init, memory: 0, processing: 0>
2023-06-23 05:44:08,574 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39921
2023-06-23 05:44:08,574 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60556
2023-06-23 05:44:08,637 - distributed.scheduler - INFO - Remove client Client-f59c7fd0-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:08,637 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60478; closing.
2023-06-23 05:44:08,637 - distributed.scheduler - INFO - Remove client Client-f59c7fd0-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:08,638 - distributed.scheduler - INFO - Close client connection: Client-f59c7fd0-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:08,643 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60556; closing.
2023-06-23 05:44:08,643 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39921', status: closing, memory: 0, processing: 0>
2023-06-23 05:44:08,644 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39921
2023-06-23 05:44:08,644 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:12,101 - distributed.scheduler - INFO - Receive client connection: Client-fb1809f2-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:12,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36774
2023-06-23 05:44:15,173 - distributed.scheduler - INFO - Remove client Client-f1e050c8-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:15,174 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34616; closing.
2023-06-23 05:44:15,174 - distributed.scheduler - INFO - Remove client Client-f1e050c8-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:15,175 - distributed.scheduler - INFO - Close client connection: Client-f1e050c8-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:15,176 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:44:15,176 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:15,177 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:15,179 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:44:15,180 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-06-23 05:44:17,324 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:17,329 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43367 instead
  warnings.warn(
2023-06-23 05:44:17,333 - distributed.scheduler - INFO - State start
2023-06-23 05:44:17,354 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:17,355 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:44:17,356 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43367/status
2023-06-23 05:44:17,372 - distributed.scheduler - INFO - Receive client connection: Client-fd33510f-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:17,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37214
2023-06-23 05:44:17,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43877'
2023-06-23 05:44:18,047 - distributed.scheduler - INFO - Receive client connection: Client-fb1809f2-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:18,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37248
2023-06-23 05:44:18,135 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34599', status: init, memory: 0, processing: 0>
2023-06-23 05:44:18,136 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34599
2023-06-23 05:44:18,136 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37254
2023-06-23 05:44:18,212 - distributed.scheduler - INFO - Remove client Client-fd33510f-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:18,212 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37214; closing.
2023-06-23 05:44:18,212 - distributed.scheduler - INFO - Remove client Client-fd33510f-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:18,213 - distributed.scheduler - INFO - Remove client Client-fb1809f2-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:18,213 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37248; closing.
2023-06-23 05:44:18,213 - distributed.scheduler - INFO - Remove client Client-fb1809f2-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:18,213 - distributed.scheduler - INFO - Close client connection: Client-fd33510f-1188-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:18,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43877'. Reason: nanny-close
2023-06-23 05:44:18,214 - distributed.scheduler - INFO - Close client connection: Client-fb1809f2-1188-11ee-88ff-d8c49764f6bb
2023-06-23 05:44:18,217 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37254; closing.
2023-06-23 05:44:18,218 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34599', status: closing, memory: 0, processing: 0>
2023-06-23 05:44:18,218 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34599
2023-06-23 05:44:18,218 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:19,073 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9kziom4c', purging
2023-06-23 05:44:19,073 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-04t1ztlp', purging
2023-06-23 05:44:19,074 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-brli4v58', purging
2023-06-23 05:44:19,074 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_66igopv', purging
2023-06-23 05:44:19,075 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-49ol84gn', purging
2023-06-23 05:44:19,075 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pd5r2l1u', purging
2023-06-23 05:44:19,075 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-68b5i9z5', purging
2023-06-23 05:44:19,076 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:19,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
2023-06-23 05:44:19,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-23 05:44:20,140 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38881
2023-06-23 05:44:20,140 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38881
2023-06-23 05:44:20,140 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-06-23 05:44:20,140 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:20,140 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:20,140 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:20,140 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:44:20,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gpfsuxr9
2023-06-23 05:44:20,141 - distributed.worker - INFO - Starting Worker plugin PreImport-7aa8d18c-acd6-435c-a10a-ae0ab1b7f85a
2023-06-23 05:44:20,141 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d8dfa35-1654-4d3a-a245-4bfaaed6b25d
2023-06-23 05:44:20,141 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70fd9758-cece-423e-af2e-e13c1273c1d5
2023-06-23 05:44:20,142 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:20,163 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38881', status: init, memory: 0, processing: 0>
2023-06-23 05:44:20,163 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38881
2023-06-23 05:44:20,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44284
2023-06-23 05:44:20,164 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:20,164 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:20,166 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:20,208 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:44:20,209 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38881. Reason: nanny-close
2023-06-23 05:44:20,211 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:44:20,211 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44284; closing.
2023-06-23 05:44:20,211 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38881', status: closing, memory: 0, processing: 0>
2023-06-23 05:44:20,211 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38881
2023-06-23 05:44:20,212 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:20,212 - distributed.nanny - INFO - Worker closed
2023-06-23 05:44:20,986 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:44:20,986 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:20,987 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:20,988 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:44:20,988 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-06-23 05:44:25,286 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:25,291 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33057 instead
  warnings.warn(
2023-06-23 05:44:25,295 - distributed.scheduler - INFO - State start
2023-06-23 05:44:25,317 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:25,318 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:44:25,318 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33057/status
2023-06-23 05:44:25,355 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35869'
2023-06-23 05:44:25,628 - distributed.scheduler - INFO - Receive client connection: Client-01d4fb6a-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:25,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44358
2023-06-23 05:44:26,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:26,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
2023-06-23 05:44:27,506 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-23 05:44:28,208 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43245
2023-06-23 05:44:28,208 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43245
2023-06-23 05:44:28,208 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39389
2023-06-23 05:44:28,208 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:28,208 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:28,208 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:28,208 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:44:28,209 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w_buze3n
2023-06-23 05:44:28,209 - distributed.worker - INFO - Starting Worker plugin PreImport-a5f8f3fd-9faa-4ce5-973f-0d56b9409598
2023-06-23 05:44:28,210 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19cfeb94-f6f1-4dd6-99a2-34612e04af4c
2023-06-23 05:44:28,210 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c347e654-f8d9-4114-9e55-23ce4bb79d6b
2023-06-23 05:44:28,211 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:28,236 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43245', status: init, memory: 0, processing: 0>
2023-06-23 05:44:28,237 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43245
2023-06-23 05:44:28,237 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44390
2023-06-23 05:44:28,238 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:28,238 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:28,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:28,307 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:44:28,310 - distributed.scheduler - INFO - Remove client Client-01d4fb6a-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:28,310 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44358; closing.
2023-06-23 05:44:28,310 - distributed.scheduler - INFO - Remove client Client-01d4fb6a-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:28,311 - distributed.scheduler - INFO - Close client connection: Client-01d4fb6a-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:28,311 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35869'. Reason: nanny-close
2023-06-23 05:44:28,312 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:44:28,314 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43245. Reason: nanny-close
2023-06-23 05:44:28,316 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44390; closing.
2023-06-23 05:44:28,316 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:44:28,316 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43245', status: closing, memory: 0, processing: 0>
2023-06-23 05:44:28,316 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43245
2023-06-23 05:44:28,316 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:28,317 - distributed.nanny - INFO - Worker closed
2023-06-23 05:44:29,329 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:44:29,329 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:29,329 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:29,330 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:44:29,330 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-06-23 05:44:31,542 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:31,546 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34827 instead
  warnings.warn(
2023-06-23 05:44:31,550 - distributed.scheduler - INFO - State start
2023-06-23 05:44:31,572 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:31,573 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:31,574 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:31,574 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-06-23 05:44:39,979 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:39,984 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34625 instead
  warnings.warn(
2023-06-23 05:44:39,988 - distributed.scheduler - INFO - State start
2023-06-23 05:44:40,009 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:40,010 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-23 05:44:40,011 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34625/status
2023-06-23 05:44:40,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42415'
2023-06-23 05:44:41,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:41,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:41,781 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:41,858 - distributed.scheduler - INFO - Receive client connection: Client-0aa9ed3e-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:41,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56776
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-23 05:44:42,169 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33495
2023-06-23 05:44:42,169 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33495
2023-06-23 05:44:42,169 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33497
2023-06-23 05:44:42,169 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:44:42,169 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:42,169 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:42,169 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:44:42,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uet6mk75
2023-06-23 05:44:42,170 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf2b471c-aad3-4b65-a7a5-0f9a0133ae20
2023-06-23 05:44:42,170 - distributed.worker - INFO - Starting Worker plugin PreImport-fc66d793-ada7-47c5-a19a-e714fc8ea35b
2023-06-23 05:44:42,170 - distributed.worker - INFO - Starting Worker plugin RMMSetup-06d73d75-64d3-46c7-8735-c2180bed5c47
2023-06-23 05:44:42,170 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:42,186 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33495', status: init, memory: 0, processing: 0>
2023-06-23 05:44:42,187 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33495
2023-06-23 05:44:42,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56800
2023-06-23 05:44:42,188 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:44:42,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:42,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:44:42,287 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:44:42,289 - distributed.scheduler - INFO - Remove client Client-0aa9ed3e-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:42,290 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56776; closing.
2023-06-23 05:44:42,290 - distributed.scheduler - INFO - Remove client Client-0aa9ed3e-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:42,290 - distributed.scheduler - INFO - Close client connection: Client-0aa9ed3e-1189-11ee-8aff-d8c49764f6bb
2023-06-23 05:44:42,291 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42415'. Reason: nanny-close
2023-06-23 05:44:42,292 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:44:42,293 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33495. Reason: nanny-close
2023-06-23 05:44:42,295 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56800; closing.
2023-06-23 05:44:42,295 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:44:42,295 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33495', status: closing, memory: 0, processing: 0>
2023-06-23 05:44:42,295 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33495
2023-06-23 05:44:42,295 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:44:42,296 - distributed.nanny - INFO - Worker closed
2023-06-23 05:44:42,856 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:44:42,857 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:42,857 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:42,858 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-23 05:44:42,858 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-06-23 05:44:44,889 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:44,894 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45021 instead
  warnings.warn(
2023-06-23 05:44:44,898 - distributed.scheduler - INFO - State start
2023-06-23 05:44:44,918 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:44:44,918 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:44:44,919 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:44:44,920 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:44:45,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36095'
2023-06-23 05:44:45,154 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46473'
2023-06-23 05:44:45,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39427'
2023-06-23 05:44:45,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33961'
2023-06-23 05:44:45,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44181'
2023-06-23 05:44:45,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43183'
2023-06-23 05:44:45,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44815'
2023-06-23 05:44:45,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34801'
2023-06-23 05:44:46,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:46,787 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:46,814 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:46,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:46,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:46,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:46,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:46,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:46,925 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:46,925 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:46,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:46,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:46,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:46,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:46,941 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:46,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:46,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:46,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:46,975 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:44:46,975 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:44:47,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:47,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:47,224 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:47,224 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:44:48,979 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39121
2023-06-23 05:44:48,979 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39121
2023-06-23 05:44:48,979 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32881
2023-06-23 05:44:48,980 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:48,980 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:48,980 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:48,980 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:48,980 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6b5bztaf
2023-06-23 05:44:48,980 - distributed.worker - INFO - Starting Worker plugin PreImport-8d824091-4dcc-4fcc-9161-954bd04eeb12
2023-06-23 05:44:48,980 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27e33211-92e0-4f96-a86e-6ba152470721
2023-06-23 05:44:48,980 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7d671ac-d284-4186-a7f8-fe951d0a24bd
2023-06-23 05:44:49,475 - distributed.worker - INFO - -------------------------------------------------
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-06-23 05:44:50,114 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35559
2023-06-23 05:44:50,114 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35559
2023-06-23 05:44:50,114 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38245
2023-06-23 05:44:50,114 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:50,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:50,114 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:50,114 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:50,114 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4scayvvc
2023-06-23 05:44:50,116 - distributed.worker - INFO - Starting Worker plugin PreImport-a56f4317-a6f4-4a14-9335-0f59981921e9
2023-06-23 05:44:50,116 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76fa98a8-fcdb-47b4-a31a-63f75de287f4
2023-06-23 05:44:50,860 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40437
2023-06-23 05:44:50,860 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40437
2023-06-23 05:44:50,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44635
2023-06-23 05:44:50,860 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:50,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:50,860 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:50,860 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:50,860 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zlw71el_
2023-06-23 05:44:50,861 - distributed.worker - INFO - Starting Worker plugin PreImport-eebd744d-54eb-4287-90ce-fc61d07c2d27
2023-06-23 05:44:50,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-319c44c8-7aea-4a42-8c87-dd01f94f6cf7
2023-06-23 05:44:50,861 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c9682bd-0109-4fa8-b9b6-8f9c8cfd222f
2023-06-23 05:44:50,861 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41875
2023-06-23 05:44:50,861 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41875
2023-06-23 05:44:50,862 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33865
2023-06-23 05:44:50,862 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:50,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:50,862 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:50,862 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:50,862 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cvsg8pem
2023-06-23 05:44:50,862 - distributed.worker - INFO - Starting Worker plugin PreImport-8bfbf994-2d9e-40d9-aba0-ff9926d5808a
2023-06-23 05:44:50,862 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b436b5a0-5b67-4830-ac8d-5588e7127ebe
2023-06-23 05:44:50,891 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36215
2023-06-23 05:44:50,891 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36215
2023-06-23 05:44:50,891 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45635
2023-06-23 05:44:50,891 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45839
2023-06-23 05:44:50,891 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:50,891 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:50,891 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45839
2023-06-23 05:44:50,892 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:50,892 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40695
2023-06-23 05:44:50,892 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:50,892 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:50,892 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8j910kpa
2023-06-23 05:44:50,892 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:50,892 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:50,892 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:50,892 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ecnkqmjb
2023-06-23 05:44:50,892 - distributed.worker - INFO - Starting Worker plugin PreImport-61c635ce-1e20-4dbc-b6eb-2cda13d1cc6e
2023-06-23 05:44:50,893 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdb00cca-7d44-4794-9d3a-1a84d45aa386
2023-06-23 05:44:50,893 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96a10a6b-92de-41ad-af52-6894e5a6bcc3
2023-06-23 05:44:50,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-962f629f-e830-4b66-b39a-0ae4fa1525d4
2023-06-23 05:44:50,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a3b696bc-0780-485c-9916-1a8f2b33771f
2023-06-23 05:44:50,910 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35963
2023-06-23 05:44:50,910 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35963
2023-06-23 05:44:50,910 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44723
2023-06-23 05:44:50,910 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:50,910 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:50,910 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:50,911 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:50,911 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-303a2u45
2023-06-23 05:44:50,911 - distributed.worker - INFO - Starting Worker plugin PreImport-85340564-167a-441f-b8c3-60be2cd312fa
2023-06-23 05:44:50,911 - distributed.worker - INFO - Starting Worker plugin RMMSetup-604559da-a41f-45e7-97e1-0bdea880c171
2023-06-23 05:44:50,939 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33413
2023-06-23 05:44:50,939 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33413
2023-06-23 05:44:50,939 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34399
2023-06-23 05:44:50,939 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:44:50,939 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:50,939 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:44:50,939 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:44:50,939 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8ly7wijg
2023-06-23 05:44:50,940 - distributed.worker - INFO - Starting Worker plugin PreImport-d92437cd-bb1b-48ac-9308-5d8de920cd02
2023-06-23 05:44:50,940 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29fe04d9-1941-4c7b-aa44-aa798044ff14
2023-06-23 05:44:50,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5b5005a-bd1d-456e-b9cf-7387d916e0e5
2023-06-23 05:44:51,249 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-568ad0ba-13bf-4cc4-97ad-78a3cf06b6fe
2023-06-23 05:44:51,249 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,266 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,267 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,268 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe50e7f9-17b2-4844-8a8f-046c2ae22c83
2023-06-23 05:44:51,268 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,268 - distributed.worker - INFO - Starting Worker plugin PreImport-a62acdfa-94a7-4c05-972e-5526c7e61a24
2023-06-23 05:44:51,269 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,283 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,386 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:51,386 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:51,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:51,401 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:51,408 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:51,408 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,411 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:51,411 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:51,411 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:51,423 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:51,423 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:44:51,493 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:44:51,493 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:44:51,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-23 05:44:51,582 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5768dcbc-be33-41f8-b902-b656faa39086
2023-06-23 05:44:51,583 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35559. Reason: worker-close
2023-06-23 05:44:51,583 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:44:51,588 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:44:51,620 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:44:51,625 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36095'. Reason: nanny-instantiate-failed
2023-06-23 05:44:51,626 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:44:51,651 - distributed.nanny - INFO - Worker process 54520 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:44:51,656 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54544 parent=54354 started daemon>
2023-06-23 05:44:51,657 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54541 parent=54354 started daemon>
2023-06-23 05:44:51,657 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54538 parent=54354 started daemon>
2023-06-23 05:44:51,657 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54535 parent=54354 started daemon>
2023-06-23 05:44:51,657 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54531 parent=54354 started daemon>
2023-06-23 05:44:51,657 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54527 parent=54354 started daemon>
2023-06-23 05:44:51,658 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54523 parent=54354 started daemon>
2023-06-23 05:44:52,051 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 54531 exit status was already read will report exitcode 255
2023-06-23 05:44:52,416 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 54535 exit status was already read will report exitcode 255
2023-06-23 05:44:52,449 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 54523 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-06-23 05:45:01,333 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:01,338 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39941 instead
  warnings.warn(
2023-06-23 05:45:01,342 - distributed.scheduler - INFO - State start
2023-06-23 05:45:01,391 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:01,392 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:45:01,393 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:45:01,393 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-06-23 05:45:01,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34659'
2023-06-23 05:45:03,413 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-303a2u45', purging
2023-06-23 05:45:03,414 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8ly7wijg', purging
2023-06-23 05:45:03,414 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8j910kpa', purging
2023-06-23 05:45:03,415 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cvsg8pem', purging
2023-06-23 05:45:03,415 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zlw71el_', purging
2023-06-23 05:45:03,415 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6b5bztaf', purging
2023-06-23 05:45:03,415 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ecnkqmjb', purging
2023-06-23 05:45:03,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:03,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:03,442 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:04,554 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38869
2023-06-23 05:45:04,554 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38869
2023-06-23 05:45:04,554 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46201
2023-06-23 05:45:04,554 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:04,555 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:04,555 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:04,555 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:45:04,555 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-96qtud2y
2023-06-23 05:45:04,555 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d27edadf-38a7-4a19-9b59-fdb8c874ecb4
2023-06-23 05:45:04,555 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26b9483f-8839-492c-9fd4-68ac0d48b226
2023-06-23 05:45:04,670 - distributed.worker - INFO - Starting Worker plugin PreImport-d10fdada-9668-4449-b4c6-373407d237fc
2023-06-23 05:45:04,671 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:04,709 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:04,709 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:04,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:05,220 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:05,330 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7f7a04dfdeb0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1318, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1348, in _ensure_connected
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-06-23 05:45:05,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34659'. Reason: nanny-close
2023-06-23 05:45:05,334 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:05,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38869. Reason: nanny-close
2023-06-23 05:45:05,337 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:05,338 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 209, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 18, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/__init__.py", line 2, in <module>
    from dask.array import backends, fft, lib, linalg, ma, overlap, random
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/backends.py", line 6, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/core.py", line 37, in <module>
    from dask.array.chunk_types import is_valid_array_chunk, is_valid_chunk_type
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/chunk_types.py", line 108, in <module>
    import cupy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/__init__.py", line 47, in <module>
    from cupy import testing  # NOQA  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/testing/__init__.py", line 8, in <module>
    from cupy.testing._attr import gpu  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/testing/_attr.py", line 4, in <module>
    from cupy.testing._pytest_impl import is_available, check_available
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/testing/_pytest_impl.py", line 6, in <module>
    import pytest
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pytest/__init__.py", line 6, in <module>
    from _pytest.assertion import register_assert_rewrite
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/assertion/__init__.py", line 9, in <module>
    from _pytest.assertion import rewrite
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/assertion/rewrite.py", line 39, in <module>
    from _pytest.main import Session
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/main.py", line 24, in <module>
    from _pytest import nodes
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/nodes.py", line 31, in <module>
    from _pytest.mark.structures import Mark
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/mark/__init__.py", line 133, in <module>
    class KeywordMatcher:
  File "/opt/conda/envs/gdf/lib/python3.9/dataclasses.py", line 1021, in dataclass
    return wrap(cls)
  File "/opt/conda/envs/gdf/lib/python3.9/dataclasses.py", line 1013, in wrap
    return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen)
  File "/opt/conda/envs/gdf/lib/python3.9/dataclasses.py", line 927, in _process_class
    _init_fn(flds,
  File "/opt/conda/envs/gdf/lib/python3.9/dataclasses.py", line 531, in _init_fn
    return _create_fn('__init__',
  File "/opt/conda/envs/gdf/lib/python3.9/dataclasses.py", line 400, in _create_fn
    exec(txt, globals, ns)
  File "<string>", line 1, in <module>
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 209, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 23, in <module>
    from dask.dataframe.core import DataFrame, Index, Scalar, Series, _Frame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 368, in <module>
    class _Frame(DaskMethodsMixin, OperatorMethodMixin):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 2337, in _Frame
    def mean(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 851, in wrapper
    method.__doc__ = _derived_from(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 812, in _derived_from
    doc = unsupported_arguments(doc, not_supported)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 749, in unsupported_arguments
    subset = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 752, in <listcomp>
    if re.match(r"^\s*" + arg + " ?:", line)
KeyboardInterrupt
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36107 instead
  warnings.warn(
2023-06-23 05:45:19,414 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:19,414 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:19,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:19,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:19,430 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:19,430 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:19,472 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:19,472 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:19,496 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:19,496 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:19,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:19,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:19,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:19,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:19,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:19,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35819 instead
  warnings.warn(
2023-06-23 05:45:39,061 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,061 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,086 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,089 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,089 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:39,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:39,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32919 instead
  warnings.warn(
2023-06-23 05:45:57,133 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,133 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,176 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,176 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:57,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:57,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36283 instead
  warnings.warn(
2023-06-23 05:46:15,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:15,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:15,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:15,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:15,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:15,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:15,595 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:15,595 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:15,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:15,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:15,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:15,660 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:15,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:15,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:15,674 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:15,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34493 instead
  warnings.warn(
2023-06-23 05:46:31,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:31,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:32,014 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:32,014 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:32,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:32,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:32,037 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:32,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:32,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:32,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:32,118 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:32,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:32,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:32,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:32,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:32,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:41,611 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #012] ep: 0x7ff2dc39a0c0, tag: 0x4adc3048549ada8e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #012] ep: 0x7ff2dc39a0c0, tag: 0x4adc3048549ada8e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-23 05:46:41,611 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #012] ep: 0x7fc9005150c0, tag: 0xdc69f4429ef2addc, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #012] ep: 0x7fc9005150c0, tag: 0xdc69f4429ef2addc, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37917 instead
  warnings.warn(
2023-06-23 05:46:48,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:48,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:48,670 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:48,670 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:48,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:48,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:48,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:48,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:48,682 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:48,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:48,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:48,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:48,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:48,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:48,742 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:48,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41679 instead
  warnings.warn(
2023-06-23 05:47:05,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:05,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:05,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:05,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:05,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:05,972 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:05,995 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:05,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:06,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:06,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:06,052 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:06,052 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:06,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:06,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:06,113 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:06,113 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46721 instead
  warnings.warn(
2023-06-23 05:47:28,339 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,463 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,463 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:28,497 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:28,497 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35893 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43121 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45669 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44267 instead
  warnings.warn(
2023-06-23 05:48:44,059 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:53758 remote=tcp://127.0.0.1:35527>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36749 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46029 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45919 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44115 instead
  warnings.warn(
2023-06-23 05:50:13,377 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-f66e9671-d923-47a0-ac69-ece51c277dcf
Function:  _run_coroutine_on_worker
args:      (197715522633511807688683057219132965130, <function shuffle_task at 0x7fa88fa99ee0>, ('explicit-comms-shuffle-bbdc2dc1e336e99c58ca2e023bb0287a', {0: {"('explicit-comms-shuffle-fe74535eebf60bb3b92c37d8dcdb0c2b', 0)"}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
