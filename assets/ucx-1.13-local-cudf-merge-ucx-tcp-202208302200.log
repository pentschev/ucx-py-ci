2022-08-30 23:41:27,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-08-30 23:41:27,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-08-30 23:41:27,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-08-30 23:41:27,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-08-30 23:41:27,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-08-30 23:41:27,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-08-30 23:41:27,384 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-08-30 23:41:27,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-08-30 23:41:27,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-08-30 23:41:27,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-08-30 23:41:27,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-08-30 23:41:27,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-08-30 23:41:27,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-08-30 23:41:27,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-08-30 23:41:27,404 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2022-08-30 23:41:27,404 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2022-08-30 23:42:30,962 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:192: Maximum pool size exceeded
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils.py", line 778, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 336, in read
    frames = [
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 337, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 138, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:192: Maximum pool size exceeded
2022-08-30 23:42:30,962 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:192: Maximum pool size exceeded
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 1983, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 2725, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils_comm.py", line 383, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils_comm.py", line 368, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/worker.py", line 2705, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/core.py", line 918, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/utils.py", line 778, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 336, in read
    frames = [
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 337, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/site-packages/distributed/comm/ucx.py", line 138, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:192: Maximum pool size exceeded
/datasets/pentschev/miniconda3/envs/gdf/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
