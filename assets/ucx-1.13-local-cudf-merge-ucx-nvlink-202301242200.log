2023-01-24 23:50:31,956 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:31,956 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:31,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:31,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:31,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:31,977 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:32,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:32,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:32,040 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:32,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:32,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:32,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:32,043 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:32,043 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:32,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:32,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:54269:0:54269] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  54269) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7fac34ff96b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7fac34ff988f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7fac34ff9bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7facaa664980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fac352751b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7fac3529e638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7fac34db10ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7fac34db1674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7fac34db3a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fac350038a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7fac34db3b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7fac35271f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7fac3551dc79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x55a8c1bd7eca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55a8c1bc5549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55a8c1bc07d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55a8c1bd2bb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55a8c1bc1e22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55a8c1bd2ec3]
19  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bc75) [0x55a8c1be1c75]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x264071) [0x55a8c1cfa071]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x55a8c1b8d187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x55a8c1bc9747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x55a8c1bc76ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55a8c1bd2ec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55a8c1bc1e22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55a8c1bd2ec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55a8c1bc1e22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55a8c1bd2ec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55a8c1bc1e22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55a8c1bd2ec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55a8c1bc1e22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55a8c1bc07d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55a8c1bd2bb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x55a8c1bc646c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55a8c1bc07d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x55a8c1be1af8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55a8c1be2254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x55a8c1cb18b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x55a8c1bcaa57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55a8c1bc5549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55a8c1bd2ec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x55a8c1be1b91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55a8c1bc5549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55a8c1bd2ec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55a8c1bc1e22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55a8c1bc07d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55a8c1bd2bb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55a8c1bc1e22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55a8c1bd2ec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x55a8c1bc1b73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55a8c1bc07d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55a8c1bd2bb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x55a8c1bc298f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55a8c1bc07d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55a8c1bc0497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55a8c1bc0449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55a8c1c7addb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x55a8c1ca9409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x55a8c1ca55a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55a8c1c9d27d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55a8c1c9d14d]
=================================
[dgx13:54261:0:54261] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  54261) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7ff8e8bd86b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7ff8e8bd888f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7ff8e8bd8bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7ff95e25e980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7ff8e8e541b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7ff8e8e7d638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7ff8e89900ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7ff8e8990674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7ff8e8992a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7ff8e8be28a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7ff8e8992b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7ff8e8e50f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7ff8e90fcc79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x561c5d130eca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x561c5d11e549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x561c5d1197d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x561c5d12bbb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x561c5d11ae22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x561c5d12bec3]
19  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bc75) [0x561c5d13ac75]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x264071) [0x561c5d253071]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x561c5d0e6187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x561c5d122747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x561c5d1206ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x561c5d12bec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x561c5d11ae22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x561c5d12bec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x561c5d11ae22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x561c5d12bec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x561c5d11ae22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x561c5d12bec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x561c5d11ae22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x561c5d1197d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x561c5d12bbb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x561c5d11f46c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x561c5d1197d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x561c5d13aaf8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x561c5d13b254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x561c5d20a8b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x561c5d123a57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x561c5d11e549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x561c5d12bec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x561c5d13ab91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x561c5d11e549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x561c5d12bec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x561c5d11ae22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x561c5d1197d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x561c5d12bbb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x561c5d11ae22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x561c5d12bec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x561c5d11ab73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x561c5d1197d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x561c5d12bbb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x561c5d11b98f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x561c5d1197d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x561c5d119497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x561c5d119449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x561c5d1d3ddb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x561c5d202409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x561c5d1fe5a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x561c5d1f627d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x561c5d1f614d]
=================================
2023-01-24 23:50:41,684 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:35591 -> ucx://127.0.0.1:59349
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f2174063380, tag: 0xaafab8667475aabd, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-24 23:50:41,684 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59349
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f572c0ae280, tag: 0xf789af551d010c3d, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f572c0ae280, tag: 0xf789af551d010c3d, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-24 23:50:41,684 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59349
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f4dc002f180, tag: 0xdbf798cb5af59964, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f4dc002f180, tag: 0xdbf798cb5af59964, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-24 23:50:41,684 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59349
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7ff970006180, tag: 0xe020f912ef58d683, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7ff970006180, tag: 0xe020f912ef58d683, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-24 23:50:41,687 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59349
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f2174063140, tag: 0xf36599495b8728e7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f2174063140, tag: 0xf36599495b8728e7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-24 23:50:41,697 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43601
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7ff970006200, tag: 0xdb4b537b3ff20d13, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7ff970006200, tag: 0xdb4b537b3ff20d13, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-24 23:50:41,697 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43601
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f4dc002f280, tag: 0xfd147ea5ecc6ee12, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f4dc002f280, tag: 0xfd147ea5ecc6ee12, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-24 23:50:41,697 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43601
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f2174063180, tag: 0xb35cc5bdb164e214, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 328, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f2174063180, tag: 0xb35cc5bdb164e214, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in _get_data
    comm = await rpc.connect(worker)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1439, in connect
    return await connect_attempt
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1375, in _connect
    comm = await connect(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 333, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:43601 after 30 s
2023-01-24 23:50:41,698 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:40309 -> ucx://127.0.0.1:43601
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f4dc002f2c0, tag: 0x7d3759f2c16c907a, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
Task exception was never retrieved
future: <Task finished name='Task-1043' coro=<_listener_handler_coroutine() done, defined at /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-01-24 23:50:41,699 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43601
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f572c0ae180, tag: 0xce37b506bfb1b902, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f572c0ae180, tag: 0xce37b506bfb1b902, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-24 23:50:41,753 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43601
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 984, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 289, in write
    raise CommClosedError("Endpoint is closed -- unable to send message")
distributed.comm.core.CommClosedError: Endpoint is closed -- unable to send message
2023-01-24 23:50:41,797 - distributed.nanny - WARNING - Restarting worker
2023-01-24 23:50:41,798 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52357 -> ucx://127.0.0.1:59349
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 629, in send
    self._ep.raise_on_error()
  File "ucp/_libs/ucx_endpoint.pyx", line 353, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXConnectionReset: Endpoint 0x7f39d40893c0 error: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-24 23:50:41,806 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59349
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f39d4089180, tag: 0xc06bebd5a3d5dc5, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f39d4089180, tag: 0xc06bebd5a3d5dc5, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
[dgx13:54262:0:54262] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  54262) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f09e59666b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f09e596688f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f09e5966bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f0a5afd1980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f09e5be21b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f09e5c0b638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f09e571e0ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f09e571e674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f09e5720a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f09e59708a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f09e5720b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f09e5bdef5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f09e5e8ac79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x55976d61feca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55976d60d549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55976d6087d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55976d61abb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55976d609e22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x153244) [0x55976d631244]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8301) [0x7f09f5601301]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x55976d612a57]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x55976d5d5187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x55976d611747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x55976d60f6ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55976d61aec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55976d609e22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55976d61aec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55976d609e22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55976d61aec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55976d609e22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55976d61aec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55976d609e22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55976d6087d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55976d61abb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x55976d60e46c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55976d6087d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x55976d629af8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55976d62a254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x55976d6f98b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x55976d612a57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55976d60d549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55976d61aec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x55976d629b91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55976d60d549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55976d61aec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55976d609e22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55976d6087d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55976d61abb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55976d609e22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55976d61aec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x55976d609b73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55976d6087d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55976d61abb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x55976d60a98f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55976d6087d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55976d608497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55976d608449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55976d6c2ddb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x55976d6f1409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x55976d6ed5a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55976d6e527d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55976d6e514d]
=================================
[dgx13:54267:0:54267] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  54267) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f39d49c96b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f39d49c988f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f39d49c9bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f3a5803e980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f39d4c451b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f39d4c6e638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f39d47810ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f39d4781674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f39d4783a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f39d49d38a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f39d4783b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f39d4c41f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f39d4eedc79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x55d100889eca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55d100877549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55d1008727d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55d100884bb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55d100873e22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55d100884ec3]
19  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bc75) [0x55d100893c75]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x264071) [0x55d1009ac071]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x55d10083f187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x55d10087b747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x55d1008796ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55d100884ec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55d100873e22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55d100884ec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55d100873e22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55d100884ec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55d100873e22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55d100884ec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55d100873e22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55d1008727d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55d100884bb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x55d10087846c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55d1008727d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x55d100893af8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55d100894254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x55d1009638b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x55d10087ca57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55d100877549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55d100884ec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x55d100893b91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55d100877549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55d100884ec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55d100873e22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55d1008727d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55d100884bb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55d100873e22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55d100884ec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x55d100873b73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55d1008727d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55d100884bb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x55d10087498f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55d1008727d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55d100872497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55d100872449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55d10092cddb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x55d10095b409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x55d1009575a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55d10094f27d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55d10094f14d]
=================================
2023-01-24 23:50:41,864 - distributed.nanny - WARNING - Restarting worker
2023-01-24 23:50:41,989 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:35591 -> ucx://127.0.0.1:34215
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f2174063400, tag: 0x5eeb2984e8b5837e, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-24 23:50:41,989 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34215
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f4dc002f240, tag: 0x5efb63eb64c74f9f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f4dc002f240, tag: 0x5efb63eb64c74f9f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-24 23:50:41,990 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34215
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7ff970006100, tag: 0xfc2680c0d1443137, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7ff970006100, tag: 0xfc2680c0d1443137, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-01-24 23:50:41,990 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34215
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f2174063200, tag: 0x67e9c3891eee6b14, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f2174063200, tag: 0x67e9c3891eee6b14, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-24 23:50:41,990 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58509 -> ucx://127.0.0.1:34215
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f572c0ae3c0, tag: 0xb73e3c071a4b4e2f, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-24 23:50:41,991 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34215
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f572c0ae100, tag: 0x167204603102ad17, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f572c0ae100, tag: 0x167204603102ad17, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-24 23:50:42,000 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52357
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f4dc002f100, tag: 0x22588ff0be530dd2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f4dc002f100, tag: 0x22588ff0be530dd2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-24 23:50:42,000 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52357
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f21740631c0, tag: 0x39abee667c5ecadb, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f21740631c0, tag: 0x39abee667c5ecadb, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-01-24 23:50:42,000 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:40309 -> ucx://127.0.0.1:52357
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f4dc002f400, tag: 0x2f9ac59443dfe9a8, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-24 23:50:42,012 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58509 -> ucx://127.0.0.1:52357
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f572c0ae440, tag: 0xbfdc23abae7a860f, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-24 23:50:42,021 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52357
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f572c0ae1c0, tag: 0xe50997cae69aff9e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f572c0ae1c0, tag: 0xe50997cae69aff9e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-24 23:50:42,092 - distributed.nanny - WARNING - Restarting worker
2023-01-24 23:50:42,111 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52357
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7ff9700061c0, tag: 0x883c36c3c2b47e5a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7ff9700061c0, tag: 0x883c36c3c2b47e5a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-24 23:50:42,141 - distributed.nanny - WARNING - Restarting worker
2023-01-24 23:50:43,482 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2023-01-24 23:50:43,482 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2023-01-24 23:50:43,566 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 4)
Function:  <dask.layers.CallableLazyImport object at 0x7f51b2
args:      ([                key   payload
11912     846054227  51925871
11915     855739578  57599926
2296         783923  85228195
11920     809356519  57784025
2303      860970085  54631523
...             ...       ...
99999185  611804044   4466638
99999210  814188904  23760894
99999219  802857140  99002308
99999142  858795630  66490519
99999164  857157503  80988079

[12500589 rows x 2 columns],                 key   payload
18986     949179996  32085336
18989     963588041  21764973
18993     961048136  94677391
39630     922202933  86652595
19003     423339716  29549128
...             ...       ...
99985913  907808871  23344049
99985917  961846151  71774644
99985832  961989022   7473763
99985850   22762560  55197992
99985855  915045478  95804315

[12501715 rows x 2 columns],                  key   payload
18537      630562998  27247348
18556      231866402  74414713
23235     1033029716  15045166
23248      726602722   3459689
23252     1001780055  51064204
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-24 23:50:43,569 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-98455e3e9b977d2e5d40bb5169c38c43', 2)
Function:  <dask.layers.CallableLazyImport object at 0x7f4865
args:      ([               key   payload
shuffle                     
0          1000434  97119370
0           912639  10928860
0           928027  11813574
0           735502  94921985
0           953116  92231946
...            ...       ...
0        799899677  47376438
0        799946407  74156884
0        799954590  35340466
0        799993786  19671530
0        799995497  90988935

[12497244 rows x 2 columns],                key   payload
shuffle                     
1           437710  42277737
1           426827   2902804
1           390941  85725135
1           354620   6420472
1           369633  72426919
...            ...       ...
1        799908354  16597348
1        799962252  21562410
1        799984751  85095523
1        799967368  85095222
1        799998162  70167721

[12500558 rows x 2 columns],                key   payload
shuffle                     
2           180422  93824358
2           147544   2325609
2           182149  20586612
2           157322  23087274
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-24 23:50:43,619 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-98455e3e9b977d2e5d40bb5169c38c43', 1)
Function:  <dask.layers.CallableLazyImport object at 0x7f1bfb
args:      ([               key   payload
shuffle                     
0          1032270  82534313
0           768408  32000388
0           904470  71695916
0           853053  62039420
0           939777  29201941
...            ...       ...
0        799927885  77574604
0        799991846  40655225
0        799997213  48952330
0        799941846  91135343
0        799862283  20814920

[12497508 rows x 2 columns],                key   payload
shuffle                     
1           512542  46066173
1           638870   9147869
1           670667    211189
1          1008061  61300898
1           918885  43786223
...            ...       ...
1        799962242  55885138
1        799962633  93268753
1        799962638  50628483
1        799889501   6181700
1        799886026   6729000

[12503907 rows x 2 columns],                key   payload
shuffle                     
2           119369  98852738
2           138262  62015600
2           186537  60646868
2             7971  92662784
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-24 23:50:43,620 - distributed.worker - ERROR - 'int' object is not subscriptable
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2862, in _get_data
    status = response["status"]
TypeError: 'int' object is not subscriptable
2023-01-24 23:50:43,630 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7ff408
args:      ([                key   payload
11911     701960091  12008331
11932     854077644  86479729
2282      832486027  54061553
2284      830590562  70850509
2285      830992572  58189368
...             ...       ...
99999149  828484708  23819737
99999150  302643333   2131887
99999152  817534383  64929524
99999159  809611534  49595881
99999167  837179035  80695966

[12498632 rows x 2 columns],                 key   payload
18977     315017709  44236051
18981     911002416  37460623
18992     115070184  32000192
39617     953130320  54343401
18999     521787738  12205539
...             ...       ...
99985830  968200984  47617405
99985841  717237637  97499789
99985844  925528825  98107587
99985845  962611615  54678046
99985848  942041262  76065070

[12502237 rows x 2 columns],                  key   payload
18533      735806578  34712744
18542     1059886019  28526152
18547     1000866857  26255516
18548     1054702684  40997610
18552     1049170384  17366961
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-24 23:50:43,680 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-98455e3e9b977d2e5d40bb5169c38c43', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7f51b2
args:      ([               key   payload
shuffle                     
0          1012106   5502294
0           821115  53561681
0           889196  94806831
0           942272  93403751
0           998892  41346801
...            ...       ...
0        799869465   6702363
0        799906841  41945836
0        799880830  64449724
0        799956708  69573487
0        799990159  78792409

[12502296 rows x 2 columns],                key   payload
shuffle                     
1           500295  44921161
1           414980  60370409
1           427473  22308167
1           645079  81276371
1           720422  56179709
...            ...       ...
1        799980754  95283365
1        799967363  70273291
1        799917131  25368022
1        799998975   6049554
1        799884921  71389201

[12499115 rows x 2 columns],                key   payload
shuffle                     
2           230555   4478826
2           181822  13241152
2           149079  73881486
2           193694  44200350
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-24 23:50:43,751 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-98455e3e9b977d2e5d40bb5169c38c43', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7ff408
args:      ([               key   payload
shuffle                     
0           987074  16083004
0           919761  34893363
0          1000437  43744336
0           947802  46717164
0           340406  79383515
...            ...       ...
0        799956079  15506874
0        799915078  79124186
0        799859880  84935616
0        799928911  31480618
0        799955613  89489867

[12497076 rows x 2 columns],                key   payload
shuffle                     
1           599807  84980980
1           602850  72964712
1           580719  33539062
1           441143  75519160
1           581149  95460000
...            ...       ...
1        799941648  32100256
1        799985223  98211098
1        799884912  95603802
1        799891139  76545047
1        799962246  23762320

[12501362 rows x 2 columns],                key   payload
shuffle                     
2           103151  67306435
2            20486  72785703
2           173781  58997675
2           186035  18456954
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-24 23:50:43,769 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7f4865
args:      ([                key   payload
11908     837813457  48510175
11910     842833530  92570282
2274        1684027  36609210
11914     812646968  34292604
2290      832365749  90079087
...             ...       ...
99999224  835798890    637450
99999147  603153955  64928007
99999160  853779714  82744652
99999161  831937341  67322708
99999165  838030468  67538028

[12497796 rows x 2 columns],                 key   payload
18976      12759581  15740809
18983     900628389  73712774
18985     925356397  66548290
39625     946150379  91674870
18988     956675688  67634461
...             ...       ...
99985831  932029242  92645517
99985835  928223957  88551298
99985838  911523889  19687515
99985839   12909793  99005502
99985842  947451319  29218063

[12497151 rows x 2 columns],                  key   payload
18534     1021453467  78363243
18540     1058153678  10195339
18544     1017930234   5252717
23232     1027232459  86965611
23238     1016401274  55742263
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-24 23:50:43,869 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-98455e3e9b977d2e5d40bb5169c38c43', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7ff408
args:      ([               key   payload
shuffle                     
0           915437   5097359
0           757512  29870583
0          1016082  20165889
0           733487  24686884
0          1013763  32636749
...            ...       ...
0        799970549  37777104
0        799860990  82754904
0        799954522  79618442
0        799861317  99473109
0        799940727  35984283

[12498811 rows x 2 columns],                key   payload
shuffle                     
1           591915  66765148
1           384960  27644337
1           619523  94114133
1           366993  42195267
1           613924  60549075
...            ...       ...
1        799879899  59279401
1        799985221  25956677
1        799956114  10428344
1        799937188  65685695
1        799998147  32312793

[12498510 rows x 2 columns],                key   payload
shuffle                     
2           151612  45285502
2            95297  92946297
2           153554  28329437
2           202083  23534219
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-24 23:50:43,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:43,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:43,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:43,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:43,947 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:43,947 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:43,947 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-24 23:50:43,947 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-24 23:50:44,296 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-98455e3e9b977d2e5d40bb5169c38c43', 7)
Function:  <dask.layers.CallableLazyImport object at 0x7f4865
args:      ([               key   payload
shuffle                     
0           923992  22785355
0           928509  59393679
0           783250  79614424
0           922350  41876944
0           912157   3149351
...            ...       ...
0        799940710  97693389
0        799995496  52727640
0        799947885  47604753
0        799994494  95593287
0        799891031  84627575

[12498151 rows x 2 columns],                key   payload
shuffle                     
1           383824   3377858
1           432006  54361780
1           627784  82508874
1           460794   5593471
1           373662  16501947
...            ...       ...
1        799954597   6961274
1        799998146  59154151
1        799984740  59410752
1        799861278  15263735
1        799997237  62390841

[12498591 rows x 2 columns],                key   payload
shuffle                     
2           155671  97780515
2           161898   9273094
2           136900  32921178
2            20492   1405254
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
