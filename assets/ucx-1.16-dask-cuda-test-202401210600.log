============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-21 06:31:16,228 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:16,233 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37925 instead
  warnings.warn(
2024-01-21 06:31:16,237 - distributed.scheduler - INFO - State start
2024-01-21 06:31:16,808 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:16,810 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-21 06:31:16,811 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37925/status
2024-01-21 06:31:16,811 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:31:16,861 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38989'
2024-01-21 06:31:16,889 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36893'
2024-01-21 06:31:16,891 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42377'
2024-01-21 06:31:16,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34787'
2024-01-21 06:31:17,865 - distributed.scheduler - INFO - Receive client connection: Client-acf01071-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:17,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54598
2024-01-21 06:31:18,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:18,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:18,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:18,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42963
2024-01-21 06:31:18,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42963
2024-01-21 06:31:18,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45823
2024-01-21 06:31:18,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:31:18,634 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,634 - distributed.worker - INFO -               Threads:                          4
2024-01-21 06:31:18,634 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-21 06:31:18,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-y0ixj7bb
2024-01-21 06:31:18,635 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98e47736-f485-4a4b-9d6e-87fb84c26d4b
2024-01-21 06:31:18,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d7d696aa-086b-4cf4-8a95-88410affddaf
2024-01-21 06:31:18,635 - distributed.worker - INFO - Starting Worker plugin PreImport-4d73d328-a012-4f7a-adc8-e062217e1aa7
2024-01-21 06:31:18,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:18,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:18,646 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:18,646 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37623
2024-01-21 06:31:18,646 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37623
2024-01-21 06:31:18,647 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42361
2024-01-21 06:31:18,647 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:31:18,647 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,647 - distributed.worker - INFO -               Threads:                          4
2024-01-21 06:31:18,647 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-21 06:31:18,647 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-vsguooi1
2024-01-21 06:31:18,647 - distributed.worker - INFO - Starting Worker plugin PreImport-f2993559-004a-4d5c-9aa2-11753a502073
2024-01-21 06:31:18,647 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f6debfa-3920-4cc2-8b93-f08d1e37573e
2024-01-21 06:31:18,647 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1bc420c8-f828-4941-9573-05b2dd07fdf9
2024-01-21 06:31:18,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:18,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:18,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:18,671 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33953
2024-01-21 06:31:18,671 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33953
2024-01-21 06:31:18,671 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33843
2024-01-21 06:31:18,671 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:31:18,672 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,672 - distributed.worker - INFO -               Threads:                          4
2024-01-21 06:31:18,672 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-21 06:31:18,672 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-tje906u0
2024-01-21 06:31:18,672 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2eedc586-8015-4e9a-bd7a-d4f2b1d1f2d8
2024-01-21 06:31:18,672 - distributed.worker - INFO - Starting Worker plugin PreImport-83acd0ef-9875-44f9-8a81-aa178bb3f2cc
2024-01-21 06:31:18,672 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c16dd72e-5bee-4d30-8cdf-32eb117a2746
2024-01-21 06:31:18,672 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,693 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42963', status: init, memory: 0, processing: 0>
2024-01-21 06:31:18,695 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42963
2024-01-21 06:31:18,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54604
2024-01-21 06:31:18,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:18,696 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:31:18,696 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:31:18,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:18,711 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:18,714 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:18,715 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41931
2024-01-21 06:31:18,715 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41931
2024-01-21 06:31:18,715 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33739
2024-01-21 06:31:18,715 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:31:18,715 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,715 - distributed.worker - INFO -               Threads:                          4
2024-01-21 06:31:18,715 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-21 06:31:18,715 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-oap2ev3f
2024-01-21 06:31:18,716 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-635b1216-3828-4cb1-bdf7-c9c98b8db1c0
2024-01-21 06:31:18,718 - distributed.worker - INFO - Starting Worker plugin PreImport-d5bc5a88-b9cf-471f-a776-14abb87dafa7
2024-01-21 06:31:18,718 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aff84ba5-c6e4-43a6-aeeb-6abff4414031
2024-01-21 06:31:18,718 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,732 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37623', status: init, memory: 0, processing: 0>
2024-01-21 06:31:18,732 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37623
2024-01-21 06:31:18,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54610
2024-01-21 06:31:18,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:18,734 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:31:18,734 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:31:18,740 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33953', status: init, memory: 0, processing: 0>
2024-01-21 06:31:18,741 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33953
2024-01-21 06:31:18,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54616
2024-01-21 06:31:18,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:18,742 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:31:18,742 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:31:18,778 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41931', status: init, memory: 0, processing: 0>
2024-01-21 06:31:18,778 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41931
2024-01-21 06:31:18,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54620
2024-01-21 06:31:18,779 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:18,780 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:31:18,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:18,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:31:18,806 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-21 06:31:18,806 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-21 06:31:18,806 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-21 06:31:18,806 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-21 06:31:18,814 - distributed.scheduler - INFO - Remove client Client-acf01071-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:18,814 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54598; closing.
2024-01-21 06:31:18,814 - distributed.scheduler - INFO - Remove client Client-acf01071-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:18,814 - distributed.scheduler - INFO - Close client connection: Client-acf01071-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:31:18,815 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38989'. Reason: nanny-close
2024-01-21 06:31:18,816 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:18,816 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36893'. Reason: nanny-close
2024-01-21 06:31:18,817 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:18,817 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42377'. Reason: nanny-close
2024-01-21 06:31:18,817 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37623. Reason: nanny-close
2024-01-21 06:31:18,817 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:18,817 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34787'. Reason: nanny-close
2024-01-21 06:31:18,817 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42963. Reason: nanny-close
2024-01-21 06:31:18,818 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:18,818 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41931. Reason: nanny-close
2024-01-21 06:31:18,818 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33953. Reason: nanny-close
2024-01-21 06:31:18,819 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:31:18,819 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54610; closing.
2024-01-21 06:31:18,819 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37623', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818678.8195455')
2024-01-21 06:31:18,819 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:31:18,820 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:31:18,820 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:18,820 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54604; closing.
2024-01-21 06:31:18,821 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:18,821 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54616; closing.
2024-01-21 06:31:18,821 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42963', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818678.8215983')
2024-01-21 06:31:18,821 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:18,822 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33953', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818678.8221607')
2024-01-21 06:31:18,822 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:31:18,823 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54620; closing.
2024-01-21 06:31:18,823 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41931', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818678.8233392')
2024-01-21 06:31:18,823 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:31:18,823 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:19,581 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:31:19,581 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:31:19,582 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:19,583 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-21 06:31:19,583 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-21 06:31:21,680 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:21,684 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41055 instead
  warnings.warn(
2024-01-21 06:31:21,689 - distributed.scheduler - INFO - State start
2024-01-21 06:31:21,711 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:21,712 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-21 06:31:21,713 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:21,714 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-21 06:31:21,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42143'
2024-01-21 06:31:21,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45419'
2024-01-21 06:31:21,919 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35621'
2024-01-21 06:31:21,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34417'
2024-01-21 06:31:21,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39411'
2024-01-21 06:31:21,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46247'
2024-01-21 06:31:21,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42861'
2024-01-21 06:31:21,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38843'
2024-01-21 06:31:24,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:24,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:24,384 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:24,385 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35617
2024-01-21 06:31:24,385 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35617
2024-01-21 06:31:24,385 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38763
2024-01-21 06:31:24,385 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,385 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,385 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:24,385 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:24,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ossx8f4p
2024-01-21 06:31:24,386 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3fad5331-7b5f-4d0c-9e7e-65f52cf9d015
2024-01-21 06:31:24,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:24,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:24,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:24,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:24,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:24,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:24,394 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:24,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:24,395 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37231
2024-01-21 06:31:24,395 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37231
2024-01-21 06:31:24,395 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43821
2024-01-21 06:31:24,395 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,395 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,395 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:24,396 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36321
2024-01-21 06:31:24,396 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:24,396 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36321
2024-01-21 06:31:24,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1lon3cp_
2024-01-21 06:31:24,396 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44865
2024-01-21 06:31:24,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:24,396 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:24,396 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,396 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:24,396 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de5e0357-81d4-4d8c-a50f-86d4413044e5
2024-01-21 06:31:24,396 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:24,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xcwlibiu
2024-01-21 06:31:24,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4185119f-e908-48eb-834b-bcc8456f3dcd
2024-01-21 06:31:24,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca91b5cb-be76-41ea-85c6-93150d9841d2
2024-01-21 06:31:24,399 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:24,400 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36831
2024-01-21 06:31:24,400 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36831
2024-01-21 06:31:24,400 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39767
2024-01-21 06:31:24,400 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,400 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,400 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:24,400 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:24,400 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-27zky3sw
2024-01-21 06:31:24,400 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b513ee9-908e-42cb-900c-e9153ad34b60
2024-01-21 06:31:24,400 - distributed.worker - INFO - Starting Worker plugin PreImport-89d681a6-f38b-4c24-b0d2-74b8e16db1d0
2024-01-21 06:31:24,400 - distributed.worker - INFO - Starting Worker plugin RMMSetup-856f5dbe-f511-4dae-8d57-625ed66e67c1
2024-01-21 06:31:24,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:24,402 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42357
2024-01-21 06:31:24,402 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42357
2024-01-21 06:31:24,402 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43015
2024-01-21 06:31:24,402 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,402 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,402 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:24,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:24,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jlacnxmp
2024-01-21 06:31:24,402 - distributed.worker - INFO - Starting Worker plugin PreImport-f62bab54-5448-4f58-a016-3c79f3dbce91
2024-01-21 06:31:24,402 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2116c110-e198-436f-9522-e2c60642941c
2024-01-21 06:31:24,404 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f086128d-afb5-43f8-b0ce-8531bc7fd2a4
2024-01-21 06:31:24,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:24,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:24,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:24,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:24,414 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:24,414 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:24,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:24,417 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39195
2024-01-21 06:31:24,417 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39195
2024-01-21 06:31:24,417 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37059
2024-01-21 06:31:24,417 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,417 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,417 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:24,417 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:24,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i4ttxc8z
2024-01-21 06:31:24,418 - distributed.worker - INFO - Starting Worker plugin PreImport-e2da0f27-f5ce-4859-a511-ed3bcccbef54
2024-01-21 06:31:24,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:24,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-358611dd-07a5-4db4-9178-aa1d8f86a119
2024-01-21 06:31:24,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:24,419 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41975
2024-01-21 06:31:24,419 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41975
2024-01-21 06:31:24,419 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39969
2024-01-21 06:31:24,419 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44805
2024-01-21 06:31:24,419 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39969
2024-01-21 06:31:24,419 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,419 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35857
2024-01-21 06:31:24,419 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,419 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:24,419 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:24,419 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:24,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:24,419 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:24,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:24,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vcy_0xa6
2024-01-21 06:31:24,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7_n3bh13
2024-01-21 06:31:24,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3cedbc61-56f0-40a0-8241-10d566dafab7
2024-01-21 06:31:24,419 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10794909-10c0-4a12-8725-7e5373abfbf7
2024-01-21 06:31:24,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6c1a053-743d-4ed4-bea2-df7717af6053
2024-01-21 06:31:24,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-918df7e3-7976-477a-ab16-f1b97788165a
2024-01-21 06:31:25,130 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42143'. Reason: nanny-close
2024-01-21 06:31:25,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45419'. Reason: nanny-close
2024-01-21 06:31:25,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35621'. Reason: nanny-close
2024-01-21 06:31:25,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34417'. Reason: nanny-close
2024-01-21 06:31:25,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39411'. Reason: nanny-close
2024-01-21 06:31:25,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46247'. Reason: nanny-close
2024-01-21 06:31:25,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42861'. Reason: nanny-close
2024-01-21 06:31:25,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38843'. Reason: nanny-close
2024-01-21 06:31:27,097 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bab22e8e-b91a-4a7c-91d9-291c26181c94
2024-01-21 06:31:27,098 - distributed.worker - INFO - Starting Worker plugin PreImport-e1d322ce-5c8c-402d-a64b-434e44b28818
2024-01-21 06:31:27,098 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:27,106 - distributed.worker - INFO - Starting Worker plugin PreImport-74943818-c365-4cb9-b172-014476a9e294
2024-01-21 06:31:27,108 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:27,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed5791f4-1de1-428f-8ca1-c474bac200b3
2024-01-21 06:31:27,138 - distributed.worker - INFO - Starting Worker plugin PreImport-4ca7fe58-318c-4c8a-ac4c-39b74f6e7a61
2024-01-21 06:31:27,139 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:27,147 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1fbe7a9c-575a-420c-a02b-b42fda76aa98
2024-01-21 06:31:27,147 - distributed.worker - INFO - Starting Worker plugin PreImport-df5f8010-8ce7-4954-816c-dee409b628d6
2024-01-21 06:31:27,148 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:27,166 - distributed.worker - INFO - Starting Worker plugin PreImport-5a676aba-af79-45a4-9f91-8cdc20f062c4
2024-01-21 06:31:27,167 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:27,168 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:27,172 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:27,183 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:29,781 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:29,782 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:29,782 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:29,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:29,828 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:29,829 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39969. Reason: nanny-close
2024-01-21 06:31:29,831 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:29,833 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:34076 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-21 06:31:30,253 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64737 parent=64540 started daemon>
2024-01-21 06:31:30,253 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64733 parent=64540 started daemon>
2024-01-21 06:31:30,253 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64726 parent=64540 started daemon>
2024-01-21 06:31:30,254 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64720 parent=64540 started daemon>
2024-01-21 06:31:30,254 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64715 parent=64540 started daemon>
2024-01-21 06:31:30,254 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64710 parent=64540 started daemon>
2024-01-21 06:31:30,254 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64706 parent=64540 started daemon>
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-21 06:31:33,256 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:33,260 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36237 instead
  warnings.warn(
2024-01-21 06:31:33,265 - distributed.scheduler - INFO - State start
2024-01-21 06:31:33,266 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-27zky3sw', purging
2024-01-21 06:31:33,267 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1lon3cp_', purging
2024-01-21 06:31:33,267 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vcy_0xa6', purging
2024-01-21 06:31:33,267 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xcwlibiu', purging
2024-01-21 06:31:33,268 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jlacnxmp', purging
2024-01-21 06:31:33,268 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ossx8f4p', purging
2024-01-21 06:31:33,268 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-i4ttxc8z', purging
2024-01-21 06:31:33,361 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:33,362 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-21 06:31:33,362 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:33,363 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-21 06:31:33,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37197'
2024-01-21 06:31:33,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32893'
2024-01-21 06:31:33,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43851'
2024-01-21 06:31:33,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42943'
2024-01-21 06:31:33,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38173'
2024-01-21 06:31:33,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35181'
2024-01-21 06:31:33,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40425'
2024-01-21 06:31:33,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46461'
2024-01-21 06:31:33,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37197'. Reason: nanny-close
2024-01-21 06:31:33,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32893'. Reason: nanny-close
2024-01-21 06:31:33,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43851'. Reason: nanny-close
2024-01-21 06:31:33,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42943'. Reason: nanny-close
2024-01-21 06:31:33,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38173'. Reason: nanny-close
2024-01-21 06:31:33,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35181'. Reason: nanny-close
2024-01-21 06:31:33,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40425'. Reason: nanny-close
2024-01-21 06:31:33,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46461'. Reason: nanny-close
2024-01-21 06:31:35,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:35,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:35,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:35,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:35,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:35,431 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:35,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:35,432 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44341
2024-01-21 06:31:35,432 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44341
2024-01-21 06:31:35,432 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39085
2024-01-21 06:31:35,432 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:35,432 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:35,432 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:35,432 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:35,432 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6s0ccwrn
2024-01-21 06:31:35,432 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:35,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23056691-fbdb-47cb-aa5c-f5f507c25e8c
2024-01-21 06:31:35,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:35,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:35,434 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45601
2024-01-21 06:31:35,434 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45601
2024-01-21 06:31:35,434 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45485
2024-01-21 06:31:35,434 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:35,434 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:35,434 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:35,434 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:35,434 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n83q8m99
2024-01-21 06:31:35,435 - distributed.worker - INFO - Starting Worker plugin PreImport-1355bec4-921b-4281-9bcd-e52c146c026b
2024-01-21 06:31:35,435 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2c032d7-c8cd-4ebc-a4f7-f443dd054429
2024-01-21 06:31:35,436 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:35,436 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36199
2024-01-21 06:31:35,436 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36199
2024-01-21 06:31:35,437 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35401
2024-01-21 06:31:35,437 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:35,437 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:35,437 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:35,437 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:35,437 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-64m2ykb6
2024-01-21 06:31:35,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:35,437 - distributed.worker - INFO - Starting Worker plugin PreImport-bbce5f36-cb4f-492d-8b30-e679dbca87aa
2024-01-21 06:31:35,437 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d0f44a6-e8de-4a7d-9b79-efc486d1e924
2024-01-21 06:31:35,438 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40753
2024-01-21 06:31:35,438 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40753
2024-01-21 06:31:35,438 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46613
2024-01-21 06:31:35,438 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:35,438 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:35,438 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:35,438 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:35,438 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jkoahtha
2024-01-21 06:31:35,438 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7da8e077-8e92-4ed5-b91b-6dafd1aaf449
2024-01-21 06:31:35,438 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da3092a7-ca44-4141-bb18-14d0a80fa96c
2024-01-21 06:31:35,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc5eeb73-b6f1-4720-a194-85c3a5663921
2024-01-21 06:31:35,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:35,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:35,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:35,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37141
2024-01-21 06:31:35,499 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37141
2024-01-21 06:31:35,499 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33895
2024-01-21 06:31:35,499 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:35,499 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:35,499 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:35,499 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:35,499 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cq7r85o9
2024-01-21 06:31:35,499 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8bbca606-1e2b-48e7-8747-9deace77d437
2024-01-21 06:31:35,513 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:35,513 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:35,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:35,518 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35991
2024-01-21 06:31:35,518 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:35,518 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:35,518 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35991
2024-01-21 06:31:35,518 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42811
2024-01-21 06:31:35,518 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:35,518 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:35,518 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:35,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:35,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v_g5vwrb
2024-01-21 06:31:35,519 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ba1b42d-1a65-4863-9b76-d684282438a9
2024-01-21 06:31:35,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:35,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:35,526 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:35,527 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35415
2024-01-21 06:31:35,528 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35415
2024-01-21 06:31:35,528 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46881
2024-01-21 06:31:35,528 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:35,528 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:35,528 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:35,528 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:35,529 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-11p8kj4h
2024-01-21 06:31:35,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:35,529 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c43ff56-e964-4828-a034-0c3d8be5738c
2024-01-21 06:31:35,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39101
2024-01-21 06:31:35,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39101
2024-01-21 06:31:35,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34025
2024-01-21 06:31:35,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:35,530 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:35,530 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:35,530 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:35,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c4j98uqy
2024-01-21 06:31:35,530 - distributed.worker - INFO - Starting Worker plugin PreImport-47af70d6-eddb-4555-9ec7-d8b6ee872d09
2024-01-21 06:31:35,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c62f83b-840f-4514-9a5d-316bf2dea838
2024-01-21 06:31:35,531 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d86b148b-5735-4a53-acaf-43074b82e044
2024-01-21 06:31:37,127 - distributed.worker - INFO - Starting Worker plugin PreImport-881901e1-c8bc-4905-ac37-09465900cdc3
2024-01-21 06:31:37,127 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,325 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a386ff79-7bfc-4e7a-965e-9f58cba23efc
2024-01-21 06:31:37,326 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,357 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40a9c259-e139-4ed3-8879-90f0cf8b35f8
2024-01-21 06:31:37,359 - distributed.worker - INFO - Starting Worker plugin PreImport-49b26409-f9d0-498e-ad74-4f82c71f4d4e
2024-01-21 06:31:37,361 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:37,425 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:37,425 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:37,452 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ddfe5d7-508b-4542-8b75-09fb75a64635
2024-01-21 06:31:37,453 - distributed.worker - INFO - Starting Worker plugin PreImport-7b3f320f-c1da-420b-bda0-58dae0b72b70
2024-01-21 06:31:37,453 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,455 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:37,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44341. Reason: nanny-close
2024-01-21 06:31:37,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:37,458 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:37,458 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,459 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:37,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:37,461 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:37,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:37,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:37,484 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:37,487 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,498 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:37,499 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:37,499 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:37,504 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-550b4d97-5ef4-4530-b22c-ddbdc4b4c4f1
2024-01-21 06:31:37,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:37,506 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:37,506 - distributed.worker - INFO - Starting Worker plugin PreImport-0aa3b9b2-242e-4c36-b9cb-5aa97689a1ab
2024-01-21 06:31:37,506 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40753. Reason: nanny-close
2024-01-21 06:31:37,507 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:37,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45601. Reason: nanny-close
2024-01-21 06:31:37,507 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37141. Reason: nanny-close
2024-01-21 06:31:37,507 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,508 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:37,509 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:37,510 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:37,511 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:37,512 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:37,514 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:37,524 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:37,525 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:37,525 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,527 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:37,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:37,551 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:37,551 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:37,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:37,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:37,558 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36199. Reason: nanny-close
2024-01-21 06:31:37,558 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35991. Reason: nanny-close
2024-01-21 06:31:37,562 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,566 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:37,566 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:37,569 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:37,569 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:37,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45007f30-4c01-4855-ba6e-b1a3b2ae68f4
2024-01-21 06:31:37,571 - distributed.worker - INFO - Starting Worker plugin PreImport-990ad3e4-d176-45f8-af8e-d291d0bdadcd
2024-01-21 06:31:37,572 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:37,607 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:37,607 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:37,610 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:37,611 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:37,611 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:37,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:37,615 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:37,615 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35415. Reason: nanny-close
2024-01-21 06:31:37,624 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:37,625 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:37,660 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:31:37,661 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39101. Reason: nanny-close
2024-01-21 06:31:37,669 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:37,671 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:50948 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-21 06:31:37,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64991 parent=64794 started daemon>
2024-01-21 06:31:37,894 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64987 parent=64794 started daemon>
2024-01-21 06:31:37,895 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64983 parent=64794 started daemon>
2024-01-21 06:31:37,895 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64979 parent=64794 started daemon>
2024-01-21 06:31:37,895 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64969 parent=64794 started daemon>
2024-01-21 06:31:37,895 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64964 parent=64794 started daemon>
2024-01-21 06:31:37,895 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64960 parent=64794 started daemon>
2024-01-21 06:31:38,224 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 64960 exit status was already read will report exitcode 255
2024-01-21 06:31:38,272 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 64991 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-21 06:31:40,394 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:40,398 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36261 instead
  warnings.warn(
2024-01-21 06:31:40,402 - distributed.scheduler - INFO - State start
2024-01-21 06:31:40,960 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:31:40,961 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-21 06:31:40,962 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:31:40,963 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-21 06:31:45,639 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34515'
2024-01-21 06:31:45,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39505'
2024-01-21 06:31:46,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43355'
2024-01-21 06:31:47,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:47,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:47,342 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:47,342 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35471
2024-01-21 06:31:47,342 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35471
2024-01-21 06:31:47,343 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43923
2024-01-21 06:31:47,343 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:47,343 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:47,343 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:47,343 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:47,343 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dr1omg0c
2024-01-21 06:31:47,343 - distributed.worker - INFO - Starting Worker plugin RMMSetup-47d9486a-2315-4f75-b971-5f9b49620a6c
2024-01-21 06:31:47,562 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:47,562 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:47,566 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:47,567 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45995
2024-01-21 06:31:47,567 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45995
2024-01-21 06:31:47,567 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39985
2024-01-21 06:31:47,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:47,567 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:47,567 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:47,567 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:47,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g6zto4q0
2024-01-21 06:31:47,567 - distributed.worker - INFO - Starting Worker plugin PreImport-2d0dee07-6706-4b40-a647-05d160ddbfaa
2024-01-21 06:31:47,568 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7735fe2e-3f1f-4859-9e3b-6e29d2ba37e2
2024-01-21 06:31:47,571 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e61a508a-4d38-4400-aa8b-0ff1f637979b
2024-01-21 06:31:47,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46829'
2024-01-21 06:31:47,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a253cf6-565c-46be-af7f-c65f2a353c87
2024-01-21 06:31:47,919 - distributed.worker - INFO - Starting Worker plugin PreImport-b10fe397-4b0d-4f13-9ef5-41e988029e50
2024-01-21 06:31:47,920 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:47,965 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:47,966 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:47,966 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:47,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:48,008 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:48,038 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:48,039 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:48,039 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:48,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:48,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:48,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:48,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:48,427 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36259
2024-01-21 06:31:48,427 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36259
2024-01-21 06:31:48,427 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36805
2024-01-21 06:31:48,427 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:48,427 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:48,427 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:48,427 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:48,427 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g_8lxbqi
2024-01-21 06:31:48,427 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6fe0b4ad-b1ad-4ccd-a52e-d219bef84b32
2024-01-21 06:31:48,428 - distributed.worker - INFO - Starting Worker plugin PreImport-e5a794bd-1f85-4407-97d0-519b46d74834
2024-01-21 06:31:48,428 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f226301-02e5-4371-922d-5a27626a1545
2024-01-21 06:31:48,660 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35471. Reason: scheduler-close
2024-01-21 06:31:48,662 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45995. Reason: scheduler-close
2024-01-21 06:31:48,668 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://127.0.0.1:34515'. Reason: scheduler-close
2024-01-21 06:31:48,668 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:56824 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:56824 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-01-21 06:31:48,671 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:9369; closing.
2024-01-21 06:31:48,671 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:48,672 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://127.0.0.1:39505'. Reason: scheduler-close
2024-01-21 06:31:48,674 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:9369; closing.
2024-01-21 06:31:48,674 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:48,814 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:49,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:31:49,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:31:49,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:31:49,394 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41389
2024-01-21 06:31:49,394 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41389
2024-01-21 06:31:49,394 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40211
2024-01-21 06:31:49,394 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:31:49,394 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:49,394 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:31:49,394 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:31:49,394 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gri2uni3
2024-01-21 06:31:49,394 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90705dd6-755f-4ffd-b342-5a959830fa26
2024-01-21 06:31:49,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-381cf771-8e9d-42ba-81e5-050a7f91222f
2024-01-21 06:31:49,685 - distributed.worker - INFO - Starting Worker plugin PreImport-83acd174-65bb-4512-99d6-9ba42fa2e638
2024-01-21 06:31:49,686 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:50,673 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-21 06:31:50,676 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-01-21 06:31:51,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39505'. Reason: nanny-close-gracefully
2024-01-21 06:31:51,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:51,117 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:51,117 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:51,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:51,146 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34515'. Reason: nanny-close-gracefully
2024-01-21 06:31:51,146 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43355'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-21 06:31:51,146 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-21 06:31:51,147 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36259. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-21 06:31:51,149 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:51,150 - distributed.nanny - INFO - Worker closed
2024-01-21 06:31:51,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43771'
2024-01-21 06:31:51,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:31:51,337 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:31:51,337 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:31:51,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:31:51,353 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46829'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-21 06:31:51,353 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-21 06:31:51,354 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41389. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-21 06:31:51,356 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:31:51,357 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:56776 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-21 06:31:51,472 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65309 parent=65083 started daemon>
2024-01-21 06:31:51,472 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65276 parent=65083 started daemon>
2024-01-21 06:31:51,541 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65276 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-21 06:32:02,289 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:02,293 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46103 instead
  warnings.warn(
2024-01-21 06:32:02,297 - distributed.scheduler - INFO - State start
2024-01-21 06:32:02,803 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:02,805 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-21 06:32:02,806 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:02,807 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-21 06:32:03,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37131'
2024-01-21 06:32:03,081 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35647'
2024-01-21 06:32:03,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40945'
2024-01-21 06:32:03,091 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36005'
2024-01-21 06:32:03,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35427'
2024-01-21 06:32:03,108 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45147'
2024-01-21 06:32:03,118 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44769'
2024-01-21 06:32:03,127 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34211'
2024-01-21 06:32:03,300 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35427'. Reason: nanny-close
2024-01-21 06:32:03,300 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45147'. Reason: nanny-close
2024-01-21 06:32:03,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44769'. Reason: nanny-close
2024-01-21 06:32:03,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34211'. Reason: nanny-close
2024-01-21 06:32:03,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37131'. Reason: nanny-close
2024-01-21 06:32:03,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35647'. Reason: nanny-close
2024-01-21 06:32:03,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40945'. Reason: nanny-close
2024-01-21 06:32:03,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36005'. Reason: nanny-close
2024-01-21 06:32:04,930 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:04,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:04,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:04,935 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34261
2024-01-21 06:32:04,935 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34261
2024-01-21 06:32:04,935 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46483
2024-01-21 06:32:04,935 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:04,936 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:04,936 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:04,936 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:04,936 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3_dyzkgw
2024-01-21 06:32:04,936 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b26bb22-8bbb-4b53-bf7c-c6a2ba49ca47
2024-01-21 06:32:04,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:04,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:04,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:04,955 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39045
2024-01-21 06:32:04,955 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39045
2024-01-21 06:32:04,955 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40179
2024-01-21 06:32:04,955 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:04,956 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:04,956 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:04,956 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:04,956 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jhg6l7zk
2024-01-21 06:32:04,956 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68df402f-f8a2-4c74-bddb-fabfabfd30ed
2024-01-21 06:32:04,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:04,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:04,962 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:04,963 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33783
2024-01-21 06:32:04,963 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33783
2024-01-21 06:32:04,963 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43721
2024-01-21 06:32:04,963 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:04,963 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:04,963 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:04,963 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:04,963 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s3gtf5gl
2024-01-21 06:32:04,963 - distributed.worker - INFO - Starting Worker plugin PreImport-dad72f14-c3a0-4639-94a8-940ae019ca0b
2024-01-21 06:32:04,963 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-949fdbcc-18cb-4101-b91b-2505b03fe396
2024-01-21 06:32:04,965 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b3a0366-84d7-42bd-b665-f5edadcfb05c
2024-01-21 06:32:05,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:05,001 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:05,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:05,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:05,005 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:05,005 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34641
2024-01-21 06:32:05,006 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34641
2024-01-21 06:32:05,006 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43249
2024-01-21 06:32:05,006 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:05,006 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:05,006 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:05,006 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:05,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-131ss6ve
2024-01-21 06:32:05,006 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3678c7a7-1d79-4ab9-9831-6a22f64a46e2
2024-01-21 06:32:05,006 - distributed.worker - INFO - Starting Worker plugin PreImport-13f84885-2a29-4abf-9f3e-d0f2e8ecccf2
2024-01-21 06:32:05,006 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86862c63-bf67-479c-8a52-fa8662d15315
2024-01-21 06:32:05,008 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:05,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:05,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:05,009 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46463
2024-01-21 06:32:05,009 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46463
2024-01-21 06:32:05,009 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39261
2024-01-21 06:32:05,009 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:05,009 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:05,009 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:05,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:05,010 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_uyiqr6j
2024-01-21 06:32:05,010 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b9367aa-6419-4996-bf59-3d83d6e2a57c
2024-01-21 06:32:05,014 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:05,016 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42905
2024-01-21 06:32:05,016 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42905
2024-01-21 06:32:05,016 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41179
2024-01-21 06:32:05,016 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:05,016 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:05,016 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:05,016 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:05,016 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vw_mn5lx
2024-01-21 06:32:05,016 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd4d29e6-362a-4449-b5ce-eafaeb68e776
2024-01-21 06:32:05,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:05,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:05,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:05,024 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36113
2024-01-21 06:32:05,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36113
2024-01-21 06:32:05,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44103
2024-01-21 06:32:05,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:05,024 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:05,024 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:05,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:05,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pqe36pgb
2024-01-21 06:32:05,025 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b7f135e-3a25-4469-8e16-dc5e4e69158b
2024-01-21 06:32:05,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-175523fd-ba92-460e-ab76-722dcd37dfa4
2024-01-21 06:32:05,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:05,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:05,201 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:05,202 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33819
2024-01-21 06:32:05,202 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33819
2024-01-21 06:32:05,202 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42973
2024-01-21 06:32:05,202 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:05,202 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:05,202 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:05,202 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:05,203 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6u0i2i58
2024-01-21 06:32:05,203 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-711e5506-e176-48c0-a543-6e64363b9477
2024-01-21 06:32:05,203 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a68645a3-8e22-4068-ab67-a0c9ccc572f7
2024-01-21 06:32:06,021 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8c269a5-05e6-4f8e-9676-a6d7dd717b87
2024-01-21 06:32:06,022 - distributed.worker - INFO - Starting Worker plugin PreImport-a7fdd93c-06bd-4d13-9e1c-2e8e477a7375
2024-01-21 06:32:06,023 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d1e35cd9-5672-46bb-bd4b-1c31a2e75f8c
2024-01-21 06:32:07,130 - distributed.worker - INFO - Starting Worker plugin PreImport-a8794d7b-8d73-45db-a5e2-175f28ab92bf
2024-01-21 06:32:07,131 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:07,176 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:07,176 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:07,180 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f7e3721-6918-4b86-a6f1-b25aeab0d6fa
2024-01-21 06:32:07,181 - distributed.worker - INFO - Starting Worker plugin PreImport-1462bd05-46cc-4892-a56e-c074412398be
2024-01-21 06:32:07,181 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,182 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,199 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:07,201 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39045. Reason: nanny-close
2024-01-21 06:32:07,208 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:07,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:07,209 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:07,209 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,210 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:07,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:07,215 - distributed.worker - INFO - Starting Worker plugin PreImport-280afb20-2f13-463c-80d6-aee6c052ac6c
2024-01-21 06:32:07,216 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:07,221 - distributed.worker - INFO - Starting Worker plugin PreImport-b25ca86e-c673-48cc-a3c4-2f0e710d7425
2024-01-21 06:32:07,221 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:07,221 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,221 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:07,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:07,228 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:07,228 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,229 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:07,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f294f896-ffbb-4c9b-bf6d-c89a7231e7bc
2024-01-21 06:32:07,242 - distributed.worker - INFO - Starting Worker plugin PreImport-6b30660b-98ee-4da3-89d0-10fe65ffaecd
2024-01-21 06:32:07,243 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,250 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:07,250 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:07,250 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:07,251 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:07,251 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,251 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:07,251 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33783. Reason: nanny-close
2024-01-21 06:32:07,251 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46463. Reason: nanny-close
2024-01-21 06:32:07,252 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:07,252 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:07,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:07,256 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:07,257 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:07,257 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:07,258 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34641. Reason: nanny-close
2024-01-21 06:32:07,261 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:07,262 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:07,263 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:07,264 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:07,283 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:07,284 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:07,284 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:07,301 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:07,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:07,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:07,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42905. Reason: nanny-close
2024-01-21 06:32:07,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33819. Reason: nanny-close
2024-01-21 06:32:07,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36113. Reason: nanny-close
2024-01-21 06:32:07,307 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:07,308 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:07,309 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:07,311 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:07,311 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:07,313 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:07,417 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:07,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:07,418 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:07,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:07,453 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:07,455 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34261. Reason: nanny-close
2024-01-21 06:32:07,457 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:07,458 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:35220 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-21 06:32:07,621 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65511 parent=65318 started daemon>
2024-01-21 06:32:07,622 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65508 parent=65318 started daemon>
2024-01-21 06:32:07,622 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65503 parent=65318 started daemon>
2024-01-21 06:32:07,622 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65499 parent=65318 started daemon>
2024-01-21 06:32:07,622 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65493 parent=65318 started daemon>
2024-01-21 06:32:07,622 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65488 parent=65318 started daemon>
2024-01-21 06:32:07,622 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65484 parent=65318 started daemon>
2024-01-21 06:32:07,740 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65493 exit status was already read will report exitcode 255
2024-01-21 06:32:07,874 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65488 exit status was already read will report exitcode 255
2024-01-21 06:32:07,909 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65484 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-21 06:32:10,019 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:10,024 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-21 06:32:10,027 - distributed.scheduler - INFO - State start
2024-01-21 06:32:10,048 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:10,049 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:32:10,050 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-21 06:32:10,050 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:10,109 - distributed.scheduler - INFO - Receive client connection: Client-ce188e2a-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:10,122 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58204
2024-01-21 06:32:10,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35461'
2024-01-21 06:32:10,154 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44715'
2024-01-21 06:32:10,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38065'
2024-01-21 06:32:10,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46661'
2024-01-21 06:32:10,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42477'
2024-01-21 06:32:10,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34533'
2024-01-21 06:32:10,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34747'
2024-01-21 06:32:10,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33681'
2024-01-21 06:32:10,479 - distributed.scheduler - INFO - Receive client connection: Client-cd00b68d-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:10,480 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58310
2024-01-21 06:32:12,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:12,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:12,014 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:12,015 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41987
2024-01-21 06:32:12,015 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41987
2024-01-21 06:32:12,015 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33611
2024-01-21 06:32:12,015 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,015 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,015 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:12,015 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:12,015 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vl3gg5cc
2024-01-21 06:32:12,015 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07822d38-0e18-4c66-9905-08b7c464f082
2024-01-21 06:32:12,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:12,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:12,250 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:12,250 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38869
2024-01-21 06:32:12,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:12,251 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38869
2024-01-21 06:32:12,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:12,251 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33571
2024-01-21 06:32:12,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:12,251 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:12,251 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,251 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:12,251 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:12,251 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2nwgnxor
2024-01-21 06:32:12,251 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17603572-09cd-4908-ad36-aba823bb7550
2024-01-21 06:32:12,251 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0112f49-9d78-42a4-bde4-a98e0bb64a83
2024-01-21 06:32:12,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:12,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:12,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44217
2024-01-21 06:32:12,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44217
2024-01-21 06:32:12,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36441
2024-01-21 06:32:12,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,256 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,256 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:12,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:12,256 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-71kluwom
2024-01-21 06:32:12,257 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33547
2024-01-21 06:32:12,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f3dd5a4-af23-4c1c-a8d4-93e4755eeec9
2024-01-21 06:32:12,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33547
2024-01-21 06:32:12,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43857
2024-01-21 06:32:12,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,257 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,257 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:12,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:12,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uvgnwhac
2024-01-21 06:32:12,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73339c8c-75a5-4f4e-aa7a-af83129eac24
2024-01-21 06:32:12,257 - distributed.worker - INFO - Starting Worker plugin PreImport-e328de46-83fb-41c8-91cd-08fdd6fdfc20
2024-01-21 06:32:12,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6a6dbf8-354a-4187-bf9b-370582f317b4
2024-01-21 06:32:12,257 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:12,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:12,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:12,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:12,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:12,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:12,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:12,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:12,263 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:12,264 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42149
2024-01-21 06:32:12,264 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42149
2024-01-21 06:32:12,264 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:12,264 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34897
2024-01-21 06:32:12,264 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,264 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:12,264 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:12,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:12,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vgi9kf6b
2024-01-21 06:32:12,264 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:12,264 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7c5e5ba-0cec-403d-85c1-c30ec0b8cb1f
2024-01-21 06:32:12,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39875
2024-01-21 06:32:12,265 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39875
2024-01-21 06:32:12,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40165
2024-01-21 06:32:12,265 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38467
2024-01-21 06:32:12,265 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,265 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40165
2024-01-21 06:32:12,265 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,265 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33661
2024-01-21 06:32:12,265 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:12,265 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,265 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,265 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:12,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j03nb4ht
2024-01-21 06:32:12,265 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:12,265 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:12,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37133
2024-01-21 06:32:12,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9gkelz31
2024-01-21 06:32:12,265 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37133
2024-01-21 06:32:12,265 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46349
2024-01-21 06:32:12,265 - distributed.worker - INFO - Starting Worker plugin PreImport-63cda74e-7ff4-4e10-8044-83ec81cde508
2024-01-21 06:32:12,265 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,265 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,266 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5a5af58-bc5d-47c9-8396-3a73e82cf9a4
2024-01-21 06:32:12,266 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c619c97f-8702-499b-af76-f84d9b2be291
2024-01-21 06:32:12,266 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:12,266 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:12,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a736a5f8-c573-43ec-8a1d-c79d91dc7b22
2024-01-21 06:32:12,266 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xhznfvtb
2024-01-21 06:32:12,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9228e5da-e6b0-42bc-a69a-0f0b58bbfece
2024-01-21 06:32:12,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e101a2f6-5116-4788-b221-7829b58a26a8
2024-01-21 06:32:12,471 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e57a2eba-dd4f-4f97-917a-82c530aed203
2024-01-21 06:32:12,472 - distributed.worker - INFO - Starting Worker plugin PreImport-31cc6242-01bc-4277-9cf7-4319b238b5df
2024-01-21 06:32:12,472 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,501 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41987', status: init, memory: 0, processing: 0>
2024-01-21 06:32:12,502 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41987
2024-01-21 06:32:12,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58334
2024-01-21 06:32:12,503 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:12,504 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:12,504 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:12,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:12,854 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-21 06:32:12,864 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:32:12,868 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:12,870 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:12,873 - distributed.scheduler - INFO - Remove client Client-ce188e2a-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:12,873 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58204; closing.
2024-01-21 06:32:12,874 - distributed.scheduler - INFO - Remove client Client-ce188e2a-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:12,874 - distributed.scheduler - INFO - Close client connection: Client-ce188e2a-b826-11ee-b5f1-d8c49764f6bb
2024-01-21 06:32:14,054 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,078 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33547', status: init, memory: 0, processing: 0>
2024-01-21 06:32:14,079 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33547
2024-01-21 06:32:14,079 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58354
2024-01-21 06:32:14,080 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:14,080 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:14,080 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:14,083 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6cead2b5-c599-4c23-b616-6f4ef24b81f7
2024-01-21 06:32:14,083 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62428de8-81e0-46b8-b733-04c55d2a95f7
2024-01-21 06:32:14,084 - distributed.worker - INFO - Starting Worker plugin PreImport-30081736-4473-4e23-82b0-58fddef37d19
2024-01-21 06:32:14,084 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,084 - distributed.worker - INFO - Starting Worker plugin PreImport-cc751fa1-adff-41a2-b961-b639ba6785dd
2024-01-21 06:32:14,084 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,094 - distributed.worker - INFO - Starting Worker plugin PreImport-cbf8bda1-9248-4b7b-9593-c06fab0a7132
2024-01-21 06:32:14,095 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,096 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ab8e3b8-946c-46f1-8cae-ca668da99846
2024-01-21 06:32:14,097 - distributed.worker - INFO - Starting Worker plugin PreImport-db218b02-cb9a-49b1-8e7b-10b300cf08df
2024-01-21 06:32:14,098 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,107 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42149', status: init, memory: 0, processing: 0>
2024-01-21 06:32:14,107 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42149
2024-01-21 06:32:14,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58366
2024-01-21 06:32:14,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:14,108 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44217', status: init, memory: 0, processing: 0>
2024-01-21 06:32:14,109 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:14,109 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,109 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44217
2024-01-21 06:32:14,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58380
2024-01-21 06:32:14,110 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:14,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:14,111 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:14,111 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,110 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:14,116 - distributed.worker - INFO - Starting Worker plugin PreImport-faca9a81-347d-440a-837c-de7974f736f1
2024-01-21 06:32:14,117 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,129 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38869', status: init, memory: 0, processing: 0>
2024-01-21 06:32:14,129 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38869
2024-01-21 06:32:14,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58384
2024-01-21 06:32:14,131 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:14,132 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:14,132 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,134 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:14,139 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37133', status: init, memory: 0, processing: 0>
2024-01-21 06:32:14,140 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37133
2024-01-21 06:32:14,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58400
2024-01-21 06:32:14,141 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40165', status: init, memory: 0, processing: 0>
2024-01-21 06:32:14,141 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40165
2024-01-21 06:32:14,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58420
2024-01-21 06:32:14,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:14,142 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:14,143 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:14,143 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,143 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:14,143 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,144 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:14,145 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:14,147 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39875', status: init, memory: 0, processing: 0>
2024-01-21 06:32:14,148 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39875
2024-01-21 06:32:14,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58408
2024-01-21 06:32:14,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:14,150 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:14,151 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:14,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:14,183 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:14,183 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:14,184 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:14,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:14,186 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:14,187 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:14,187 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:14,188 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:14,191 - distributed.scheduler - INFO - Remove client Client-cd00b68d-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:14,192 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58310; closing.
2024-01-21 06:32:14,192 - distributed.scheduler - INFO - Remove client Client-cd00b68d-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:14,193 - distributed.scheduler - INFO - Close client connection: Client-cd00b68d-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:14,194 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35461'. Reason: nanny-close
2024-01-21 06:32:14,194 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,195 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44715'. Reason: nanny-close
2024-01-21 06:32:14,196 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,196 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38065'. Reason: nanny-close
2024-01-21 06:32:14,196 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41987. Reason: nanny-close
2024-01-21 06:32:14,196 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,196 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46661'. Reason: nanny-close
2024-01-21 06:32:14,197 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38869. Reason: nanny-close
2024-01-21 06:32:14,197 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42477'. Reason: nanny-close
2024-01-21 06:32:14,197 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42149. Reason: nanny-close
2024-01-21 06:32:14,197 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34533'. Reason: nanny-close
2024-01-21 06:32:14,197 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33547. Reason: nanny-close
2024-01-21 06:32:14,198 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34747'. Reason: nanny-close
2024-01-21 06:32:14,198 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,198 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39875. Reason: nanny-close
2024-01-21 06:32:14,198 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58334; closing.
2024-01-21 06:32:14,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33681'. Reason: nanny-close
2024-01-21 06:32:14,198 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37133. Reason: nanny-close
2024-01-21 06:32:14,199 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41987', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.1989827')
2024-01-21 06:32:14,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:14,199 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,199 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,199 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40165. Reason: nanny-close
2024-01-21 06:32:14,199 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,199 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44217. Reason: nanny-close
2024-01-21 06:32:14,199 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:14,200 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:14,200 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,200 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:14,200 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58354; closing.
2024-01-21 06:32:14,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58384; closing.
2024-01-21 06:32:14,201 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:14,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58366; closing.
2024-01-21 06:32:14,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:14,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33547', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.2021675')
2024-01-21 06:32:14,202 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:14,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38869', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.2025602')
2024-01-21 06:32:14,202 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:14,203 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42149', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.202946')
2024-01-21 06:32:14,203 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58408; closing.
2024-01-21 06:32:14,203 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:14,203 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:14,203 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.2039173')
2024-01-21 06:32:14,204 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58400; closing.
2024-01-21 06:32:14,204 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58420; closing.
2024-01-21 06:32:14,205 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37133', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.2050426')
2024-01-21 06:32:14,205 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.205447')
2024-01-21 06:32:14,205 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58380; closing.
2024-01-21 06:32:14,206 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44217', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.2061696')
2024-01-21 06:32:14,206 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:14,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35189', status: init, memory: 0, processing: 0>
2024-01-21 06:32:14,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35189
2024-01-21 06:32:14,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58424
2024-01-21 06:32:14,533 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58424; closing.
2024-01-21 06:32:14,533 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35189', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818734.5335386')
2024-01-21 06:32:14,533 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:15,110 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:15,110 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:15,111 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:15,112 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:15,112 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-21 06:32:17,174 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:17,179 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-21 06:32:17,183 - distributed.scheduler - INFO - State start
2024-01-21 06:32:17,205 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:17,206 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:32:17,207 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-21 06:32:17,207 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:17,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39175'
2024-01-21 06:32:18,798 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:18,798 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:19,270 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:19,270 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38715
2024-01-21 06:32:19,270 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38715
2024-01-21 06:32:19,271 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-21 06:32:19,271 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:19,271 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:19,271 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:19,271 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:32:19,271 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z4xxk6hk
2024-01-21 06:32:19,271 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1167c012-030f-4e61-acc7-31f36720ac07
2024-01-21 06:32:19,271 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10e30d1e-bf5a-4ae3-935a-f42592111305
2024-01-21 06:32:19,272 - distributed.worker - INFO - Starting Worker plugin PreImport-b8d9a418-ee00-4bc5-a37e-66996c6b2965
2024-01-21 06:32:19,272 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:19,328 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38715', status: init, memory: 0, processing: 0>
2024-01-21 06:32:19,343 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38715
2024-01-21 06:32:19,343 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58592
2024-01-21 06:32:19,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:19,346 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:19,346 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:19,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:20,063 - distributed.scheduler - INFO - Receive client connection: Client-d14a8a0f-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:20,064 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59038
2024-01-21 06:32:20,070 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:20,076 - distributed.scheduler - INFO - Remove client Client-d14a8a0f-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:20,076 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59038; closing.
2024-01-21 06:32:20,077 - distributed.scheduler - INFO - Remove client Client-d14a8a0f-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:20,077 - distributed.scheduler - INFO - Close client connection: Client-d14a8a0f-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:20,078 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39175'. Reason: nanny-close
2024-01-21 06:32:20,078 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:20,080 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38715. Reason: nanny-close
2024-01-21 06:32:20,081 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58592; closing.
2024-01-21 06:32:20,081 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:20,082 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818740.082061')
2024-01-21 06:32:20,082 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:20,083 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:20,794 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:20,794 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:20,794 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:20,796 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:20,796 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-21 06:32:24,750 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:24,754 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42623 instead
  warnings.warn(
2024-01-21 06:32:24,758 - distributed.scheduler - INFO - State start
2024-01-21 06:32:24,779 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:24,780 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:32:24,780 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42623/status
2024-01-21 06:32:24,780 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:24,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34351'
2024-01-21 06:32:26,520 - distributed.scheduler - INFO - Receive client connection: Client-d5d54076-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:26,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59146
2024-01-21 06:32:26,723 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:26,723 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:27,301 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:27,302 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42993
2024-01-21 06:32:27,302 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42993
2024-01-21 06:32:27,302 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42625
2024-01-21 06:32:27,302 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:27,302 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:27,302 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:27,302 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:32:27,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7njux0uj
2024-01-21 06:32:27,303 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01ff6879-ff96-4970-a804-bf5245add989
2024-01-21 06:32:27,303 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7d6aec6-d497-4816-bd3c-d9ca14991975
2024-01-21 06:32:27,303 - distributed.worker - INFO - Starting Worker plugin PreImport-790177bc-f82b-498d-9459-142ac59ba9a1
2024-01-21 06:32:27,304 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:27,357 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42993', status: init, memory: 0, processing: 0>
2024-01-21 06:32:27,358 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42993
2024-01-21 06:32:27,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59156
2024-01-21 06:32:27,358 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:27,359 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:27,359 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:27,360 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:27,458 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:27,461 - distributed.scheduler - INFO - Remove client Client-d5d54076-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:27,462 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59146; closing.
2024-01-21 06:32:27,462 - distributed.scheduler - INFO - Remove client Client-d5d54076-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:27,463 - distributed.scheduler - INFO - Close client connection: Client-d5d54076-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:27,463 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34351'. Reason: nanny-close
2024-01-21 06:32:27,464 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:27,465 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42993. Reason: nanny-close
2024-01-21 06:32:27,467 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59156; closing.
2024-01-21 06:32:27,467 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:27,467 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818747.4674337')
2024-01-21 06:32:27,467 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:27,468 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:28,179 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:28,179 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:28,180 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:28,182 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:28,182 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-21 06:32:30,440 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:30,445 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32807 instead
  warnings.warn(
2024-01-21 06:32:30,449 - distributed.scheduler - INFO - State start
2024-01-21 06:32:30,472 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:30,473 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:32:30,474 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32807/status
2024-01-21 06:32:30,474 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:33,455 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:42176'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42176>: Stream is closed
2024-01-21 06:32:33,721 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:33,721 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:33,722 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:33,722 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:33,723 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-21 06:32:35,911 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:35,915 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-21 06:32:35,918 - distributed.scheduler - INFO - State start
2024-01-21 06:32:35,941 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:35,942 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-21 06:32:35,943 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-21 06:32:35,943 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:36,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36081'
2024-01-21 06:32:36,512 - distributed.scheduler - INFO - Receive client connection: Client-dc5efbb4-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:36,532 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35430
2024-01-21 06:32:37,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:37,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:37,758 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:37,759 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38317
2024-01-21 06:32:37,759 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38317
2024-01-21 06:32:37,759 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43397
2024-01-21 06:32:37,759 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-21 06:32:37,759 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:37,759 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:37,759 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:32:37,759 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-s664urnd
2024-01-21 06:32:37,759 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1853399d-137b-4ba5-aef1-b3c1c2e8da15
2024-01-21 06:32:37,759 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be547c7a-8614-4453-a114-8389878934eb
2024-01-21 06:32:37,759 - distributed.worker - INFO - Starting Worker plugin PreImport-0630c325-2175-43ce-9428-6a79ec604e7b
2024-01-21 06:32:37,760 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:37,810 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38317', status: init, memory: 0, processing: 0>
2024-01-21 06:32:37,811 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38317
2024-01-21 06:32:37,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35446
2024-01-21 06:32:37,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:37,812 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-21 06:32:37,812 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:37,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-21 06:32:37,859 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:37,864 - distributed.scheduler - INFO - Remove client Client-dc5efbb4-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:37,864 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35430; closing.
2024-01-21 06:32:37,864 - distributed.scheduler - INFO - Remove client Client-dc5efbb4-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:37,865 - distributed.scheduler - INFO - Close client connection: Client-dc5efbb4-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:37,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36081'. Reason: nanny-close
2024-01-21 06:32:37,866 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:37,867 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38317. Reason: nanny-close
2024-01-21 06:32:37,868 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-21 06:32:37,868 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35446; closing.
2024-01-21 06:32:37,869 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38317', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818757.8691695')
2024-01-21 06:32:37,869 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:37,870 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:38,381 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:38,381 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:38,382 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:38,384 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-21 06:32:38,384 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-21 06:32:40,727 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:40,731 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37303 instead
  warnings.warn(
2024-01-21 06:32:40,736 - distributed.scheduler - INFO - State start
2024-01-21 06:32:40,824 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:40,825 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:32:40,826 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37303/status
2024-01-21 06:32:40,826 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:40,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38099'
2024-01-21 06:32:40,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35445'
2024-01-21 06:32:40,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45481'
2024-01-21 06:32:40,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43947'
2024-01-21 06:32:40,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45007'
2024-01-21 06:32:40,998 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40283'
2024-01-21 06:32:41,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36131'
2024-01-21 06:32:41,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45429'
2024-01-21 06:32:42,254 - distributed.scheduler - INFO - Receive client connection: Client-df4093af-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:42,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50580
2024-01-21 06:32:43,002 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:43,002 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:43,007 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:43,007 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:43,007 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:43,007 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:43,007 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:43,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:43,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:43,008 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43157
2024-01-21 06:32:43,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43157
2024-01-21 06:32:43,008 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39295
2024-01-21 06:32:43,008 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:43,008 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:43,008 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:43,008 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:43,008 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9nlxf4uo
2024-01-21 06:32:43,009 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68649539-a349-45d3-be93-156c75063e30
2024-01-21 06:32:43,011 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:43,012 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:43,012 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:43,012 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43489
2024-01-21 06:32:43,012 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43489
2024-01-21 06:32:43,012 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36235
2024-01-21 06:32:43,012 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:43,012 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:43,013 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:43,013 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:43,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nsvppm0e
2024-01-21 06:32:43,013 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46749
2024-01-21 06:32:43,013 - distributed.worker - INFO - Starting Worker plugin PreImport-8326de26-2590-4077-96ad-6a36e0414c95
2024-01-21 06:32:43,013 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36085
2024-01-21 06:32:43,013 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46749
2024-01-21 06:32:43,013 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36085
2024-01-21 06:32:43,013 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46335
2024-01-21 06:32:43,013 - distributed.worker - INFO - Starting Worker plugin RMMSetup-47a2365c-f8a2-4460-adcb-fb8f67c5bae6
2024-01-21 06:32:43,013 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39957
2024-01-21 06:32:43,013 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:43,013 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:43,013 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:43,013 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:43,013 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:43,013 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:43,013 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:43,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-atup6mfe
2024-01-21 06:32:43,013 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:43,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_7e50z7k
2024-01-21 06:32:43,013 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c689a45d-ffc3-4bda-8f3f-73c0033d2b44
2024-01-21 06:32:43,014 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d8bde6f-4dbf-471b-b525-92fe41b7316f
2024-01-21 06:32:43,014 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c20c229-701c-43ae-9aff-0a87fb9d4f01
2024-01-21 06:32:43,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:43,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:43,021 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:43,021 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:43,021 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:43,022 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:43,022 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:43,022 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46557
2024-01-21 06:32:43,022 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46557
2024-01-21 06:32:43,022 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36629
2024-01-21 06:32:43,022 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:43,022 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:43,023 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:43,023 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:43,023 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jbcvgpyi
2024-01-21 06:32:43,023 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6f50ed0-9cc7-45ef-ae0b-426fc39cb849
2024-01-21 06:32:43,023 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:43,023 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:43,023 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80f6b9a2-23da-4520-acb2-61127f3da968
2024-01-21 06:32:43,025 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:43,026 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:43,026 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41153
2024-01-21 06:32:43,026 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41153
2024-01-21 06:32:43,026 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46767
2024-01-21 06:32:43,026 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:43,026 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:43,027 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:43,027 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:43,027 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3mbrleau
2024-01-21 06:32:43,027 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33157
2024-01-21 06:32:43,027 - distributed.worker - INFO - Starting Worker plugin PreImport-17f12937-6bde-4008-bb16-98f7a54d178d
2024-01-21 06:32:43,027 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33157
2024-01-21 06:32:43,027 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45515
2024-01-21 06:32:43,027 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec54abe0-094e-4e5e-b8d4-a6953eb806f5
2024-01-21 06:32:43,027 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:43,027 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:43,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1e4a332-d5a2-4d80-b3b1-ab09f5e2bcdb
2024-01-21 06:32:43,027 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:43,027 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:43,027 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vg4a06pd
2024-01-21 06:32:43,027 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:43,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48a62020-4e0b-4d23-9195-a6c1830208ed
2024-01-21 06:32:43,028 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40195
2024-01-21 06:32:43,028 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40195
2024-01-21 06:32:43,028 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38155
2024-01-21 06:32:43,028 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:43,028 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:43,028 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:43,029 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:32:43,029 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hom63qf2
2024-01-21 06:32:43,029 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4139b58a-3f87-4652-bc80-e5f2fe3f1230
2024-01-21 06:32:46,157 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0033d10-fd22-460e-84c1-b0811298f051
2024-01-21 06:32:46,158 - distributed.worker - INFO - Starting Worker plugin PreImport-016f34c8-0ca4-4703-a4e3-159f38cc67b1
2024-01-21 06:32:46,159 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,191 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36085', status: init, memory: 0, processing: 0>
2024-01-21 06:32:46,193 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36085
2024-01-21 06:32:46,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50596
2024-01-21 06:32:46,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:46,196 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:46,196 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:46,267 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cda1d470-a35e-4128-83e9-7e09f3761a79
2024-01-21 06:32:46,269 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,274 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,295 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f63ecfe-6588-4c0b-9d64-c642a243459f
2024-01-21 06:32:46,296 - distributed.worker - INFO - Starting Worker plugin PreImport-02c010e6-a7bb-4110-b31a-f987b3530234
2024-01-21 06:32:46,297 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,298 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41153', status: init, memory: 0, processing: 0>
2024-01-21 06:32:46,299 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41153
2024-01-21 06:32:46,300 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:46,301 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:46,301 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50600
2024-01-21 06:32:46,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:46,309 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43489', status: init, memory: 0, processing: 0>
2024-01-21 06:32:46,309 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43489
2024-01-21 06:32:46,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50598
2024-01-21 06:32:46,309 - distributed.worker - INFO - Starting Worker plugin PreImport-8233f34a-479d-4edd-9ac2-bcd67fd60fcd
2024-01-21 06:32:46,311 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,311 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:46,312 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:46,312 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:46,336 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-295b1738-7e11-4f7d-882c-574e07f41a2c
2024-01-21 06:32:46,337 - distributed.worker - INFO - Starting Worker plugin PreImport-9c8d7ea8-3870-412f-8263-e23e87a38108
2024-01-21 06:32:46,337 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,338 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33157', status: init, memory: 0, processing: 0>
2024-01-21 06:32:46,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33157
2024-01-21 06:32:46,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50614
2024-01-21 06:32:46,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:46,342 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:46,342 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,344 - distributed.worker - INFO - Starting Worker plugin PreImport-4b1dff1a-4973-4a05-b66d-e1333bb04ca1
2024-01-21 06:32:46,345 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,347 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46557', status: init, memory: 0, processing: 0>
2024-01-21 06:32:46,348 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46557
2024-01-21 06:32:46,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50630
2024-01-21 06:32:46,349 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14ed6b3c-d7c0-447d-a41e-3634754e0860
2024-01-21 06:32:46,349 - distributed.worker - INFO - Starting Worker plugin PreImport-fd40a635-2609-4946-8926-4bc39514f885
2024-01-21 06:32:46,350 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,350 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:46,351 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:46,351 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:46,360 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:46,367 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40195', status: init, memory: 0, processing: 0>
2024-01-21 06:32:46,368 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40195
2024-01-21 06:32:46,368 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50642
2024-01-21 06:32:46,369 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:46,370 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:46,370 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:46,374 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46749', status: init, memory: 0, processing: 0>
2024-01-21 06:32:46,374 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46749
2024-01-21 06:32:46,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50644
2024-01-21 06:32:46,375 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43157', status: init, memory: 0, processing: 0>
2024-01-21 06:32:46,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:46,376 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43157
2024-01-21 06:32:46,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50652
2024-01-21 06:32:46,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:46,376 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:46,378 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:46,378 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:46,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:46,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:46,391 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:46,391 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:46,391 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:46,392 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:46,392 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:46,392 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:46,392 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:46,395 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-21 06:32:46,408 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:46,408 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:46,408 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:46,408 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:46,408 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:46,409 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:46,409 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:46,409 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:46,414 - distributed.scheduler - INFO - Remove client Client-df4093af-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:46,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50580; closing.
2024-01-21 06:32:46,415 - distributed.scheduler - INFO - Remove client Client-df4093af-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:46,415 - distributed.scheduler - INFO - Close client connection: Client-df4093af-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:46,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38099'. Reason: nanny-close
2024-01-21 06:32:46,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:46,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35445'. Reason: nanny-close
2024-01-21 06:32:46,417 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:46,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45481'. Reason: nanny-close
2024-01-21 06:32:46,417 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43157. Reason: nanny-close
2024-01-21 06:32:46,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:46,418 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43947'. Reason: nanny-close
2024-01-21 06:32:46,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:46,418 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45007'. Reason: nanny-close
2024-01-21 06:32:46,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46749. Reason: nanny-close
2024-01-21 06:32:46,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:46,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36085. Reason: nanny-close
2024-01-21 06:32:46,419 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40283'. Reason: nanny-close
2024-01-21 06:32:46,419 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:46,419 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36131'. Reason: nanny-close
2024-01-21 06:32:46,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43489. Reason: nanny-close
2024-01-21 06:32:46,419 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:46,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41153. Reason: nanny-close
2024-01-21 06:32:46,419 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45429'. Reason: nanny-close
2024-01-21 06:32:46,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50652; closing.
2024-01-21 06:32:46,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:46,420 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40195. Reason: nanny-close
2024-01-21 06:32:46,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:46,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43157', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818766.420176')
2024-01-21 06:32:46,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:46,420 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46557. Reason: nanny-close
2024-01-21 06:32:46,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33157. Reason: nanny-close
2024-01-21 06:32:46,421 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:46,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:46,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:46,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:46,421 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:46,422 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50644; closing.
2024-01-21 06:32:46,422 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:46,422 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:46,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50596; closing.
2024-01-21 06:32:46,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46749', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818766.4232407')
2024-01-21 06:32:46,423 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:46,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50600; closing.
2024-01-21 06:32:46,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:46,423 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:46,423 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:46,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50598; closing.
2024-01-21 06:32:46,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:46,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36085', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818766.4242988')
2024-01-21 06:32:46,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41153', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818766.4246433')
2024-01-21 06:32:46,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43489', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818766.4249227')
2024-01-21 06:32:46,425 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50642; closing.
2024-01-21 06:32:46,425 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:46,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40195', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818766.4259086')
2024-01-21 06:32:46,425 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:46,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50630; closing.
2024-01-21 06:32:46,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50614; closing.
2024-01-21 06:32:46,426 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46557', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818766.4268472')
2024-01-21 06:32:46,427 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33157', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818766.4272244')
2024-01-21 06:32:46,427 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:47,583 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:47,583 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:47,583 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:47,585 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:47,585 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-21 06:32:49,671 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:49,675 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-21 06:32:49,678 - distributed.scheduler - INFO - State start
2024-01-21 06:32:49,699 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:49,700 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:32:49,701 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-21 06:32:49,701 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:49,751 - distributed.scheduler - INFO - Receive client connection: Client-e4b74fb2-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:49,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50756
2024-01-21 06:32:49,837 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41529'
2024-01-21 06:32:51,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:51,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:51,497 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:51,498 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37995
2024-01-21 06:32:51,498 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37995
2024-01-21 06:32:51,498 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33991
2024-01-21 06:32:51,498 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:51,498 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:51,498 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:51,498 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:32:51,498 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ag1jhwj4
2024-01-21 06:32:51,498 - distributed.worker - INFO - Starting Worker plugin PreImport-4a49fff4-21f3-488c-88f3-2e274aed452a
2024-01-21 06:32:51,498 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5611479c-a043-4f15-b2b2-a1aacfa51c46
2024-01-21 06:32:51,775 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1dd71c33-e5c6-4702-bd6a-3a2ddd71fce2
2024-01-21 06:32:51,775 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:51,831 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37995', status: init, memory: 0, processing: 0>
2024-01-21 06:32:51,832 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37995
2024-01-21 06:32:51,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42356
2024-01-21 06:32:51,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:51,834 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:51,834 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:51,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:51,910 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:32:51,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:51,918 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:51,921 - distributed.scheduler - INFO - Remove client Client-e4b74fb2-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:51,921 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50756; closing.
2024-01-21 06:32:51,922 - distributed.scheduler - INFO - Remove client Client-e4b74fb2-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:51,922 - distributed.scheduler - INFO - Close client connection: Client-e4b74fb2-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:51,923 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41529'. Reason: nanny-close
2024-01-21 06:32:51,924 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:51,925 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37995. Reason: nanny-close
2024-01-21 06:32:51,927 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42356; closing.
2024-01-21 06:32:51,927 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:51,927 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37995', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818771.9278004')
2024-01-21 06:32:51,928 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:51,928 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:52,789 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:52,790 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:52,790 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:52,791 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:52,792 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-21 06:32:54,975 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:54,979 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-21 06:32:54,983 - distributed.scheduler - INFO - State start
2024-01-21 06:32:55,031 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-21 06:32:55,032 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-21 06:32:55,032 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-21 06:32:55,032 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-21 06:32:55,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41695'
2024-01-21 06:32:56,130 - distributed.scheduler - INFO - Receive client connection: Client-e7c8f1b1-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:56,146 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42446
2024-01-21 06:32:56,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:32:56,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:32:56,684 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:32:56,685 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37293
2024-01-21 06:32:56,685 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37293
2024-01-21 06:32:56,685 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42223
2024-01-21 06:32:56,685 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-21 06:32:56,685 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:56,685 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:32:56,685 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:32:56,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-de17tlbv
2024-01-21 06:32:56,685 - distributed.worker - INFO - Starting Worker plugin PreImport-bed50769-9145-4b5d-81c5-4b0f29b1fa61
2024-01-21 06:32:56,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee49e079-8596-4c6a-b2c2-3eab6121f328
2024-01-21 06:32:57,025 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1bed027-ea73-4f3e-a085-841be8184741
2024-01-21 06:32:57,025 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:57,096 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37293', status: init, memory: 0, processing: 0>
2024-01-21 06:32:57,097 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37293
2024-01-21 06:32:57,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42470
2024-01-21 06:32:57,097 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:32:57,098 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-21 06:32:57,098 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:32:57,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-21 06:32:57,168 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-21 06:32:57,174 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-21 06:32:57,178 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:57,179 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:32:57,182 - distributed.scheduler - INFO - Remove client Client-e7c8f1b1-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:57,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42446; closing.
2024-01-21 06:32:57,182 - distributed.scheduler - INFO - Remove client Client-e7c8f1b1-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:57,182 - distributed.scheduler - INFO - Close client connection: Client-e7c8f1b1-b826-11ee-ba92-d8c49764f6bb
2024-01-21 06:32:57,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41695'. Reason: nanny-close
2024-01-21 06:32:57,184 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-21 06:32:57,184 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37293. Reason: nanny-close
2024-01-21 06:32:57,186 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42470; closing.
2024-01-21 06:32:57,186 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-21 06:32:57,186 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37293', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705818777.1866071')
2024-01-21 06:32:57,186 - distributed.scheduler - INFO - Lost all workers
2024-01-21 06:32:57,187 - distributed.nanny - INFO - Worker closed
2024-01-21 06:32:57,899 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-21 06:32:57,899 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-21 06:32:57,900 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-21 06:32:57,900 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-21 06:32:57,901 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45453 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36467 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41603 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46081 instead
  warnings.warn(
[dgx13:68643:0:68733] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x7fb9a32d389d)
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43621 instead
  warnings.warn(
2024-01-21 06:35:00,536 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40335 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36163 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45513 instead
  warnings.warn(
2024-01-21 06:35:53,738 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40623 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37379 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44367 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38789 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40735 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] [1705819119.909144] [dgx13:73853:0]            sock.c:481  UCX  ERROR bind(fd=133 addr=0.0.0.0:46539) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33325 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36175 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46343 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35535 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35053 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33915 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42487 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36767 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44647 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36823 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38905 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36269 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44129 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35825 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40257 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33621 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] 2024-01-21 06:48:51,311 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40081 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44277 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33953 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44435 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41765 instead
  warnings.warn(
2024-01-21 06:51:21,919 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:34300 remote=tcp://127.0.0.1:36775>: Stream is closed
2024-01-21 06:51:21,920 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:34302 remote=tcp://127.0.0.1:36775>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39753 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39741 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42355 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45449 instead
  warnings.warn(
2024-01-21 06:52:15,435 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39487 instead
  warnings.warn(
2024-01-21 06:52:23,443 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-21 06:52:23,443 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37143 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43893 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34131 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45993 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33215 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33417 instead
  warnings.warn(
[1705820040.416344] [dgx13:87309:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:39746) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41367 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44505 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33461 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46541 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39583 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33227 instead
  warnings.warn(
[1705820139.687049] [dgx13:88776:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:42023) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] 2024-01-21 06:56:06,863 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f479424a0c0, tag: 0x954513e91a563578, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f479424a0c0, tag: 0x954513e91a563578, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42871 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34261 instead
  warnings.warn(
2024-01-21 06:56:33,250 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
[1705820193.563547] [dgx13:89581] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44779 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42397 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32851 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37547 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35339 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36119 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1705820255.084631] [dgx13:89987:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:33078) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-21 06:58:21,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:21,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:21,291 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:21,291 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:21,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:21,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:21,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:21,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:21,457 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:21,458 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:21,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:21,549 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:21,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:21,598 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:21,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:58:21,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:58:21,997 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:21,998 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33359
2024-01-21 06:58:21,998 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33359
2024-01-21 06:58:21,998 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45059
2024-01-21 06:58:21,998 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36355
2024-01-21 06:58:21,998 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:21,998 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:21,999 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-25xe_daq
2024-01-21 06:58:21,999 - distributed.worker - INFO - Starting Worker plugin PreImport-448c8f5a-b724-4925-b925-50a60dd3f89f
2024-01-21 06:58:21,999 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ac86ae5-5f61-4d96-b082-65ffaff2e774
2024-01-21 06:58:21,999 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf40c1cc-132e-457e-9a80-4d67f29a20fc
2024-01-21 06:58:21,999 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:22,030 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33559
2024-01-21 06:58:22,030 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33559
2024-01-21 06:58:22,030 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45699
2024-01-21 06:58:22,030 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,030 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,030 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:22,030 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-utfmv_jn
2024-01-21 06:58:22,030 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b999df2f-4659-4c5e-9e09-c87e9d9e7971
2024-01-21 06:58:22,031 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f88c9b4-74ed-4912-b1af-8b54dad67b93
2024-01-21 06:58:22,031 - distributed.worker - INFO - Starting Worker plugin PreImport-cdef2c82-314e-4bb4-8218-32ffac5c4205
2024-01-21 06:58:22,031 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,068 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:22,069 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41209
2024-01-21 06:58:22,069 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41209
2024-01-21 06:58:22,069 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33433
2024-01-21 06:58:22,069 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,069 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:22,069 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ts1q20gk
2024-01-21 06:58:22,070 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d986af41-768d-4e71-a2e2-5b3e6e0f69cc
2024-01-21 06:58:22,070 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db9d7b61-0faa-48f7-8577-f4d48ebe67b0
2024-01-21 06:58:22,071 - distributed.worker - INFO - Starting Worker plugin PreImport-ce3e0575-9c60-4b2e-9ee3-8a4099065fac
2024-01-21 06:58:22,071 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:22,077 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,077 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36355
2024-01-21 06:58:22,091 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:22,092 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38731
2024-01-21 06:58:22,092 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38731
2024-01-21 06:58:22,092 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41445
2024-01-21 06:58:22,092 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,092 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,092 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:22,092 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ejyrgqq0
2024-01-21 06:58:22,092 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a200256-1fe5-4320-bb48-9aa50416f956
2024-01-21 06:58:22,093 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08680af7-9b7b-4f71-9dba-7f7f63eb3be4
2024-01-21 06:58:22,093 - distributed.worker - INFO - Starting Worker plugin PreImport-60bbad66-051b-4a0c-9964-19638aea344b
2024-01-21 06:58:22,093 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:22,105 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,105 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36355
2024-01-21 06:58:22,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:22,169 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,169 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:22,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36355
2024-01-21 06:58:22,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36655
2024-01-21 06:58:22,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36655
2024-01-21 06:58:22,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39613
2024-01-21 06:58:22,171 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,171 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,171 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:22,171 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3c1oq_us
2024-01-21 06:58:22,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4609490d-3234-4f26-8f7d-3732dcd86072
2024-01-21 06:58:22,172 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aaada744-467f-4772-a82e-8d5ff9ca1d16
2024-01-21 06:58:22,172 - distributed.worker - INFO - Starting Worker plugin PreImport-f829db29-3bc7-4909-ab6a-ec3367818eff
2024-01-21 06:58:22,172 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,223 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:22,224 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45173
2024-01-21 06:58:22,224 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45173
2024-01-21 06:58:22,224 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41207
2024-01-21 06:58:22,224 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,224 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,224 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:22,224 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u64npq1h
2024-01-21 06:58:22,225 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ce043a4-096f-4dc9-95ac-3c2160d43420
2024-01-21 06:58:22,225 - distributed.worker - INFO - Starting Worker plugin PreImport-cf86ce7e-1923-465f-af3e-149f716123c7
2024-01-21 06:58:22,225 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e3f5e23-69ae-4b8a-bc25-0542df24175d
2024-01-21 06:58:22,225 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:22,248 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,249 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36355
2024-01-21 06:58:22,282 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:22,283 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33727
2024-01-21 06:58:22,284 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33727
2024-01-21 06:58:22,284 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39609
2024-01-21 06:58:22,284 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,284 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,284 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:22,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qh4s2dpw
2024-01-21 06:58:22,284 - distributed.worker - INFO - Starting Worker plugin PreImport-bc6e9fee-871c-4afa-811f-9106b397edbd
2024-01-21 06:58:22,285 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae9a3e67-0997-4c98-a40f-ad08156ea942
2024-01-21 06:58:22,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3305ebe1-3177-4e6d-9451-3d9e9f0bf121
2024-01-21 06:58:22,286 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,299 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:58:22,300 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42643
2024-01-21 06:58:22,300 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42643
2024-01-21 06:58:22,300 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35931
2024-01-21 06:58:22,300 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,301 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,301 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:58:22,301 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nzrm1mr7
2024-01-21 06:58:22,301 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ad85018-2ca0-4e55-bdbe-80e288385986
2024-01-21 06:58:22,301 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47e4e548-25c2-4eec-a545-c9e7b19d5100
2024-01-21 06:58:22,301 - distributed.worker - INFO - Starting Worker plugin PreImport-a4b8fcce-f94a-4e8e-a86b-59773edad322
2024-01-21 06:58:22,301 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,521 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:22,522 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,522 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,523 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36355
2024-01-21 06:58:22,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:22,598 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36355
2024-01-21 06:58:22,611 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:22,611 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,612 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,613 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36355
2024-01-21 06:58:22,614 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:58:22,614 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36355
2024-01-21 06:58:22,615 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:58:22,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36355
2024-01-21 06:58:22,635 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:58:22,635 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:58:22,635 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:58:22,636 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:58:22,636 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:58:22,636 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:58:22,636 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:58:22,637 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-21 06:58:22,641 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33359. Reason: nanny-close
2024-01-21 06:58:22,642 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33559. Reason: nanny-close
2024-01-21 06:58:22,643 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38731. Reason: nanny-close
2024-01-21 06:58:22,644 - distributed.core - INFO - Connection to tcp://127.0.0.1:36355 has been closed.
2024-01-21 06:58:22,644 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41209. Reason: nanny-close
2024-01-21 06:58:22,644 - distributed.core - INFO - Connection to tcp://127.0.0.1:36355 has been closed.
2024-01-21 06:58:22,645 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36655. Reason: nanny-close
2024-01-21 06:58:22,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:36355 has been closed.
2024-01-21 06:58:22,646 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45173. Reason: nanny-close
2024-01-21 06:58:22,646 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:22,646 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:22,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:36355 has been closed.
2024-01-21 06:58:22,647 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33727. Reason: nanny-close
2024-01-21 06:58:22,647 - distributed.core - INFO - Connection to tcp://127.0.0.1:36355 has been closed.
2024-01-21 06:58:22,647 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:22,647 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42643. Reason: nanny-close
2024-01-21 06:58:22,647 - distributed.core - INFO - Connection to tcp://127.0.0.1:36355 has been closed.
2024-01-21 06:58:22,648 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:22,648 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:22,648 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:22,649 - distributed.core - INFO - Connection to tcp://127.0.0.1:36355 has been closed.
2024-01-21 06:58:22,649 - distributed.core - INFO - Connection to tcp://127.0.0.1:36355 has been closed.
2024-01-21 06:58:22,650 - distributed.nanny - INFO - Worker closed
2024-01-21 06:58:22,651 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-21 06:59:07,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:07,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:07,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:07,200 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46281
2024-01-21 06:59:07,200 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46281
2024-01-21 06:59:07,201 - distributed.worker - INFO -           Worker name:                          0
2024-01-21 06:59:07,201 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44363
2024-01-21 06:59:07,201 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39007
2024-01-21 06:59:07,201 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:07,201 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:07,201 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-21 06:59:07,201 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hk8_bpaf
2024-01-21 06:59:07,201 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9de143d5-61fa-4480-83dc-ba143deac51c
2024-01-21 06:59:07,201 - distributed.worker - INFO - Starting Worker plugin PreImport-c44f7bc1-4181-4d41-b6a0-ac99254316c4
2024-01-21 06:59:07,204 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-21 06:59:07,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c2fe540-c37d-434e-acc9-db9de824acb3
2024-01-21 06:59:07,205 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46281. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-21 06:59:07,205 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-21 06:59:07,207 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-21 06:59:11,272 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:11,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:11,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:11,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:11,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:11,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:11,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:11,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:11,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:11,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:11,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:11,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:11,445 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:11,445 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:11,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-21 06:59:11,450 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-21 06:59:11,902 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:11,903 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34093
2024-01-21 06:59:11,903 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34093
2024-01-21 06:59:11,903 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45947
2024-01-21 06:59:11,903 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32897
2024-01-21 06:59:11,903 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:11,903 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:11,903 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:59:11,903 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-46ur68sp
2024-01-21 06:59:11,904 - distributed.worker - INFO - Starting Worker plugin PreImport-a49c4222-edb8-40c4-a299-fc01b054ef55
2024-01-21 06:59:11,904 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9883adc0-4919-4952-a406-b4be515f96ef
2024-01-21 06:59:11,904 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec28b753-2151-4a74-b47c-17710a1884e8
2024-01-21 06:59:11,904 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:11,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:59:11,965 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32897
2024-01-21 06:59:11,965 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:11,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32897
2024-01-21 06:59:11,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:11,973 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46029
2024-01-21 06:59:11,973 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46029
2024-01-21 06:59:11,973 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37449
2024-01-21 06:59:11,973 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32897
2024-01-21 06:59:11,973 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:11,973 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:11,973 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:59:11,973 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2_z52nps
2024-01-21 06:59:11,974 - distributed.worker - INFO - Starting Worker plugin PreImport-9de80099-d7ec-4255-bcbe-0d0685cc8be1
2024-01-21 06:59:11,974 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1e0cdcb-b6ac-44c4-a332-8f33c6c1d01b
2024-01-21 06:59:11,974 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97698661-9fd2-4c6e-935e-bcb15a4ec2cc
2024-01-21 06:59:11,974 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:11,981 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:11,982 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40841
2024-01-21 06:59:11,982 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40841
2024-01-21 06:59:11,982 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40973
2024-01-21 06:59:11,982 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32897
2024-01-21 06:59:11,982 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:11,983 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:11,983 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:59:11,983 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xtk6lwvq
2024-01-21 06:59:11,983 - distributed.worker - INFO - Starting Worker plugin PreImport-9b68cbdb-a986-4b76-8623-704b6952fe4a
2024-01-21 06:59:11,983 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b86cac0-44d3-41d0-bd0d-05ceff14278d
2024-01-21 06:59:11,983 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bcce2e25-c1b0-476d-a053-b2e648d8dbd3
2024-01-21 06:59:11,984 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:11,986 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:11,987 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46427
2024-01-21 06:59:11,987 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46427
2024-01-21 06:59:11,987 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46349
2024-01-21 06:59:11,987 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32897
2024-01-21 06:59:11,987 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:11,987 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:11,987 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:59:11,988 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6inqf9e0
2024-01-21 06:59:11,988 - distributed.worker - INFO - Starting Worker plugin PreImport-a5dc60d6-f359-4254-8a29-d5792ef7da7a
2024-01-21 06:59:11,988 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e58b316e-b5b6-4916-a5c7-cbc61731c86f
2024-01-21 06:59:11,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8a97b7b-4960-4ad3-99a2-f5e1c74b419f
2024-01-21 06:59:11,988 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:12,063 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34267
2024-01-21 06:59:12,063 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34267
2024-01-21 06:59:12,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38455
2024-01-21 06:59:12,063 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,064 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,064 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:12,064 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:59:12,064 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7h6vyt4e
2024-01-21 06:59:12,064 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25bc11a3-a7e6-43a2-a95b-fe56f0fa104e
2024-01-21 06:59:12,065 - distributed.worker - INFO - Starting Worker plugin PreImport-c2c623bc-44a6-4493-a6f3-28de421f4dd1
2024-01-21 06:59:12,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6b5db0e-7b82-4302-8059-ffd2784ddb1a
2024-01-21 06:59:12,065 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:12,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:12,067 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38115
2024-01-21 06:59:12,067 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38115
2024-01-21 06:59:12,067 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42319
2024-01-21 06:59:12,067 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,067 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,067 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43875
2024-01-21 06:59:12,067 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:12,067 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43875
2024-01-21 06:59:12,067 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:59:12,067 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39113
2024-01-21 06:59:12,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pgy1hmbw
2024-01-21 06:59:12,067 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,067 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,067 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:12,067 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:59:12,068 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x6xewmpd
2024-01-21 06:59:12,068 - distributed.worker - INFO - Starting Worker plugin PreImport-6758a869-4e2e-4df8-a14a-327d3a9fbf18
2024-01-21 06:59:12,068 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77103761-daf4-429a-aace-e6d2431980a5
2024-01-21 06:59:12,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f074eda2-9fa6-4348-8601-5fb970269970
2024-01-21 06:59:12,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84cd8cb3-7984-4849-bdee-41c985635ba6
2024-01-21 06:59:12,069 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d318ad69-02de-45c7-a4b3-898e98542c66
2024-01-21 06:59:12,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,069 - distributed.worker - INFO - Starting Worker plugin PreImport-53f64904-edd7-4ea3-ba2c-cd398a60f287
2024-01-21 06:59:12,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-21 06:59:12,070 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43791
2024-01-21 06:59:12,071 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43791
2024-01-21 06:59:12,071 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38289
2024-01-21 06:59:12,071 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,071 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,071 - distributed.worker - INFO -               Threads:                          1
2024-01-21 06:59:12,071 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-21 06:59:12,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4l6cxtfg
2024-01-21 06:59:12,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8776750e-732c-42e3-90a9-dfdb86ded3fd
2024-01-21 06:59:12,071 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:59:12,071 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70bade70-b600-4534-bf02-38e277ca0009
2024-01-21 06:59:12,071 - distributed.worker - INFO - Starting Worker plugin PreImport-b7d70c47-eef2-4597-ae06-79e580abf877
2024-01-21 06:59:12,072 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,072 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,072 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,073 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32897
2024-01-21 06:59:12,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:59:12,196 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,196 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32897
2024-01-21 06:59:12,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:59:12,202 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,202 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32897
2024-01-21 06:59:12,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:59:12,367 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,367 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,368 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32897
2024-01-21 06:59:12,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:59:12,369 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,369 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32897
2024-01-21 06:59:12,388 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:59:12,389 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,389 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-21 06:59:12,391 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32897
2024-01-21 06:59:12,391 - distributed.worker - INFO - -------------------------------------------------
2024-01-21 06:59:12,391 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32897
2024-01-21 06:59:12,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32897
2024-01-21 06:59:12,446 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34093. Reason: nanny-close
2024-01-21 06:59:12,446 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46427. Reason: nanny-close
2024-01-21 06:59:12,447 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40841. Reason: nanny-close
2024-01-21 06:59:12,448 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46029. Reason: nanny-close
2024-01-21 06:59:12,448 - distributed.core - INFO - Connection to tcp://127.0.0.1:32897 has been closed.
2024-01-21 06:59:12,448 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38115. Reason: nanny-close
2024-01-21 06:59:12,448 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43791. Reason: nanny-close
2024-01-21 06:59:12,448 - distributed.core - INFO - Connection to tcp://127.0.0.1:32897 has been closed.
2024-01-21 06:59:12,449 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43875. Reason: nanny-close
2024-01-21 06:59:12,449 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34267. Reason: nanny-close
2024-01-21 06:59:12,449 - distributed.nanny - INFO - Worker closed
2024-01-21 06:59:12,450 - distributed.core - INFO - Connection to tcp://127.0.0.1:32897 has been closed.
2024-01-21 06:59:12,450 - distributed.nanny - INFO - Worker closed
2024-01-21 06:59:12,450 - distributed.core - INFO - Connection to tcp://127.0.0.1:32897 has been closed.
2024-01-21 06:59:12,450 - distributed.core - INFO - Connection to tcp://127.0.0.1:32897 has been closed.
2024-01-21 06:59:12,451 - distributed.core - INFO - Connection to tcp://127.0.0.1:32897 has been closed.
2024-01-21 06:59:12,451 - distributed.nanny - INFO - Worker closed
2024-01-21 06:59:12,451 - distributed.nanny - INFO - Worker closed
2024-01-21 06:59:12,451 - distributed.core - INFO - Connection to tcp://127.0.0.1:32897 has been closed.
2024-01-21 06:59:12,452 - distributed.core - INFO - Connection to tcp://127.0.0.1:32897 has been closed.
2024-01-21 06:59:12,452 - distributed.nanny - INFO - Worker closed
2024-01-21 06:59:12,453 - distributed.nanny - INFO - Worker closed
2024-01-21 06:59:12,453 - distributed.nanny - INFO - Worker closed
2024-01-21 06:59:12,454 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-01-21 06:59:18,025 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-21 06:59:18,028 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration 2024-01-21 06:59:22,211 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-21 06:59:22,213 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-01-21 06:59:24,501 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-21 06:59:24,505 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] [1705820364.796526] [dgx13:64146:0]            sock.c:481  UCX  ERROR bind(fd=254 addr=0.0.0.0:35792) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-01-21 06:59:42,580 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 06:59:42,589 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:46729'. Shutting down.
2024-01-21 06:59:42,590 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f4c135ddbb0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 06:59:44,595 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-01-21 07:00:03,015 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:00:03,022 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ff75eb86be0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:00:05,026 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-01-21 07:00:33,373 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:00:33,380 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fd333581c40>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-01-21 07:01:03,849 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:01:03,856 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ff3f7db7c40>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:01:05,860 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-01-21 07:01:34,411 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:01:34,418 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f0731e66be0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:01:36,423 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-01-21 07:02:04,893 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:02:04,900 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f6137629be0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:02:06,904 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-01-21 07:02:35,319 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-21 07:02:35,326 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fdd1fe77be0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-21 07:03:09,436 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-21 07:03:09,440 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 51 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
