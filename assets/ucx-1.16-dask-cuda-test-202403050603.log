============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.0.2, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-03-05 06:54:54,006 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:54:54,011 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-05 06:54:54,015 - distributed.scheduler - INFO - State start
2024-03-05 06:54:54,093 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:54:54,095 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-05 06:54:54,096 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-05 06:54:54,096 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:54:54,217 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41355'
2024-03-05 06:54:54,238 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37185'
2024-03-05 06:54:54,242 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43343'
2024-03-05 06:54:54,250 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35613'
2024-03-05 06:54:55,309 - distributed.scheduler - INFO - Receive client connection: Client-440e4aeb-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:54:55,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40808
2024-03-05 06:54:56,211 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:54:56,211 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:54:56,215 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:54:56,216 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33253
2024-03-05 06:54:56,216 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33253
2024-03-05 06:54:56,216 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39465
2024-03-05 06:54:56,217 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-05 06:54:56,217 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:56,217 - distributed.worker - INFO -               Threads:                          4
2024-03-05 06:54:56,217 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-05 06:54:56,217 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zkspxac8
2024-03-05 06:54:56,217 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac6c9eec-3098-4b94-9e07-01258f762642
2024-03-05 06:54:56,217 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efe965c3-20ca-43e0-a4cd-21de8b66f20d
2024-03-05 06:54:56,217 - distributed.worker - INFO - Starting Worker plugin PreImport-63e27e31-9179-4f7a-86b9-56c66c00a078
2024-03-05 06:54:56,218 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:56,237 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:54:56,237 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:54:56,241 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:54:56,242 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45491
2024-03-05 06:54:56,242 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45491
2024-03-05 06:54:56,242 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37627
2024-03-05 06:54:56,242 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-05 06:54:56,242 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:56,242 - distributed.worker - INFO -               Threads:                          4
2024-03-05 06:54:56,242 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-05 06:54:56,242 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-afg82tn1
2024-03-05 06:54:56,242 - distributed.worker - INFO - Starting Worker plugin PreImport-7eac7d1d-eb43-42de-b988-9b9dc1e66b7d
2024-03-05 06:54:56,243 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14a61c18-46f6-4098-a593-1d0f5297b899
2024-03-05 06:54:56,243 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c779e7e9-cae3-4fcb-935b-435e14c03d5f
2024-03-05 06:54:56,247 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:56,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:54:56,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:54:56,254 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:54:56,255 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33513
2024-03-05 06:54:56,255 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33513
2024-03-05 06:54:56,255 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32903
2024-03-05 06:54:56,255 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-05 06:54:56,255 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:56,255 - distributed.worker - INFO -               Threads:                          4
2024-03-05 06:54:56,255 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-05 06:54:56,255 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qr6oakso
2024-03-05 06:54:56,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04d6608b-ed73-4d4a-bdad-d3d7a64c04b7
2024-03-05 06:54:56,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e8a0411-8016-4c43-940c-36455f4cc481
2024-03-05 06:54:56,256 - distributed.worker - INFO - Starting Worker plugin PreImport-cbab455a-fc0e-4d49-8f2c-66a9c271edab
2024-03-05 06:54:56,256 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:56,271 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:54:56,271 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:54:56,275 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:54:56,276 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46349
2024-03-05 06:54:56,276 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46349
2024-03-05 06:54:56,276 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35749
2024-03-05 06:54:56,276 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-05 06:54:56,276 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:56,276 - distributed.worker - INFO -               Threads:                          4
2024-03-05 06:54:56,276 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-05 06:54:56,276 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-m2n7q2i8
2024-03-05 06:54:56,276 - distributed.worker - INFO - Starting Worker plugin PreImport-aa9e540c-07ff-4729-a0c9-a5e9692c6a5e
2024-03-05 06:54:56,276 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28e4c1fe-ea26-4b4b-b9f5-0dc8739d048a
2024-03-05 06:54:56,276 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96cd671b-ef0e-4cbe-aa2d-5157e434ca91
2024-03-05 06:54:56,277 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:57,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46349', status: init, memory: 0, processing: 0>
2024-03-05 06:54:57,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46349
2024-03-05 06:54:57,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40858
2024-03-05 06:54:57,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:54:57,346 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-05 06:54:57,346 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:57,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-05 06:54:57,358 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33253', status: init, memory: 0, processing: 0>
2024-03-05 06:54:57,359 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33253
2024-03-05 06:54:57,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40828
2024-03-05 06:54:57,359 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45491', status: init, memory: 0, processing: 0>
2024-03-05 06:54:57,360 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45491
2024-03-05 06:54:57,360 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40840
2024-03-05 06:54:57,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:54:57,361 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-05 06:54:57,361 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:57,361 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:54:57,362 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-05 06:54:57,363 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:57,363 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33513', status: init, memory: 0, processing: 0>
2024-03-05 06:54:57,363 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-05 06:54:57,364 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33513
2024-03-05 06:54:57,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40842
2024-03-05 06:54:57,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-05 06:54:57,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:54:57,365 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-05 06:54:57,366 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:54:57,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-05 06:54:57,465 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-05 06:54:57,465 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-05 06:54:57,465 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-05 06:54:57,466 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-05 06:54:57,472 - distributed.scheduler - INFO - Remove client Client-440e4aeb-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:54:57,472 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40808; closing.
2024-03-05 06:54:57,472 - distributed.scheduler - INFO - Remove client Client-440e4aeb-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:54:57,472 - distributed.scheduler - INFO - Close client connection: Client-440e4aeb-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:54:57,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41355'. Reason: nanny-close
2024-03-05 06:54:57,475 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:54:57,475 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37185'. Reason: nanny-close
2024-03-05 06:54:57,476 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:54:57,476 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43343'. Reason: nanny-close
2024-03-05 06:54:57,476 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33253. Reason: nanny-close
2024-03-05 06:54:57,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:54:57,477 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35613'. Reason: nanny-close
2024-03-05 06:54:57,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:54:57,478 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45491. Reason: nanny-close
2024-03-05 06:54:57,478 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40828; closing.
2024-03-05 06:54:57,478 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-05 06:54:57,478 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33253', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621697.4786882')
2024-03-05 06:54:57,478 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46349. Reason: nanny-close
2024-03-05 06:54:57,480 - distributed.nanny - INFO - Worker closed
2024-03-05 06:54:57,480 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-05 06:54:57,480 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40840; closing.
2024-03-05 06:54:57,480 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45491', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621697.4806926')
2024-03-05 06:54:57,481 - distributed.nanny - INFO - Worker closed
2024-03-05 06:54:57,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40858; closing.
2024-03-05 06:54:57,482 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-05 06:54:57,482 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621697.482155')
2024-03-05 06:54:57,484 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33513. Reason: nanny-close
2024-03-05 06:54:57,484 - distributed.nanny - INFO - Worker closed
2024-03-05 06:54:57,487 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40842; closing.
2024-03-05 06:54:57,487 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-05 06:54:57,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621697.4875746')
2024-03-05 06:54:57,487 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:54:57,489 - distributed.nanny - INFO - Worker closed
2024-03-05 06:54:58,239 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:54:58,239 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:54:58,240 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:54:58,241 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-05 06:54:58,242 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-05 06:55:00,589 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:55:00,593 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35259 instead
  warnings.warn(
2024-03-05 06:55:00,597 - distributed.scheduler - INFO - State start
2024-03-05 06:55:00,619 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:55:00,620 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-05 06:55:00,621 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:55:00,621 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-05 06:55:00,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41879'
2024-03-05 06:55:00,782 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37895'
2024-03-05 06:55:00,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43379'
2024-03-05 06:55:00,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39215'
2024-03-05 06:55:00,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39245'
2024-03-05 06:55:00,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38853'
2024-03-05 06:55:00,828 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45489'
2024-03-05 06:55:00,837 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37069'
2024-03-05 06:55:02,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:02,748 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:02,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:02,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46499
2024-03-05 06:55:02,754 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46499
2024-03-05 06:55:02,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33241
2024-03-05 06:55:02,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:02,754 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:02,754 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:02,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:02,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fon1govp
2024-03-05 06:55:02,754 - distributed.worker - INFO - Starting Worker plugin PreImport-9891a0ca-63f9-4500-985d-f36e16f0fb1d
2024-03-05 06:55:02,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aec26118-4abb-4e37-80f4-ede9fc52aa93
2024-03-05 06:55:02,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01abff6f-9951-401e-b97b-0dd9c833d5d8
2024-03-05 06:55:02,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:02,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:02,766 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:02,767 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43729
2024-03-05 06:55:02,767 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43729
2024-03-05 06:55:02,767 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34487
2024-03-05 06:55:02,767 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:02,767 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:02,768 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:02,768 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:02,768 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yxzj2xor
2024-03-05 06:55:02,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d26f014a-e238-406b-964e-fc2ec52f898b
2024-03-05 06:55:02,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:02,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:02,769 - distributed.worker - INFO - Starting Worker plugin PreImport-3374e551-71f4-4cc7-9451-ab312fa9d1db
2024-03-05 06:55:02,769 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a335bd5-8af6-4f31-bc92-a5b27cc80bb2
2024-03-05 06:55:02,773 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:02,774 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39023
2024-03-05 06:55:02,774 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39023
2024-03-05 06:55:02,774 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41481
2024-03-05 06:55:02,774 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:02,774 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:02,774 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:02,774 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:02,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-65o34rw5
2024-03-05 06:55:02,774 - distributed.worker - INFO - Starting Worker plugin PreImport-dec27017-0bf3-4c43-8f5d-74d6b544004b
2024-03-05 06:55:02,775 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b925e60-c005-45a8-9042-dc8f8468095b
2024-03-05 06:55:02,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cbe6a5f7-596b-4c40-8ee2-99eae12b956e
2024-03-05 06:55:02,783 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:02,783 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:02,788 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:02,789 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34625
2024-03-05 06:55:02,789 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34625
2024-03-05 06:55:02,789 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38709
2024-03-05 06:55:02,789 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:02,789 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:02,789 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:02,789 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:02,789 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3e741y0z
2024-03-05 06:55:02,789 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02c4417f-3f23-4e76-aa62-9a37b8cc02e5
2024-03-05 06:55:02,946 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:02,947 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:02,959 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:02,962 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42871
2024-03-05 06:55:02,962 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42871
2024-03-05 06:55:02,962 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46451
2024-03-05 06:55:02,962 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:02,962 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:02,962 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:02,962 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:02,962 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-42gofhra
2024-03-05 06:55:02,963 - distributed.worker - INFO - Starting Worker plugin RMMSetup-317357b0-0818-4fd7-b1d5-7a45302dea6e
2024-03-05 06:55:02,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:02,977 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:02,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:02,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:02,985 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:02,987 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35571
2024-03-05 06:55:02,987 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35571
2024-03-05 06:55:02,987 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37107
2024-03-05 06:55:02,987 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:02,987 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:02,987 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:02,987 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:02,987 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5rhrw4ul
2024-03-05 06:55:02,988 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-118c0634-8f6a-44f5-918f-52c4fe93e570
2024-03-05 06:55:02,989 - distributed.worker - INFO - Starting Worker plugin PreImport-cd70dd96-a17e-4031-9771-237d04ac082b
2024-03-05 06:55:02,989 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a55853b-3b19-4ca2-be7c-aecc1a6e2053
2024-03-05 06:55:02,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:02,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:02,994 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:02,997 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44725
2024-03-05 06:55:02,997 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44725
2024-03-05 06:55:02,997 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45081
2024-03-05 06:55:02,997 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:02,997 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:02,997 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:02,998 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:02,998 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yi5j_pvo
2024-03-05 06:55:02,998 - distributed.worker - INFO - Starting Worker plugin PreImport-ca53fa84-7aea-4c02-be23-676fe43c842b
2024-03-05 06:55:02,999 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4944544a-fb76-440f-baff-f2d913f2a8b7
2024-03-05 06:55:02,999 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae2de6b4-0552-458b-b632-27214f41c5a4
2024-03-05 06:55:03,003 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:03,005 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36791
2024-03-05 06:55:03,006 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36791
2024-03-05 06:55:03,006 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39161
2024-03-05 06:55:03,006 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:03,006 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:03,006 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:03,006 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:03,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s88spj8p
2024-03-05 06:55:03,007 - distributed.worker - INFO - Starting Worker plugin PreImport-2a83a613-9012-4ee7-8f7d-7e65faef7b3e
2024-03-05 06:55:03,007 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-181c5413-9b52-4710-a712-b89439636e1e
2024-03-05 06:55:03,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09760e87-5172-42cb-aba3-857cbd855a60
2024-03-05 06:55:05,042 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,073 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,115 - distributed.worker - INFO - Starting Worker plugin PreImport-0f434be1-6f43-49a2-8475-4140f108b7d2
2024-03-05 06:55:05,116 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7e089e3-a17c-4848-9a39-7d669c3cc9ad
2024-03-05 06:55:05,116 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,126 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6cb8f47-beed-4597-b819-93a73859d3b5
2024-03-05 06:55:05,138 - distributed.worker - INFO - Starting Worker plugin PreImport-c1076c39-2439-4f90-a8cd-78a908b60608
2024-03-05 06:55:05,139 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,142 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,165 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,169 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,842 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:05,842 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:05,843 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:05,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43379'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:05,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:05,871 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34625. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:05,871 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:05,872 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:05,872 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:05,873 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:05,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:05,875 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:05,893 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38853'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:05,893 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:05,894 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39023. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:05,896 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:05,898 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:06,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:06,011 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:06,011 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:06,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:06,038 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:06,039 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:06,039 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:06,039 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:06,040 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:06,040 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:06,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:06,042 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:06,046 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39215'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,046 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,047 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39245'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,047 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,047 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36791. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,048 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35571. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,049 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:06,051 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:06,053 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:06,055 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:06,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:06,059 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:06,059 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:06,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:06,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41879'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,074 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43729. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:06,080 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:06,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45489'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,098 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,100 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42871. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:06,102 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:06,104 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:53994 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-05 06:55:06,184 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46088 parent=45889 started daemon>
2024-03-05 06:55:06,184 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46084 parent=45889 started daemon>
2024-03-05 06:55:06,185 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46080 parent=45889 started daemon>
2024-03-05 06:55:06,185 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46076 parent=45889 started daemon>
2024-03-05 06:55:06,185 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46071 parent=45889 started daemon>
2024-03-05 06:55:06,185 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46061 parent=45889 started daemon>
2024-03-05 06:55:06,186 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46057 parent=45889 started daemon>
2024-03-05 06:55:06,321 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46061 exit status was already read will report exitcode 255
2024-03-05 06:55:06,449 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46057 exit status was already read will report exitcode 255
2024-03-05 06:55:06,477 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46084 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-05 06:55:16,887 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:55:16,892 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36801 instead
  warnings.warn(
2024-03-05 06:55:16,895 - distributed.scheduler - INFO - State start
2024-03-05 06:55:16,897 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yi5j_pvo', purging
2024-03-05 06:55:16,898 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fon1govp', purging
2024-03-05 06:55:16,917 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:55:16,918 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:55:16,919 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36801/status
2024-03-05 06:55:16,919 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:55:17,047 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33735'
2024-03-05 06:55:17,057 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40195'
2024-03-05 06:55:17,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45053'
2024-03-05 06:55:17,078 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33705'
2024-03-05 06:55:17,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34625'
2024-03-05 06:55:17,091 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34787'
2024-03-05 06:55:17,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37773'
2024-03-05 06:55:17,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35445'
2024-03-05 06:55:19,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:19,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:19,439 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:19,439 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:19,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:19,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:19,444 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:19,445 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34915
2024-03-05 06:55:19,446 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34915
2024-03-05 06:55:19,446 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:19,446 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36739
2024-03-05 06:55:19,446 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:19,446 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:19,446 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:19,446 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:19,446 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7s2oj2gn
2024-03-05 06:55:19,446 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72b1a76f-380b-47dc-9476-7d2db0894f16
2024-03-05 06:55:19,447 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40981
2024-03-05 06:55:19,447 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40981
2024-03-05 06:55:19,447 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35103
2024-03-05 06:55:19,447 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:19,447 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:19,447 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:19,447 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:19,447 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vqmccmc3
2024-03-05 06:55:19,448 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a3385e3-8311-4d75-b241-2df705fea9b5
2024-03-05 06:55:19,448 - distributed.worker - INFO - Starting Worker plugin PreImport-2d6b4630-b00e-45c2-b59a-1e761f7bc1c0
2024-03-05 06:55:19,448 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e64fdc82-eb86-4277-b01a-ed1fbced022a
2024-03-05 06:55:19,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:19,452 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33447
2024-03-05 06:55:19,452 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33447
2024-03-05 06:55:19,452 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38675
2024-03-05 06:55:19,452 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:19,452 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:19,452 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:19,452 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:19,452 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o5851fau
2024-03-05 06:55:19,452 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9927e641-b68e-4aa8-b81f-00d719bc0cf2
2024-03-05 06:55:19,455 - distributed.worker - INFO - Starting Worker plugin PreImport-08dbaa8c-eac0-44d0-8678-e8ccd6b2f0a0
2024-03-05 06:55:19,456 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bba6ee4a-d76d-46b1-a9b6-2fc7c5961639
2024-03-05 06:55:19,460 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:19,460 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:19,466 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:19,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38981
2024-03-05 06:55:19,468 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38981
2024-03-05 06:55:19,468 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37185
2024-03-05 06:55:19,468 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:19,468 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:19,468 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:19,468 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:19,468 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fqcwkhj4
2024-03-05 06:55:19,469 - distributed.worker - INFO - Starting Worker plugin PreImport-615560a2-8dd2-4225-bc4f-680ec14a1743
2024-03-05 06:55:19,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0e5d708-a177-4263-b67c-ec47721bfdd0
2024-03-05 06:55:19,470 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ed1ad66-2b83-425b-a225-a6f30194de0f
2024-03-05 06:55:19,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:19,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:19,477 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:19,478 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44117
2024-03-05 06:55:19,478 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44117
2024-03-05 06:55:19,478 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35903
2024-03-05 06:55:19,479 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:19,479 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:19,479 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:19,479 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:19,479 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1_biyqua
2024-03-05 06:55:19,479 - distributed.worker - INFO - Starting Worker plugin PreImport-2a94e05f-0781-455a-b46e-38f8bf19db68
2024-03-05 06:55:19,479 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4649159-fbc4-4309-a205-6c062ab19b60
2024-03-05 06:55:19,479 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fe33a5e-a204-4421-9981-5b15bafb17c7
2024-03-05 06:55:19,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:19,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:19,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:19,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:19,487 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:19,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:19,489 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34435
2024-03-05 06:55:19,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34435
2024-03-05 06:55:19,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44589
2024-03-05 06:55:19,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:19,489 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:19,489 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:19,489 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:19,489 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jcrp1dbc
2024-03-05 06:55:19,490 - distributed.worker - INFO - Starting Worker plugin PreImport-45267c0b-86c9-4c99-8d5a-63741499c557
2024-03-05 06:55:19,490 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-305685f4-3acf-4c7d-b345-5e8b1b1ad104
2024-03-05 06:55:19,490 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39711
2024-03-05 06:55:19,490 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39711
2024-03-05 06:55:19,490 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7959b88d-7a9e-4e0c-ba95-b5eeb70295bf
2024-03-05 06:55:19,490 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39645
2024-03-05 06:55:19,490 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:19,490 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:19,490 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:19,490 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:19,490 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gto1uf9g
2024-03-05 06:55:19,491 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0993237a-79be-4714-9b6e-3f349e3200be
2024-03-05 06:55:19,494 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:19,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:19,500 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:19,501 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37279
2024-03-05 06:55:19,502 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37279
2024-03-05 06:55:19,502 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33945
2024-03-05 06:55:19,502 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:19,502 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:19,502 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:19,502 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:19,502 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bs101thl
2024-03-05 06:55:19,502 - distributed.worker - INFO - Starting Worker plugin PreImport-dab1d212-88c8-491a-9ac5-08ace11235b7
2024-03-05 06:55:19,502 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13e35f8b-09ae-4a6e-8eeb-24102f3cb143
2024-03-05 06:55:19,503 - distributed.worker - INFO - Starting Worker plugin RMMSetup-340769ea-ba48-4864-9a3b-557fdacec449
2024-03-05 06:55:20,692 - distributed.scheduler - INFO - Receive client connection: Client-51c87a1b-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:55:20,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35338
2024-03-05 06:55:21,584 - distributed.scheduler - INFO - Receive client connection: Client-55a7b5ea-dabd-11ee-96cf-d8c49764f6bb
2024-03-05 06:55:21,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35346
2024-03-05 06:55:22,214 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59ee3742-1c76-4e13-89e6-df186dc5a791
2024-03-05 06:55:22,215 - distributed.worker - INFO - Starting Worker plugin PreImport-f53bdd97-6329-40d8-a088-fc4fd03843f0
2024-03-05 06:55:22,215 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,238 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34915', status: init, memory: 0, processing: 0>
2024-03-05 06:55:22,240 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34915
2024-03-05 06:55:22,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35366
2024-03-05 06:55:22,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:22,241 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:22,241 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,243 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:22,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fdb0c6d1-99fd-42b0-8337-041e0e0e83f5
2024-03-05 06:55:22,274 - distributed.worker - INFO - Starting Worker plugin PreImport-5eef4778-b275-4a48-a5e3-a9a14e8667c1
2024-03-05 06:55:22,275 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,300 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,304 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,309 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,309 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39711', status: init, memory: 0, processing: 0>
2024-03-05 06:55:22,310 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39711
2024-03-05 06:55:22,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35374
2024-03-05 06:55:22,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:22,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:22,313 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:22,319 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,324 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,330 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,333 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44117', status: init, memory: 0, processing: 0>
2024-03-05 06:55:22,334 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44117
2024-03-05 06:55:22,334 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35400
2024-03-05 06:55:22,335 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38981', status: init, memory: 0, processing: 0>
2024-03-05 06:55:22,335 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:22,336 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38981
2024-03-05 06:55:22,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35384
2024-03-05 06:55:22,336 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:22,336 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:22,338 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40981', status: init, memory: 0, processing: 0>
2024-03-05 06:55:22,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:22,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40981
2024-03-05 06:55:22,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35392
2024-03-05 06:55:22,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:22,339 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:22,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:22,341 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:22,341 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,343 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:22,346 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37279', status: init, memory: 0, processing: 0>
2024-03-05 06:55:22,347 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37279
2024-03-05 06:55:22,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35420
2024-03-05 06:55:22,348 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:22,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:22,349 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:22,352 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34435', status: init, memory: 0, processing: 0>
2024-03-05 06:55:22,353 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34435
2024-03-05 06:55:22,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35422
2024-03-05 06:55:22,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:22,354 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33447', status: init, memory: 0, processing: 0>
2024-03-05 06:55:22,354 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33447
2024-03-05 06:55:22,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35406
2024-03-05 06:55:22,354 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:22,355 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:22,356 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:22,357 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:22,357 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:22,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:22,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,412 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,413 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,413 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,417 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,418 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:55:22,423 - distributed.scheduler - INFO - Remove client Client-51c87a1b-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:55:22,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35338; closing.
2024-03-05 06:55:22,423 - distributed.scheduler - INFO - Remove client Client-51c87a1b-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:55:22,424 - distributed.scheduler - INFO - Remove client Client-55a7b5ea-dabd-11ee-96cf-d8c49764f6bb
2024-03-05 06:55:22,424 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35346; closing.
2024-03-05 06:55:22,424 - distributed.scheduler - INFO - Close client connection: Client-51c87a1b-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:55:22,425 - distributed.scheduler - INFO - Remove client Client-55a7b5ea-dabd-11ee-96cf-d8c49764f6bb
2024-03-05 06:55:22,425 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33735'. Reason: nanny-close
2024-03-05 06:55:22,425 - distributed.scheduler - INFO - Close client connection: Client-55a7b5ea-dabd-11ee-96cf-d8c49764f6bb
2024-03-05 06:55:22,425 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:55:22,426 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40195'. Reason: nanny-close
2024-03-05 06:55:22,426 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:55:22,426 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45053'. Reason: nanny-close
2024-03-05 06:55:22,427 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40981. Reason: nanny-close
2024-03-05 06:55:22,427 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:55:22,427 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33705'. Reason: nanny-close
2024-03-05 06:55:22,427 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:55:22,427 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33447. Reason: nanny-close
2024-03-05 06:55:22,427 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34625'. Reason: nanny-close
2024-03-05 06:55:22,427 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34435. Reason: nanny-close
2024-03-05 06:55:22,428 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:55:22,428 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34787'. Reason: nanny-close
2024-03-05 06:55:22,428 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44117. Reason: nanny-close
2024-03-05 06:55:22,428 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:55:22,428 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37773'. Reason: nanny-close
2024-03-05 06:55:22,429 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38981. Reason: nanny-close
2024-03-05 06:55:22,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:55:22,429 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35445'. Reason: nanny-close
2024-03-05 06:55:22,429 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39711. Reason: nanny-close
2024-03-05 06:55:22,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:55:22,429 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:22,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35422; closing.
2024-03-05 06:55:22,429 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37279. Reason: nanny-close
2024-03-05 06:55:22,430 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:22,430 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34435', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621722.4300017')
2024-03-05 06:55:22,430 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:22,430 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:22,430 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34915. Reason: nanny-close
2024-03-05 06:55:22,430 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35392; closing.
2024-03-05 06:55:22,431 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:22,431 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:22,431 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:22,431 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:22,431 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:22,431 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:22,431 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:22,432 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40981', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621722.431975')
2024-03-05 06:55:22,432 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35406; closing.
2024-03-05 06:55:22,432 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35400; closing.
2024-03-05 06:55:22,432 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:22,433 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:22,433 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:22,433 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33447', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621722.433462')
2024-03-05 06:55:22,433 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:22,433 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44117', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621722.4338186')
2024-03-05 06:55:22,434 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:22,434 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35384; closing.
2024-03-05 06:55:22,434 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35374; closing.
2024-03-05 06:55:22,435 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38981', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621722.4349864')
2024-03-05 06:55:22,435 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39711', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621722.4352977')
2024-03-05 06:55:22,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35420; closing.
2024-03-05 06:55:22,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35366; closing.
2024-03-05 06:55:22,436 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37279', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621722.4362438')
2024-03-05 06:55:22,436 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34915', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621722.4366627')
2024-03-05 06:55:22,436 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:55:22,850 - distributed.scheduler - INFO - Receive client connection: Client-5668bc03-dabd-11ee-96cf-d8c49764f6bb
2024-03-05 06:55:22,850 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35434
2024-03-05 06:55:23,391 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:55:23,391 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:55:23,392 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:55:23,394 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:55:23,395 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-05 06:55:25,648 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:55:25,652 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37317 instead
  warnings.warn(
2024-03-05 06:55:25,655 - distributed.scheduler - INFO - State start
2024-03-05 06:55:25,675 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:55:25,676 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-05 06:55:25,677 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:55:25,677 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-05 06:55:25,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36007'
2024-03-05 06:55:25,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45453'
2024-03-05 06:55:25,841 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45111'
2024-03-05 06:55:25,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42831'
2024-03-05 06:55:25,858 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38221'
2024-03-05 06:55:25,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35507'
2024-03-05 06:55:25,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41097'
2024-03-05 06:55:25,885 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35841'
2024-03-05 06:55:27,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:27,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:27,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:27,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:27,761 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:27,761 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:27,762 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46199
2024-03-05 06:55:27,762 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46559
2024-03-05 06:55:27,762 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46199
2024-03-05 06:55:27,762 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46559
2024-03-05 06:55:27,762 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33859
2024-03-05 06:55:27,762 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41913
2024-03-05 06:55:27,762 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:27,762 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:27,763 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:27,763 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:27,763 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:27,763 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:27,763 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:27,763 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:27,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k2qiq4y1
2024-03-05 06:55:27,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ws9joanx
2024-03-05 06:55:27,763 - distributed.worker - INFO - Starting Worker plugin PreImport-9bfd9dca-738d-4b4d-932f-021a42919be2
2024-03-05 06:55:27,763 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-234e4849-5d0b-4854-a7ed-c943a87f70a1
2024-03-05 06:55:27,763 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93fda64d-bbd5-4ba7-8550-e78d33f26be3
2024-03-05 06:55:27,763 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e423cd4-2133-4def-85ea-d1696471302c
2024-03-05 06:55:27,763 - distributed.worker - INFO - Starting Worker plugin PreImport-4bad8466-7fd0-4bd9-84bd-b18bd2417766
2024-03-05 06:55:27,763 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2ab463f-51e3-402e-b2e5-036b54bc3563
2024-03-05 06:55:27,766 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:27,766 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:27,771 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:27,772 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44075
2024-03-05 06:55:27,772 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44075
2024-03-05 06:55:27,772 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41801
2024-03-05 06:55:27,772 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:27,772 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:27,772 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:27,772 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:27,772 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nnzi8waj
2024-03-05 06:55:27,773 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-753efda8-04cc-4753-af67-f2fae99903b8
2024-03-05 06:55:27,775 - distributed.worker - INFO - Starting Worker plugin PreImport-651408ae-4d01-4f11-8958-634385a82425
2024-03-05 06:55:27,776 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0194149b-a45b-4f82-9ee9-67865beea558
2024-03-05 06:55:27,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:27,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:27,792 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:27,792 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:27,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:27,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33743
2024-03-05 06:55:27,796 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33743
2024-03-05 06:55:27,796 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34743
2024-03-05 06:55:27,796 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:27,796 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:27,796 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:27,796 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:27,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vy7jn_ad
2024-03-05 06:55:27,796 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13fca238-30c1-4fad-9dd9-8513d0e0cec0
2024-03-05 06:55:27,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:27,798 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46097
2024-03-05 06:55:27,798 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46097
2024-03-05 06:55:27,798 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46209
2024-03-05 06:55:27,798 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:27,798 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:27,798 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:27,798 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:27,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l41bi5xa
2024-03-05 06:55:27,798 - distributed.worker - INFO - Starting Worker plugin PreImport-c755882f-d9f7-4eaf-808a-a3af4b1db6f4
2024-03-05 06:55:27,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49b722db-3d13-4307-aec4-971cbf5e78f3
2024-03-05 06:55:27,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1de2eae-689e-4e2b-a7dd-66fbe2832e0c
2024-03-05 06:55:27,822 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:27,822 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:27,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:27,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:27,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:55:27,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:55:27,827 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:27,827 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33663
2024-03-05 06:55:27,828 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33663
2024-03-05 06:55:27,828 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38657
2024-03-05 06:55:27,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:27,828 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:27,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:55:27,828 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:27,828 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:27,828 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:27,828 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-orsyz46e
2024-03-05 06:55:27,828 - distributed.worker - INFO - Starting Worker plugin PreImport-8acf8ed6-01c6-4fe9-9e44-918bf089ffd3
2024-03-05 06:55:27,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3be158aa-9e8d-4bcb-be0c-9a3e2cb41eab
2024-03-05 06:55:27,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da206b70-419f-418c-96a4-217cde32a036
2024-03-05 06:55:27,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42921
2024-03-05 06:55:27,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40569
2024-03-05 06:55:27,829 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42921
2024-03-05 06:55:27,829 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40569
2024-03-05 06:55:27,829 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33863
2024-03-05 06:55:27,829 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35915
2024-03-05 06:55:27,829 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:27,829 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:55:27,829 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:27,829 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:27,829 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:27,829 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:55:27,829 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:27,829 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:55:27,829 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qy8k7y2h
2024-03-05 06:55:27,829 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t61rjmgl
2024-03-05 06:55:27,829 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c35afceb-adff-45c4-89e0-c99d0f9a7c09
2024-03-05 06:55:27,829 - distributed.worker - INFO - Starting Worker plugin PreImport-932d1c37-e82c-482a-833b-bc45d29abdf8
2024-03-05 06:55:27,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9e30dae-bcc8-4d8d-ad9b-4b19ba3849b3
2024-03-05 06:55:27,830 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f60729d3-6bc1-4e58-ad94-a249f3b1fe69
2024-03-05 06:55:29,994 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,000 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,016 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,044 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,088 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3620d2fa-7589-4e06-9065-c62655dc0ec4
2024-03-05 06:55:30,089 - distributed.worker - INFO - Starting Worker plugin PreImport-78fcd45d-b798-4d8f-a007-9cbb25d09d4a
2024-03-05 06:55:30,090 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,104 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,109 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e24b017c-19f4-429b-b57e-9103a322cab6
2024-03-05 06:55:30,109 - distributed.worker - INFO - Starting Worker plugin PreImport-cc200960-f6fd-4d5b-9ac9-906e4100a168
2024-03-05 06:55:30,110 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,113 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,748 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:30,749 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:30,749 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:30,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:30,752 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:30,752 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:30,765 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36007'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,765 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,766 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42831'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,767 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,767 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44075. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,767 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46199. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,769 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:30,769 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:30,770 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:30,771 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:30,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:30,802 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:30,802 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:30,824 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35841'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,824 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,825 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42921. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,827 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:30,828 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:30,844 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:30,846 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:30,846 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:30,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35507'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,869 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,870 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33743. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,873 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:30,875 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:30,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:30,950 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:30,950 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:30,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:30,970 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38221'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,970 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,971 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40569. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:30,974 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:30,977 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:31,406 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:31,407 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:31,407 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:31,409 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:31,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45111'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,423 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46097. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,425 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:31,426 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:31,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:31,545 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:31,546 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:31,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:31,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45453'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46559. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:31,580 - distributed.nanny - INFO - Worker closed
2024-03-05 06:55:31,746 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:55:31,747 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:55:31,747 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:55:31,748 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:55:31,778 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41097'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,778 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,779 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33663. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-05 06:55:31,780 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:55:31,782 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:35790 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-05 06:55:31,981 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46677 parent=46478 started daemon>
2024-03-05 06:55:31,981 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46673 parent=46478 started daemon>
2024-03-05 06:55:31,981 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46666 parent=46478 started daemon>
2024-03-05 06:55:31,981 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46660 parent=46478 started daemon>
2024-03-05 06:55:31,981 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46655 parent=46478 started daemon>
2024-03-05 06:55:31,981 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46650 parent=46478 started daemon>
2024-03-05 06:55:31,981 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46646 parent=46478 started daemon>
2024-03-05 06:55:32,048 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46646 exit status was already read will report exitcode 255
2024-03-05 06:55:32,084 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46677 exit status was already read will report exitcode 255
2024-03-05 06:55:32,421 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46650 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-05 06:56:00,119 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:00,125 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34133 instead
  warnings.warn(
2024-03-05 06:56:00,129 - distributed.scheduler - INFO - State start
2024-03-05 06:56:00,155 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:00,155 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:56:00,156 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34133/status
2024-03-05 06:56:00,157 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:00,239 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42627'
2024-03-05 06:56:00,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33135'
2024-03-05 06:56:00,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39251'
2024-03-05 06:56:00,320 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37025'
2024-03-05 06:56:00,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39261'
2024-03-05 06:56:00,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34399'
2024-03-05 06:56:00,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40769'
2024-03-05 06:56:00,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38903'
2024-03-05 06:56:02,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:02,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:02,151 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:02,152 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45831
2024-03-05 06:56:02,152 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45831
2024-03-05 06:56:02,152 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42979
2024-03-05 06:56:02,152 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,152 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,152 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:02,152 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:02,152 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pbwllf11
2024-03-05 06:56:02,152 - distributed.worker - INFO - Starting Worker plugin PreImport-65c64acd-a04d-4b32-ac26-230ba95f01ce
2024-03-05 06:56:02,152 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1fd582d5-1b5b-46f9-bc46-edaca7b69181
2024-03-05 06:56:02,153 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a471701b-a82a-481e-b862-695bbc694653
2024-03-05 06:56:02,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:02,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:02,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:02,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:02,382 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:02,383 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40453
2024-03-05 06:56:02,383 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40453
2024-03-05 06:56:02,383 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40091
2024-03-05 06:56:02,383 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,383 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,383 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:02,383 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:02,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xj0riei0
2024-03-05 06:56:02,383 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:02,384 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c433ede2-1fec-4d94-8eae-d371ae05ea65
2024-03-05 06:56:02,384 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41321
2024-03-05 06:56:02,384 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41321
2024-03-05 06:56:02,384 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43851
2024-03-05 06:56:02,385 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,385 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,385 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:02,385 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:02,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z2qbwuyl
2024-03-05 06:56:02,385 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22a17f8f-084f-41bf-a80c-504b9ba6b13d
2024-03-05 06:56:02,385 - distributed.worker - INFO - Starting Worker plugin PreImport-cd66bba9-6ad2-4375-ab75-29e835295608
2024-03-05 06:56:02,386 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac85fb1d-fbf9-41e4-8444-e3436fff289a
2024-03-05 06:56:02,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:02,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:02,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:02,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:02,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:02,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:02,397 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:02,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41523
2024-03-05 06:56:02,399 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41523
2024-03-05 06:56:02,399 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41407
2024-03-05 06:56:02,399 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,399 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,399 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:02,399 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:02,399 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zacqqwxr
2024-03-05 06:56:02,400 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b203152f-499d-4e7b-8cd7-a52b153bf016
2024-03-05 06:56:02,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:02,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:02,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44565
2024-03-05 06:56:02,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44565
2024-03-05 06:56:02,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42879
2024-03-05 06:56:02,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,402 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,402 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:02,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:02,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nsg_a0fu
2024-03-05 06:56:02,402 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42067
2024-03-05 06:56:02,402 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42067
2024-03-05 06:56:02,402 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43979
2024-03-05 06:56:02,402 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,402 - distributed.worker - INFO - Starting Worker plugin PreImport-61c0517d-0af2-4ebf-8e0f-ef3b437f3017
2024-03-05 06:56:02,402 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,402 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:02,402 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e4c45f1-331a-4e0e-9592-0dc4b3968fa1
2024-03-05 06:56:02,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:02,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eff6ca8x
2024-03-05 06:56:02,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29ec933e-0084-40c1-8fb9-f2ec74a17593
2024-03-05 06:56:02,403 - distributed.worker - INFO - Starting Worker plugin PreImport-45f7e673-1657-4e0b-84ff-98574d27ea87
2024-03-05 06:56:02,403 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3a860bf-cd10-464b-a3ec-895bd76af6c5
2024-03-05 06:56:02,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d3bd99f-60f5-4ab6-860f-4bf0783df2ce
2024-03-05 06:56:02,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:02,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:02,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:02,433 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38593
2024-03-05 06:56:02,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38593
2024-03-05 06:56:02,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34703
2024-03-05 06:56:02,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,434 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,434 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:02,434 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:02,434 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g10klhry
2024-03-05 06:56:02,434 - distributed.worker - INFO - Starting Worker plugin PreImport-ebd7cc6a-a3a3-48a0-8187-d8ee7c421cc0
2024-03-05 06:56:02,434 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-856e50e3-38f4-4fdf-901d-94b087441fe3
2024-03-05 06:56:02,435 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32e3f72e-7a08-411f-8cd8-714000442d15
2024-03-05 06:56:02,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:02,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:02,595 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:02,596 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41339
2024-03-05 06:56:02,596 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41339
2024-03-05 06:56:02,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45057
2024-03-05 06:56:02,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,597 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,597 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:02,597 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:02,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3ngif5nk
2024-03-05 06:56:02,597 - distributed.worker - INFO - Starting Worker plugin PreImport-24350ee1-9a13-4b9d-bc27-e94e0438370c
2024-03-05 06:56:02,597 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cea0df00-9463-4247-9eb2-d07e6e282f45
2024-03-05 06:56:02,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb8ea910-181b-4eff-a2ac-4ea7a4db1dc4
2024-03-05 06:56:02,669 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,701 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45831', status: init, memory: 0, processing: 0>
2024-03-05 06:56:02,714 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45831
2024-03-05 06:56:02,714 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35324
2024-03-05 06:56:02,715 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:02,716 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:02,717 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:02,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:02,968 - distributed.scheduler - INFO - Receive client connection: Client-6b62893b-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:02,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35326
2024-03-05 06:56:04,493 - distributed.worker - INFO - Starting Worker plugin PreImport-c9526d3f-5f46-4ee4-8a46-14b667a70b6a
2024-03-05 06:56:04,494 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efffd26b-190f-4f62-b2b4-ca2b605fe47d
2024-03-05 06:56:04,494 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,502 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,517 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40453', status: init, memory: 0, processing: 0>
2024-03-05 06:56:04,518 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40453
2024-03-05 06:56:04,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35346
2024-03-05 06:56:04,519 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:04,520 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:04,520 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,521 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:04,536 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41321', status: init, memory: 0, processing: 0>
2024-03-05 06:56:04,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41321
2024-03-05 06:56:04,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35356
2024-03-05 06:56:04,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:04,540 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:04,540 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,542 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:04,547 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,550 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,551 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,567 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44565', status: init, memory: 0, processing: 0>
2024-03-05 06:56:04,568 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44565
2024-03-05 06:56:04,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35372
2024-03-05 06:56:04,569 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:04,569 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,570 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:04,570 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:04,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-137caaa0-c02b-4ea3-9671-59e456289f82
2024-03-05 06:56:04,575 - distributed.worker - INFO - Starting Worker plugin PreImport-04bac4ba-e21a-48e2-aae9-1abc2f8b22ab
2024-03-05 06:56:04,575 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,579 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42067', status: init, memory: 0, processing: 0>
2024-03-05 06:56:04,580 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42067
2024-03-05 06:56:04,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35386
2024-03-05 06:56:04,581 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:04,582 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:04,582 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:04,587 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38593', status: init, memory: 0, processing: 0>
2024-03-05 06:56:04,587 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38593
2024-03-05 06:56:04,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35388
2024-03-05 06:56:04,589 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:04,590 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:04,590 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:04,594 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41339', status: init, memory: 0, processing: 0>
2024-03-05 06:56:04,594 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41339
2024-03-05 06:56:04,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35396
2024-03-05 06:56:04,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:04,596 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:04,596 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:04,599 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41523', status: init, memory: 0, processing: 0>
2024-03-05 06:56:04,599 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41523
2024-03-05 06:56:04,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35412
2024-03-05 06:56:04,600 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:04,601 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:04,601 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:04,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:04,641 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,641 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,642 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,642 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,642 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,642 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,643 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,643 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,654 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:04,654 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:04,654 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:04,654 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:04,654 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:04,654 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:04,654 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:04,655 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:04,765 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,767 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:04,770 - distributed.scheduler - INFO - Remove client Client-6b62893b-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:04,770 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35326; closing.
2024-03-05 06:56:04,770 - distributed.scheduler - INFO - Remove client Client-6b62893b-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:04,770 - distributed.scheduler - INFO - Close client connection: Client-6b62893b-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:04,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42627'. Reason: nanny-close
2024-03-05 06:56:04,772 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:04,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37025'. Reason: nanny-close
2024-03-05 06:56:04,773 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:04,773 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39261'. Reason: nanny-close
2024-03-05 06:56:04,774 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45831. Reason: nanny-close
2024-03-05 06:56:04,774 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:04,774 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38903'. Reason: nanny-close
2024-03-05 06:56:04,774 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42067. Reason: nanny-close
2024-03-05 06:56:04,774 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:04,775 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34399'. Reason: nanny-close
2024-03-05 06:56:04,775 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40453. Reason: nanny-close
2024-03-05 06:56:04,775 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:04,775 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40769'. Reason: nanny-close
2024-03-05 06:56:04,775 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41339. Reason: nanny-close
2024-03-05 06:56:04,775 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:04,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33135'. Reason: nanny-close
2024-03-05 06:56:04,776 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41321. Reason: nanny-close
2024-03-05 06:56:04,776 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:04,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39251'. Reason: nanny-close
2024-03-05 06:56:04,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:04,776 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38593. Reason: nanny-close
2024-03-05 06:56:04,776 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:04,776 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35324; closing.
2024-03-05 06:56:04,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:04,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:04,776 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41523. Reason: nanny-close
2024-03-05 06:56:04,777 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45831', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621764.7770662')
2024-03-05 06:56:04,777 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:04,777 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44565. Reason: nanny-close
2024-03-05 06:56:04,778 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35346; closing.
2024-03-05 06:56:04,778 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:04,778 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35386; closing.
2024-03-05 06:56:04,778 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:04,778 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:04,778 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:04,778 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:04,778 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:04,779 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:04,779 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621764.7792423')
2024-03-05 06:56:04,779 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42067', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621764.7796202')
2024-03-05 06:56:04,779 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:04,780 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35396; closing.
2024-03-05 06:56:04,780 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:04,780 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:04,780 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:04,781 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41339', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621764.781007')
2024-03-05 06:56:04,781 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:04,781 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35356; closing.
2024-03-05 06:56:04,781 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35412; closing.
2024-03-05 06:56:04,782 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621764.7821271')
2024-03-05 06:56:04,782 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41523', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621764.7824645')
2024-03-05 06:56:04,782 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35388; closing.
2024-03-05 06:56:04,783 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35372; closing.
2024-03-05 06:56:04,783 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38593', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621764.7833614')
2024-03-05 06:56:04,783 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44565', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621764.7838252')
2024-03-05 06:56:04,784 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:56:05,688 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:05,688 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:05,688 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:05,690 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:56:05,690 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-05 06:56:07,726 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:07,730 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-05 06:56:07,734 - distributed.scheduler - INFO - State start
2024-03-05 06:56:07,754 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:07,755 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:56:07,755 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-05 06:56:07,756 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:07,821 - distributed.scheduler - INFO - Receive client connection: Client-7039d723-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:07,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35492
2024-03-05 06:56:08,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43697'
2024-03-05 06:56:08,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41955'
2024-03-05 06:56:08,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45203'
2024-03-05 06:56:08,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35101'
2024-03-05 06:56:08,379 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42679'
2024-03-05 06:56:08,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38689'
2024-03-05 06:56:08,396 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45863'
2024-03-05 06:56:08,404 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46671'
2024-03-05 06:56:10,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:10,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:10,310 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:10,311 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39777
2024-03-05 06:56:10,311 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39777
2024-03-05 06:56:10,311 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33061
2024-03-05 06:56:10,311 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:10,311 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:10,311 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:10,311 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:10,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d4uf8g5t
2024-03-05 06:56:10,311 - distributed.worker - INFO - Starting Worker plugin PreImport-9553684b-c523-427f-a240-7a9f86c0e629
2024-03-05 06:56:10,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1bd461d6-1e7e-487a-9324-2087e3259819
2024-03-05 06:56:10,313 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec7d8c1b-f160-496d-9634-56a8d746da54
2024-03-05 06:56:10,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:10,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:10,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:10,320 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46663
2024-03-05 06:56:10,320 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46663
2024-03-05 06:56:10,320 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37595
2024-03-05 06:56:10,320 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:10,320 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:10,320 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:10,320 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:10,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bp12ktya
2024-03-05 06:56:10,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:10,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:10,321 - distributed.worker - INFO - Starting Worker plugin PreImport-a9117075-73b5-43ca-900e-ca1bc1b9c42c
2024-03-05 06:56:10,321 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-454b51ad-10bf-450d-9912-65171bfe6736
2024-03-05 06:56:10,322 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7215ed58-0ce4-4192-974e-7dfa8b91cb87
2024-03-05 06:56:10,325 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:10,326 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44129
2024-03-05 06:56:10,326 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44129
2024-03-05 06:56:10,326 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46533
2024-03-05 06:56:10,326 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:10,326 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:10,326 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:10,326 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:10,326 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a10d2_kc
2024-03-05 06:56:10,326 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06004c87-c120-4e1b-a640-3c6635829589
2024-03-05 06:56:10,327 - distributed.worker - INFO - Starting Worker plugin PreImport-1df0256c-ab3e-4ff3-baee-99f8219a9f35
2024-03-05 06:56:10,327 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0d7ac1e-d4c4-46ce-836c-a11af58062bc
2024-03-05 06:56:10,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:10,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:10,350 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:10,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:10,353 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:10,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41307
2024-03-05 06:56:10,354 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41307
2024-03-05 06:56:10,354 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38921
2024-03-05 06:56:10,354 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:10,354 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:10,354 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:10,354 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:10,354 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6eb2yuon
2024-03-05 06:56:10,354 - distributed.worker - INFO - Starting Worker plugin PreImport-fcbf08bf-05ff-44c4-a3f0-65328695203d
2024-03-05 06:56:10,354 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:10,354 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-362261d6-4338-418b-a628-553933c48f1b
2024-03-05 06:56:10,354 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ed86bae-ae47-4e3a-b7fa-cf690fd340e0
2024-03-05 06:56:10,355 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40919
2024-03-05 06:56:10,355 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40919
2024-03-05 06:56:10,355 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36071
2024-03-05 06:56:10,355 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:10,355 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:10,355 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:10,355 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:10,355 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0is4hi9t
2024-03-05 06:56:10,356 - distributed.worker - INFO - Starting Worker plugin PreImport-dc56069a-6017-4542-9c7c-c563b038f5e7
2024-03-05 06:56:10,356 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4eef5734-51ff-443f-9876-2f171575a0b2
2024-03-05 06:56:10,356 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27ed22e7-0bd5-499f-9e2c-165c947e587f
2024-03-05 06:56:10,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:10,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:10,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:10,413 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38359
2024-03-05 06:56:10,413 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38359
2024-03-05 06:56:10,413 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37307
2024-03-05 06:56:10,414 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:10,414 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:10,414 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:10,414 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:10,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w1oumpwq
2024-03-05 06:56:10,414 - distributed.worker - INFO - Starting Worker plugin PreImport-092601d6-a761-42d6-b019-95e02e745d7f
2024-03-05 06:56:10,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-281c2f70-2806-4701-a386-d8417d6c40ad
2024-03-05 06:56:10,414 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90243570-2418-4aaf-a7a2-146f7ed7a804
2024-03-05 06:56:10,438 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:10,438 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:10,443 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:10,444 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45741
2024-03-05 06:56:10,444 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45741
2024-03-05 06:56:10,444 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39993
2024-03-05 06:56:10,444 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:10,444 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:10,444 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:10,444 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:10,444 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gyjhg_4h
2024-03-05 06:56:10,444 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38265e61-c293-4ed9-ae38-e3b719a2e378
2024-03-05 06:56:10,465 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:10,465 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:10,474 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:10,475 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32999
2024-03-05 06:56:10,476 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32999
2024-03-05 06:56:10,476 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35231
2024-03-05 06:56:10,476 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:10,476 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:10,476 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:10,476 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:10,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kkw_k_hc
2024-03-05 06:56:10,476 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99a35a26-c2a9-4bd4-a77f-d40496cb54e4
2024-03-05 06:56:10,477 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b99a93f-083f-45b3-a0be-3f79aa017510
2024-03-05 06:56:12,904 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3239542c-2195-4091-a756-a6c52e7f21e9
2024-03-05 06:56:12,919 - distributed.worker - INFO - Starting Worker plugin PreImport-daa871dc-e521-4b0b-b064-3a4309446919
2024-03-05 06:56:12,920 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,926 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,927 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,932 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,936 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,938 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39777', status: init, memory: 0, processing: 0>
2024-03-05 06:56:12,940 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39777
2024-03-05 06:56:12,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39706
2024-03-05 06:56:12,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:12,942 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:12,942 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,944 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:12,945 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,949 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41307', status: init, memory: 0, processing: 0>
2024-03-05 06:56:12,949 - distributed.worker - INFO - Starting Worker plugin PreImport-5082c3c0-36d2-43e2-bf75-57b6f1f1f6dd
2024-03-05 06:56:12,949 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41307
2024-03-05 06:56:12,949 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39720
2024-03-05 06:56:12,950 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:12,951 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:12,951 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,952 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45741', status: init, memory: 0, processing: 0>
2024-03-05 06:56:12,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:12,953 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45741
2024-03-05 06:56:12,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39716
2024-03-05 06:56:12,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:12,955 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:12,955 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46663', status: init, memory: 0, processing: 0>
2024-03-05 06:56:12,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:12,958 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46663
2024-03-05 06:56:12,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39732
2024-03-05 06:56:12,959 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:12,960 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40919', status: init, memory: 0, processing: 0>
2024-03-05 06:56:12,960 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:12,960 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,960 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40919
2024-03-05 06:56:12,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39756
2024-03-05 06:56:12,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:12,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:12,962 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:12,962 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,964 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:12,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44129', status: init, memory: 0, processing: 0>
2024-03-05 06:56:12,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44129
2024-03-05 06:56:12,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39746
2024-03-05 06:56:12,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:12,967 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:12,967 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:12,971 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32999', status: init, memory: 0, processing: 0>
2024-03-05 06:56:12,972 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32999
2024-03-05 06:56:12,972 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39774
2024-03-05 06:56:12,973 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:12,974 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:12,974 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:12,978 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38359', status: init, memory: 0, processing: 0>
2024-03-05 06:56:12,979 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38359
2024-03-05 06:56:12,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39762
2024-03-05 06:56:12,980 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:12,981 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:12,981 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:12,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:13,088 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:13,088 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:13,088 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:13,089 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:13,089 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:13,089 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:13,089 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:13,090 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:13,094 - distributed.scheduler - INFO - Remove client Client-7039d723-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:13,094 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35492; closing.
2024-03-05 06:56:13,095 - distributed.scheduler - INFO - Remove client Client-7039d723-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:13,095 - distributed.scheduler - INFO - Close client connection: Client-7039d723-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:13,096 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43697'. Reason: nanny-close
2024-03-05 06:56:13,097 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:13,097 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41955'. Reason: nanny-close
2024-03-05 06:56:13,098 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:13,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45203'. Reason: nanny-close
2024-03-05 06:56:13,098 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39777. Reason: nanny-close
2024-03-05 06:56:13,098 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:13,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35101'. Reason: nanny-close
2024-03-05 06:56:13,098 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40919. Reason: nanny-close
2024-03-05 06:56:13,099 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:13,099 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42679'. Reason: nanny-close
2024-03-05 06:56:13,099 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44129. Reason: nanny-close
2024-03-05 06:56:13,099 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:13,099 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38689'. Reason: nanny-close
2024-03-05 06:56:13,100 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:13,100 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38359. Reason: nanny-close
2024-03-05 06:56:13,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45863'. Reason: nanny-close
2024-03-05 06:56:13,100 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32999. Reason: nanny-close
2024-03-05 06:56:13,100 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:13,100 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:13,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46671'. Reason: nanny-close
2024-03-05 06:56:13,101 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39756; closing.
2024-03-05 06:56:13,101 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46663. Reason: nanny-close
2024-03-05 06:56:13,101 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:13,101 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:13,101 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40919', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621773.1013026')
2024-03-05 06:56:13,101 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45741. Reason: nanny-close
2024-03-05 06:56:13,101 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:13,101 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39706; closing.
2024-03-05 06:56:13,102 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:13,102 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621773.102457')
2024-03-05 06:56:13,102 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:13,102 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:13,102 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41307. Reason: nanny-close
2024-03-05 06:56:13,102 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:13,103 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:13,103 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:13,103 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39746; closing.
2024-03-05 06:56:13,104 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:13,104 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:13,104 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:13,105 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:13,105 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:13,104 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39706>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-05 06:56:13,106 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39762; closing.
2024-03-05 06:56:13,106 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:13,106 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39774; closing.
2024-03-05 06:56:13,106 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44129', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621773.1065977')
2024-03-05 06:56:13,106 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:13,107 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38359', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621773.1074426')
2024-03-05 06:56:13,107 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621773.1078155')
2024-03-05 06:56:13,108 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39732; closing.
2024-03-05 06:56:13,108 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39716; closing.
2024-03-05 06:56:13,109 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46663', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621773.1090338')
2024-03-05 06:56:13,109 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621773.109358')
2024-03-05 06:56:13,109 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39720; closing.
2024-03-05 06:56:13,110 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41307', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621773.110123')
2024-03-05 06:56:13,110 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:56:13,110 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39720>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-05 06:56:14,113 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:14,113 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:14,113 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:14,114 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:56:14,115 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-05 06:56:16,214 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:16,218 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-05 06:56:16,221 - distributed.scheduler - INFO - State start
2024-03-05 06:56:16,242 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:16,242 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:56:16,243 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-05 06:56:16,243 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:16,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45269'
2024-03-05 06:56:16,996 - distributed.scheduler - INFO - Receive client connection: Client-75253c54-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:17,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39852
2024-03-05 06:56:18,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:18,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:18,603 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:18,604 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42509
2024-03-05 06:56:18,604 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42509
2024-03-05 06:56:18,605 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-05 06:56:18,605 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:18,605 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:18,605 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:18,605 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-05 06:56:18,605 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qp978qum
2024-03-05 06:56:18,605 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f5cf434-fb35-491b-a1cb-fba3666b1437
2024-03-05 06:56:18,606 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3f86aa3-b7c3-40fd-aef7-5df1344b2f02
2024-03-05 06:56:18,606 - distributed.worker - INFO - Starting Worker plugin PreImport-826b570f-047f-4952-9716-3e1bebf5020b
2024-03-05 06:56:18,606 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:18,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42509', status: init, memory: 0, processing: 0>
2024-03-05 06:56:18,742 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42509
2024-03-05 06:56:18,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39872
2024-03-05 06:56:18,744 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:18,745 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:18,745 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:18,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:18,841 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:18,843 - distributed.scheduler - INFO - Remove client Client-75253c54-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:18,844 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39852; closing.
2024-03-05 06:56:18,844 - distributed.scheduler - INFO - Remove client Client-75253c54-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:18,845 - distributed.scheduler - INFO - Close client connection: Client-75253c54-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:18,845 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45269'. Reason: nanny-close
2024-03-05 06:56:18,846 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:18,847 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42509. Reason: nanny-close
2024-03-05 06:56:18,849 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39872; closing.
2024-03-05 06:56:18,849 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:18,850 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42509', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621778.8500488')
2024-03-05 06:56:18,850 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:56:18,851 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:19,511 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:19,511 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:19,511 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:19,512 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:56:19,513 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-05 06:56:23,979 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:23,984 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44641 instead
  warnings.warn(
2024-03-05 06:56:23,988 - distributed.scheduler - INFO - State start
2024-03-05 06:56:24,088 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:24,089 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:56:24,090 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44641/status
2024-03-05 06:56:24,090 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:24,271 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36691'
2024-03-05 06:56:25,711 - distributed.scheduler - INFO - Receive client connection: Client-79b2441f-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:25,723 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40788
2024-03-05 06:56:26,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:26,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:26,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:26,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36445
2024-03-05 06:56:26,686 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36445
2024-03-05 06:56:26,686 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43489
2024-03-05 06:56:26,686 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:26,687 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:26,687 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:26,687 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-05 06:56:26,687 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uq1l9qg7
2024-03-05 06:56:26,687 - distributed.worker - INFO - Starting Worker plugin PreImport-4046edd2-5538-410c-a13a-2dd5e4fed86e
2024-03-05 06:56:26,688 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c0cabd4-be3b-47d0-b955-150981b42fc1
2024-03-05 06:56:26,688 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eaa5775b-97cf-4743-a4e2-4a4fdafb417f
2024-03-05 06:56:26,688 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:26,737 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36445', status: init, memory: 0, processing: 0>
2024-03-05 06:56:26,738 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36445
2024-03-05 06:56:26,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40808
2024-03-05 06:56:26,739 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:26,739 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:26,739 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:26,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:26,746 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:26,748 - distributed.scheduler - INFO - Remove client Client-79b2441f-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:26,748 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40788; closing.
2024-03-05 06:56:26,749 - distributed.scheduler - INFO - Remove client Client-79b2441f-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:26,749 - distributed.scheduler - INFO - Close client connection: Client-79b2441f-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:26,750 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36691'. Reason: nanny-close
2024-03-05 06:56:26,751 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:26,752 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36445. Reason: nanny-close
2024-03-05 06:56:26,753 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:26,753 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40808; closing.
2024-03-05 06:56:26,754 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36445', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621786.754108')
2024-03-05 06:56:26,754 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:56:26,755 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:27,365 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:27,365 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:27,366 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:27,366 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:56:27,367 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-05 06:56:29,429 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:29,433 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38823 instead
  warnings.warn(
2024-03-05 06:56:29,437 - distributed.scheduler - INFO - State start
2024-03-05 06:56:29,458 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:29,458 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:56:29,459 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38823/status
2024-03-05 06:56:29,459 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:32,111 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:40822'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40822>: Stream is closed
2024-03-05 06:56:32,417 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:32,418 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:32,418 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:32,419 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:56:32,420 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-05 06:56:34,433 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:34,437 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-05 06:56:34,440 - distributed.scheduler - INFO - State start
2024-03-05 06:56:34,764 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:34,765 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-05 06:56:34,765 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-05 06:56:34,766 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:34,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45993'
2024-03-05 06:56:35,100 - distributed.scheduler - INFO - Receive client connection: Client-801b9bd7-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:35,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37864
2024-03-05 06:56:36,710 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:36,710 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:36,714 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:36,715 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42871
2024-03-05 06:56:36,715 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42871
2024-03-05 06:56:36,715 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35173
2024-03-05 06:56:36,715 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-05 06:56:36,715 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:36,715 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:36,715 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-05 06:56:36,715 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-prq9kylf
2024-03-05 06:56:36,715 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8cbf0d3f-5366-42a2-b5bf-08a14c37d8ca
2024-03-05 06:56:36,715 - distributed.worker - INFO - Starting Worker plugin PreImport-57d28a60-aa21-4241-8aca-3c944fd08a20
2024-03-05 06:56:36,716 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2aab5607-3abc-4870-add2-f5399ccdb369
2024-03-05 06:56:36,716 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:36,780 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42871', status: init, memory: 0, processing: 0>
2024-03-05 06:56:36,781 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42871
2024-03-05 06:56:36,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37882
2024-03-05 06:56:36,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:36,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-05 06:56:36,784 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:36,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-05 06:56:36,850 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:36,853 - distributed.scheduler - INFO - Remove client Client-801b9bd7-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:36,853 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37864; closing.
2024-03-05 06:56:36,853 - distributed.scheduler - INFO - Remove client Client-801b9bd7-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:36,854 - distributed.scheduler - INFO - Close client connection: Client-801b9bd7-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:36,854 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45993'. Reason: nanny-close
2024-03-05 06:56:36,855 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:36,856 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42871. Reason: nanny-close
2024-03-05 06:56:36,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37882; closing.
2024-03-05 06:56:36,858 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-05 06:56:36,858 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621796.8587284')
2024-03-05 06:56:36,859 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:56:36,859 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:37,670 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:37,671 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:37,671 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:37,672 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-05 06:56:37,672 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-03-05 06:56:39,912 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:39,916 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-05 06:56:39,920 - distributed.scheduler - INFO - State start
2024-03-05 06:56:39,942 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:39,943 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:56:39,943 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-05 06:56:39,944 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:39,947 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34253'
2024-03-05 06:56:39,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36993'
2024-03-05 06:56:39,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34371'
2024-03-05 06:56:40,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34729'
2024-03-05 06:56:40,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40753'
2024-03-05 06:56:40,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39619'
2024-03-05 06:56:40,088 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41273'
2024-03-05 06:56:40,190 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38987'
2024-03-05 06:56:40,311 - distributed.scheduler - INFO - Receive client connection: Client-834061c8-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:40,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39934
2024-03-05 06:56:41,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:41,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:41,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:41,848 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40733
2024-03-05 06:56:41,849 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40733
2024-03-05 06:56:41,849 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41013
2024-03-05 06:56:41,849 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:41,849 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:41,849 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:41,849 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:41,849 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ukia0ucl
2024-03-05 06:56:41,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6304102f-e211-4699-9232-e8c616fe5d63
2024-03-05 06:56:41,854 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:41,854 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:41,859 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:41,860 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42415
2024-03-05 06:56:41,860 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42415
2024-03-05 06:56:41,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33339
2024-03-05 06:56:41,860 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:41,860 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:41,860 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:41,860 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:41,860 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vbkanjue
2024-03-05 06:56:41,860 - distributed.worker - INFO - Starting Worker plugin PreImport-07996499-14c9-4e40-a35d-948c5582796e
2024-03-05 06:56:41,861 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7fd832d1-ca5d-4655-abee-03cbeded4b4b
2024-03-05 06:56:41,861 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40f4c6c2-be34-4ad0-bbc2-466c1edb9bb6
2024-03-05 06:56:41,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:41,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:41,893 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:41,894 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33105
2024-03-05 06:56:41,894 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33105
2024-03-05 06:56:41,895 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41179
2024-03-05 06:56:41,895 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:41,895 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:41,895 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:41,895 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:41,895 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r9b8a7uo
2024-03-05 06:56:41,895 - distributed.worker - INFO - Starting Worker plugin PreImport-122815c0-0295-49f8-8696-781bf9ecae12
2024-03-05 06:56:41,895 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a74ab38f-2b75-48ad-a71a-a7314a8506af
2024-03-05 06:56:41,895 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0097ca98-19bc-4683-b2b3-ab257b807f92
2024-03-05 06:56:42,122 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:42,122 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:42,124 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:42,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:42,127 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:42,128 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33847
2024-03-05 06:56:42,128 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33847
2024-03-05 06:56:42,128 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38825
2024-03-05 06:56:42,128 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:42,128 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:42,128 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:42,128 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:42,128 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-owvudzqc
2024-03-05 06:56:42,129 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5babc35b-1f35-4f47-935a-2de88cb6d5e8
2024-03-05 06:56:42,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:42,129 - distributed.worker - INFO - Starting Worker plugin PreImport-056664a2-27cc-45f0-b4e5-c84e217ec1a1
2024-03-05 06:56:42,130 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f521e43-720d-417e-9dcd-92bd4293e129
2024-03-05 06:56:42,130 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35837
2024-03-05 06:56:42,130 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35837
2024-03-05 06:56:42,130 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34841
2024-03-05 06:56:42,130 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:42,130 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:42,130 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:42,130 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:42,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iht44exg
2024-03-05 06:56:42,130 - distributed.worker - INFO - Starting Worker plugin RMMSetup-831ad3e6-8483-4cbb-b3a8-91f4d63e3738
2024-03-05 06:56:42,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:42,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:42,139 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:42,140 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46841
2024-03-05 06:56:42,140 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46841
2024-03-05 06:56:42,140 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35759
2024-03-05 06:56:42,140 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:42,140 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:42,140 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:42,140 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:42,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ag1gtxq3
2024-03-05 06:56:42,141 - distributed.worker - INFO - Starting Worker plugin PreImport-08d47716-c255-4c81-bf9d-007da8b92275
2024-03-05 06:56:42,141 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d23dd35-06b5-422d-9b0d-754f023fba1b
2024-03-05 06:56:42,141 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35dc1f40-c2d6-4f9f-93c8-ddffef1475ad
2024-03-05 06:56:42,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:42,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:42,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:42,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:42,146 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:42,146 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36799
2024-03-05 06:56:42,147 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36799
2024-03-05 06:56:42,147 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42653
2024-03-05 06:56:42,147 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:42,147 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:42,147 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:42,147 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:42,147 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ucuqv4u7
2024-03-05 06:56:42,147 - distributed.worker - INFO - Starting Worker plugin PreImport-b40367b2-0840-4df5-b9e9-516f151e67a5
2024-03-05 06:56:42,147 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-badc8dfd-ea5b-46bd-a163-906df48ed10e
2024-03-05 06:56:42,147 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f609d9c7-e267-4feb-ad3c-5eaecda62567
2024-03-05 06:56:42,148 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:42,149 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45943
2024-03-05 06:56:42,149 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45943
2024-03-05 06:56:42,149 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39221
2024-03-05 06:56:42,149 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:42,150 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:42,150 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:42,150 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 06:56:42,150 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2xrma6ir
2024-03-05 06:56:42,150 - distributed.worker - INFO - Starting Worker plugin PreImport-8c49d073-085d-4e7c-b9ee-fda8f0090154
2024-03-05 06:56:42,150 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b7049157-d368-4787-8b6e-2df658b0cf40
2024-03-05 06:56:42,151 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da76c7f5-8f62-45ab-99e0-944512ab251f
2024-03-05 06:56:42,730 - distributed.worker - INFO - Starting Worker plugin PreImport-1b5f0f18-346e-43ce-b7d3-d198b9b8f3c8
2024-03-05 06:56:42,731 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13746710-43d7-4b9b-9b04-fb84a21aded6
2024-03-05 06:56:42,731 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:42,751 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40733', status: init, memory: 0, processing: 0>
2024-03-05 06:56:42,752 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40733
2024-03-05 06:56:42,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39960
2024-03-05 06:56:42,753 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:42,754 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:42,754 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:42,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:43,762 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:43,793 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42415', status: init, memory: 0, processing: 0>
2024-03-05 06:56:43,794 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42415
2024-03-05 06:56:43,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39974
2024-03-05 06:56:43,795 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:43,796 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:43,796 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:43,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:43,946 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:43,972 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33105', status: init, memory: 0, processing: 0>
2024-03-05 06:56:43,973 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33105
2024-03-05 06:56:43,973 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39980
2024-03-05 06:56:43,974 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:43,975 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:43,975 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:43,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:43,994 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,005 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,012 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df191b33-845f-4669-ae1d-f4add7857be3
2024-03-05 06:56:44,013 - distributed.worker - INFO - Starting Worker plugin PreImport-53064d4c-55a0-4a88-8fa6-4ffa1910fbc5
2024-03-05 06:56:44,013 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,021 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,026 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33847', status: init, memory: 0, processing: 0>
2024-03-05 06:56:44,027 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33847
2024-03-05 06:56:44,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39994
2024-03-05 06:56:44,027 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,028 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:44,029 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:44,029 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,031 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46841', status: init, memory: 0, processing: 0>
2024-03-05 06:56:44,031 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46841
2024-03-05 06:56:44,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40010
2024-03-05 06:56:44,032 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:44,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:44,033 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:44,033 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:44,036 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35837', status: init, memory: 0, processing: 0>
2024-03-05 06:56:44,036 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35837
2024-03-05 06:56:44,036 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40026
2024-03-05 06:56:44,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:44,038 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:44,038 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:44,048 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36799', status: init, memory: 0, processing: 0>
2024-03-05 06:56:44,049 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36799
2024-03-05 06:56:44,049 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40030
2024-03-05 06:56:44,050 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:44,051 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:44,051 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,052 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:44,053 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45943', status: init, memory: 0, processing: 0>
2024-03-05 06:56:44,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45943
2024-03-05 06:56:44,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40036
2024-03-05 06:56:44,055 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:44,056 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:44,056 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:44,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:44,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:44,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:44,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:44,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:44,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:44,134 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:44,135 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:44,135 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-05 06:56:44,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:44,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:44,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:44,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:44,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:44,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:44,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:44,148 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:44,152 - distributed.scheduler - INFO - Remove client Client-834061c8-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:44,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39934; closing.
2024-03-05 06:56:44,153 - distributed.scheduler - INFO - Remove client Client-834061c8-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:44,153 - distributed.scheduler - INFO - Close client connection: Client-834061c8-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:44,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34253'. Reason: nanny-close
2024-03-05 06:56:44,154 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:44,155 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34729'. Reason: nanny-close
2024-03-05 06:56:44,155 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:44,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36993'. Reason: nanny-close
2024-03-05 06:56:44,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42415. Reason: nanny-close
2024-03-05 06:56:44,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:44,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40753'. Reason: nanny-close
2024-03-05 06:56:44,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45943. Reason: nanny-close
2024-03-05 06:56:44,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:44,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41273'. Reason: nanny-close
2024-03-05 06:56:44,157 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40733. Reason: nanny-close
2024-03-05 06:56:44,157 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:44,157 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38987'. Reason: nanny-close
2024-03-05 06:56:44,157 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46841. Reason: nanny-close
2024-03-05 06:56:44,157 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:44,157 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39619'. Reason: nanny-close
2024-03-05 06:56:44,157 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:44,158 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33847. Reason: nanny-close
2024-03-05 06:56:44,158 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34371'. Reason: nanny-close
2024-03-05 06:56:44,158 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36799. Reason: nanny-close
2024-03-05 06:56:44,158 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:44,158 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35837. Reason: nanny-close
2024-03-05 06:56:44,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:44,158 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39960; closing.
2024-03-05 06:56:44,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:44,159 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:44,159 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:44,159 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33105. Reason: nanny-close
2024-03-05 06:56:44,159 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40733', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621804.159612')
2024-03-05 06:56:44,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:44,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:44,160 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40036; closing.
2024-03-05 06:56:44,160 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:44,160 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40010; closing.
2024-03-05 06:56:44,160 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:44,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:44,160 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39974; closing.
2024-03-05 06:56:44,160 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:44,160 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:44,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:44,161 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621804.1613026')
2024-03-05 06:56:44,161 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:44,161 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46841', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621804.1615996')
2024-03-05 06:56:44,161 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:44,161 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:44,162 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42415', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621804.1619637')
2024-03-05 06:56:44,162 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:44,163 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39994; closing.
2024-03-05 06:56:44,163 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40030; closing.
2024-03-05 06:56:44,163 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40026; closing.
2024-03-05 06:56:44,164 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39974>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-05 06:56:44,165 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40036>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-05 06:56:44,165 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40010>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-05 06:56:44,166 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33847', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621804.1661425')
2024-03-05 06:56:44,166 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36799', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621804.1665118')
2024-03-05 06:56:44,166 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35837', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621804.1668582')
2024-03-05 06:56:44,167 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39980; closing.
2024-03-05 06:56:44,167 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33105', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621804.167777')
2024-03-05 06:56:44,168 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:56:45,170 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:45,171 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:45,171 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:45,173 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:56:45,173 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-05 06:56:47,553 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:47,558 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-05 06:56:47,562 - distributed.scheduler - INFO - State start
2024-03-05 06:56:47,584 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:47,585 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:56:47,586 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-05 06:56:47,586 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:47,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46571'
2024-03-05 06:56:48,710 - distributed.scheduler - INFO - Receive client connection: Client-87b81ea9-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:48,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40178
2024-03-05 06:56:49,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:49,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:49,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:49,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37243
2024-03-05 06:56:49,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37243
2024-03-05 06:56:49,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42505
2024-03-05 06:56:49,525 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:49,525 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:49,525 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:49,525 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-05 06:56:49,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q34l6ub3
2024-03-05 06:56:49,525 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-773c9a3a-ad8c-4168-abe3-a3907de293c7
2024-03-05 06:56:49,526 - distributed.worker - INFO - Starting Worker plugin PreImport-69c545b7-a45e-4d38-bd5d-51b25dbe24fe
2024-03-05 06:56:49,527 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5391f04-ad45-4108-b400-5294dd89b006
2024-03-05 06:56:49,832 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:49,902 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37243', status: init, memory: 0, processing: 0>
2024-03-05 06:56:49,904 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37243
2024-03-05 06:56:49,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40198
2024-03-05 06:56:49,905 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:49,905 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:49,906 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:49,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:49,945 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:49,949 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:49,951 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:49,953 - distributed.scheduler - INFO - Remove client Client-87b81ea9-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:49,953 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40178; closing.
2024-03-05 06:56:49,954 - distributed.scheduler - INFO - Remove client Client-87b81ea9-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:49,954 - distributed.scheduler - INFO - Close client connection: Client-87b81ea9-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:49,955 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46571'. Reason: nanny-close
2024-03-05 06:56:49,956 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:49,957 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37243. Reason: nanny-close
2024-03-05 06:56:49,959 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40198; closing.
2024-03-05 06:56:49,959 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:49,959 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37243', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621809.9592965')
2024-03-05 06:56:49,959 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:56:49,960 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:50,570 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:50,571 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:50,571 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:50,572 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:56:50,573 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-05 06:56:52,630 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:52,634 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-05 06:56:52,638 - distributed.scheduler - INFO - State start
2024-03-05 06:56:52,660 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-05 06:56:52,661 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-05 06:56:52,661 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-05 06:56:52,662 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-05 06:56:52,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43419'
2024-03-05 06:56:52,895 - distributed.scheduler - INFO - Receive client connection: Client-8ae8b566-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:52,906 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60644
2024-03-05 06:56:54,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 06:56:54,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 06:56:54,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 06:56:54,366 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43745
2024-03-05 06:56:54,366 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43745
2024-03-05 06:56:54,366 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34283
2024-03-05 06:56:54,366 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-05 06:56:54,366 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:54,366 - distributed.worker - INFO -               Threads:                          1
2024-03-05 06:56:54,367 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-05 06:56:54,367 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4cndrqr0
2024-03-05 06:56:54,367 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6eaea739-ad0a-4f69-8319-15276c927730
2024-03-05 06:56:54,367 - distributed.worker - INFO - Starting Worker plugin PreImport-32773555-e714-48b9-8e96-d03462b45899
2024-03-05 06:56:54,367 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27b5b6ca-bcf8-431b-b0b6-8c1652e235e8
2024-03-05 06:56:54,662 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:54,719 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43745', status: init, memory: 0, processing: 0>
2024-03-05 06:56:54,720 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43745
2024-03-05 06:56:54,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60664
2024-03-05 06:56:54,721 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 06:56:54,721 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-05 06:56:54,721 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 06:56:54,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-05 06:56:54,737 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-03-05 06:56:54,742 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-05 06:56:54,745 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:54,747 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 06:56:54,749 - distributed.scheduler - INFO - Remove client Client-8ae8b566-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:54,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60644; closing.
2024-03-05 06:56:54,749 - distributed.scheduler - INFO - Remove client Client-8ae8b566-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:54,750 - distributed.scheduler - INFO - Close client connection: Client-8ae8b566-dabd-11ee-b1b0-d8c49764f6bb
2024-03-05 06:56:54,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43419'. Reason: nanny-close
2024-03-05 06:56:54,772 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-05 06:56:54,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43745. Reason: nanny-close
2024-03-05 06:56:54,775 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60664; closing.
2024-03-05 06:56:54,775 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-05 06:56:54,775 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43745', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1709621814.7755613')
2024-03-05 06:56:54,775 - distributed.scheduler - INFO - Lost all workers
2024-03-05 06:56:54,776 - distributed.nanny - INFO - Worker closed
2024-03-05 06:56:55,516 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-05 06:56:55,517 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-05 06:56:55,517 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-05 06:56:55,518 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-05 06:56:55,519 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46221 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40399 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36933 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] [1709621895.806286] [dgx13:50726:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:32776) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34585 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] 2024-03-05 06:58:39,078 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-11:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 37, in _test_local_cluster
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39873 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40999 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38135 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41181 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32867 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33409 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41409 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34683 instead
  warnings.warn(
2024-03-05 07:04:37,400 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33291 instead
  warnings.warn(
2024-03-05 07:04:39,622 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36961 instead
  warnings.warn(
2024-03-05 07:04:41,934 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43411 instead
  warnings.warn(
2024-03-05 07:04:45,380 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34681 instead
  warnings.warn(
2024-03-05 07:04:48,736 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44751 instead
  warnings.warn(
2024-03-05 07:04:52,095 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] 2024-03-05 07:05:14,843 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:50964 remote=tcp://127.0.0.1:38385>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42941 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40789 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45585 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33423 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] [1709622475.520004] [dgx13:60329:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:44474) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38903 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38569 instead
  warnings.warn(
2024-03-05 07:08:54,754 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-43:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37029 instead
  warnings.warn(
2024-03-05 07:08:57,108 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-44:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45045 instead
  warnings.warn(
2024-03-05 07:08:59,466 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-45:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40347 instead
  warnings.warn(
2024-03-05 07:09:04,090 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-46:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] 2024-03-05 07:09:09,150 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-47:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44093 instead
  warnings.warn(
2024-03-05 07:09:14,422 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-48:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46775 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40251 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33917 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43215 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37049 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34877 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39139 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39671 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38263 instead
  warnings.warn(
[1709622649.907901] [dgx13:63508:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:33214) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39909 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42753 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37285 instead
  warnings.warn(
[1709622705.087414] [dgx13:64196:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:55122) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39539 instead
  warnings.warn(
2024-03-05 07:11:55,845 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-63:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43555 instead
  warnings.warn(
2024-03-05 07:11:57,947 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-64:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46657 instead
  warnings.warn(
2024-03-05 07:12:00,021 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-65:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46111 instead
  warnings.warn(
2024-03-05 07:12:03,413 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-66:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45993 instead
  warnings.warn(
2024-03-05 07:12:06,607 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-67:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42371 instead
  warnings.warn(
2024-03-05 07:12:10,041 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-68:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34573 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43497 instead
  warnings.warn(
[1709622746.642784] [dgx13:65255:0]            sock.c:481  UCX  ERROR bind(fd=165 addr=0.0.0.0:57818) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46317 instead
  warnings.warn(
2024-03-05 07:12:41,469 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-71:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 302, in _test_jit_unspill
    with dask_cuda.LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1709622782.759840] [dgx13:66004:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:52401) failed: Address already in use
2024-03-05 07:13:04,043 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-03-05 07:13:04,073 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] SKIPPED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42985 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45341 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34119 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40967 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1709622833.211219] [dgx13:45488:0]            sock.c:481  UCX  ERROR bind(fd=169 addr=0.0.0.0:38424) failed: Address already in use
[1709622833.212013] [dgx13:45488:0]            sock.c:481  UCX  ERROR bind(fd=174 addr=0.0.0.0:33010) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] [1709622841.613896] [dgx13:45488:0]            sock.c:481  UCX  ERROR bind(fd=175 addr=0.0.0.0:38095) failed: Address already in use
2024-03-05 07:14:07,895 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-03-05 07:14:23,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:14:23,655 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:14:23,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:14:23,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:14:23,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:14:23,660 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:14:23,692 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:14:23,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:14:23,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:14:23,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:14:23,730 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:14:23,730 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:14:23,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:14:23,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:14:23,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:14:23,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:14:24,337 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:14:24,338 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32779
2024-03-05 07:14:24,338 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32779
2024-03-05 07:14:24,338 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42801
2024-03-05 07:14:24,338 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,338 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,338 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:14:24,338 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3m457nbd
2024-03-05 07:14:24,338 - distributed.worker - INFO - Starting Worker plugin PreImport-ac355ff2-6b87-4017-93d4-0cc40f80aae3
2024-03-05 07:14:24,338 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6570fad-24e3-4626-9da3-218b0982b8c5
2024-03-05 07:14:24,338 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37219c20-1022-4fae-bbd3-8a39a4afd90b
2024-03-05 07:14:24,339 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:14:24,340 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35555
2024-03-05 07:14:24,340 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35555
2024-03-05 07:14:24,340 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40159
2024-03-05 07:14:24,340 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,340 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,340 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:14:24,341 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8ktxv32x
2024-03-05 07:14:24,341 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea667318-c7e2-4df3-9df7-ca6cb08580ab
2024-03-05 07:14:24,341 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d3876379-f89b-405b-81b1-5866746646fc
2024-03-05 07:14:24,341 - distributed.worker - INFO - Starting Worker plugin PreImport-fc8ce820-0a8d-4443-8e58-dc39ecb4961c
2024-03-05 07:14:24,341 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,375 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:14:24,376 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42131
2024-03-05 07:14:24,376 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42131
2024-03-05 07:14:24,376 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45197
2024-03-05 07:14:24,377 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,377 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,377 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:14:24,377 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0kwjb35b
2024-03-05 07:14:24,377 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65325db9-28d6-422a-9e95-3f4929fbefcf
2024-03-05 07:14:24,377 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ea7ee8a-eebb-4702-b992-a143566062fe
2024-03-05 07:14:24,377 - distributed.worker - INFO - Starting Worker plugin PreImport-7d81704c-c8d4-4ca4-ba01-6718a8a83d64
2024-03-05 07:14:24,378 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,383 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:14:24,384 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39115
2024-03-05 07:14:24,384 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39115
2024-03-05 07:14:24,385 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36105
2024-03-05 07:14:24,385 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,385 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,385 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:14:24,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ztve4mnv
2024-03-05 07:14:24,385 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3351bc4-3e7e-42bb-93de-15a1a9270583
2024-03-05 07:14:24,386 - distributed.worker - INFO - Starting Worker plugin PreImport-f162e77c-2786-4f2b-a0f1-fed71c23e02e
2024-03-05 07:14:24,386 - distributed.worker - INFO - Starting Worker plugin RMMSetup-088a35ef-9a6d-4eff-b2e9-ee679bcf61b2
2024-03-05 07:14:24,386 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:14:24,404 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39035
2024-03-05 07:14:24,404 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39035
2024-03-05 07:14:24,404 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46601
2024-03-05 07:14:24,404 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,404 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,404 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:14:24,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-05rx6crf
2024-03-05 07:14:24,405 - distributed.worker - INFO - Starting Worker plugin PreImport-3348eef2-140b-460a-b631-26e1be5fb7c3
2024-03-05 07:14:24,405 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3572d81b-b28c-4a0c-83a0-dfd5415f41bd
2024-03-05 07:14:24,405 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32d3584c-5896-4447-b644-adb3cf0a3155
2024-03-05 07:14:24,405 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,497 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:14:24,498 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,498 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34319
2024-03-05 07:14:24,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:14:24,501 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,501 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,503 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34319
2024-03-05 07:14:24,512 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:14:24,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37137
2024-03-05 07:14:24,513 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37137
2024-03-05 07:14:24,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33329
2024-03-05 07:14:24,513 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,513 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,513 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:14:24,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9hysaelq
2024-03-05 07:14:24,514 - distributed.worker - INFO - Starting Worker plugin PreImport-493984e3-643c-4ecb-8412-217171174229
2024-03-05 07:14:24,514 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fdd0a27f-9142-4125-933f-bda420b319c6
2024-03-05 07:14:24,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5df51e2-1d5e-43df-a9dd-3f25ee719e63
2024-03-05 07:14:24,514 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:14:24,518 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42443
2024-03-05 07:14:24,518 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42443
2024-03-05 07:14:24,518 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40247
2024-03-05 07:14:24,518 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,518 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,518 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:14:24,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c8v60ywj
2024-03-05 07:14:24,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebbe61ed-7dfc-4993-ba9c-9f2d775fbe48
2024-03-05 07:14:24,519 - distributed.worker - INFO - Starting Worker plugin PreImport-aba19de8-212b-4b2d-9aa8-bff1052623e0
2024-03-05 07:14:24,519 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59bbb9cf-80f2-423c-8cda-c1e79fc6b86c
2024-03-05 07:14:24,520 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,587 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:14:24,588 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,588 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,588 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:14:24,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44521
2024-03-05 07:14:24,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44521
2024-03-05 07:14:24,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35615
2024-03-05 07:14:24,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34319
2024-03-05 07:14:24,590 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,590 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,590 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:14:24,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-41a7glgg
2024-03-05 07:14:24,590 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a6664d0-f17b-4f84-9d3c-3936e048a820
2024-03-05 07:14:24,590 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7da42639-28f9-497f-8c01-1d91cd7577e7
2024-03-05 07:14:24,590 - distributed.worker - INFO - Starting Worker plugin PreImport-299acddf-8fef-4763-874d-602c12723973
2024-03-05 07:14:24,591 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:14:24,597 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,597 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34319
2024-03-05 07:14:24,602 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:14:24,603 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,603 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34319
2024-03-05 07:14:24,667 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:14:24,668 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,668 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34319
2024-03-05 07:14:24,672 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:14:24,673 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,673 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34319
2024-03-05 07:14:24,693 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:14:24,694 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34319
2024-03-05 07:14:24,694 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:14:24,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34319
2024-03-05 07:14:24,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 07:14:24,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 07:14:24,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 07:14:24,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 07:14:24,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 07:14:24,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 07:14:24,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 07:14:24,717 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-05 07:14:24,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32779. Reason: nanny-close
2024-03-05 07:14:24,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35555. Reason: nanny-close
2024-03-05 07:14:24,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39115. Reason: nanny-close
2024-03-05 07:14:24,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:34319 has been closed.
2024-03-05 07:14:24,726 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42131. Reason: nanny-close
2024-03-05 07:14:24,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:34319 has been closed.
2024-03-05 07:14:24,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39035. Reason: nanny-close
2024-03-05 07:14:24,727 - distributed.nanny - INFO - Worker closed
2024-03-05 07:14:24,727 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37137. Reason: nanny-close
2024-03-05 07:14:24,728 - distributed.nanny - INFO - Worker closed
2024-03-05 07:14:24,728 - distributed.core - INFO - Connection to tcp://127.0.0.1:34319 has been closed.
2024-03-05 07:14:24,728 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42443. Reason: nanny-close
2024-03-05 07:14:24,728 - distributed.core - INFO - Connection to tcp://127.0.0.1:34319 has been closed.
2024-03-05 07:14:24,729 - distributed.core - INFO - Connection to tcp://127.0.0.1:34319 has been closed.
2024-03-05 07:14:24,729 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44521. Reason: nanny-close
2024-03-05 07:14:24,729 - distributed.nanny - INFO - Worker closed
2024-03-05 07:14:24,730 - distributed.nanny - INFO - Worker closed
2024-03-05 07:14:24,730 - distributed.core - INFO - Connection to tcp://127.0.0.1:34319 has been closed.
2024-03-05 07:14:24,730 - distributed.nanny - INFO - Worker closed
2024-03-05 07:14:24,730 - distributed.core - INFO - Connection to tcp://127.0.0.1:34319 has been closed.
2024-03-05 07:14:24,731 - distributed.core - INFO - Connection to tcp://127.0.0.1:34319 has been closed.
2024-03-05 07:14:24,732 - distributed.nanny - INFO - Worker closed
2024-03-05 07:14:24,732 - distributed.nanny - INFO - Worker closed
2024-03-05 07:14:24,732 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-03-05 07:15:03,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:03,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:03,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:03,946 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41115
2024-03-05 07:15:03,947 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41115
2024-03-05 07:15:03,947 - distributed.worker - INFO -           Worker name:                          0
2024-03-05 07:15:03,947 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35807
2024-03-05 07:15:03,947 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45865
2024-03-05 07:15:03,947 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:03,947 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:03,947 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-05 07:15:03,947 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k_w1b7_0
2024-03-05 07:15:03,947 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09d666a7-2f63-46e9-b3dc-be3c76a46431
2024-03-05 07:15:03,948 - distributed.worker - INFO - Starting Worker plugin PreImport-3e921d25-26f9-453e-8336-783f7c65d8c7
2024-03-05 07:15:03,965 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-03-05 07:15:03,966 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad780b03-daaa-43aa-8df8-c7dc3993cf3f
2024-03-05 07:15:03,967 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41115. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-03-05 07:15:03,967 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-05 07:15:03,970 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-03-05 07:15:09,176 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:09,176 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:09,179 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:09,179 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:09,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:09,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:09,217 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:09,217 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:09,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:09,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:09,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:09,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:09,454 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:09,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:09,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-05 07:15:09,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-05 07:15:09,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:09,863 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36171
2024-03-05 07:15:09,864 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36171
2024-03-05 07:15:09,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40933
2024-03-05 07:15:09,864 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39133
2024-03-05 07:15:09,864 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,864 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:09,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 07:15:09,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ndt0_iz
2024-03-05 07:15:09,864 - distributed.worker - INFO - Starting Worker plugin PreImport-e2c817ea-ac69-45c2-ba21-d0cdcd06e122
2024-03-05 07:15:09,864 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84ea67bc-5bed-4c33-8042-06bd1c896867
2024-03-05 07:15:09,864 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ce9c4e1-17b8-479e-8ef4-7fd71f1e8b4e
2024-03-05 07:15:09,865 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,867 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:09,868 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39827
2024-03-05 07:15:09,868 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39827
2024-03-05 07:15:09,868 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44525
2024-03-05 07:15:09,868 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39133
2024-03-05 07:15:09,868 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,868 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:09,868 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 07:15:09,868 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1iks2vsj
2024-03-05 07:15:09,869 - distributed.worker - INFO - Starting Worker plugin PreImport-8debe0a8-c87f-4cd6-a261-d480501ded15
2024-03-05 07:15:09,869 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0de4d91f-49f7-40b7-a765-c984c95c53a0
2024-03-05 07:15:09,869 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-587a94e5-3505-46d1-b57a-806f8936446b
2024-03-05 07:15:09,870 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,871 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:09,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44577
2024-03-05 07:15:09,872 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44577
2024-03-05 07:15:09,872 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46043
2024-03-05 07:15:09,872 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39133
2024-03-05 07:15:09,872 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,872 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:09,872 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 07:15:09,872 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1kag4lvl
2024-03-05 07:15:09,872 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e49332d-4ea0-4f5f-ba1b-b54cbdaec7ec
2024-03-05 07:15:09,873 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1050bbdc-2c05-4813-8629-8ecb40b2a125
2024-03-05 07:15:09,873 - distributed.worker - INFO - Starting Worker plugin PreImport-ac5756cd-b32f-4171-94d1-bb0a7360e945
2024-03-05 07:15:09,873 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,924 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:09,925 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44369
2024-03-05 07:15:09,925 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44369
2024-03-05 07:15:09,925 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35955
2024-03-05 07:15:09,925 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39133
2024-03-05 07:15:09,925 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,925 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:09,925 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 07:15:09,925 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a9fgpew_
2024-03-05 07:15:09,926 - distributed.worker - INFO - Starting Worker plugin PreImport-cab9e381-f811-4879-8bcf-bec6c2f7609c
2024-03-05 07:15:09,926 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f89d5598-624e-4fbc-9f91-50ab15e33edc
2024-03-05 07:15:09,926 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67621010-6d5d-4c1d-b936-db5edf4d53ee
2024-03-05 07:15:09,926 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:09,984 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41009
2024-03-05 07:15:09,984 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41009
2024-03-05 07:15:09,984 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45833
2024-03-05 07:15:09,984 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39133
2024-03-05 07:15:09,984 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:09,984 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:09,984 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 07:15:09,985 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yp532jdn
2024-03-05 07:15:09,985 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fdf56b8-effd-40d0-83e1-625a35dfc7e0
2024-03-05 07:15:09,988 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26abab23-d087-4c55-b062-d80799e806df
2024-03-05 07:15:09,988 - distributed.worker - INFO - Starting Worker plugin PreImport-c9d38901-e9f0-4756-9484-80e40a3ab241
2024-03-05 07:15:09,988 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:10,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43253
2024-03-05 07:15:10,075 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43253
2024-03-05 07:15:10,075 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44243
2024-03-05 07:15:10,075 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,075 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,075 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:10,075 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 07:15:10,075 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oqmprx3s
2024-03-05 07:15:10,075 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8df25db9-6e41-4031-a6e4-a82586af40f4
2024-03-05 07:15:10,075 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1d648f96-27ed-4077-bf70-a3b2dca94b68
2024-03-05 07:15:10,076 - distributed.worker - INFO - Starting Worker plugin PreImport-5a91a29d-dcbc-40b9-a47a-4599f80b775e
2024-03-05 07:15:10,076 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,183 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:10,185 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33657
2024-03-05 07:15:10,185 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33657
2024-03-05 07:15:10,185 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33103
2024-03-05 07:15:10,185 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,185 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,185 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:10,185 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 07:15:10,185 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wv5ywu1i
2024-03-05 07:15:10,185 - distributed.worker - INFO - Starting Worker plugin PreImport-e8869438-7884-40a3-99eb-bc10f1a7cd32
2024-03-05 07:15:10,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5776002-4c38-4054-98f4-45654a730a65
2024-03-05 07:15:10,186 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd6dcb8c-a1c5-40d0-aed5-891024eddaaa
2024-03-05 07:15:10,186 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,187 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-05 07:15:10,188 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46481
2024-03-05 07:15:10,188 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46481
2024-03-05 07:15:10,188 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40829
2024-03-05 07:15:10,188 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,188 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,188 - distributed.worker - INFO -               Threads:                          1
2024-03-05 07:15:10,188 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-05 07:15:10,188 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xeijocj2
2024-03-05 07:15:10,189 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c07fbcc-6cd6-481f-af34-4dca7d744346
2024-03-05 07:15:10,189 - distributed.worker - INFO - Starting Worker plugin PreImport-87f0b020-b560-457a-bcc2-da34677c7cc9
2024-03-05 07:15:10,189 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-971006ec-e7e6-48c3-ac68-2494da0a0238
2024-03-05 07:15:10,191 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:15:10,637 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,637 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39133
2024-03-05 07:15:10,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:15:10,745 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,746 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39133
2024-03-05 07:15:10,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:15:10,768 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,768 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39133
2024-03-05 07:15:10,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:15:10,800 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,800 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39133
2024-03-05 07:15:10,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:15:10,841 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,841 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,843 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39133
2024-03-05 07:15:10,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:15:10,903 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,903 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39133
2024-03-05 07:15:10,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:15:10,915 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,915 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,916 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-05 07:15:10,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39133
2024-03-05 07:15:10,918 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39133
2024-03-05 07:15:10,918 - distributed.worker - INFO - -------------------------------------------------
2024-03-05 07:15:10,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39133
2024-03-05 07:15:10,959 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44577. Reason: nanny-close
2024-03-05 07:15:10,959 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36171. Reason: nanny-close
2024-03-05 07:15:10,960 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39827. Reason: nanny-close
2024-03-05 07:15:10,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41009. Reason: nanny-close
2024-03-05 07:15:10,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44369. Reason: nanny-close
2024-03-05 07:15:10,961 - distributed.core - INFO - Connection to tcp://127.0.0.1:39133 has been closed.
2024-03-05 07:15:10,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43253. Reason: nanny-close
2024-03-05 07:15:10,962 - distributed.core - INFO - Connection to tcp://127.0.0.1:39133 has been closed.
2024-03-05 07:15:10,962 - distributed.nanny - INFO - Worker closed
2024-03-05 07:15:10,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:39133 has been closed.
2024-03-05 07:15:10,963 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33657. Reason: nanny-close
2024-03-05 07:15:10,963 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46481. Reason: nanny-close
2024-03-05 07:15:10,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:39133 has been closed.
2024-03-05 07:15:10,963 - distributed.nanny - INFO - Worker closed
2024-03-05 07:15:10,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:39133 has been closed.
2024-03-05 07:15:10,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:39133 has been closed.
2024-03-05 07:15:10,964 - distributed.nanny - INFO - Worker closed
2024-03-05 07:15:10,964 - distributed.nanny - INFO - Worker closed
2024-03-05 07:15:10,965 - distributed.nanny - INFO - Worker closed
2024-03-05 07:15:10,966 - distributed.nanny - INFO - Worker closed
2024-03-05 07:15:10,966 - distributed.core - INFO - Connection to tcp://127.0.0.1:39133 has been closed.
2024-03-05 07:15:10,966 - distributed.core - INFO - Connection to tcp://127.0.0.1:39133 has been closed.
2024-03-05 07:15:10,967 - distributed.nanny - INFO - Worker closed
2024-03-05 07:15:10,968 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-03-05 07:15:55,729 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-f385206b075b1ba5a317a4172fdba511', 0)
Function:  subgraph_callable-6f9cb824-5e81-4980-abcc-58c8ad75
args:      (   key
0    0
1    1
2    2
3    3
4    4
5    5
6    6
7    7
8    8
9    9, '_partitions', 'getitem-1854ee8e9e99669ed828c701ddc744bf', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory')"

FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-03-05 07:16:01,057 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-266746600c3b6705bb83008947330318', 1)
Function:  subgraph_callable-40e0d343-afb9-49d8-b8d6-51581d54
args:      (   key
5    5
6    6
7    7
8    8
9    9, '_partitions', 'getitem-43824e46bd4ee4016e40265f05cb448f', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory')"

2024-03-05 07:16:01,075 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('assign-266746600c3b6705bb83008947330318', 0))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-03-05 07:16:05,661 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-05 07:16:05,668 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fb3b6b7e6d0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-05 07:16:07,672 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-03-05 07:16:36,026 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-05 07:16:36,033 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7efd89d646d0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-05 07:16:38,037 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-03-05 07:17:06,522 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-05 07:17:06,528 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f71f92416d0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-05 07:17:08,532 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-03-05 07:17:37,106 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-05 07:17:37,118 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fbd9469f6a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-05 07:17:39,122 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-03-05 07:18:12,047 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 936, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 996, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 376, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-05 07:18:12,050 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 936, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 996, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 376, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
