============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.0.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-22 07:03:16,197 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:16,202 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36403 instead
  warnings.warn(
2024-02-22 07:03:16,206 - distributed.scheduler - INFO - State start
2024-02-22 07:03:16,228 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:16,229 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-22 07:03:16,230 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36403/status
2024-02-22 07:03:16,230 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:03:16,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34861'
2024-02-22 07:03:16,330 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39185'
2024-02-22 07:03:16,334 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37821'
2024-02-22 07:03:16,341 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38735'
2024-02-22 07:03:16,852 - distributed.scheduler - INFO - Receive client connection: Client-727b2451-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:16,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36938
2024-02-22 07:03:18,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:18,193 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:18,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:18,193 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:18,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:18,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:18,198 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37835
2024-02-22 07:03:18,198 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42669
2024-02-22 07:03:18,198 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37835
2024-02-22 07:03:18,198 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42669
2024-02-22 07:03:18,198 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40779
2024-02-22 07:03:18,198 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40669
2024-02-22 07:03:18,198 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-22 07:03:18,198 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-22 07:03:18,198 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,198 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,198 - distributed.worker - INFO -               Threads:                          4
2024-02-22 07:03:18,198 - distributed.worker - INFO -               Threads:                          4
2024-02-22 07:03:18,198 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-22 07:03:18,198 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-22 07:03:18,198 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-d27unasc
2024-02-22 07:03:18,198 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-_ycr1w4p
2024-02-22 07:03:18,199 - distributed.worker - INFO - Starting Worker plugin PreImport-db38f0a0-8ff4-43d2-b7db-bad2e441925e
2024-02-22 07:03:18,199 - distributed.worker - INFO - Starting Worker plugin PreImport-40020f4f-53d0-4c1f-908a-09dadf129a6f
2024-02-22 07:03:18,199 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b9ff55f-f774-437f-8099-2421f0c1b884
2024-02-22 07:03:18,199 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-306bc998-9731-499c-bef2-b9ae9aba588c
2024-02-22 07:03:18,199 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a55a2f3-06fc-47cc-a83b-62f0e379e602
2024-02-22 07:03:18,199 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,199 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6dadaafe-6d2f-465e-a45e-bd91dda489ec
2024-02-22 07:03:18,199 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:18,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:18,218 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:18,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38599
2024-02-22 07:03:18,218 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38599
2024-02-22 07:03:18,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44733
2024-02-22 07:03:18,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-22 07:03:18,219 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,219 - distributed.worker - INFO -               Threads:                          4
2024-02-22 07:03:18,219 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-22 07:03:18,219 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-e2fyjncd
2024-02-22 07:03:18,219 - distributed.worker - INFO - Starting Worker plugin PreImport-411baead-b90c-4c05-b93e-4f6ade96a6f8
2024-02-22 07:03:18,219 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54014b75-bd98-448c-ab7c-ede882dacab7
2024-02-22 07:03:18,219 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ca30199-b8d6-4e0d-b2a3-d5174ef2bb74
2024-02-22 07:03:18,219 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:18,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:18,227 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:18,228 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38773
2024-02-22 07:03:18,228 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38773
2024-02-22 07:03:18,228 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39191
2024-02-22 07:03:18,228 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-22 07:03:18,228 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,228 - distributed.worker - INFO -               Threads:                          4
2024-02-22 07:03:18,228 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-22 07:03:18,228 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-__ed8tuf
2024-02-22 07:03:18,228 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82118e96-e317-469d-8cb0-0f57e173cb68
2024-02-22 07:03:18,229 - distributed.worker - INFO - Starting Worker plugin PreImport-13161edc-07de-4b4f-8ec1-b522a3891c0d
2024-02-22 07:03:18,229 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9b696de-04db-4e57-aa2f-19f119cfc99f
2024-02-22 07:03:18,229 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,310 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37835', status: init, memory: 0, processing: 0>
2024-02-22 07:03:18,312 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37835
2024-02-22 07:03:18,312 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36964
2024-02-22 07:03:18,312 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:18,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-22 07:03:18,313 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-22 07:03:18,318 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42669', status: init, memory: 0, processing: 0>
2024-02-22 07:03:18,318 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42669
2024-02-22 07:03:18,318 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36972
2024-02-22 07:03:18,319 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:18,320 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-22 07:03:18,320 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-22 07:03:18,340 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38599', status: init, memory: 0, processing: 0>
2024-02-22 07:03:18,340 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38599
2024-02-22 07:03:18,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36976
2024-02-22 07:03:18,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:18,342 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-22 07:03:18,342 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,343 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-22 07:03:18,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38773', status: init, memory: 0, processing: 0>
2024-02-22 07:03:18,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38773
2024-02-22 07:03:18,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36984
2024-02-22 07:03:18,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:18,346 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-22 07:03:18,346 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:18,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-22 07:03:18,403 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-22 07:03:18,403 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-22 07:03:18,403 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-22 07:03:18,403 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-22 07:03:18,408 - distributed.scheduler - INFO - Remove client Client-727b2451-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:18,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36938; closing.
2024-02-22 07:03:18,409 - distributed.scheduler - INFO - Remove client Client-727b2451-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:18,409 - distributed.scheduler - INFO - Close client connection: Client-727b2451-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:18,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34861'. Reason: nanny-close
2024-02-22 07:03:18,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:18,411 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39185'. Reason: nanny-close
2024-02-22 07:03:18,411 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:18,411 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37821'. Reason: nanny-close
2024-02-22 07:03:18,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37835. Reason: nanny-close
2024-02-22 07:03:18,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:18,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38735'. Reason: nanny-close
2024-02-22 07:03:18,412 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42669. Reason: nanny-close
2024-02-22 07:03:18,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:18,412 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38599. Reason: nanny-close
2024-02-22 07:03:18,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38773. Reason: nanny-close
2024-02-22 07:03:18,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-22 07:03:18,413 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36964; closing.
2024-02-22 07:03:18,413 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585398.4138372')
2024-02-22 07:03:18,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-22 07:03:18,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-22 07:03:18,414 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:18,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36976; closing.
2024-02-22 07:03:18,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-22 07:03:18,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38599', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585398.4152555')
2024-02-22 07:03:18,415 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:18,415 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:18,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36972; closing.
2024-02-22 07:03:18,416 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:18,415 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:36976>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:36976>: Stream is closed
2024-02-22 07:03:18,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36984; closing.
2024-02-22 07:03:18,418 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42669', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585398.4181604')
2024-02-22 07:03:18,418 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38773', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585398.418561')
2024-02-22 07:03:18,418 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:03:19,076 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:03:19,076 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:03:19,076 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:03:19,077 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-22 07:03:19,078 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-22 07:03:21,416 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:21,420 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46137 instead
  warnings.warn(
2024-02-22 07:03:21,424 - distributed.scheduler - INFO - State start
2024-02-22 07:03:21,447 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:21,447 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:03:21,448 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46137/status
2024-02-22 07:03:21,448 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:03:21,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39977'
2024-02-22 07:03:21,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45985'
2024-02-22 07:03:21,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38593'
2024-02-22 07:03:21,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44563'
2024-02-22 07:03:21,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41461'
2024-02-22 07:03:21,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33953'
2024-02-22 07:03:21,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41941'
2024-02-22 07:03:21,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43619'
2024-02-22 07:03:22,710 - distributed.scheduler - INFO - Receive client connection: Client-7588a52c-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:22,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41438
2024-02-22 07:03:23,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:23,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:23,478 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:23,479 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39925
2024-02-22 07:03:23,479 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39925
2024-02-22 07:03:23,479 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36387
2024-02-22 07:03:23,479 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:23,479 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:23,480 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:23,480 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:23,480 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3pongiat
2024-02-22 07:03:23,480 - distributed.worker - INFO - Starting Worker plugin PreImport-7336436a-dcc7-453b-8fcc-9a3de724b5db
2024-02-22 07:03:23,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8bf9251-1b2e-4dc5-8cae-f70a90209ecc
2024-02-22 07:03:23,480 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ed196c9-45a3-4143-baf2-defe6e6a2001
2024-02-22 07:03:23,501 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:23,501 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:23,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:23,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38581
2024-02-22 07:03:23,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38581
2024-02-22 07:03:23,506 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46631
2024-02-22 07:03:23,506 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:23,506 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:23,507 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:23,507 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:23,507 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wqsgbqnk
2024-02-22 07:03:23,507 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad81e1e6-aab9-4e8b-b026-ca4de92ea459
2024-02-22 07:03:23,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:23,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:23,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:23,517 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35777
2024-02-22 07:03:23,517 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35777
2024-02-22 07:03:23,517 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40361
2024-02-22 07:03:23,517 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:23,517 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:23,517 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:23,517 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:23,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qiba8325
2024-02-22 07:03:23,518 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6469736-bc76-4dcf-a7df-cbd846c91e62
2024-02-22 07:03:23,518 - distributed.worker - INFO - Starting Worker plugin PreImport-3d7a3e07-ea4c-4dbb-9896-53e35ecb1605
2024-02-22 07:03:23,518 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09814034-3020-476f-8ff4-062a80ba43fd
2024-02-22 07:03:23,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:23,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:23,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:23,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:23,523 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:23,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:23,524 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43541
2024-02-22 07:03:23,524 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43541
2024-02-22 07:03:23,524 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40349
2024-02-22 07:03:23,524 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:23,524 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:23,524 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:23,524 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:23,524 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-se1kwoym
2024-02-22 07:03:23,524 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a678186a-a503-4175-8bfd-fdd5833b593e
2024-02-22 07:03:23,524 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43855
2024-02-22 07:03:23,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43855
2024-02-22 07:03:23,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41803
2024-02-22 07:03:23,525 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:23,525 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:23,525 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:23,525 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:23,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3vlb55bw
2024-02-22 07:03:23,525 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2b6c8f1-8cc3-483e-afd0-9e122fa7f52b
2024-02-22 07:03:23,525 - distributed.worker - INFO - Starting Worker plugin PreImport-5169378d-88f9-41fd-89d5-1d97d33cf151
2024-02-22 07:03:23,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f36dd669-8131-4e31-838b-b8536c0cca0c
2024-02-22 07:03:23,531 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:23,531 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:23,535 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:23,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45141
2024-02-22 07:03:23,536 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45141
2024-02-22 07:03:23,536 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38053
2024-02-22 07:03:23,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:23,536 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:23,536 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:23,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:23,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pvcwmrnz
2024-02-22 07:03:23,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa0d13e3-44a3-4099-95ee-b46979d95fb2
2024-02-22 07:03:23,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:23,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:23,769 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:23,770 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44675
2024-02-22 07:03:23,770 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44675
2024-02-22 07:03:23,770 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36559
2024-02-22 07:03:23,770 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:23,770 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:23,770 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:23,770 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:23,770 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-byg_fb2c
2024-02-22 07:03:23,771 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5bfafe23-26b6-41e8-968c-85d9a0878cea
2024-02-22 07:03:23,776 - distributed.worker - INFO - Starting Worker plugin PreImport-5f5f34b3-8b3d-4fed-bd50-12ca72fecfda
2024-02-22 07:03:23,776 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4bf9cd5-b90d-4962-95cb-26d772c474f4
2024-02-22 07:03:23,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:23,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:23,798 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:23,799 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36203
2024-02-22 07:03:23,799 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36203
2024-02-22 07:03:23,799 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43365
2024-02-22 07:03:23,799 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:23,799 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:23,799 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:23,799 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:23,800 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eu1gvb4o
2024-02-22 07:03:23,800 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb67093e-3927-416a-8d03-335fb2c3cd83
2024-02-22 07:03:25,430 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,454 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39925', status: init, memory: 0, processing: 0>
2024-02-22 07:03:25,455 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39925
2024-02-22 07:03:25,455 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41450
2024-02-22 07:03:25,456 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:25,457 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:25,457 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:25,506 - distributed.worker - INFO - Starting Worker plugin PreImport-439d209f-f98d-42ba-a535-5c2c5fa5d56e
2024-02-22 07:03:25,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98d0a2a1-6edc-4614-952f-9ac1383a948d
2024-02-22 07:03:25,509 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,541 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43541', status: init, memory: 0, processing: 0>
2024-02-22 07:03:25,541 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43541
2024-02-22 07:03:25,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41452
2024-02-22 07:03:25,543 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:25,544 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:25,544 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:25,551 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54b5bbeb-94a9-4813-aa97-c26023b95461
2024-02-22 07:03:25,551 - distributed.worker - INFO - Starting Worker plugin PreImport-589f02d8-9095-4a35-ae89-7081a803fb13
2024-02-22 07:03:25,552 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,563 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,573 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38581', status: init, memory: 0, processing: 0>
2024-02-22 07:03:25,573 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38581
2024-02-22 07:03:25,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41466
2024-02-22 07:03:25,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:25,575 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:25,575 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:25,585 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43855', status: init, memory: 0, processing: 0>
2024-02-22 07:03:25,585 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43855
2024-02-22 07:03:25,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41482
2024-02-22 07:03:25,586 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:25,587 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:25,587 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:25,601 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f7bf14d-7aee-468e-b671-3cf7d6be0672
2024-02-22 07:03:25,610 - distributed.worker - INFO - Starting Worker plugin PreImport-3f92286a-613f-4ab8-878e-ef1d4fc6468d
2024-02-22 07:03:25,610 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,625 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6860181e-a674-4e05-9730-7646a5c51b0e
2024-02-22 07:03:25,626 - distributed.worker - INFO - Starting Worker plugin PreImport-f3550191-e862-4a77-b4c1-fd3e62cb193a
2024-02-22 07:03:25,627 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,632 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45141', status: init, memory: 0, processing: 0>
2024-02-22 07:03:25,632 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45141
2024-02-22 07:03:25,632 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41498
2024-02-22 07:03:25,633 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35777', status: init, memory: 0, processing: 0>
2024-02-22 07:03:25,633 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:25,634 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35777
2024-02-22 07:03:25,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41484
2024-02-22 07:03:25,634 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:25,634 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,635 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:25,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:25,636 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:25,636 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:25,641 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,659 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36203', status: init, memory: 0, processing: 0>
2024-02-22 07:03:25,659 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36203
2024-02-22 07:03:25,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41506
2024-02-22 07:03:25,661 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:25,662 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:25,662 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:25,671 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44675', status: init, memory: 0, processing: 0>
2024-02-22 07:03:25,672 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44675
2024-02-22 07:03:25,672 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41512
2024-02-22 07:03:25,673 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:25,674 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:25,674 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:25,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:25,734 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:25,735 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:25,735 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:25,735 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:25,735 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:25,735 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:25,735 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:25,735 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:25,740 - distributed.scheduler - INFO - Remove client Client-7588a52c-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:25,740 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41438; closing.
2024-02-22 07:03:25,740 - distributed.scheduler - INFO - Remove client Client-7588a52c-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:25,741 - distributed.scheduler - INFO - Close client connection: Client-7588a52c-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:25,742 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39977'. Reason: nanny-close
2024-02-22 07:03:25,742 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:25,742 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45985'. Reason: nanny-close
2024-02-22 07:03:25,743 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:25,743 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38593'. Reason: nanny-close
2024-02-22 07:03:25,743 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39925. Reason: nanny-close
2024-02-22 07:03:25,743 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:25,743 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44563'. Reason: nanny-close
2024-02-22 07:03:25,744 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38581. Reason: nanny-close
2024-02-22 07:03:25,744 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:25,744 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41461'. Reason: nanny-close
2024-02-22 07:03:25,744 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43541. Reason: nanny-close
2024-02-22 07:03:25,744 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:25,744 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33953'. Reason: nanny-close
2024-02-22 07:03:25,744 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35777. Reason: nanny-close
2024-02-22 07:03:25,745 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:25,745 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41941'. Reason: nanny-close
2024-02-22 07:03:25,745 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45141. Reason: nanny-close
2024-02-22 07:03:25,745 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41450; closing.
2024-02-22 07:03:25,745 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:25,745 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:25,745 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43619'. Reason: nanny-close
2024-02-22 07:03:25,745 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43855. Reason: nanny-close
2024-02-22 07:03:25,745 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585405.745675')
2024-02-22 07:03:25,745 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:25,745 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:25,746 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36203. Reason: nanny-close
2024-02-22 07:03:25,746 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44675. Reason: nanny-close
2024-02-22 07:03:25,746 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:25,747 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:25,747 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:25,747 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:25,747 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:25,747 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:25,747 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41452; closing.
2024-02-22 07:03:25,747 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41466; closing.
2024-02-22 07:03:25,748 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:25,748 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:25,748 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:25,749 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:25,749 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:25,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43541', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585405.7490015')
2024-02-22 07:03:25,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585405.7494943')
2024-02-22 07:03:25,749 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:25,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41484; closing.
2024-02-22 07:03:25,750 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41498; closing.
2024-02-22 07:03:25,750 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:25,750 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585405.7508833')
2024-02-22 07:03:25,751 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:25,751 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45141', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585405.7513602')
2024-02-22 07:03:25,751 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41482; closing.
2024-02-22 07:03:25,752 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43855', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585405.7528048')
2024-02-22 07:03:25,753 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41506; closing.
2024-02-22 07:03:25,753 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41512; closing.
2024-02-22 07:03:25,754 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36203', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585405.7542546')
2024-02-22 07:03:25,754 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585405.754837')
2024-02-22 07:03:25,755 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:03:26,657 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:03:26,658 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:03:26,658 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:03:26,659 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:03:26,660 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-22 07:03:28,985 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:28,990 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42813 instead
  warnings.warn(
2024-02-22 07:03:28,994 - distributed.scheduler - INFO - State start
2024-02-22 07:03:29,016 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:29,017 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:03:29,018 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42813/status
2024-02-22 07:03:29,018 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:03:29,097 - distributed.scheduler - INFO - Receive client connection: Client-7a0f01f7-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:29,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41572
2024-02-22 07:03:29,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35373'
2024-02-22 07:03:29,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35011'
2024-02-22 07:03:29,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37941'
2024-02-22 07:03:29,224 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33083'
2024-02-22 07:03:29,227 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39769'
2024-02-22 07:03:29,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46759'
2024-02-22 07:03:29,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33681'
2024-02-22 07:03:29,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39099'
2024-02-22 07:03:31,081 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:31,082 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:31,086 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:31,087 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37717
2024-02-22 07:03:31,087 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37717
2024-02-22 07:03:31,087 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43553
2024-02-22 07:03:31,087 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,087 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,087 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:31,087 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:31,087 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xcrvrvgk
2024-02-22 07:03:31,087 - distributed.worker - INFO - Starting Worker plugin PreImport-0c1c7fe1-d735-422f-a460-b78a39456b74
2024-02-22 07:03:31,087 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a6e9cf7-ac4c-40d1-986c-abe9a8df4f2b
2024-02-22 07:03:31,088 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27d382ea-43d7-48d1-94d5-a77c8a919ea0
2024-02-22 07:03:31,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:31,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:31,142 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:31,143 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38969
2024-02-22 07:03:31,143 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38969
2024-02-22 07:03:31,143 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34651
2024-02-22 07:03:31,143 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,143 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,143 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:31,143 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:31,143 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pynx0uej
2024-02-22 07:03:31,143 - distributed.worker - INFO - Starting Worker plugin PreImport-adf82b91-50a1-4a1c-a4d8-899f6270f8aa
2024-02-22 07:03:31,143 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-249e2397-b5bb-4fbe-9da0-e95002cbb84f
2024-02-22 07:03:31,143 - distributed.worker - INFO - Starting Worker plugin RMMSetup-553b2a6d-1fbc-453d-9db7-e6f596b4e441
2024-02-22 07:03:31,144 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:31,144 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:31,148 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:31,149 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32795
2024-02-22 07:03:31,149 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32795
2024-02-22 07:03:31,149 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46443
2024-02-22 07:03:31,150 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,150 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,150 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:31,150 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:31,150 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4myvibou
2024-02-22 07:03:31,150 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12bf2e0c-e469-4997-8d80-66d05e602517
2024-02-22 07:03:31,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:31,325 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:31,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:31,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:31,329 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:31,330 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36807
2024-02-22 07:03:31,330 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36807
2024-02-22 07:03:31,330 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34667
2024-02-22 07:03:31,330 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,330 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,330 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:31,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:31,331 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qapyb5i7
2024-02-22 07:03:31,331 - distributed.worker - INFO - Starting Worker plugin PreImport-f621d88e-a4b1-4b5a-9cef-f003b6e5ac16
2024-02-22 07:03:31,331 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12d4bcf1-4117-4f82-b1ad-85d78a7dc2d1
2024-02-22 07:03:31,331 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd577e23-d3b7-4b85-8f51-428d176245c0
2024-02-22 07:03:31,333 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:31,334 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43157
2024-02-22 07:03:31,334 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43157
2024-02-22 07:03:31,334 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40189
2024-02-22 07:03:31,334 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,334 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,334 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:31,334 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:31,334 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kr_lfm7n
2024-02-22 07:03:31,334 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1242dcb-945b-40ef-a6f8-f8bb9324a7cd
2024-02-22 07:03:31,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:31,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:31,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:31,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:31,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:31,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:31,346 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:31,347 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46289
2024-02-22 07:03:31,347 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46289
2024-02-22 07:03:31,347 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39735
2024-02-22 07:03:31,347 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,347 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,347 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:31,347 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:31,347 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pr8gemsq
2024-02-22 07:03:31,347 - distributed.worker - INFO - Starting Worker plugin RMMSetup-beee488e-4145-4e56-84fb-bf7f178153df
2024-02-22 07:03:31,349 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:31,350 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:31,350 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33689
2024-02-22 07:03:31,350 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33689
2024-02-22 07:03:31,350 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32813
2024-02-22 07:03:31,350 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,350 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,350 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:31,350 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:31,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pwjhnbx9
2024-02-22 07:03:31,350 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41733
2024-02-22 07:03:31,350 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41733
2024-02-22 07:03:31,350 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1697bca-42d7-4c6d-a2df-56ba751326da
2024-02-22 07:03:31,350 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45521
2024-02-22 07:03:31,351 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,351 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,351 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:31,351 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:31,351 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yq9z_m35
2024-02-22 07:03:31,351 - distributed.worker - INFO - Starting Worker plugin PreImport-8b871e48-f180-4b89-b138-8b8c7678706d
2024-02-22 07:03:31,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c44037d3-c43f-4ddc-97f0-023330dee976
2024-02-22 07:03:31,352 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da0608ce-5b2f-43af-9012-0b0e6ba12b80
2024-02-22 07:03:31,558 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,590 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37717', status: init, memory: 0, processing: 0>
2024-02-22 07:03:31,591 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37717
2024-02-22 07:03:31,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53294
2024-02-22 07:03:31,593 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:31,594 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:31,594 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:31,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:33,167 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6c805c1-0c71-4bd8-b7af-d115b53e2baa
2024-02-22 07:03:33,168 - distributed.worker - INFO - Starting Worker plugin PreImport-e7019b43-1020-4a66-a98e-91f19ce29679
2024-02-22 07:03:33,169 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32795', status: init, memory: 0, processing: 0>
2024-02-22 07:03:33,198 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32795
2024-02-22 07:03:33,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53308
2024-02-22 07:03:33,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:33,201 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:33,201 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:33,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a63bdd14-3dd3-470e-bc1d-0e48f3b42f1b
2024-02-22 07:03:33,353 - distributed.worker - INFO - Starting Worker plugin PreImport-af60e744-079b-4d7c-9e1b-572946ba1b59
2024-02-22 07:03:33,353 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,364 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb27a90b-e3dc-4459-92a3-40c45b637bdc
2024-02-22 07:03:33,364 - distributed.worker - INFO - Starting Worker plugin PreImport-61c3e760-af09-48a9-9297-e82f2bde765c
2024-02-22 07:03:33,365 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,377 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43157', status: init, memory: 0, processing: 0>
2024-02-22 07:03:33,377 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43157
2024-02-22 07:03:33,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53320
2024-02-22 07:03:33,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:33,379 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:33,379 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:33,381 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,395 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33689', status: init, memory: 0, processing: 0>
2024-02-22 07:03:33,396 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33689
2024-02-22 07:03:33,396 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53334
2024-02-22 07:03:33,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:33,399 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:33,399 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:33,401 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38969', status: init, memory: 0, processing: 0>
2024-02-22 07:03:33,402 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38969
2024-02-22 07:03:33,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53340
2024-02-22 07:03:33,403 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:33,403 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:33,404 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,405 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:33,625 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4cfa118-4301-4017-88a3-0e71b1574ceb
2024-02-22 07:03:33,642 - distributed.worker - INFO - Starting Worker plugin PreImport-b8b7cb6e-c032-45a0-96ac-70728975fb47
2024-02-22 07:03:33,642 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,646 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36807', status: init, memory: 0, processing: 0>
2024-02-22 07:03:33,647 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36807
2024-02-22 07:03:33,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53344
2024-02-22 07:03:33,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:33,648 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:33,648 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:33,662 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46289', status: init, memory: 0, processing: 0>
2024-02-22 07:03:33,663 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46289
2024-02-22 07:03:33,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53356
2024-02-22 07:03:33,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:33,664 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:33,664 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:33,666 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41733', status: init, memory: 0, processing: 0>
2024-02-22 07:03:33,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41733
2024-02-22 07:03:33,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53360
2024-02-22 07:03:33,698 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:33,699 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:33,699 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:33,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:33,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:33,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:33,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:33,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:33,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:33,807 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:33,808 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:33,810 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:33,816 - distributed.scheduler - INFO - Remove client Client-7a0f01f7-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:33,817 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41572; closing.
2024-02-22 07:03:33,817 - distributed.scheduler - INFO - Remove client Client-7a0f01f7-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:33,817 - distributed.scheduler - INFO - Close client connection: Client-7a0f01f7-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:33,818 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35373'. Reason: nanny-close
2024-02-22 07:03:33,819 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:33,819 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35011'. Reason: nanny-close
2024-02-22 07:03:33,820 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:33,820 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37941'. Reason: nanny-close
2024-02-22 07:03:33,821 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37717. Reason: nanny-close
2024-02-22 07:03:33,821 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:33,821 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33083'. Reason: nanny-close
2024-02-22 07:03:33,821 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:33,821 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33689. Reason: nanny-close
2024-02-22 07:03:33,821 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39769'. Reason: nanny-close
2024-02-22 07:03:33,821 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46289. Reason: nanny-close
2024-02-22 07:03:33,821 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:33,822 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46759'. Reason: nanny-close
2024-02-22 07:03:33,822 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38969. Reason: nanny-close
2024-02-22 07:03:33,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:33,822 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33681'. Reason: nanny-close
2024-02-22 07:03:33,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:33,822 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41733. Reason: nanny-close
2024-02-22 07:03:33,823 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39099'. Reason: nanny-close
2024-02-22 07:03:33,823 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:33,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32795. Reason: nanny-close
2024-02-22 07:03:33,823 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:33,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36807. Reason: nanny-close
2024-02-22 07:03:33,823 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53356; closing.
2024-02-22 07:03:33,823 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:33,823 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:33,824 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:33,824 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585413.8239532')
2024-02-22 07:03:33,824 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43157. Reason: nanny-close
2024-02-22 07:03:33,825 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:33,825 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53294; closing.
2024-02-22 07:03:33,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:33,825 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53340; closing.
2024-02-22 07:03:33,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:33,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:33,825 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:33,825 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53334; closing.
2024-02-22 07:03:33,826 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:33,826 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:33,826 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37717', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585413.8265545')
2024-02-22 07:03:33,826 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:33,827 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585413.8269813')
2024-02-22 07:03:33,827 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:33,827 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:33,827 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33689', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585413.8273296')
2024-02-22 07:03:33,827 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:33,828 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53360; closing.
2024-02-22 07:03:33,828 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53308; closing.
2024-02-22 07:03:33,828 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53344; closing.
2024-02-22 07:03:33,829 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41733', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585413.8292427')
2024-02-22 07:03:33,829 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:33,829 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32795', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585413.829595')
2024-02-22 07:03:33,829 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585413.8299406')
2024-02-22 07:03:33,830 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53320; closing.
2024-02-22 07:03:33,830 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43157', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585413.8306704')
2024-02-22 07:03:33,830 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:03:34,834 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:03:34,834 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:03:34,835 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:03:34,836 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:03:34,836 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-22 07:03:37,198 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:37,203 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33783 instead
  warnings.warn(
2024-02-22 07:03:37,207 - distributed.scheduler - INFO - State start
2024-02-22 07:03:37,237 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:37,238 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:03:37,240 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33783/status
2024-02-22 07:03:37,240 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:03:37,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42993'
2024-02-22 07:03:37,400 - distributed.scheduler - INFO - Receive client connection: Client-7ef0ae1b-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:37,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53508
2024-02-22 07:03:37,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43777'
2024-02-22 07:03:37,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44573'
2024-02-22 07:03:37,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39369'
2024-02-22 07:03:37,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37957'
2024-02-22 07:03:37,454 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44609'
2024-02-22 07:03:37,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46541'
2024-02-22 07:03:37,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33601'
2024-02-22 07:03:39,314 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:39,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:39,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:39,319 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43895
2024-02-22 07:03:39,319 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43895
2024-02-22 07:03:39,319 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33397
2024-02-22 07:03:39,320 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,320 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,320 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:39,320 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:39,320 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tzhisqib
2024-02-22 07:03:39,320 - distributed.worker - INFO - Starting Worker plugin PreImport-3b4f455b-ea8b-4b22-bc63-90a2906e6d10
2024-02-22 07:03:39,320 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69bb912b-77df-4024-a771-0852a51c441f
2024-02-22 07:03:39,320 - distributed.worker - INFO - Starting Worker plugin RMMSetup-402ab1fc-5a10-4bcc-b5dc-bab75f2ce4c7
2024-02-22 07:03:39,552 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:39,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:39,554 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:39,554 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:39,557 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:39,558 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35769
2024-02-22 07:03:39,558 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35769
2024-02-22 07:03:39,558 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45725
2024-02-22 07:03:39,558 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,558 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,558 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:39,558 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:39,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-07lpe5b2
2024-02-22 07:03:39,558 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a95ca423-cf5a-487b-a810-ab4ecd6ab73b
2024-02-22 07:03:39,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:39,566 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43321
2024-02-22 07:03:39,566 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43321
2024-02-22 07:03:39,566 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39261
2024-02-22 07:03:39,566 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,566 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,566 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:39,566 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:39,566 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zsg_58x3
2024-02-22 07:03:39,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60734726-995a-4ed5-a4a7-3f3b38dae505
2024-02-22 07:03:39,570 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:39,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:39,571 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:39,571 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:39,571 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:39,571 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:39,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:39,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:39,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:39,575 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:39,576 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:39,577 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:39,578 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33107
2024-02-22 07:03:39,578 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:39,578 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33107
2024-02-22 07:03:39,578 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35455
2024-02-22 07:03:39,578 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,578 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,578 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:39,578 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:39,578 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gfbhv1ja
2024-02-22 07:03:39,578 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:39,578 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eaa98440-6623-4105-94cf-1f0f067bf9de
2024-02-22 07:03:39,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38595
2024-02-22 07:03:39,579 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38595
2024-02-22 07:03:39,579 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33101
2024-02-22 07:03:39,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,579 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,579 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:39,579 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:39,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35515
2024-02-22 07:03:39,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xsjmi79l
2024-02-22 07:03:39,579 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35515
2024-02-22 07:03:39,579 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38751
2024-02-22 07:03:39,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,579 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,579 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:39,579 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:39,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9t6oyfzp
2024-02-22 07:03:39,579 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20bd50ef-f98f-4fa2-a212-20944ce91eae
2024-02-22 07:03:39,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34089
2024-02-22 07:03:39,579 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34089
2024-02-22 07:03:39,580 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43457
2024-02-22 07:03:39,580 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,580 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c860584-0026-4e69-b9ff-d8ef09db2471
2024-02-22 07:03:39,580 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,580 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:39,580 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:39,580 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-itb4t72h
2024-02-22 07:03:39,580 - distributed.worker - INFO - Starting Worker plugin RMMSetup-581efd81-1799-48c9-8531-fdd20087df5c
2024-02-22 07:03:39,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:39,586 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36013
2024-02-22 07:03:39,586 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36013
2024-02-22 07:03:39,586 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38441
2024-02-22 07:03:39,586 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,587 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,587 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:39,587 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:39,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k7eweigm
2024-02-22 07:03:39,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7516f78f-f6bf-4fe9-838e-60d85986e04c
2024-02-22 07:03:39,874 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,906 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43895', status: init, memory: 0, processing: 0>
2024-02-22 07:03:39,907 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43895
2024-02-22 07:03:39,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53526
2024-02-22 07:03:39,908 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:39,909 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:39,909 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:39,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:41,568 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72aea7d3-cc76-4888-9a73-e78539b64e2e
2024-02-22 07:03:41,569 - distributed.worker - INFO - Starting Worker plugin PreImport-e53e64de-039e-497c-beb3-a400b0831180
2024-02-22 07:03:41,570 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,582 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b922cd8-cf4d-4705-a95c-08b7bc3334b7
2024-02-22 07:03:41,582 - distributed.worker - INFO - Starting Worker plugin PreImport-015d7ce7-48aa-4188-b67f-23e249b74e66
2024-02-22 07:03:41,582 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,594 - distributed.worker - INFO - Starting Worker plugin PreImport-15e54d0e-93f3-44e2-96aa-dad4df9ec5c9
2024-02-22 07:03:41,594 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd30789e-bc93-41c1-8704-3742640fe437
2024-02-22 07:03:41,594 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,601 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db7a50fd-e0e2-4b6f-abee-c34135fb9c7a
2024-02-22 07:03:41,601 - distributed.worker - INFO - Starting Worker plugin PreImport-9dd1244e-4325-4c68-826d-fc3a4f3be86f
2024-02-22 07:03:41,601 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,603 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35769', status: init, memory: 0, processing: 0>
2024-02-22 07:03:41,603 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35769
2024-02-22 07:03:41,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44120
2024-02-22 07:03:41,604 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cbe34ac-75a4-44b9-ac9f-42e4e5357f04
2024-02-22 07:03:41,604 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab3fb4c6-6ad2-437e-b80a-02904cef9a92
2024-02-22 07:03:41,605 - distributed.worker - INFO - Starting Worker plugin PreImport-c1a9f5b2-9d5b-4d8b-a1ba-6b4b864de5b2
2024-02-22 07:03:41,605 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:41,605 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79471a80-c5ed-4818-bc02-fd21ddca5ed6
2024-02-22 07:03:41,605 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,605 - distributed.worker - INFO - Starting Worker plugin PreImport-0259dca0-3427-4be3-a9aa-6b1385d07fa9
2024-02-22 07:03:41,605 - distributed.worker - INFO - Starting Worker plugin PreImport-76d3db0b-1325-4db4-b781-6c3a92d7256a
2024-02-22 07:03:41,606 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,606 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,606 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:41,606 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:41,611 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43321', status: init, memory: 0, processing: 0>
2024-02-22 07:03:41,611 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43321
2024-02-22 07:03:41,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44122
2024-02-22 07:03:41,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:41,614 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:41,614 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,615 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33107', status: init, memory: 0, processing: 0>
2024-02-22 07:03:41,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:41,616 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33107
2024-02-22 07:03:41,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44132
2024-02-22 07:03:41,616 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:41,617 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:41,617 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,619 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:41,628 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38595', status: init, memory: 0, processing: 0>
2024-02-22 07:03:41,629 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38595
2024-02-22 07:03:41,629 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44158
2024-02-22 07:03:41,630 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35515', status: init, memory: 0, processing: 0>
2024-02-22 07:03:41,630 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:41,630 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35515
2024-02-22 07:03:41,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44172
2024-02-22 07:03:41,631 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:41,631 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,632 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36013', status: init, memory: 0, processing: 0>
2024-02-22 07:03:41,632 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:41,632 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:41,632 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36013
2024-02-22 07:03:41,632 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44148
2024-02-22 07:03:41,632 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:41,633 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:41,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:41,635 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:41,635 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:41,644 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34089', status: init, memory: 0, processing: 0>
2024-02-22 07:03:41,645 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34089
2024-02-22 07:03:41,645 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44184
2024-02-22 07:03:41,646 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:41,648 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:41,648 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:41,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:41,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:41,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:41,665 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:41,665 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:41,665 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:41,665 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:41,665 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:41,666 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:41,677 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:41,677 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:41,678 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:41,678 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:41,678 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:41,678 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:41,678 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:41,678 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:41,687 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:41,689 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:41,691 - distributed.scheduler - INFO - Remove client Client-7ef0ae1b-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:41,691 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53508; closing.
2024-02-22 07:03:41,691 - distributed.scheduler - INFO - Remove client Client-7ef0ae1b-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:41,692 - distributed.scheduler - INFO - Close client connection: Client-7ef0ae1b-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:41,693 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42993'. Reason: nanny-close
2024-02-22 07:03:41,693 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:41,693 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43777'. Reason: nanny-close
2024-02-22 07:03:41,694 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:41,694 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44573'. Reason: nanny-close
2024-02-22 07:03:41,695 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43895. Reason: nanny-close
2024-02-22 07:03:41,695 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:41,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39369'. Reason: nanny-close
2024-02-22 07:03:41,695 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35769. Reason: nanny-close
2024-02-22 07:03:41,695 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:41,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37957'. Reason: nanny-close
2024-02-22 07:03:41,695 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33107. Reason: nanny-close
2024-02-22 07:03:41,696 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:41,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44609'. Reason: nanny-close
2024-02-22 07:03:41,696 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35515. Reason: nanny-close
2024-02-22 07:03:41,696 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:41,696 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46541'. Reason: nanny-close
2024-02-22 07:03:41,696 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43321. Reason: nanny-close
2024-02-22 07:03:41,696 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:41,697 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33601'. Reason: nanny-close
2024-02-22 07:03:41,697 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:41,697 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34089. Reason: nanny-close
2024-02-22 07:03:41,697 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:41,697 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53526; closing.
2024-02-22 07:03:41,697 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:41,697 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36013. Reason: nanny-close
2024-02-22 07:03:41,697 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:41,697 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43895', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585421.6977582')
2024-02-22 07:03:41,697 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:41,697 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38595. Reason: nanny-close
2024-02-22 07:03:41,698 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44132; closing.
2024-02-22 07:03:41,698 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:41,698 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:41,698 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33107', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585421.6988788')
2024-02-22 07:03:41,698 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:41,699 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44120; closing.
2024-02-22 07:03:41,699 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:41,699 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:41,699 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:41,699 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:41,699 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:41,700 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35769', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585421.700222')
2024-02-22 07:03:41,700 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:41,700 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44172; closing.
2024-02-22 07:03:41,701 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:41,701 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:41,701 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:41,701 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44132>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-22 07:03:41,703 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44122; closing.
2024-02-22 07:03:41,703 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35515', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585421.703475')
2024-02-22 07:03:41,703 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44184; closing.
2024-02-22 07:03:41,704 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44148; closing.
2024-02-22 07:03:41,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585421.7044191')
2024-02-22 07:03:41,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34089', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585421.704819')
2024-02-22 07:03:41,705 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36013', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585421.7051802')
2024-02-22 07:03:41,705 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44158; closing.
2024-02-22 07:03:41,705 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585421.7059155')
2024-02-22 07:03:41,706 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:03:42,758 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:03:42,759 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:03:42,759 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:03:42,761 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:03:42,761 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-22 07:03:45,121 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:45,125 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44107 instead
  warnings.warn(
2024-02-22 07:03:45,130 - distributed.scheduler - INFO - State start
2024-02-22 07:03:45,153 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:45,154 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:03:45,154 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44107/status
2024-02-22 07:03:45,155 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:03:45,430 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46617'
2024-02-22 07:03:45,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42523'
2024-02-22 07:03:45,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45805'
2024-02-22 07:03:45,461 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37405'
2024-02-22 07:03:45,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34073'
2024-02-22 07:03:45,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42335'
2024-02-22 07:03:45,486 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40685'
2024-02-22 07:03:45,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37365'
2024-02-22 07:03:47,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:47,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:47,330 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:47,331 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33787
2024-02-22 07:03:47,331 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33787
2024-02-22 07:03:47,331 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43023
2024-02-22 07:03:47,331 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:47,331 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:47,331 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:47,331 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:47,332 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jq0rodqv
2024-02-22 07:03:47,332 - distributed.worker - INFO - Starting Worker plugin PreImport-c7aa2031-be91-4477-9d6a-954b664e07b4
2024-02-22 07:03:47,332 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-937cc30a-c5ef-454f-acf9-5d31d9e67290
2024-02-22 07:03:47,332 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a498e18-19f0-4ee6-b5ac-d21219d43ddf
2024-02-22 07:03:47,372 - distributed.scheduler - INFO - Receive client connection: Client-83aee228-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:47,384 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:47,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:47,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44346
2024-02-22 07:03:47,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:47,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:47,388 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:47,389 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44115
2024-02-22 07:03:47,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:47,389 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44115
2024-02-22 07:03:47,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:47,389 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38059
2024-02-22 07:03:47,389 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:47,389 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:47,389 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:47,390 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:47,390 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wypdhe30
2024-02-22 07:03:47,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a70c680-4db8-4d4a-a211-d053be235361
2024-02-22 07:03:47,392 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:47,393 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36183
2024-02-22 07:03:47,393 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36183
2024-02-22 07:03:47,393 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37709
2024-02-22 07:03:47,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:47,393 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:47,393 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:47,394 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:47,394 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:47,394 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s55ltapd
2024-02-22 07:03:47,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4e8629a3-58c8-4cf3-bc7f-bcc0f1824d17
2024-02-22 07:03:47,394 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34141
2024-02-22 07:03:47,394 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34141
2024-02-22 07:03:47,394 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38001
2024-02-22 07:03:47,394 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:47,394 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:47,394 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:47,394 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:47,395 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7io19edy
2024-02-22 07:03:47,395 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88d28172-4ecf-4d19-bcdc-48c0b41eaa4a
2024-02-22 07:03:47,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:47,400 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:47,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:47,405 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44405
2024-02-22 07:03:47,405 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44405
2024-02-22 07:03:47,405 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38629
2024-02-22 07:03:47,405 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:47,405 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:47,405 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:47,405 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:47,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zexrmu9v
2024-02-22 07:03:47,405 - distributed.worker - INFO - Starting Worker plugin PreImport-eabd37e9-2c50-48ed-a185-2d4216a74a26
2024-02-22 07:03:47,406 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57f23d94-8247-4e67-b23c-76ece5a9fc94
2024-02-22 07:03:47,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37bdfe1e-f699-4627-ba64-03cff92f70c4
2024-02-22 07:03:47,562 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:47,562 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:47,567 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:47,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45839
2024-02-22 07:03:47,568 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45839
2024-02-22 07:03:47,568 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38857
2024-02-22 07:03:47,568 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:47,568 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:47,568 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:47,568 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:47,568 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-43bulp5h
2024-02-22 07:03:47,568 - distributed.worker - INFO - Starting Worker plugin RMMSetup-429a8f0a-46c5-4b96-ab2c-cb509df9cd14
2024-02-22 07:03:47,573 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:47,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:47,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:47,575 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:47,578 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:47,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35851
2024-02-22 07:03:47,579 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35851
2024-02-22 07:03:47,579 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32831
2024-02-22 07:03:47,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:47,579 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:47,579 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:47,579 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:47,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cp_ke95o
2024-02-22 07:03:47,579 - distributed.worker - INFO - Starting Worker plugin PreImport-d3d4c7a5-02d7-4b6e-8b35-18232cc368cd
2024-02-22 07:03:47,579 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93ec9d02-c99e-4d6e-9df2-48dd47c2917f
2024-02-22 07:03:47,579 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90b258ea-12d3-4edc-ae42-501fee76417c
2024-02-22 07:03:47,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:47,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39007
2024-02-22 07:03:47,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39007
2024-02-22 07:03:47,583 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42027
2024-02-22 07:03:47,583 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:47,583 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:47,583 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:47,583 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:47,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n2yeqfjf
2024-02-22 07:03:47,584 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d1ac1d2-9835-45aa-a107-3f90f0fc76b0
2024-02-22 07:03:48,039 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:48,065 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33787', status: init, memory: 0, processing: 0>
2024-02-22 07:03:48,066 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33787
2024-02-22 07:03:48,066 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44374
2024-02-22 07:03:48,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:48,069 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:48,069 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:48,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:49,521 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,529 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6e298cd-4734-4cdd-ab30-0ded2bcc3a7c
2024-02-22 07:03:49,530 - distributed.worker - INFO - Starting Worker plugin PreImport-340e41b8-a509-4048-9ded-24688a308389
2024-02-22 07:03:49,531 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,537 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f15711f5-8185-41b8-88df-c0a5f7191f9d
2024-02-22 07:03:49,538 - distributed.worker - INFO - Starting Worker plugin PreImport-dba7d74d-6ec5-4e6c-87bd-6aa83afbd2ff
2024-02-22 07:03:49,539 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,553 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44405', status: init, memory: 0, processing: 0>
2024-02-22 07:03:49,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44405
2024-02-22 07:03:49,554 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44392
2024-02-22 07:03:49,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:49,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:49,556 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,559 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:49,563 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34141', status: init, memory: 0, processing: 0>
2024-02-22 07:03:49,564 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34141
2024-02-22 07:03:49,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44400
2024-02-22 07:03:49,566 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:49,567 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:49,567 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,568 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,569 - distributed.worker - INFO - Starting Worker plugin PreImport-77e28308-a86c-4404-991c-b4c628ca5e89
2024-02-22 07:03:49,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:49,569 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d2f40e7-3e2e-4e0c-8c37-0d0432314c9b
2024-02-22 07:03:49,570 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,571 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44115', status: init, memory: 0, processing: 0>
2024-02-22 07:03:49,572 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44115
2024-02-22 07:03:49,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44406
2024-02-22 07:03:49,573 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25d58b77-2b6b-4523-b0c7-a05dbbc69d17
2024-02-22 07:03:49,573 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:49,573 - distributed.worker - INFO - Starting Worker plugin PreImport-aa928d65-0751-47a4-a2d3-b0d90d45aacd
2024-02-22 07:03:49,574 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,574 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:49,574 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:49,579 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-653802be-a650-4d5c-b903-cbf02f6d8e62
2024-02-22 07:03:49,579 - distributed.worker - INFO - Starting Worker plugin PreImport-8ebf04c9-b615-4ef7-b5ce-ce27c09bdcb4
2024-02-22 07:03:49,580 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,593 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35851', status: init, memory: 0, processing: 0>
2024-02-22 07:03:49,594 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35851
2024-02-22 07:03:49,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44416
2024-02-22 07:03:49,595 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39007', status: init, memory: 0, processing: 0>
2024-02-22 07:03:49,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:49,595 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39007
2024-02-22 07:03:49,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44430
2024-02-22 07:03:49,596 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:49,596 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:49,597 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:49,597 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:49,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:49,602 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45839', status: init, memory: 0, processing: 0>
2024-02-22 07:03:49,603 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45839
2024-02-22 07:03:49,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44422
2024-02-22 07:03:49,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36183', status: init, memory: 0, processing: 0>
2024-02-22 07:03:49,604 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36183
2024-02-22 07:03:49,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44438
2024-02-22 07:03:49,604 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:49,606 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:49,606 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,606 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:49,606 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:49,607 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:49,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:49,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:49,684 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,684 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,685 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,685 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,685 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,685 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,686 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,686 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,697 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:49,698 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:49,698 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:49,698 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:49,698 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:49,698 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:49,699 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:49,699 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:03:49,707 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,709 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:03:49,711 - distributed.scheduler - INFO - Remove client Client-83aee228-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:49,711 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44346; closing.
2024-02-22 07:03:49,712 - distributed.scheduler - INFO - Remove client Client-83aee228-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:49,712 - distributed.scheduler - INFO - Close client connection: Client-83aee228-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:49,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46617'. Reason: nanny-close
2024-02-22 07:03:49,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:49,714 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42523'. Reason: nanny-close
2024-02-22 07:03:49,714 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:49,714 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45805'. Reason: nanny-close
2024-02-22 07:03:49,714 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33787. Reason: nanny-close
2024-02-22 07:03:49,715 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:49,715 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37405'. Reason: nanny-close
2024-02-22 07:03:49,715 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36183. Reason: nanny-close
2024-02-22 07:03:49,715 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:49,715 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34073'. Reason: nanny-close
2024-02-22 07:03:49,716 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45839. Reason: nanny-close
2024-02-22 07:03:49,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:49,716 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42335'. Reason: nanny-close
2024-02-22 07:03:49,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:49,716 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34141. Reason: nanny-close
2024-02-22 07:03:49,716 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44374; closing.
2024-02-22 07:03:49,716 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:49,716 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40685'. Reason: nanny-close
2024-02-22 07:03:49,716 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:49,717 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35851. Reason: nanny-close
2024-02-22 07:03:49,717 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33787', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585429.7169697')
2024-02-22 07:03:49,717 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:49,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37365'. Reason: nanny-close
2024-02-22 07:03:49,717 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:49,717 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39007. Reason: nanny-close
2024-02-22 07:03:49,718 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:49,718 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44405. Reason: nanny-close
2024-02-22 07:03:49,718 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44115. Reason: nanny-close
2024-02-22 07:03:49,718 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:49,718 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44438; closing.
2024-02-22 07:03:49,718 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:49,719 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36183', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585429.7193654')
2024-02-22 07:03:49,719 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:49,719 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:49,719 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:49,719 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44422; closing.
2024-02-22 07:03:49,720 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45839', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585429.7202826')
2024-02-22 07:03:49,720 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44400; closing.
2024-02-22 07:03:49,720 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:49,720 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:49,720 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:49,721 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:49,721 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34141', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585429.721254')
2024-02-22 07:03:49,721 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:49,721 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:49,721 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44416; closing.
2024-02-22 07:03:49,721 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44430; closing.
2024-02-22 07:03:49,722 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585429.722433')
2024-02-22 07:03:49,722 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39007', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585429.7228632')
2024-02-22 07:03:49,723 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:49,723 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:49,723 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44406; closing.
2024-02-22 07:03:49,723 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44392; closing.
2024-02-22 07:03:49,723 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585429.723795')
2024-02-22 07:03:49,724 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44405', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585429.724077')
2024-02-22 07:03:49,724 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:03:50,779 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:03:50,780 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:03:50,780 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:03:50,781 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:03:50,782 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-22 07:03:53,125 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:53,130 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41127 instead
  warnings.warn(
2024-02-22 07:03:53,135 - distributed.scheduler - INFO - State start
2024-02-22 07:03:53,158 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:03:53,160 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:03:53,160 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41127/status
2024-02-22 07:03:53,161 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:03:53,265 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34975'
2024-02-22 07:03:53,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39609'
2024-02-22 07:03:53,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33799'
2024-02-22 07:03:53,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38771'
2024-02-22 07:03:53,305 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33593'
2024-02-22 07:03:53,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45115'
2024-02-22 07:03:53,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38781'
2024-02-22 07:03:53,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39705'
2024-02-22 07:03:53,672 - distributed.scheduler - INFO - Receive client connection: Client-886f0511-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:53,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37718
2024-02-22 07:03:55,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:55,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:55,173 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:55,174 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43119
2024-02-22 07:03:55,174 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43119
2024-02-22 07:03:55,174 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34361
2024-02-22 07:03:55,174 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:55,174 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:55,174 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:55,174 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:55,174 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q3xrmpdb
2024-02-22 07:03:55,174 - distributed.worker - INFO - Starting Worker plugin RMMSetup-daa204f4-3465-4d89-a8fa-9e3e0265b873
2024-02-22 07:03:55,176 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:55,177 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:55,181 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:55,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44783
2024-02-22 07:03:55,182 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44783
2024-02-22 07:03:55,182 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36461
2024-02-22 07:03:55,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:55,182 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:55,182 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:55,182 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:55,182 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3cmve397
2024-02-22 07:03:55,182 - distributed.worker - INFO - Starting Worker plugin PreImport-d3a9288e-6542-4ced-86ad-d7404ea89d0a
2024-02-22 07:03:55,182 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-277798a6-1fa6-4ce5-a735-2a4983de2eff
2024-02-22 07:03:55,183 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc1effbf-597a-4b6a-8991-e4acb447757e
2024-02-22 07:03:55,239 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:55,239 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:55,244 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:55,245 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39697
2024-02-22 07:03:55,245 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39697
2024-02-22 07:03:55,245 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39079
2024-02-22 07:03:55,245 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:55,245 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:55,245 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:55,245 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:55,245 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b7yiwe05
2024-02-22 07:03:55,245 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e156beaf-c097-47f9-b637-989ec458b594
2024-02-22 07:03:55,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:55,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:55,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:55,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:55,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:55,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:55,252 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:55,253 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44849
2024-02-22 07:03:55,253 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44849
2024-02-22 07:03:55,253 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42399
2024-02-22 07:03:55,253 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:55,253 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:55,253 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:55,253 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:55,253 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3bmza8nd
2024-02-22 07:03:55,254 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b9a0fb3-f99b-4e08-a114-e4a5fd40771d
2024-02-22 07:03:55,254 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:55,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:55,255 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39423
2024-02-22 07:03:55,255 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39423
2024-02-22 07:03:55,255 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46619
2024-02-22 07:03:55,255 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:55,255 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:55,255 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:55,255 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:55,255 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6pnlbzk7
2024-02-22 07:03:55,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe96924e-043f-4718-93d0-e3fd264252ca
2024-02-22 07:03:55,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43237
2024-02-22 07:03:55,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43237
2024-02-22 07:03:55,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35207
2024-02-22 07:03:55,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:55,256 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:55,256 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:55,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:55,256 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-leytumfs
2024-02-22 07:03:55,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cec5eb47-32e8-4dbd-be40-40d9729f445a
2024-02-22 07:03:55,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:55,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:55,266 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:55,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41319
2024-02-22 07:03:55,267 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41319
2024-02-22 07:03:55,267 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42849
2024-02-22 07:03:55,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:55,267 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:55,267 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:55,267 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:55,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rsufncib
2024-02-22 07:03:55,267 - distributed.worker - INFO - Starting Worker plugin RMMSetup-410b9c64-03d3-4ff5-b050-55ec32c08760
2024-02-22 07:03:55,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:03:55,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:03:55,479 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:03:55,481 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34795
2024-02-22 07:03:55,481 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34795
2024-02-22 07:03:55,481 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43239
2024-02-22 07:03:55,481 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:03:55,481 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:55,481 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:03:55,481 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:03:55,481 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xe_jycpn
2024-02-22 07:03:55,481 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf2af6d9-c6c7-4d14-ad57-387edf62153e
2024-02-22 07:03:56,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04cbaa23-61a6-418d-b5ba-7aa128ce1ad6
2024-02-22 07:03:56,082 - distributed.worker - INFO - Starting Worker plugin PreImport-0d62ae98-e245-42d7-8d44-cad06789b450
2024-02-22 07:03:56,083 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:56,114 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43119', status: init, memory: 0, processing: 0>
2024-02-22 07:03:56,116 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43119
2024-02-22 07:03:56,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37726
2024-02-22 07:03:56,117 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:56,118 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:56,118 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:56,120 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:57,155 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,186 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44783', status: init, memory: 0, processing: 0>
2024-02-22 07:03:57,186 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44783
2024-02-22 07:03:57,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37746
2024-02-22 07:03:57,188 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:57,189 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:57,189 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,191 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:57,281 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-488a95df-9154-4215-9506-589513137355
2024-02-22 07:03:57,282 - distributed.worker - INFO - Starting Worker plugin PreImport-368eca70-195b-4e66-8161-6d164004d64f
2024-02-22 07:03:57,283 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,315 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43237', status: init, memory: 0, processing: 0>
2024-02-22 07:03:57,315 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43237
2024-02-22 07:03:57,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37762
2024-02-22 07:03:57,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:57,318 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:57,318 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:57,402 - distributed.worker - INFO - Starting Worker plugin PreImport-00e42509-41f9-4210-9596-0b7d020e91c9
2024-02-22 07:03:57,403 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c34a34ff-f26c-4093-8be2-fe894fa8cefd
2024-02-22 07:03:57,403 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,411 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f70d931-0741-4aa3-a3e9-4c934df3834e
2024-02-22 07:03:57,412 - distributed.worker - INFO - Starting Worker plugin PreImport-6f3a55cd-b417-491d-9286-29810df09d41
2024-02-22 07:03:57,412 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,421 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-692c4bf8-0b3a-4d6d-905b-db881c52ff42
2024-02-22 07:03:57,422 - distributed.worker - INFO - Starting Worker plugin PreImport-bedf57f9-3ede-4993-b3c5-fa3753c188f6
2024-02-22 07:03:57,423 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,424 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44849', status: init, memory: 0, processing: 0>
2024-02-22 07:03:57,425 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44849
2024-02-22 07:03:57,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37776
2024-02-22 07:03:57,426 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:57,426 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:57,427 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,428 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:57,432 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39423', status: init, memory: 0, processing: 0>
2024-02-22 07:03:57,433 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39423
2024-02-22 07:03:57,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37792
2024-02-22 07:03:57,434 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:57,435 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:57,435 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:57,439 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d025cc65-c668-4c40-8ecd-f47fbfb93d76
2024-02-22 07:03:57,439 - distributed.worker - INFO - Starting Worker plugin PreImport-507edb4a-31cc-4c5d-bcfc-2f750aa2574d
2024-02-22 07:03:57,440 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,444 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41319', status: init, memory: 0, processing: 0>
2024-02-22 07:03:57,445 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41319
2024-02-22 07:03:57,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37796
2024-02-22 07:03:57,446 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:57,446 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:57,446 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,446 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5358bd56-8322-4c53-8bf1-9210b7bca2f2
2024-02-22 07:03:57,447 - distributed.worker - INFO - Starting Worker plugin PreImport-2ca70c91-4c3c-4b3c-b896-a6fd59822d09
2024-02-22 07:03:57,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:57,448 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,461 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34795', status: init, memory: 0, processing: 0>
2024-02-22 07:03:57,461 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34795
2024-02-22 07:03:57,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37804
2024-02-22 07:03:57,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:57,463 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:57,463 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:57,480 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39697', status: init, memory: 0, processing: 0>
2024-02-22 07:03:57,481 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39697
2024-02-22 07:03:57,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37806
2024-02-22 07:03:57,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:03:57,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:03:57,484 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:03:57,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:03:57,555 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:57,555 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:57,555 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:57,555 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:57,555 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:57,556 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:57,556 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:57,556 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:03:57,560 - distributed.scheduler - INFO - Remove client Client-886f0511-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:57,561 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37718; closing.
2024-02-22 07:03:57,561 - distributed.scheduler - INFO - Remove client Client-886f0511-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:57,562 - distributed.scheduler - INFO - Close client connection: Client-886f0511-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:03:57,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34975'. Reason: nanny-close
2024-02-22 07:03:57,563 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:57,563 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39609'. Reason: nanny-close
2024-02-22 07:03:57,564 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:57,564 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33799'. Reason: nanny-close
2024-02-22 07:03:57,564 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44783. Reason: nanny-close
2024-02-22 07:03:57,564 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:57,565 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38771'. Reason: nanny-close
2024-02-22 07:03:57,565 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43119. Reason: nanny-close
2024-02-22 07:03:57,565 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:57,565 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33593'. Reason: nanny-close
2024-02-22 07:03:57,565 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44849. Reason: nanny-close
2024-02-22 07:03:57,565 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:57,566 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45115'. Reason: nanny-close
2024-02-22 07:03:57,566 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41319. Reason: nanny-close
2024-02-22 07:03:57,566 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:57,566 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38781'. Reason: nanny-close
2024-02-22 07:03:57,566 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39697. Reason: nanny-close
2024-02-22 07:03:57,566 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:57,566 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39705'. Reason: nanny-close
2024-02-22 07:03:57,567 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43237. Reason: nanny-close
2024-02-22 07:03:57,567 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:03:57,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:57,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:57,567 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39423. Reason: nanny-close
2024-02-22 07:03:57,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:57,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37726; closing.
2024-02-22 07:03:57,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37746; closing.
2024-02-22 07:03:57,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:57,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37776; closing.
2024-02-22 07:03:57,568 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34795. Reason: nanny-close
2024-02-22 07:03:57,568 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43119', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585437.5682166')
2024-02-22 07:03:57,568 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:57,568 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585437.5688086')
2024-02-22 07:03:57,568 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:57,569 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:57,569 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:57,569 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44849', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585437.5692763')
2024-02-22 07:03:57,569 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:57,569 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:57,569 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:57,569 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:03:57,570 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37796; closing.
2024-02-22 07:03:57,570 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:57,570 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:57,571 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37806; closing.
2024-02-22 07:03:57,571 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:57,571 - distributed.nanny - INFO - Worker closed
2024-02-22 07:03:57,571 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41319', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585437.5713365')
2024-02-22 07:03:57,572 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585437.5724282')
2024-02-22 07:03:57,572 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37792; closing.
2024-02-22 07:03:57,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37762; closing.
2024-02-22 07:03:57,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37804; closing.
2024-02-22 07:03:57,573 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39423', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585437.5735314')
2024-02-22 07:03:57,573 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43237', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585437.5738795')
2024-02-22 07:03:57,574 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34795', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585437.5742218')
2024-02-22 07:03:57,574 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:03:58,679 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:03:58,679 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:03:58,679 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:03:58,681 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:03:58,682 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-22 07:04:01,061 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:01,067 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41993 instead
  warnings.warn(
2024-02-22 07:04:01,072 - distributed.scheduler - INFO - State start
2024-02-22 07:04:01,096 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:01,097 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:04:01,098 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41993/status
2024-02-22 07:04:01,098 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:04:01,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37387'
2024-02-22 07:04:02,749 - distributed.scheduler - INFO - Receive client connection: Client-8d2f4a8f-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:02,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43568
2024-02-22 07:04:03,281 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:03,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:03,995 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:03,995 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45633
2024-02-22 07:04:03,996 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45633
2024-02-22 07:04:03,996 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-22 07:04:03,996 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:03,996 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:03,996 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:03,996 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-22 07:04:03,996 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_jlg3nwg
2024-02-22 07:04:03,996 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4fd5f37-f91c-49d1-ab95-25c1f1d41f00
2024-02-22 07:04:03,996 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6838eb20-f419-4478-8b31-2f32236cc67b
2024-02-22 07:04:03,997 - distributed.worker - INFO - Starting Worker plugin PreImport-c0c2839a-f4bb-471a-b72a-ed648bdaa57c
2024-02-22 07:04:03,997 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:04,052 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45633', status: init, memory: 0, processing: 0>
2024-02-22 07:04:04,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45633
2024-02-22 07:04:04,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43596
2024-02-22 07:04:04,054 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:04,055 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:04,055 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:04,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:04,096 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:04,099 - distributed.scheduler - INFO - Remove client Client-8d2f4a8f-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:04,100 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43568; closing.
2024-02-22 07:04:04,102 - distributed.scheduler - INFO - Remove client Client-8d2f4a8f-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:04,102 - distributed.scheduler - INFO - Close client connection: Client-8d2f4a8f-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:04,103 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37387'. Reason: nanny-close
2024-02-22 07:04:04,103 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:04,104 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45633. Reason: nanny-close
2024-02-22 07:04:04,106 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43596; closing.
2024-02-22 07:04:04,106 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:04,106 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585444.1067731')
2024-02-22 07:04:04,107 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:04:04,107 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:04,768 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:04:04,769 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:04:04,769 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:04:04,770 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:04:04,770 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-22 07:04:09,482 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:09,487 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39703 instead
  warnings.warn(
2024-02-22 07:04:09,491 - distributed.scheduler - INFO - State start
2024-02-22 07:04:09,514 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:09,514 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:04:09,515 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39703/status
2024-02-22 07:04:09,515 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:04:09,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34109'
2024-02-22 07:04:10,168 - distributed.scheduler - INFO - Receive client connection: Client-92387e3e-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:10,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60624
2024-02-22 07:04:11,565 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:11,565 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:12,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:12,200 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34587
2024-02-22 07:04:12,200 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34587
2024-02-22 07:04:12,200 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37287
2024-02-22 07:04:12,200 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:12,200 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:12,200 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:12,200 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-22 07:04:12,200 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h3alvg85
2024-02-22 07:04:12,201 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14370f54-7727-4db3-94d4-885f08c1510b
2024-02-22 07:04:12,201 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8828edcc-3a8d-45a9-abb2-fb427ec78035
2024-02-22 07:04:12,201 - distributed.worker - INFO - Starting Worker plugin PreImport-bd6a06a6-dd60-43be-8007-6ab0e7fbd72f
2024-02-22 07:04:12,202 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:12,253 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34587', status: init, memory: 0, processing: 0>
2024-02-22 07:04:12,254 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34587
2024-02-22 07:04:12,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60642
2024-02-22 07:04:12,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:12,256 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:12,256 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:12,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:12,341 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:12,344 - distributed.scheduler - INFO - Remove client Client-92387e3e-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:12,344 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60624; closing.
2024-02-22 07:04:12,345 - distributed.scheduler - INFO - Remove client Client-92387e3e-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:12,345 - distributed.scheduler - INFO - Close client connection: Client-92387e3e-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:12,346 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34109'. Reason: nanny-close
2024-02-22 07:04:12,347 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:12,348 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34587. Reason: nanny-close
2024-02-22 07:04:12,349 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:12,349 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60642; closing.
2024-02-22 07:04:12,350 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585452.350017')
2024-02-22 07:04:12,350 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:04:12,350 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:12,911 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:04:12,911 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:04:12,912 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:04:12,913 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:04:12,914 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-22 07:04:15,270 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:15,275 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44021 instead
  warnings.warn(
2024-02-22 07:04:15,279 - distributed.scheduler - INFO - State start
2024-02-22 07:04:15,303 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:15,303 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:04:15,304 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44021/status
2024-02-22 07:04:15,304 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:04:18,236 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:60656'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60656>: Stream is closed
2024-02-22 07:04:18,525 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:04:18,525 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:04:18,526 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:04:18,526 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:04:18,527 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-22 07:04:20,868 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:20,877 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46783 instead
  warnings.warn(
2024-02-22 07:04:20,884 - distributed.scheduler - INFO - State start
2024-02-22 07:04:20,914 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:20,915 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-22 07:04:20,916 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46783/status
2024-02-22 07:04:20,916 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:04:21,009 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35623'
2024-02-22 07:04:22,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:22,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:22,827 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:22,827 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40285
2024-02-22 07:04:22,828 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40285
2024-02-22 07:04:22,828 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33305
2024-02-22 07:04:22,828 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-22 07:04:22,828 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:22,828 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:22,828 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-22 07:04:22,828 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-_uzbn0v4
2024-02-22 07:04:22,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1cb4d5a4-6b12-4d97-990a-3f56a5d8b26e
2024-02-22 07:04:22,828 - distributed.worker - INFO - Starting Worker plugin PreImport-5bd82fa0-8623-4e9f-8ee2-0576e81ede45
2024-02-22 07:04:22,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-368fc94d-09cb-4eaf-8435-8619a3cb84a7
2024-02-22 07:04:22,829 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:22,878 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40285', status: init, memory: 0, processing: 0>
2024-02-22 07:04:22,890 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40285
2024-02-22 07:04:22,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42032
2024-02-22 07:04:22,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:22,892 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-22 07:04:22,892 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:22,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-22 07:04:26,604 - distributed.scheduler - INFO - Receive client connection: Client-98f9b99e-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:26,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42052
2024-02-22 07:04:26,614 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:26,616 - distributed.scheduler - INFO - Remove client Client-98f9b99e-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:26,616 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42052; closing.
2024-02-22 07:04:26,617 - distributed.scheduler - INFO - Remove client Client-98f9b99e-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:26,617 - distributed.scheduler - INFO - Close client connection: Client-98f9b99e-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:26,618 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35623'. Reason: nanny-close
2024-02-22 07:04:26,618 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:26,619 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40285. Reason: nanny-close
2024-02-22 07:04:26,621 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-22 07:04:26,621 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42032; closing.
2024-02-22 07:04:26,621 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40285', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585466.621828')
2024-02-22 07:04:26,622 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:04:26,622 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:27,183 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:04:27,183 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:04:27,184 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:04:27,185 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-22 07:04:27,185 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-22 07:04:29,471 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:29,477 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39391 instead
  warnings.warn(
2024-02-22 07:04:29,481 - distributed.scheduler - INFO - State start
2024-02-22 07:04:29,505 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:29,506 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:04:29,507 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39391/status
2024-02-22 07:04:29,507 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:04:29,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35579'
2024-02-22 07:04:29,702 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37307'
2024-02-22 07:04:29,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37663'
2024-02-22 07:04:29,727 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39563'
2024-02-22 07:04:29,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36739'
2024-02-22 07:04:29,739 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45417'
2024-02-22 07:04:29,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36369'
2024-02-22 07:04:29,759 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36627'
2024-02-22 07:04:30,600 - distributed.scheduler - INFO - Receive client connection: Client-9e209dc9-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:30,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52280
2024-02-22 07:04:31,659 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:31,659 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:31,659 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:31,659 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:31,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:31,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:31,664 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44635
2024-02-22 07:04:31,664 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45861
2024-02-22 07:04:31,664 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45861
2024-02-22 07:04:31,664 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44635
2024-02-22 07:04:31,664 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38251
2024-02-22 07:04:31,664 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37447
2024-02-22 07:04:31,664 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:31,664 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:31,664 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:31,664 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:31,664 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:31,664 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:31,664 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:04:31,664 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:04:31,664 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8jfk29sd
2024-02-22 07:04:31,664 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xn47zw07
2024-02-22 07:04:31,665 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45c59a81-639a-4081-bbdc-bfe753a546e5
2024-02-22 07:04:31,665 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa39ddea-e7de-49f0-bc07-af4dc8ace2b0
2024-02-22 07:04:31,665 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01834b48-93ef-45f3-a28f-60b32f1df2a5
2024-02-22 07:04:31,670 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:31,670 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:31,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:31,675 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35199
2024-02-22 07:04:31,676 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35199
2024-02-22 07:04:31,676 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44755
2024-02-22 07:04:31,676 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:31,676 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:31,676 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:31,676 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:04:31,676 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i5xaym1n
2024-02-22 07:04:31,676 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3044711-4e21-4e4f-b092-f9fc43fe5bfd
2024-02-22 07:04:31,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:31,677 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:31,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:31,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:31,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:31,682 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46573
2024-02-22 07:04:31,682 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46573
2024-02-22 07:04:31,682 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46083
2024-02-22 07:04:31,682 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:31,682 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:31,682 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:31,682 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:04:31,682 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oprry7vl
2024-02-22 07:04:31,683 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed241d3f-531a-42af-a8ff-77cd14d03ffd
2024-02-22 07:04:31,683 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:31,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:31,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:31,684 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33589
2024-02-22 07:04:31,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33589
2024-02-22 07:04:31,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34391
2024-02-22 07:04:31,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:31,684 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:31,685 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:31,685 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:04:31,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uu9pjn1g
2024-02-22 07:04:31,685 - distributed.worker - INFO - Starting Worker plugin PreImport-74c9fa04-f077-49c6-beb0-739fda90dc19
2024-02-22 07:04:31,685 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-838bd56e-6be6-48ee-a318-defa6217cd93
2024-02-22 07:04:31,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-094dfa32-dd04-4048-b58a-cdcf60f6093f
2024-02-22 07:04:31,689 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:31,689 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36827
2024-02-22 07:04:31,689 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36827
2024-02-22 07:04:31,690 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43721
2024-02-22 07:04:31,690 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:31,690 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:31,690 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:31,690 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:04:31,690 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pvsn_79h
2024-02-22 07:04:31,690 - distributed.worker - INFO - Starting Worker plugin RMMSetup-500f72ad-72b7-4c58-8ef1-a4a31e1458fd
2024-02-22 07:04:31,691 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:31,691 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:31,695 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:31,696 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36335
2024-02-22 07:04:31,696 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36335
2024-02-22 07:04:31,696 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39065
2024-02-22 07:04:31,696 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:31,696 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:31,696 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:31,696 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:04:31,696 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t5ju1qjx
2024-02-22 07:04:31,697 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d36b5a35-c352-427a-8929-6077092d0252
2024-02-22 07:04:31,697 - distributed.worker - INFO - Starting Worker plugin PreImport-7fd8444c-bf7a-4606-a405-b8ded1042997
2024-02-22 07:04:31,697 - distributed.worker - INFO - Starting Worker plugin RMMSetup-daea1ff1-e068-474a-82c6-90430b059be5
2024-02-22 07:04:31,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:31,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:31,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:31,746 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41443
2024-02-22 07:04:31,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41443
2024-02-22 07:04:31,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44545
2024-02-22 07:04:31,746 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:31,747 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:31,747 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:31,747 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:04:31,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g9m0ka41
2024-02-22 07:04:31,747 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d908671-745e-47b6-9fa3-2244e4bc24aa
2024-02-22 07:04:33,676 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5483dfec-c1df-4654-badc-4e6755e1d49e
2024-02-22 07:04:33,677 - distributed.worker - INFO - Starting Worker plugin PreImport-fa484046-e6a5-44ba-8dd3-2e83f3adea8f
2024-02-22 07:04:33,679 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,711 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44635', status: init, memory: 0, processing: 0>
2024-02-22 07:04:33,712 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44635
2024-02-22 07:04:33,712 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52296
2024-02-22 07:04:33,712 - distributed.worker - INFO - Starting Worker plugin PreImport-100dd33a-0a53-4221-90e6-f561e5be0f8a
2024-02-22 07:04:33,714 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:33,714 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,715 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:33,715 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:33,744 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45861', status: init, memory: 0, processing: 0>
2024-02-22 07:04:33,745 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45861
2024-02-22 07:04:33,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52306
2024-02-22 07:04:33,746 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:33,747 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:33,747 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:33,759 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7026e535-cc20-4aad-947c-51431699ded3
2024-02-22 07:04:33,760 - distributed.worker - INFO - Starting Worker plugin PreImport-38770a34-3e25-48cb-ac35-2f903f5a9e82
2024-02-22 07:04:33,761 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-721c885e-de31-431b-b8fe-fea33e8242c9
2024-02-22 07:04:33,769 - distributed.worker - INFO - Starting Worker plugin PreImport-977ce531-bb73-43bf-98e7-c0da83a38120
2024-02-22 07:04:33,769 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,781 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a3be3be-b465-4243-9898-ffba1591ecea
2024-02-22 07:04:33,782 - distributed.worker - INFO - Starting Worker plugin PreImport-983a26d8-845e-4ed7-ae01-fcba60fd78bb
2024-02-22 07:04:33,782 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,791 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46573', status: init, memory: 0, processing: 0>
2024-02-22 07:04:33,791 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46573
2024-02-22 07:04:33,791 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52328
2024-02-22 07:04:33,792 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:33,793 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35199', status: init, memory: 0, processing: 0>
2024-02-22 07:04:33,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a72e5996-c98b-4fb0-b3ce-d51b5b854301
2024-02-22 07:04:33,793 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:33,793 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,793 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,793 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35199
2024-02-22 07:04:33,793 - distributed.worker - INFO - Starting Worker plugin PreImport-95154b37-0354-4e29-8ceb-3f1f5df0b87b
2024-02-22 07:04:33,793 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52320
2024-02-22 07:04:33,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:33,795 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,795 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:33,796 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:33,796 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,797 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:33,803 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36827', status: init, memory: 0, processing: 0>
2024-02-22 07:04:33,804 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36827
2024-02-22 07:04:33,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52340
2024-02-22 07:04:33,805 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:33,806 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:33,806 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:33,819 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36335', status: init, memory: 0, processing: 0>
2024-02-22 07:04:33,820 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36335
2024-02-22 07:04:33,820 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52346
2024-02-22 07:04:33,821 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33589', status: init, memory: 0, processing: 0>
2024-02-22 07:04:33,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:33,821 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33589
2024-02-22 07:04:33,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52358
2024-02-22 07:04:33,822 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:33,822 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:33,823 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:33,823 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,823 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:33,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:33,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41443', status: init, memory: 0, processing: 0>
2024-02-22 07:04:33,830 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41443
2024-02-22 07:04:33,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52366
2024-02-22 07:04:33,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:33,833 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:33,833 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:33,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:33,878 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:04:33,878 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:04:33,878 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:04:33,879 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:04:33,879 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:04:33,879 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:04:33,879 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:04:33,879 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-22 07:04:33,893 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:33,893 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:33,893 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:33,893 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:33,893 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:33,893 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:33,893 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:33,893 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:33,897 - distributed.scheduler - INFO - Remove client Client-9e209dc9-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:33,898 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52280; closing.
2024-02-22 07:04:33,898 - distributed.scheduler - INFO - Remove client Client-9e209dc9-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:33,898 - distributed.scheduler - INFO - Close client connection: Client-9e209dc9-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:33,899 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35579'. Reason: nanny-close
2024-02-22 07:04:33,900 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:33,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37307'. Reason: nanny-close
2024-02-22 07:04:33,900 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:33,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37663'. Reason: nanny-close
2024-02-22 07:04:33,901 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33589. Reason: nanny-close
2024-02-22 07:04:33,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:33,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39563'. Reason: nanny-close
2024-02-22 07:04:33,901 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36335. Reason: nanny-close
2024-02-22 07:04:33,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:33,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36739'. Reason: nanny-close
2024-02-22 07:04:33,902 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45861. Reason: nanny-close
2024-02-22 07:04:33,902 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:33,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45417'. Reason: nanny-close
2024-02-22 07:04:33,902 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44635. Reason: nanny-close
2024-02-22 07:04:33,902 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:33,902 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52358; closing.
2024-02-22 07:04:33,902 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:33,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36369'. Reason: nanny-close
2024-02-22 07:04:33,902 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36827. Reason: nanny-close
2024-02-22 07:04:33,903 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33589', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585473.9029493')
2024-02-22 07:04:33,903 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:33,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36627'. Reason: nanny-close
2024-02-22 07:04:33,903 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46573. Reason: nanny-close
2024-02-22 07:04:33,903 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:33,903 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:33,903 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35199. Reason: nanny-close
2024-02-22 07:04:33,904 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:33,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:33,904 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52346; closing.
2024-02-22 07:04:33,904 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41443. Reason: nanny-close
2024-02-22 07:04:33,904 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:33,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:33,905 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:33,905 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:33,905 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585473.9054554')
2024-02-22 07:04:33,905 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52306; closing.
2024-02-22 07:04:33,906 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:33,906 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:33,906 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45861', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585473.906451')
2024-02-22 07:04:33,906 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:33,906 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:33,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52340; closing.
2024-02-22 07:04:33,907 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52296; closing.
2024-02-22 07:04:33,907 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:33,907 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:33,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585473.9076087')
2024-02-22 07:04:33,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585473.9079244')
2024-02-22 07:04:33,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52328; closing.
2024-02-22 07:04:33,908 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:33,908 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:33,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46573', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585473.9088867')
2024-02-22 07:04:33,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52320; closing.
2024-02-22 07:04:33,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52366; closing.
2024-02-22 07:04:33,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585473.9098878')
2024-02-22 07:04:33,910 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41443', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585473.9102945')
2024-02-22 07:04:33,910 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:04:34,865 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:04:34,865 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:04:34,866 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:04:34,867 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:04:34,867 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-22 07:04:37,153 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:37,158 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45701 instead
  warnings.warn(
2024-02-22 07:04:37,163 - distributed.scheduler - INFO - State start
2024-02-22 07:04:37,186 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:37,187 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:04:37,187 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45701/status
2024-02-22 07:04:37,188 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:04:37,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42159'
2024-02-22 07:04:38,376 - distributed.scheduler - INFO - Receive client connection: Client-a2b793a3-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:38,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52480
2024-02-22 07:04:39,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:39,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:39,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:39,172 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35541
2024-02-22 07:04:39,172 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35541
2024-02-22 07:04:39,172 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35327
2024-02-22 07:04:39,172 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:39,172 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:39,173 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:39,173 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-22 07:04:39,173 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g_5iv27x
2024-02-22 07:04:39,173 - distributed.worker - INFO - Starting Worker plugin RMMSetup-435cbc5a-77e4-4056-8b78-a17bfc25723c
2024-02-22 07:04:39,462 - distributed.worker - INFO - Starting Worker plugin PreImport-42a61a59-6534-4fcb-84de-7da7e49102b2
2024-02-22 07:04:39,463 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-247977d2-d150-4a43-95c6-579dc2f48f35
2024-02-22 07:04:39,463 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:39,527 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35541', status: init, memory: 0, processing: 0>
2024-02-22 07:04:39,529 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35541
2024-02-22 07:04:39,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52486
2024-02-22 07:04:39,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:39,530 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:39,531 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:39,532 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:39,615 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:04:39,619 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:39,620 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:39,623 - distributed.scheduler - INFO - Remove client Client-a2b793a3-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:39,623 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52480; closing.
2024-02-22 07:04:39,623 - distributed.scheduler - INFO - Remove client Client-a2b793a3-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:39,624 - distributed.scheduler - INFO - Close client connection: Client-a2b793a3-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:39,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42159'. Reason: nanny-close
2024-02-22 07:04:39,625 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:39,626 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35541. Reason: nanny-close
2024-02-22 07:04:39,628 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:39,628 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52486; closing.
2024-02-22 07:04:39,628 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35541', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585479.6286044')
2024-02-22 07:04:39,628 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:04:39,629 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:40,191 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:04:40,191 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:04:40,192 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:04:40,193 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:04:40,193 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-22 07:04:42,491 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:42,496 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42111 instead
  warnings.warn(
2024-02-22 07:04:42,500 - distributed.scheduler - INFO - State start
2024-02-22 07:04:42,522 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-22 07:04:42,523 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-22 07:04:42,524 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42111/status
2024-02-22 07:04:42,524 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-22 07:04:42,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33077'
2024-02-22 07:04:44,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:04:44,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:04:44,480 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:04:44,481 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35227
2024-02-22 07:04:44,481 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35227
2024-02-22 07:04:44,481 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37023
2024-02-22 07:04:44,481 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-22 07:04:44,481 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:44,481 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:04:44,481 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-22 07:04:44,481 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e4odgc2b
2024-02-22 07:04:44,482 - distributed.worker - INFO - Starting Worker plugin PreImport-60a9f7bb-bd1e-47af-b58e-106bbf6390c8
2024-02-22 07:04:44,482 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8cd8bf39-c0f3-4b04-adf3-7a93566d456d
2024-02-22 07:04:44,482 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7694b3a-0c24-4708-b46d-f39882d0e247
2024-02-22 07:04:44,683 - distributed.scheduler - INFO - Receive client connection: Client-a5e19cfe-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:44,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53472
2024-02-22 07:04:44,768 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:44,817 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35227', status: init, memory: 0, processing: 0>
2024-02-22 07:04:44,818 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35227
2024-02-22 07:04:44,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53490
2024-02-22 07:04:44,819 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:04:44,820 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-22 07:04:44,820 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:04:44,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-22 07:04:44,904 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-22 07:04:44,908 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-22 07:04:44,912 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:44,914 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:04:44,916 - distributed.scheduler - INFO - Remove client Client-a5e19cfe-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:44,916 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53472; closing.
2024-02-22 07:04:44,916 - distributed.scheduler - INFO - Remove client Client-a5e19cfe-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:44,917 - distributed.scheduler - INFO - Close client connection: Client-a5e19cfe-d150-11ee-89f6-d8c49764f6bb
2024-02-22 07:04:44,917 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33077'. Reason: nanny-close
2024-02-22 07:04:44,918 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-22 07:04:44,919 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35227. Reason: nanny-close
2024-02-22 07:04:44,921 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-22 07:04:44,921 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53490; closing.
2024-02-22 07:04:44,921 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35227', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1708585484.9213688')
2024-02-22 07:04:44,921 - distributed.scheduler - INFO - Lost all workers
2024-02-22 07:04:44,922 - distributed.nanny - INFO - Worker closed
2024-02-22 07:04:45,432 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-22 07:04:45,433 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-22 07:04:45,433 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-22 07:04:45,434 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-22 07:04:45,435 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42019 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46659 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43337 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45397 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39373 instead
  warnings.warn(
2024-02-22 07:05:35,299 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f476413d0c0, tag: 0xd6e88e33e6e28885, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f476413d0c0, tag: 0xd6e88e33e6e28885, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35669 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33137 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46115 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34435 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34801 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44467 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39627 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38973 instead
  warnings.warn(
[1708585628.886665] [dgx13:57957:0]            sock.c:481  UCX  ERROR bind(fd=179 addr=0.0.0.0:35155) failed: Address already in use
[1708585633.813719] [dgx13:58069:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:38654) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39193 instead
  warnings.warn(
[1708585641.811962] [dgx13:58184:0]            sock.c:481  UCX  ERROR bind(fd=175 addr=0.0.0.0:40606) failed: Address already in use
[1708585641.813887] [dgx13:58184:0]            sock.c:481  UCX  ERROR bind(fd=178 addr=0.0.0.0:42924) failed: Address already in use
[1708585646.697648] [dgx13:58274:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:38133) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39863 instead
  warnings.warn(
[1708585663.251805] [dgx13:58666:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:46242) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] [1708585673.817789] [dgx13:58842:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:46858) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36265 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39699 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34963 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43817 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f52edd50140, tag: 0x33bc9363fb15594d>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f52edd50140, tag: 0x33bc9363fb15594d>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-3893' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33073 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45743 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38373 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34975 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41925 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42031 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38465 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33877 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33129 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42983 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36705 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41387 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36911 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42223 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45829 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37127 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41239 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40867 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39447 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42301 instead
  warnings.warn(
[1708586495.946036] [dgx13:70335:0]            sock.c:481  UCX  ERROR bind(fd=158 addr=0.0.0.0:47360) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46743 instead
  warnings.warn(
[1708586517.216077] [dgx13:70664:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:43585) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32823 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44749 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40465 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34459 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38049 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36741 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42243 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39303 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35451 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34929 instead
  warnings.warn(
2024-02-22 07:25:49,928 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-22 07:25:49,928 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-677' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35031 instead
  warnings.warn(
2024-02-22 07:26:03,958 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
2024-02-22 07:26:03,960 - distributed.comm.ucx - ERROR - 
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError
2024-02-22 07:26:03,965 - distributed.comm.ucx - ERROR - unpack(b) received extra data.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
2024-02-22 07:26:03,966 - distributed.core - ERROR - Exception while reading from ucx://127.0.0.1:53411
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 900, in _handle_comm
    msg = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
Task exception was never retrieved
future: <Task finished name='Task-662' coro=<Server._handle_comm() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py:876> exception=ExtraData(-16, b'\x07\x00\xb4\xc9\x7f\x00\x00\xf0\x07\x00\xb4\xc9\x7f\x00\x00\xa0\xae\xf2\xb5\xc9\x7f\x00\x00\xa0\xae\xf2\xb5\xc9\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xf0)\x9et\xcc\x7f\x00\x00 D\xd7\xb5\xc9\x7f\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00 \xf2\xfaw\xcc\x7f\x00\x00\x81\xd7\x00\x00\x00\x00\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xa1\xd1\x00\x00\x00\x00\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00a\xd1\x00\x00\x00\x00\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x001\xd1\x00\x00\x00\x00\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xc1\xc9\x00\x00\x00\x00\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xf0V\x9dt\xcc\x7f\x00\x00 >\x18\xc3\x80U\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00 \xf2\xfaw\xcc\x7f\x00\x00q\xbf\x00\x00\x00\x00\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x80\x00\x00\xb4\xc9\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 900, in _handle_comm
    msg = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
[1708586763.999924] [dgx13:73964:0]           mpool.c:54   UCX  WARN  object 0x557295fe7380 {{cb|snd_tag|rk_use} send length 9660 ucp_proto_progress_tag_rndv_rts() comp:???()host memory} was not returned to mpool ucp_requests
[1708586764.497600] [dgx13:73964:0]         ptr_map.c:18   UCX  WARN  ptr hash 0x557295c4e600 contains 1 elements on destroy
[1708586764.497915] [dgx13:73964:0]          rcache.c:695  UCX  WARN  ucp_rcache: destroying inuse region 0x557297619d10 [0x5572976c8f90..0x5572976cb550] g- rw ref 1 md[6]=mlx5_0
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42401 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36905 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44917 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42431 instead
  warnings.warn(
[1708586842.018124] [dgx13:74967:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:47040) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37915 instead
  warnings.warn(
2024-02-22 07:27:38,324 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 439, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 445, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37697 instead
  warnings.warn(
[1708586880.420220] [dgx13:75402:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:37608) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41199 instead
  warnings.warn(
[1708586903.675448] [dgx13:75792:0]            sock.c:481  UCX  ERROR bind(fd=155 addr=0.0.0.0:36135) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34967 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38067 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46443 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45885 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44339 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42141 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43293 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43105 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37289 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43597 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1708587058.380489] [dgx13:51702:0]            sock.c:481  UCX  ERROR bind(fd=244 addr=0.0.0.0:43892) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1708587069.531911] [dgx13:77791:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:58470) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-02-22 07:31:40,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:31:40,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:31:40,709 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:31:40,710 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:31:40,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:31:40,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:31:41,145 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:31:41,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:31:41,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:31:41,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:31:41,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:31:41,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:31:41,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:31:41,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:31:41,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:31:41,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:31:41,296 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:31:41,296 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32787
2024-02-22 07:31:41,296 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32787
2024-02-22 07:31:41,297 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41817
2024-02-22 07:31:41,297 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,297 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,297 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:31:41,297 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p66h4mjx
2024-02-22 07:31:41,297 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e103045-f879-495d-a6d7-94a312747021
2024-02-22 07:31:41,297 - distributed.worker - INFO - Starting Worker plugin PreImport-d14ddd3d-567f-4eed-aa50-8e23845c947e
2024-02-22 07:31:41,297 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e50863a4-56fb-46fe-983c-976ff38221b6
2024-02-22 07:31:41,297 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,355 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:31:41,355 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,355 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40589
2024-02-22 07:31:41,362 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:31:41,363 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41237
2024-02-22 07:31:41,363 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41237
2024-02-22 07:31:41,363 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40111
2024-02-22 07:31:41,363 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,363 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,363 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:31:41,363 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jtmh7o9b
2024-02-22 07:31:41,363 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73ccaa76-c4c0-4905-8d8c-d858e9985607
2024-02-22 07:31:41,364 - distributed.worker - INFO - Starting Worker plugin PreImport-641687b8-99d2-4989-9420-d03061dfd015
2024-02-22 07:31:41,364 - distributed.worker - INFO - Starting Worker plugin RMMSetup-661a6c1c-0a36-4b71-92cc-aadbf257b602
2024-02-22 07:31:41,364 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:31:41,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40205
2024-02-22 07:31:41,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40205
2024-02-22 07:31:41,386 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43403
2024-02-22 07:31:41,386 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,386 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,386 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:31:41,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-50b0b57r
2024-02-22 07:31:41,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07ae0694-f891-49af-ba69-427a36cce0ae
2024-02-22 07:31:41,387 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3c47e8d-7890-4227-8dd9-a6c0060b0e61
2024-02-22 07:31:41,387 - distributed.worker - INFO - Starting Worker plugin PreImport-55112b73-2d66-4cf0-b878-61a3831d2fdb
2024-02-22 07:31:41,388 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:31:41,429 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,429 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,430 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40589
2024-02-22 07:31:41,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:31:41,458 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,458 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40589
2024-02-22 07:31:41,802 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:31:41,803 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45991
2024-02-22 07:31:41,803 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45991
2024-02-22 07:31:41,803 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46691
2024-02-22 07:31:41,804 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,804 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,804 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:31:41,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-swp_hh_6
2024-02-22 07:31:41,804 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2978514d-fdbf-409a-8abc-db8ec10d3f34
2024-02-22 07:31:41,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-064a2fe8-815f-466f-96f5-0b15cf79adca
2024-02-22 07:31:41,805 - distributed.worker - INFO - Starting Worker plugin PreImport-73fe5aa4-74f5-46a1-adc7-e923b218d6dd
2024-02-22 07:31:41,805 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:31:41,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35317
2024-02-22 07:31:41,808 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35317
2024-02-22 07:31:41,808 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33947
2024-02-22 07:31:41,808 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,808 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,808 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:31:41,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k8bwu97l
2024-02-22 07:31:41,809 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23e16aef-f316-48ce-901a-361a2b3f0fdd
2024-02-22 07:31:41,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:31:41,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:31:41,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45877
2024-02-22 07:31:41,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45877
2024-02-22 07:31:41,810 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46813
2024-02-22 07:31:41,810 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,810 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,810 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:31:41,810 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8rste22v
2024-02-22 07:31:41,810 - distributed.worker - INFO - Starting Worker plugin PreImport-918496aa-4869-41d9-ac0f-8b8bc555e43b
2024-02-22 07:31:41,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed665d9a-fc5e-4b64-96a6-a0f74ba6358d
2024-02-22 07:31:41,811 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45479
2024-02-22 07:31:41,811 - distributed.worker - INFO - Starting Worker plugin PreImport-b3b248c8-b4cc-4afe-af77-0357256082cf
2024-02-22 07:31:41,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45479
2024-02-22 07:31:41,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b2252d4-e70c-4e4b-b805-cd072c8f3bce
2024-02-22 07:31:41,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-186954fe-e489-4d8d-944e-9fff12ea69ed
2024-02-22 07:31:41,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35027
2024-02-22 07:31:41,811 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,811 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,811 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,811 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,811 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:31:41,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p6l3sr78
2024-02-22 07:31:41,811 - distributed.worker - INFO - Starting Worker plugin PreImport-f53b6730-87c4-4c66-8906-251ae8644df3
2024-02-22 07:31:41,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b9a7096-51c3-4cbb-b03d-4f13f05d22a4
2024-02-22 07:31:41,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e7c09ed-e6bc-4790-bae1-0515319d3593
2024-02-22 07:31:41,812 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,872 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:31:41,873 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38097
2024-02-22 07:31:41,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38097
2024-02-22 07:31:41,873 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42819
2024-02-22 07:31:41,873 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,873 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,873 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:31:41,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r75z5v6n
2024-02-22 07:31:41,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-386f0397-f1a4-43bd-b296-c51d342325cb
2024-02-22 07:31:41,875 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60a70f0e-db3e-45b1-89bc-8679c965411b
2024-02-22 07:31:41,876 - distributed.worker - INFO - Starting Worker plugin PreImport-0d4dd05e-59b3-4355-a6f0-caaccb712de6
2024-02-22 07:31:41,876 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:31:41,982 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40589
2024-02-22 07:31:41,982 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:41,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40589
2024-02-22 07:31:42,001 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:31:42,001 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40589
2024-02-22 07:31:42,002 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:42,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40589
2024-02-22 07:31:42,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:31:42,006 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40589
2024-02-22 07:31:42,006 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:42,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:31:42,007 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40589
2024-02-22 07:31:42,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40589
2024-02-22 07:31:42,007 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:42,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40589
2024-02-22 07:31:42,019 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:31:42,020 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40589
2024-02-22 07:31:42,020 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:31:42,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40589
2024-02-22 07:31:42,053 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:31:42,053 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:31:42,053 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:31:42,053 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:31:42,053 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:31:42,054 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:31:42,054 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:31:42,054 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-22 07:31:42,060 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32787. Reason: nanny-close
2024-02-22 07:31:42,061 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41237. Reason: nanny-close
2024-02-22 07:31:42,061 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40205. Reason: nanny-close
2024-02-22 07:31:42,062 - distributed.core - INFO - Connection to tcp://127.0.0.1:40589 has been closed.
2024-02-22 07:31:42,062 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45479. Reason: nanny-close
2024-02-22 07:31:42,062 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45877. Reason: nanny-close
2024-02-22 07:31:42,063 - distributed.core - INFO - Connection to tcp://127.0.0.1:40589 has been closed.
2024-02-22 07:31:42,063 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38097. Reason: nanny-close
2024-02-22 07:31:42,063 - distributed.nanny - INFO - Worker closed
2024-02-22 07:31:42,063 - distributed.core - INFO - Connection to tcp://127.0.0.1:40589 has been closed.
2024-02-22 07:31:42,064 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35317. Reason: nanny-close
2024-02-22 07:31:42,064 - distributed.nanny - INFO - Worker closed
2024-02-22 07:31:42,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:40589 has been closed.
2024-02-22 07:31:42,064 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45991. Reason: nanny-close
2024-02-22 07:31:42,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:40589 has been closed.
2024-02-22 07:31:42,065 - distributed.nanny - INFO - Worker closed
2024-02-22 07:31:42,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:40589 has been closed.
2024-02-22 07:31:42,065 - distributed.nanny - INFO - Worker closed
2024-02-22 07:31:42,065 - distributed.nanny - INFO - Worker closed
2024-02-22 07:31:42,066 - distributed.core - INFO - Connection to tcp://127.0.0.1:40589 has been closed.
2024-02-22 07:31:42,066 - distributed.core - INFO - Connection to tcp://127.0.0.1:40589 has been closed.
2024-02-22 07:31:42,066 - distributed.nanny - INFO - Worker closed
2024-02-22 07:31:42,067 - distributed.nanny - INFO - Worker closed
2024-02-22 07:31:42,067 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed 2024-02-22 07:31:55,580 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-02-22 07:31:55,585 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:45379', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 939, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-02-22 07:32:16,641 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:16,641 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:16,645 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:16,647 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41181
2024-02-22 07:32:16,647 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41181
2024-02-22 07:32:16,647 - distributed.worker - INFO -           Worker name:                          0
2024-02-22 07:32:16,647 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46755
2024-02-22 07:32:16,647 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41359
2024-02-22 07:32:16,647 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:16,647 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:16,647 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-22 07:32:16,647 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vid0tont
2024-02-22 07:32:16,647 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3aef74fe-e3fa-4ac9-acbd-e9055d2ceaf8
2024-02-22 07:32:16,648 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0470f28-423c-42a3-9077-79862fa122e5
2024-02-22 07:32:16,648 - distributed.worker - INFO - Starting Worker plugin PreImport-d9f501c0-2dbb-4c53-86ac-2c7b70aaa94a
2024-02-22 07:32:16,652 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-02-22 07:32:16,652 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41181. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-02-22 07:32:16,652 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-02-22 07:32:16,654 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-02-22 07:32:21,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:21,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:21,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:21,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:21,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:21,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:21,368 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:21,368 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:21,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:21,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:21,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:21,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:21,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:21,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:21,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-22 07:32:21,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-22 07:32:21,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:21,936 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41087
2024-02-22 07:32:21,937 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41087
2024-02-22 07:32:21,937 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41061
2024-02-22 07:32:21,937 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33795
2024-02-22 07:32:21,937 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:21,937 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:21,937 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:32:21,937 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7af1_qtn
2024-02-22 07:32:21,937 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3060cd17-d5ab-4b5c-8ab1-b1baa626526a
2024-02-22 07:32:21,937 - distributed.worker - INFO - Starting Worker plugin PreImport-89b369b6-c478-432a-b4e6-0cf21856c18a
2024-02-22 07:32:21,937 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4d9e555-e516-4821-a677-f919d3bf626a
2024-02-22 07:32:21,937 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:21,944 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:21,945 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43909
2024-02-22 07:32:21,945 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43909
2024-02-22 07:32:21,945 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45119
2024-02-22 07:32:21,945 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33795
2024-02-22 07:32:21,945 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:21,945 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:21,945 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:32:21,945 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kut3kot5
2024-02-22 07:32:21,946 - distributed.worker - INFO - Starting Worker plugin PreImport-4d2743ef-6f40-4412-b467-173909c66a95
2024-02-22 07:32:21,946 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47b8b5fc-2cb6-452b-94a4-cc9030a2ca11
2024-02-22 07:32:21,946 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d447c04-2393-4559-8770-2147d69e332c
2024-02-22 07:32:21,946 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:22,024 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38349
2024-02-22 07:32:22,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38349
2024-02-22 07:32:22,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45521
2024-02-22 07:32:22,024 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,024 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,024 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:22,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:32:22,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3juxi7wc
2024-02-22 07:32:22,024 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ae97f18-f2b2-42cb-b981-fa57e96de7c5
2024-02-22 07:32:22,025 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a3f369d-c49d-4227-97a8-c117f1319ef8
2024-02-22 07:32:22,025 - distributed.worker - INFO - Starting Worker plugin PreImport-50fc6577-37dd-4b6c-b9b8-bb069e8790da
2024-02-22 07:32:22,025 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,044 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:32:22,045 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,045 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,046 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33795
2024-02-22 07:32:22,051 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:32:22,052 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,052 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33795
2024-02-22 07:32:22,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:22,062 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42127
2024-02-22 07:32:22,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42127
2024-02-22 07:32:22,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33937
2024-02-22 07:32:22,063 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,063 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,063 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:22,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:32:22,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-48utpvuc
2024-02-22 07:32:22,063 - distributed.worker - INFO - Starting Worker plugin PreImport-b9365a75-0ce0-4634-8058-665ec43a6a7b
2024-02-22 07:32:22,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d18b1554-0b41-4e9d-886d-ca5be1085ff3
2024-02-22 07:32:22,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-756c412d-5455-4773-8a6b-4d3bc678b4b3
2024-02-22 07:32:22,064 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:22,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36965
2024-02-22 07:32:22,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36965
2024-02-22 07:32:22,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42293
2024-02-22 07:32:22,084 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,084 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,084 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:22,085 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:32:22,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ugl1n3rx
2024-02-22 07:32:22,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b744b21a-7a58-4630-8f6c-be06a4841556
2024-02-22 07:32:22,085 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35f19f33-efcc-40d8-8591-193ff322fa1a
2024-02-22 07:32:22,085 - distributed.worker - INFO - Starting Worker plugin PreImport-6ef71422-bc96-42e1-b97f-065ec7315342
2024-02-22 07:32:22,086 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,105 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:32:22,106 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,106 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33795
2024-02-22 07:32:22,136 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:22,137 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39167
2024-02-22 07:32:22,137 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39167
2024-02-22 07:32:22,137 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42375
2024-02-22 07:32:22,137 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,137 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,137 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:22,137 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:32:22,137 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h63ukezj
2024-02-22 07:32:22,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63390edb-df2f-4074-b904-b11c5b0ddad7
2024-02-22 07:32:22,138 - distributed.worker - INFO - Starting Worker plugin PreImport-3b59bfe4-89e4-46b5-9de7-01885258089a
2024-02-22 07:32:22,138 - distributed.worker - INFO - Starting Worker plugin RMMSetup-462a7cd0-5054-4346-9e8e-377e44397f86
2024-02-22 07:32:22,138 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:32:22,168 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,168 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33795
2024-02-22 07:32:22,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:32:22,210 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,210 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33795
2024-02-22 07:32:22,230 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:22,231 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43575
2024-02-22 07:32:22,231 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43575
2024-02-22 07:32:22,231 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37899
2024-02-22 07:32:22,231 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,231 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,231 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:22,231 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:32:22,231 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ni4tyazn
2024-02-22 07:32:22,231 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b10907de-3fe7-4f3e-a816-3c6eeffb8b3a
2024-02-22 07:32:22,232 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d38ba320-fcd2-4987-9351-c8a458dadf08
2024-02-22 07:32:22,232 - distributed.worker - INFO - Starting Worker plugin PreImport-111fb1b8-5a5a-48d9-87b7-0d2f931013fa
2024-02-22 07:32:22,232 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,238 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:32:22,239 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,239 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33795
2024-02-22 07:32:22,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-22 07:32:22,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45079
2024-02-22 07:32:22,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45079
2024-02-22 07:32:22,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39527
2024-02-22 07:32:22,272 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,272 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,272 - distributed.worker - INFO -               Threads:                          1
2024-02-22 07:32:22,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-22 07:32:22,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cex61y5h
2024-02-22 07:32:22,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a47788d-3a4c-46dc-8e83-bd5d1d3f0051
2024-02-22 07:32:22,273 - distributed.worker - INFO - Starting Worker plugin PreImport-d2ecad73-e6e0-4678-86ae-1ba5d96c4bff
2024-02-22 07:32:22,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-751288cb-3704-40e3-ad46-1c64e90016be
2024-02-22 07:32:22,273 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,310 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:32:22,310 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,311 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,312 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33795
2024-02-22 07:32:22,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-22 07:32:22,354 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33795
2024-02-22 07:32:22,354 - distributed.worker - INFO - -------------------------------------------------
2024-02-22 07:32:22,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33795
2024-02-22 07:32:22,391 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41087. Reason: nanny-close
2024-02-22 07:32:22,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43909. Reason: nanny-close
2024-02-22 07:32:22,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42127. Reason: nanny-close
2024-02-22 07:32:22,393 - distributed.core - INFO - Connection to tcp://127.0.0.1:33795 has been closed.
2024-02-22 07:32:22,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38349. Reason: nanny-close
2024-02-22 07:32:22,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39167. Reason: nanny-close
2024-02-22 07:32:22,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:33795 has been closed.
2024-02-22 07:32:22,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43575. Reason: nanny-close
2024-02-22 07:32:22,395 - distributed.nanny - INFO - Worker closed
2024-02-22 07:32:22,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:33795 has been closed.
2024-02-22 07:32:22,396 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36965. Reason: nanny-close
2024-02-22 07:32:22,396 - distributed.nanny - INFO - Worker closed
2024-02-22 07:32:22,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:33795 has been closed.
2024-02-22 07:32:22,396 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45079. Reason: nanny-close
2024-02-22 07:32:22,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:33795 has been closed.
2024-02-22 07:32:22,397 - distributed.nanny - INFO - Worker closed
2024-02-22 07:32:22,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:33795 has been closed.
2024-02-22 07:32:22,397 - distributed.nanny - INFO - Worker closed
2024-02-22 07:32:22,397 - distributed.nanny - INFO - Worker closed
2024-02-22 07:32:22,398 - distributed.core - INFO - Connection to tcp://127.0.0.1:33795 has been closed.
2024-02-22 07:32:22,398 - distributed.nanny - INFO - Worker closed
2024-02-22 07:32:22,398 - distributed.core - INFO - Connection to tcp://127.0.0.1:33795 has been closed.
2024-02-22 07:32:22,399 - distributed.nanny - INFO - Worker closed
2024-02-22 07:32:22,399 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-02-22 07:32:30,331 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-22 07:32:30,335 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-02-22 07:32:30,574 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-22 07:32:30,579 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-02-22 07:33:04,514 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:33:04,524 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:45783'. Shutting down.
2024-02-22 07:33:04,526 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fa3c007d9d0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:33:06,530 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-02-22 07:33:24,518 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:33:24,525 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fb38c124a30>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-02-22 07:33:55,046 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:33:55,053 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fc9c8235a00>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:33:57,057 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-02-22 07:34:25,683 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:34:25,690 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fd8471d0a00>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-02-22 07:34:56,351 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:34:56,359 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f71c020ba30>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-02-22 07:35:26,985 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:35:26,992 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f598c0aca00>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-02-22 07:35:57,375 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-22 07:35:57,383 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f39b4b659d0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-02-22 07:36:32,444 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-22 07:36:32,450 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
