============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.4
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-09 06:35:08,433 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:35:08,437 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44783 instead
  warnings.warn(
2024-02-09 06:35:08,441 - distributed.scheduler - INFO - State start
2024-02-09 06:35:08,463 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:35:08,464 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-09 06:35:08,465 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44783/status
2024-02-09 06:35:08,465 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:35:08,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33107'
2024-02-09 06:35:08,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35181'
2024-02-09 06:35:08,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38901'
2024-02-09 06:35:08,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38079'
2024-02-09 06:35:09,554 - distributed.scheduler - INFO - Receive client connection: Client-5d21f997-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:35:09,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57346
2024-02-09 06:35:10,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:10,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:10,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:10,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:10,399 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:10,400 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38479
2024-02-09 06:35:10,400 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38479
2024-02-09 06:35:10,400 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43667
2024-02-09 06:35:10,400 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-09 06:35:10,400 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,400 - distributed.worker - INFO -               Threads:                          4
2024-02-09 06:35:10,400 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-09 06:35:10,400 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-82ubbqzj
2024-02-09 06:35:10,400 - distributed.worker - INFO - Starting Worker plugin PreImport-0d12fc47-0c7c-4d26-8828-d57c5150c820
2024-02-09 06:35:10,401 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e8297f11-2961-4742-ae7e-5a5eca12b4ca
2024-02-09 06:35:10,401 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b634de5-edcf-485a-8d02-0038590f5057
2024-02-09 06:35:10,401 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:10,404 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34491
2024-02-09 06:35:10,404 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34491
2024-02-09 06:35:10,404 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37259
2024-02-09 06:35:10,404 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-09 06:35:10,404 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,404 - distributed.worker - INFO -               Threads:                          4
2024-02-09 06:35:10,404 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-09 06:35:10,404 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ymo8qxof
2024-02-09 06:35:10,404 - distributed.worker - INFO - Starting Worker plugin PreImport-fe772ba2-9c5a-435f-97b3-a602510a6204
2024-02-09 06:35:10,404 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61029306-7d88-4bdc-a54d-c2bca3a9dc3c
2024-02-09 06:35:10,404 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ecc9aa3c-f27c-422e-8729-366c97ad2f37
2024-02-09 06:35:10,405 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:10,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:10,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:10,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:10,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:10,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:10,450 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46407
2024-02-09 06:35:10,451 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46407
2024-02-09 06:35:10,451 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41391
2024-02-09 06:35:10,451 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-09 06:35:10,451 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,451 - distributed.worker - INFO -               Threads:                          4
2024-02-09 06:35:10,451 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41003
2024-02-09 06:35:10,451 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-09 06:35:10,451 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41003
2024-02-09 06:35:10,451 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-5juhz0cz
2024-02-09 06:35:10,451 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34767
2024-02-09 06:35:10,451 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-09 06:35:10,451 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,451 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efa5052e-bee0-4628-969c-52cd577c391c
2024-02-09 06:35:10,451 - distributed.worker - INFO -               Threads:                          4
2024-02-09 06:35:10,451 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-09 06:35:10,451 - distributed.worker - INFO - Starting Worker plugin PreImport-320f60e6-7510-4fdd-9792-a2005e28e28a
2024-02-09 06:35:10,451 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-d4qo2czj
2024-02-09 06:35:10,451 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ecf98c9b-8116-42e8-8e2f-50ede748c206
2024-02-09 06:35:10,451 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d20f95d-0c9b-40a3-a8c4-adf4389c0b96
2024-02-09 06:35:10,451 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,452 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d6eca7c-1988-449b-90ed-c63eb4754b6c
2024-02-09 06:35:10,452 - distributed.worker - INFO - Starting Worker plugin PreImport-5f21d985-8ed0-4f7f-ac0a-abf71310b45b
2024-02-09 06:35:10,452 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,492 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34491', status: init, memory: 0, processing: 0>
2024-02-09 06:35:10,493 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34491
2024-02-09 06:35:10,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53836
2024-02-09 06:35:10,494 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:10,494 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38479', status: init, memory: 0, processing: 0>
2024-02-09 06:35:10,494 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-09 06:35:10,494 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,494 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38479
2024-02-09 06:35:10,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53834
2024-02-09 06:35:10,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-09 06:35:10,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:10,496 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-09 06:35:10,496 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-09 06:35:10,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41003', status: init, memory: 0, processing: 0>
2024-02-09 06:35:10,547 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41003
2024-02-09 06:35:10,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53848
2024-02-09 06:35:10,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46407', status: init, memory: 0, processing: 0>
2024-02-09 06:35:10,548 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:10,548 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46407
2024-02-09 06:35:10,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53846
2024-02-09 06:35:10,549 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-09 06:35:10,549 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,549 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:10,550 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-09 06:35:10,550 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:10,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-09 06:35:10,551 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-09 06:35:10,596 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-09 06:35:10,597 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-09 06:35:10,597 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-09 06:35:10,597 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-09 06:35:10,602 - distributed.scheduler - INFO - Remove client Client-5d21f997-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:35:10,602 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57346; closing.
2024-02-09 06:35:10,602 - distributed.scheduler - INFO - Remove client Client-5d21f997-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:35:10,602 - distributed.scheduler - INFO - Close client connection: Client-5d21f997-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:35:10,604 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33107'. Reason: nanny-close
2024-02-09 06:35:10,605 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:10,605 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35181'. Reason: nanny-close
2024-02-09 06:35:10,605 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:10,606 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38901'. Reason: nanny-close
2024-02-09 06:35:10,607 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34491. Reason: nanny-close
2024-02-09 06:35:10,607 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:10,607 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38079'. Reason: nanny-close
2024-02-09 06:35:10,607 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38479. Reason: nanny-close
2024-02-09 06:35:10,607 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:10,608 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46407. Reason: nanny-close
2024-02-09 06:35:10,608 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-09 06:35:10,609 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41003. Reason: nanny-close
2024-02-09 06:35:10,609 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53836; closing.
2024-02-09 06:35:10,609 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34491', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460510.6094341')
2024-02-09 06:35:10,609 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-09 06:35:10,609 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:10,610 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-09 06:35:10,611 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-09 06:35:10,611 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:10,611 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53846; closing.
2024-02-09 06:35:10,611 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53834; closing.
2024-02-09 06:35:10,612 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:10,612 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46407', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460510.612621')
2024-02-09 06:35:10,612 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:10,613 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460510.6131527')
2024-02-09 06:35:10,613 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53848; closing.
2024-02-09 06:35:10,614 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41003', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460510.61419')
2024-02-09 06:35:10,614 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:35:11,419 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:35:11,420 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:35:11,420 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:35:11,421 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-09 06:35:11,421 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-09 06:35:13,539 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:35:13,543 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-09 06:35:13,547 - distributed.scheduler - INFO - State start
2024-02-09 06:35:13,568 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:35:13,569 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-09 06:35:13,570 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:35:13,571 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-09 06:35:13,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45465'
2024-02-09 06:35:13,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35461'
2024-02-09 06:35:13,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43969'
2024-02-09 06:35:13,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37951'
2024-02-09 06:35:13,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39577'
2024-02-09 06:35:13,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32795'
2024-02-09 06:35:13,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34797'
2024-02-09 06:35:13,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44265'
2024-02-09 06:35:15,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:15,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:15,340 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:15,341 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33543
2024-02-09 06:35:15,341 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33543
2024-02-09 06:35:15,341 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38029
2024-02-09 06:35:15,341 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:15,341 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:15,341 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:15,341 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:15,341 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0jpfy4dj
2024-02-09 06:35:15,341 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97fba300-316b-4dae-8827-450ee73dcca3
2024-02-09 06:35:15,580 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:15,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:15,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:15,581 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:15,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:15,581 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:15,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:15,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:15,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:15,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:15,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45109
2024-02-09 06:35:15,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45109
2024-02-09 06:35:15,585 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39761
2024-02-09 06:35:15,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:15,585 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:15,585 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:15,585 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:15,585 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:15,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j2pjhcfn
2024-02-09 06:35:15,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afc5542b-259b-4965-9dc5-dd8b2f823a96
2024-02-09 06:35:15,586 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35339
2024-02-09 06:35:15,586 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35339
2024-02-09 06:35:15,586 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45319
2024-02-09 06:35:15,586 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:15,586 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:15,586 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:15,586 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43537
2024-02-09 06:35:15,586 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:15,586 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43537
2024-02-09 06:35:15,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:15,586 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u7gk50f5
2024-02-09 06:35:15,586 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41127
2024-02-09 06:35:15,586 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:15,586 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:15,586 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-542ce1c8-4a7b-4be5-b1d1-644382ffe391
2024-02-09 06:35:15,586 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:15,587 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:15,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-znglqsrx
2024-02-09 06:35:15,587 - distributed.worker - INFO - Starting Worker plugin PreImport-47ba2343-28f0-4c68-b073-485b19da67ac
2024-02-09 06:35:15,587 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:15,587 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53ad8e4c-63ae-4fb3-8114-4eacf074eb10
2024-02-09 06:35:15,587 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:15,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d6e7c4f-b7b5-47e5-908b-2713c2e1218a
2024-02-09 06:35:15,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3bb4f7bd-60b2-47b2-9fb5-c679ddeba024
2024-02-09 06:35:15,587 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42971
2024-02-09 06:35:15,587 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42971
2024-02-09 06:35:15,587 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38349
2024-02-09 06:35:15,587 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:15,587 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:15,588 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:15,588 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:15,588 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hd_squk6
2024-02-09 06:35:15,588 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ca3d11d-1c38-40f2-b1f3-61560ad44aff
2024-02-09 06:35:15,588 - distributed.worker - INFO - Starting Worker plugin PreImport-ee558bbd-b90a-44d4-b62b-5a7ac21cf117
2024-02-09 06:35:15,588 - distributed.worker - INFO - Starting Worker plugin RMMSetup-124d6d91-72ea-4ab8-bcbd-b5f19822b154
2024-02-09 06:35:15,591 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:15,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45113
2024-02-09 06:35:15,592 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45113
2024-02-09 06:35:15,592 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41403
2024-02-09 06:35:15,592 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:15,592 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:15,592 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:15,592 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:15,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7pt5e2r4
2024-02-09 06:35:15,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0b9b88f-442f-4611-8930-3f5c5e6c3a25
2024-02-09 06:35:15,593 - distributed.worker - INFO - Starting Worker plugin PreImport-5f10abcd-a385-46ab-b9f9-272842bf4044
2024-02-09 06:35:15,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fcfce699-1451-414c-8376-825cb779b6b3
2024-02-09 06:35:15,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:15,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:15,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:15,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:15,667 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:15,668 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46027
2024-02-09 06:35:15,668 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46027
2024-02-09 06:35:15,668 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46789
2024-02-09 06:35:15,668 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:15,668 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:15,668 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:15,668 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:15,668 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-02gvp_42
2024-02-09 06:35:15,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9293e891-b246-4468-9c73-8e4451b8c0fe
2024-02-09 06:35:15,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:15,670 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46857
2024-02-09 06:35:15,670 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46857
2024-02-09 06:35:15,671 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43305
2024-02-09 06:35:15,671 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:15,671 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:15,671 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:15,671 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:15,671 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ex8z7yy_
2024-02-09 06:35:15,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af102893-74c2-4cb3-9435-e15996db5041
2024-02-09 06:35:15,780 - distributed.worker - INFO - Starting Worker plugin PreImport-3fe2ea51-0309-42a8-8c8c-02bf6858e914
2024-02-09 06:35:15,781 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12f87063-9c25-4dd2-8fc7-a3f387e57cae
2024-02-09 06:35:15,782 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,519 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,527 - distributed.worker - INFO - Starting Worker plugin PreImport-ef1186b9-806f-417c-b9cc-2765a65823c5
2024-02-09 06:35:17,528 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,563 - distributed.worker - INFO - Starting Worker plugin PreImport-4fc2e353-78d8-479e-8aac-ce6f318527e3
2024-02-09 06:35:17,564 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6501f83-f1c9-4aad-9243-459a662fe344
2024-02-09 06:35:17,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:17,565 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,566 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:17,566 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,567 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:17,568 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:17,568 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:17,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:17,586 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32795'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,587 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,587 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34797'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,588 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,588 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35339. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,589 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43537. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,590 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:17,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:17,592 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:17,592 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:17,593 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:17,593 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:17,593 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:17,603 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,615 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:17,616 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:17,616 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,617 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:17,626 - distributed.worker - INFO - Starting Worker plugin PreImport-9ff1aea4-fde1-4c38-a393-1fd44ce0b45d
2024-02-09 06:35:17,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe852645-e50c-49ed-8ac1-3f5b298459dc
2024-02-09 06:35:17,627 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,637 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43969'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,638 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,638 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37951'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,638 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:17,638 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42971. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,639 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45109. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,639 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:17,640 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,641 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:17,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:17,642 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:17,642 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:17,644 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:17,653 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:17,654 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:17,654 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,654 - distributed.worker - INFO - Starting Worker plugin PreImport-ec206dbe-cb7e-4b04-b3e0-838453abec97
2024-02-09 06:35:17,655 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9854b067-5342-4a58-96cf-9529d1359b2f
2024-02-09 06:35:17,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:17,656 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,689 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:17,690 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39577'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,690 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,690 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:17,690 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:17,691 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45113. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,692 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:17,694 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44265'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,694 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,695 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:17,695 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46857. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,697 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:17,697 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:17,699 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:17,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45465'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,742 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46027. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-09 06:35:17,745 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:17,747 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:47194 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-09 06:35:17,928 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55568 parent=55371 started daemon>
2024-02-09 06:35:17,928 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55560 parent=55371 started daemon>
2024-02-09 06:35:17,928 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55556 parent=55371 started daemon>
2024-02-09 06:35:17,928 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55551 parent=55371 started daemon>
2024-02-09 06:35:17,929 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55546 parent=55371 started daemon>
2024-02-09 06:35:17,929 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55541 parent=55371 started daemon>
2024-02-09 06:35:17,929 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55537 parent=55371 started daemon>
2024-02-09 06:35:18,070 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 55541 exit status was already read will report exitcode 255
2024-02-09 06:35:18,188 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 55551 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-09 06:35:52,644 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:35:52,648 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41041 instead
  warnings.warn(
2024-02-09 06:35:52,653 - distributed.scheduler - INFO - State start
2024-02-09 06:35:52,654 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0jpfy4dj', purging
2024-02-09 06:35:52,676 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:35:52,677 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:35:52,677 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41041/status
2024-02-09 06:35:52,677 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:35:52,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40351'
2024-02-09 06:35:52,808 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37177'
2024-02-09 06:35:52,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34575'
2024-02-09 06:35:52,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41727'
2024-02-09 06:35:52,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41341'
2024-02-09 06:35:52,844 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42425'
2024-02-09 06:35:52,853 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37229'
2024-02-09 06:35:52,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42279'
2024-02-09 06:35:54,359 - distributed.scheduler - INFO - Receive client connection: Client-777c4a6b-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:35:54,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59426
2024-02-09 06:35:54,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:54,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:54,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:54,645 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35821
2024-02-09 06:35:54,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35821
2024-02-09 06:35:54,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44241
2024-02-09 06:35:54,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:54,645 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:54,645 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:54,646 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:54,646 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-91kflqjb
2024-02-09 06:35:54,646 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff77bac0-5bbd-4830-8c8f-ae1ab7eca6c9
2024-02-09 06:35:54,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:54,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:54,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:54,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:54,654 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:54,654 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:54,655 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37637
2024-02-09 06:35:54,655 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34365
2024-02-09 06:35:54,655 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37637
2024-02-09 06:35:54,655 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34365
2024-02-09 06:35:54,655 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42275
2024-02-09 06:35:54,655 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42397
2024-02-09 06:35:54,655 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:54,655 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:54,655 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:54,655 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:54,655 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:54,655 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:54,655 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:54,655 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:54,655 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-28yo7tkc
2024-02-09 06:35:54,655 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w75oakk8
2024-02-09 06:35:54,655 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b6ddcd7-9f61-4242-ae7e-f43af49b592e
2024-02-09 06:35:54,655 - distributed.worker - INFO - Starting Worker plugin PreImport-2323e6d6-9880-4cfe-ac91-9553a7fbd40c
2024-02-09 06:35:54,656 - distributed.worker - INFO - Starting Worker plugin RMMSetup-948b8957-0abe-4a92-8886-ee75b02b97f5
2024-02-09 06:35:54,656 - distributed.worker - INFO - Starting Worker plugin PreImport-c58b1021-7cc4-42fd-8097-6757557ed543
2024-02-09 06:35:54,656 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0d27448-bb47-4bdd-adf3-6534152ff834
2024-02-09 06:35:54,700 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:54,700 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:54,704 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:54,705 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38525
2024-02-09 06:35:54,705 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38525
2024-02-09 06:35:54,705 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44591
2024-02-09 06:35:54,706 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:54,706 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:54,706 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:54,706 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:54,706 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gkyriwya
2024-02-09 06:35:54,706 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a53f9e84-b1cf-4e3d-9aaf-9681fc603d53
2024-02-09 06:35:54,707 - distributed.worker - INFO - Starting Worker plugin PreImport-9b56f962-ae1d-46b2-a48e-cb7a2c79a458
2024-02-09 06:35:54,708 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd9c192e-e473-4eb5-8c42-0217f875fcca
2024-02-09 06:35:54,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:54,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:54,724 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:54,725 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36071
2024-02-09 06:35:54,725 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36071
2024-02-09 06:35:54,725 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37133
2024-02-09 06:35:54,726 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:54,726 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:54,726 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:54,726 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:54,726 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ssrfqyhl
2024-02-09 06:35:54,726 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67cd0fdc-bbeb-47a4-b77d-476d51731771
2024-02-09 06:35:54,726 - distributed.worker - INFO - Starting Worker plugin PreImport-d43bd86e-242f-407b-b78c-9311c5269f00
2024-02-09 06:35:54,726 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b7c04e5-406f-4968-9116-d4fa4704224c
2024-02-09 06:35:54,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:54,770 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:54,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:54,776 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44925
2024-02-09 06:35:54,776 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44925
2024-02-09 06:35:54,776 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42109
2024-02-09 06:35:54,776 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:54,776 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:54,776 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:54,776 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:54,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-idpcpb0n
2024-02-09 06:35:54,776 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0cae3f17-ce74-4cc6-b788-a5e3b4ddc00b
2024-02-09 06:35:54,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:54,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:54,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:54,965 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44441
2024-02-09 06:35:54,965 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44441
2024-02-09 06:35:54,965 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43177
2024-02-09 06:35:54,965 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:54,965 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:54,965 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:54,966 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:54,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7_vm2oun
2024-02-09 06:35:54,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96d558f5-9b0e-4a1c-9660-b0a9ff8a09cc
2024-02-09 06:35:54,966 - distributed.worker - INFO - Starting Worker plugin PreImport-dc4048f5-3ad5-40e2-8c5f-ccfe6dcf31c7
2024-02-09 06:35:54,966 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6f4b0cc-27c9-4669-bb17-6597122b1d5e
2024-02-09 06:35:54,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:35:54,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:35:54,993 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:35:54,995 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42169
2024-02-09 06:35:54,995 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42169
2024-02-09 06:35:54,995 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46187
2024-02-09 06:35:54,995 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:35:54,995 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:54,995 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:35:54,995 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:35:54,995 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-34l0eys7
2024-02-09 06:35:54,996 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb83cc2b-d7e7-44cf-a208-faffbeb5594c
2024-02-09 06:35:54,996 - distributed.worker - INFO - Starting Worker plugin PreImport-86aa6aa5-0517-4f68-bcd8-39abd8c87a1a
2024-02-09 06:35:54,996 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fdb16429-a01f-47ad-81f1-4177f902d150
2024-02-09 06:35:55,681 - distributed.worker - INFO - Starting Worker plugin PreImport-e7e561b6-41c3-47e7-8a75-7206e659aeb3
2024-02-09 06:35:55,682 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9256a8e4-98c9-4179-b5ec-a36b2e988c56
2024-02-09 06:35:55,682 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:55,708 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35821', status: init, memory: 0, processing: 0>
2024-02-09 06:35:55,712 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35821
2024-02-09 06:35:55,712 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59454
2024-02-09 06:35:55,713 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:55,713 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:55,714 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:55,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:56,692 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33709d6e-c7ee-45ea-b224-0e505d717f98
2024-02-09 06:35:56,696 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,697 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,730 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34365', status: init, memory: 0, processing: 0>
2024-02-09 06:35:56,731 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34365
2024-02-09 06:35:56,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59458
2024-02-09 06:35:56,732 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:56,733 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:56,733 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:56,736 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37637', status: init, memory: 0, processing: 0>
2024-02-09 06:35:56,737 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37637
2024-02-09 06:35:56,737 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59456
2024-02-09 06:35:56,739 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:56,740 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:56,740 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:56,797 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,805 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,820 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36071', status: init, memory: 0, processing: 0>
2024-02-09 06:35:56,821 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36071
2024-02-09 06:35:56,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59474
2024-02-09 06:35:56,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:56,822 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:56,822 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,823 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:56,837 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38525', status: init, memory: 0, processing: 0>
2024-02-09 06:35:56,837 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38525
2024-02-09 06:35:56,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59486
2024-02-09 06:35:56,839 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:56,840 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:56,840 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:56,849 - distributed.worker - INFO - Starting Worker plugin PreImport-9ab86260-b1f3-4030-a278-fec6a253a680
2024-02-09 06:35:56,849 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2feebe24-e1b9-4c61-85c8-74cba3f0d1e3
2024-02-09 06:35:56,850 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,862 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,882 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44925', status: init, memory: 0, processing: 0>
2024-02-09 06:35:56,882 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44925
2024-02-09 06:35:56,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59492
2024-02-09 06:35:56,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:56,885 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:56,885 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,885 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:56,888 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44441', status: init, memory: 0, processing: 0>
2024-02-09 06:35:56,888 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44441
2024-02-09 06:35:56,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59500
2024-02-09 06:35:56,889 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:56,890 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:56,890 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:56,905 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42169', status: init, memory: 0, processing: 0>
2024-02-09 06:35:56,905 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42169
2024-02-09 06:35:56,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59502
2024-02-09 06:35:56,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:35:56,907 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:35:56,907 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:35:56,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:35:56,919 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:35:56,919 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:35:56,919 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:35:56,919 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:35:56,920 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:35:56,920 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:35:56,920 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:35:56,920 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:35:56,925 - distributed.scheduler - INFO - Remove client Client-777c4a6b-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:35:56,925 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59426; closing.
2024-02-09 06:35:56,926 - distributed.scheduler - INFO - Remove client Client-777c4a6b-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:35:56,926 - distributed.scheduler - INFO - Close client connection: Client-777c4a6b-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:35:56,927 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40351'. Reason: nanny-close
2024-02-09 06:35:56,927 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:56,927 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37177'. Reason: nanny-close
2024-02-09 06:35:56,928 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:56,928 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34575'. Reason: nanny-close
2024-02-09 06:35:56,928 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37637. Reason: nanny-close
2024-02-09 06:35:56,928 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:56,929 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41727'. Reason: nanny-close
2024-02-09 06:35:56,929 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:56,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34365. Reason: nanny-close
2024-02-09 06:35:56,929 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41341'. Reason: nanny-close
2024-02-09 06:35:56,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35821. Reason: nanny-close
2024-02-09 06:35:56,929 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:56,929 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42425'. Reason: nanny-close
2024-02-09 06:35:56,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36071. Reason: nanny-close
2024-02-09 06:35:56,930 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:56,930 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37229'. Reason: nanny-close
2024-02-09 06:35:56,930 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:56,930 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44925. Reason: nanny-close
2024-02-09 06:35:56,930 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42279'. Reason: nanny-close
2024-02-09 06:35:56,930 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:35:56,931 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38525. Reason: nanny-close
2024-02-09 06:35:56,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:56,931 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44441. Reason: nanny-close
2024-02-09 06:35:56,931 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59456; closing.
2024-02-09 06:35:56,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:56,931 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42169. Reason: nanny-close
2024-02-09 06:35:56,932 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37637', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460556.9319637')
2024-02-09 06:35:56,932 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:56,932 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:56,932 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59458; closing.
2024-02-09 06:35:56,933 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34365', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460556.933205')
2024-02-09 06:35:56,933 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:56,933 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:56,933 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:56,933 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:56,933 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59454; closing.
2024-02-09 06:35:56,933 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:35:56,933 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:56,933 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:56,934 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:56,934 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35821', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460556.9346428')
2024-02-09 06:35:56,934 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:56,934 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:56,935 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59474; closing.
2024-02-09 06:35:56,935 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:56,936 - distributed.nanny - INFO - Worker closed
2024-02-09 06:35:56,935 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59458>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-09 06:35:56,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59492; closing.
2024-02-09 06:35:56,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460556.938309')
2024-02-09 06:35:56,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59486; closing.
2024-02-09 06:35:56,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59500; closing.
2024-02-09 06:35:56,939 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59502; closing.
2024-02-09 06:35:56,939 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460556.9393196')
2024-02-09 06:35:56,939 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38525', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460556.9397266')
2024-02-09 06:35:56,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44441', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460556.9401016')
2024-02-09 06:35:56,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42169', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460556.940469')
2024-02-09 06:35:56,940 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:35:57,793 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:35:57,793 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:35:57,794 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:35:57,795 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:35:57,795 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-09 06:36:00,023 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:00,028 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-09 06:36:00,031 - distributed.scheduler - INFO - State start
2024-02-09 06:36:00,052 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:00,053 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:36:00,053 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-09 06:36:00,054 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:00,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34511'
2024-02-09 06:36:00,225 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45451'
2024-02-09 06:36:00,233 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40251'
2024-02-09 06:36:00,247 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43851'
2024-02-09 06:36:00,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36521'
2024-02-09 06:36:00,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38985'
2024-02-09 06:36:00,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40861'
2024-02-09 06:36:00,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41159'
2024-02-09 06:36:00,789 - distributed.scheduler - INFO - Receive client connection: Client-7bf8b6e0-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:00,805 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39522
2024-02-09 06:36:02,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:02,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:02,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:02,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:02,103 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:02,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:02,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:02,104 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42675
2024-02-09 06:36:02,104 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42675
2024-02-09 06:36:02,104 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45949
2024-02-09 06:36:02,104 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:02,104 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:02,104 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:02,104 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:02,104 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b6hqci_h
2024-02-09 06:36:02,105 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2ffb83c-669f-4f24-8790-e8822058fda3
2024-02-09 06:36:02,106 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:02,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:02,107 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:02,108 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36355
2024-02-09 06:36:02,108 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36355
2024-02-09 06:36:02,108 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36951
2024-02-09 06:36:02,108 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:02,108 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:02,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:02,108 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:02,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:02,108 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:02,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:02,108 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z1vap49d
2024-02-09 06:36:02,109 - distributed.worker - INFO - Starting Worker plugin PreImport-631fa6bc-4b8e-4242-b71d-a6f00d7af5e2
2024-02-09 06:36:02,109 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86feeb91-516e-4f01-86fb-cb2367ae48cd
2024-02-09 06:36:02,109 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46591
2024-02-09 06:36:02,109 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46591
2024-02-09 06:36:02,109 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46695
2024-02-09 06:36:02,109 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:02,109 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:02,109 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:02,109 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:02,109 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vhc9qhgi
2024-02-09 06:36:02,110 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a1cafa6-6c56-4beb-8776-cf9605bf4ec3
2024-02-09 06:36:02,110 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5de66e69-8848-4ca4-9f3f-904d455069f3
2024-02-09 06:36:02,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:02,111 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42061
2024-02-09 06:36:02,111 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42061
2024-02-09 06:36:02,111 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37785
2024-02-09 06:36:02,111 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:02,112 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:02,112 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:02,112 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:02,112 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jn6do6an
2024-02-09 06:36:02,112 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a63a9491-f8f6-4be2-9756-fbd2e7010e58
2024-02-09 06:36:02,112 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10605ed0-3af8-4af3-b3d4-aafad1caa207
2024-02-09 06:36:02,113 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:02,113 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38019
2024-02-09 06:36:02,113 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38019
2024-02-09 06:36:02,114 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36659
2024-02-09 06:36:02,114 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:02,114 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:02,114 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:02,114 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:02,114 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e76l21iw
2024-02-09 06:36:02,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e8dccdd-2b00-4b8f-b22e-172ba37fe9ae
2024-02-09 06:36:02,114 - distributed.worker - INFO - Starting Worker plugin PreImport-33e52d11-b8c9-47b4-b519-f64acf3e02f8
2024-02-09 06:36:02,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-faf4550d-82d2-4223-9ffa-aef398d72612
2024-02-09 06:36:02,180 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:02,180 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:02,185 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:02,186 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32901
2024-02-09 06:36:02,186 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32901
2024-02-09 06:36:02,186 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38757
2024-02-09 06:36:02,186 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:02,186 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:02,186 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:02,186 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:02,186 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-twe5ldn_
2024-02-09 06:36:02,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e9c0513-d442-4107-8acf-2f571d89b4fb
2024-02-09 06:36:02,186 - distributed.worker - INFO - Starting Worker plugin PreImport-72c30777-72e0-42e0-a678-cc744ae7827b
2024-02-09 06:36:02,186 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3dfa076d-4e45-4760-83a4-3364bfd30c79
2024-02-09 06:36:02,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:02,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:02,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:02,192 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46653
2024-02-09 06:36:02,192 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46653
2024-02-09 06:36:02,192 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45003
2024-02-09 06:36:02,192 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:02,192 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:02,192 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:02,192 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:02,192 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t09bsd56
2024-02-09 06:36:02,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2625e54-a1cd-41f4-a4ad-35549bcf8825
2024-02-09 06:36:02,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:02,193 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:02,198 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:02,198 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40969
2024-02-09 06:36:02,198 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40969
2024-02-09 06:36:02,199 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39449
2024-02-09 06:36:02,199 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:02,199 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:02,199 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:02,199 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:02,199 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5pwit6br
2024-02-09 06:36:02,199 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4addb6f-9756-4776-87d8-a0ada17d6b22
2024-02-09 06:36:02,200 - distributed.worker - INFO - Starting Worker plugin PreImport-5fa8bb06-6a49-4443-bfc6-e89a71b7809b
2024-02-09 06:36:02,200 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ab3a64c-106e-44ea-9bcb-d72c60bb1e94
2024-02-09 06:36:04,317 - distributed.worker - INFO - Starting Worker plugin PreImport-21b0fe7c-3a89-4f30-8d87-c01991d1867c
2024-02-09 06:36:04,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-336c6580-e250-4228-9983-35530f4989ba
2024-02-09 06:36:04,317 - distributed.worker - INFO - Starting Worker plugin PreImport-2a672648-bd61-4b06-bed3-cc43ea111079
2024-02-09 06:36:04,319 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,319 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d06d5fe-9648-4060-8475-9f369a5ff3e4
2024-02-09 06:36:04,320 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,344 - distributed.worker - INFO - Starting Worker plugin PreImport-339c88da-c398-4400-8330-f5dea45625c6
2024-02-09 06:36:04,345 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,350 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42675', status: init, memory: 0, processing: 0>
2024-02-09 06:36:04,351 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42675
2024-02-09 06:36:04,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39546
2024-02-09 06:36:04,352 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46591', status: init, memory: 0, processing: 0>
2024-02-09 06:36:04,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:04,353 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46591
2024-02-09 06:36:04,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39560
2024-02-09 06:36:04,353 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:04,353 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:04,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:04,355 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:04,355 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:04,371 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42061', status: init, memory: 0, processing: 0>
2024-02-09 06:36:04,372 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42061
2024-02-09 06:36:04,372 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39574
2024-02-09 06:36:04,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:04,374 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:04,374 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,375 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:04,390 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,422 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38019', status: init, memory: 0, processing: 0>
2024-02-09 06:36:04,421 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,422 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38019
2024-02-09 06:36:04,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39576
2024-02-09 06:36:04,424 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:04,425 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:04,425 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,426 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:04,432 - distributed.worker - INFO - Starting Worker plugin PreImport-c7c5d9aa-34a0-4bb8-8aa5-7e1749132ec2
2024-02-09 06:36:04,432 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f82b8d75-accb-48bb-ae89-1371c5e23736
2024-02-09 06:36:04,433 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,434 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,455 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46653', status: init, memory: 0, processing: 0>
2024-02-09 06:36:04,456 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46653
2024-02-09 06:36:04,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39610
2024-02-09 06:36:04,457 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36355', status: init, memory: 0, processing: 0>
2024-02-09 06:36:04,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:04,457 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36355
2024-02-09 06:36:04,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39586
2024-02-09 06:36:04,458 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:04,458 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,458 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40969', status: init, memory: 0, processing: 0>
2024-02-09 06:36:04,459 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40969
2024-02-09 06:36:04,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39602
2024-02-09 06:36:04,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:04,459 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:04,460 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:04,460 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:04,461 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:04,461 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:04,462 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32901', status: init, memory: 0, processing: 0>
2024-02-09 06:36:04,463 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32901
2024-02-09 06:36:04,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39626
2024-02-09 06:36:04,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:04,464 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:04,465 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:04,465 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:04,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:04,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:04,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:04,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:04,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:04,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:04,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:04,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:04,529 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:04,540 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:04,540 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:04,540 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:04,540 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:04,540 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:04,540 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:04,540 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:04,540 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:04,549 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:04,551 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:04,553 - distributed.scheduler - INFO - Remove client Client-7bf8b6e0-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:04,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39522; closing.
2024-02-09 06:36:04,554 - distributed.scheduler - INFO - Remove client Client-7bf8b6e0-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:04,555 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34511'. Reason: nanny-close
2024-02-09 06:36:04,555 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:04,556 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45451'. Reason: nanny-close
2024-02-09 06:36:04,556 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:04,556 - distributed.scheduler - INFO - Close client connection: Client-7bf8b6e0-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:04,557 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40251'. Reason: nanny-close
2024-02-09 06:36:04,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:04,557 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36355. Reason: nanny-close
2024-02-09 06:36:04,557 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43851'. Reason: nanny-close
2024-02-09 06:36:04,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:04,557 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46591. Reason: nanny-close
2024-02-09 06:36:04,557 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36521'. Reason: nanny-close
2024-02-09 06:36:04,558 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32901. Reason: nanny-close
2024-02-09 06:36:04,558 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:04,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38985'. Reason: nanny-close
2024-02-09 06:36:04,558 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42675. Reason: nanny-close
2024-02-09 06:36:04,558 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:04,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40861'. Reason: nanny-close
2024-02-09 06:36:04,559 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:04,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38019. Reason: nanny-close
2024-02-09 06:36:04,559 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41159'. Reason: nanny-close
2024-02-09 06:36:04,559 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:04,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40969. Reason: nanny-close
2024-02-09 06:36:04,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42061. Reason: nanny-close
2024-02-09 06:36:04,559 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:04,559 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39586; closing.
2024-02-09 06:36:04,560 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36355', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460564.5602105')
2024-02-09 06:36:04,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:04,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:04,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:04,560 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46653. Reason: nanny-close
2024-02-09 06:36:04,561 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39546; closing.
2024-02-09 06:36:04,561 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:04,561 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:04,561 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:04,561 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:04,561 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:04,562 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:04,562 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:04,562 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:04,562 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460564.5623367')
2024-02-09 06:36:04,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39576; closing.
2024-02-09 06:36:04,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39626; closing.
2024-02-09 06:36:04,562 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:04,563 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39560; closing.
2024-02-09 06:36:04,563 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:04,563 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:04,563 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:04,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460564.563919')
2024-02-09 06:36:04,564 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32901', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460564.5642865')
2024-02-09 06:36:04,564 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46591', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460564.5646398')
2024-02-09 06:36:04,565 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39574; closing.
2024-02-09 06:36:04,565 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39602; closing.
2024-02-09 06:36:04,565 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42061', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460564.5656347')
2024-02-09 06:36:04,566 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460564.5659943')
2024-02-09 06:36:04,566 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39610; closing.
2024-02-09 06:36:04,566 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460564.566728')
2024-02-09 06:36:04,566 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:36:05,772 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:36:05,772 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:36:05,773 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:36:05,774 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:36:05,774 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-09 06:36:08,132 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:08,138 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-09 06:36:08,142 - distributed.scheduler - INFO - State start
2024-02-09 06:36:08,166 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:08,168 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:36:08,169 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-09 06:36:08,169 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:08,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40411'
2024-02-09 06:36:08,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41033'
2024-02-09 06:36:08,310 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36407'
2024-02-09 06:36:08,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40367'
2024-02-09 06:36:08,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42513'
2024-02-09 06:36:08,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45229'
2024-02-09 06:36:08,342 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41299'
2024-02-09 06:36:08,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33491'
2024-02-09 06:36:09,294 - distributed.scheduler - INFO - Receive client connection: Client-80b3994b-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:09,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39762
2024-02-09 06:36:10,138 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:10,138 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:10,144 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:10,144 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45401
2024-02-09 06:36:10,145 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45401
2024-02-09 06:36:10,145 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35167
2024-02-09 06:36:10,145 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:10,145 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:10,145 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:10,145 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:10,145 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-69392kj7
2024-02-09 06:36:10,145 - distributed.worker - INFO - Starting Worker plugin PreImport-0f10da99-497b-421e-a830-f633cfc9890c
2024-02-09 06:36:10,145 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5fbb2da8-f312-4e23-b4c8-a537990bf255
2024-02-09 06:36:10,146 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df8fae90-877c-435e-bf81-130fb5ca5def
2024-02-09 06:36:10,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:10,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:10,201 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:10,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:10,203 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:10,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37453
2024-02-09 06:36:10,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37453
2024-02-09 06:36:10,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34315
2024-02-09 06:36:10,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:10,204 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:10,204 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:10,204 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:10,204 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u3cbjr3v
2024-02-09 06:36:10,204 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11f16979-bc01-47f0-a25c-38e21727b37a
2024-02-09 06:36:10,207 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:10,208 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37821
2024-02-09 06:36:10,208 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37821
2024-02-09 06:36:10,208 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36887
2024-02-09 06:36:10,208 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:10,208 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:10,208 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:10,208 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:10,208 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o3gf40rc
2024-02-09 06:36:10,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90009426-e84c-4787-b8eb-585c1b52f94a
2024-02-09 06:36:10,210 - distributed.worker - INFO - Starting Worker plugin PreImport-dde59886-9142-42ae-ba15-fc8e6370d91a
2024-02-09 06:36:10,210 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a1a7675-2e83-4a5f-b447-e6c30c78508c
2024-02-09 06:36:10,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:10,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:10,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:10,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46673
2024-02-09 06:36:10,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46673
2024-02-09 06:36:10,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34879
2024-02-09 06:36:10,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:10,401 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:10,401 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:10,401 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:10,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ggwe2emp
2024-02-09 06:36:10,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a275fb9a-928e-450c-a5ff-2448d403155e
2024-02-09 06:36:10,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:10,410 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:10,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:10,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39569
2024-02-09 06:36:10,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39569
2024-02-09 06:36:10,418 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35915
2024-02-09 06:36:10,418 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:10,418 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:10,418 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:10,418 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:10,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5v5bhlz7
2024-02-09 06:36:10,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6a71388-7d32-4e36-81b5-695b1ecb6729
2024-02-09 06:36:10,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:10,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:10,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:10,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:10,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:10,433 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46539
2024-02-09 06:36:10,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46539
2024-02-09 06:36:10,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40783
2024-02-09 06:36:10,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:10,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:10,433 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:10,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:10,433 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:10,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:10,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hayvszrb
2024-02-09 06:36:10,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-638205e3-b409-4d5c-b9c9-afbe02056417
2024-02-09 06:36:10,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-417261b6-755e-4c57-a612-9df5316fbae4
2024-02-09 06:36:10,439 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:10,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:10,441 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40867
2024-02-09 06:36:10,441 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40867
2024-02-09 06:36:10,441 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35457
2024-02-09 06:36:10,441 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:10,441 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:10,441 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:10,441 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:10,441 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4cdlvb_8
2024-02-09 06:36:10,441 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38051
2024-02-09 06:36:10,441 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38051
2024-02-09 06:36:10,441 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36099
2024-02-09 06:36:10,441 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:10,441 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:10,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ecb0721f-64fc-4978-8b36-058fa137efad
2024-02-09 06:36:10,441 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:10,441 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:10,441 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xq060e1n
2024-02-09 06:36:10,442 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-198ec6ba-7908-4e4f-8e9e-098207457ec7
2024-02-09 06:36:10,442 - distributed.worker - INFO - Starting Worker plugin PreImport-aa49a950-a389-46b7-9821-de656153c1d2
2024-02-09 06:36:10,442 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5516d4c-f51d-4416-9f09-ec02a088de29
2024-02-09 06:36:10,442 - distributed.worker - INFO - Starting Worker plugin PreImport-536fcc3a-6094-4831-8038-3f59f21803b5
2024-02-09 06:36:10,442 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ebb64c3-d2eb-41b6-a46f-b5c7df09fb4b
2024-02-09 06:36:11,125 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:11,157 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45401', status: init, memory: 0, processing: 0>
2024-02-09 06:36:11,159 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45401
2024-02-09 06:36:11,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58286
2024-02-09 06:36:11,160 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:11,161 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:11,161 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:11,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:12,925 - distributed.worker - INFO - Starting Worker plugin PreImport-a49394fe-994c-4cdf-a3a7-d885fbc68761
2024-02-09 06:36:12,925 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd1ea11a-6db5-40c8-b827-0f7582a67c4b
2024-02-09 06:36:12,927 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,931 - distributed.worker - INFO - Starting Worker plugin PreImport-83b15c80-4a41-4e8e-b7c4-fcc975cfe93f
2024-02-09 06:36:12,931 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6953aece-a973-4c43-9afb-7828eea4f4c4
2024-02-09 06:36:12,932 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46673', status: init, memory: 0, processing: 0>
2024-02-09 06:36:12,955 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46673
2024-02-09 06:36:12,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58310
2024-02-09 06:36:12,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:12,956 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:12,957 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:12,964 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,974 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37453', status: init, memory: 0, processing: 0>
2024-02-09 06:36:12,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37453
2024-02-09 06:36:12,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58320
2024-02-09 06:36:12,975 - distributed.worker - INFO - Starting Worker plugin PreImport-2fa6e499-2eb8-4802-9ef1-b742ef4c7bc2
2024-02-09 06:36:12,976 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-883145c8-0350-4722-bff8-c870c48e4c62
2024-02-09 06:36:12,976 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,977 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:12,978 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:12,979 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,980 - distributed.worker - INFO - Starting Worker plugin PreImport-110a49c9-45ed-4326-a935-de8f727d8edb
2024-02-09 06:36:12,981 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:12,987 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,987 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:12,998 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39569', status: init, memory: 0, processing: 0>
2024-02-09 06:36:12,999 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39569
2024-02-09 06:36:12,999 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58334
2024-02-09 06:36:13,000 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37821', status: init, memory: 0, processing: 0>
2024-02-09 06:36:13,000 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:13,000 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37821
2024-02-09 06:36:13,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58326
2024-02-09 06:36:13,001 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:13,001 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:13,002 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:13,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:13,003 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:13,003 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:13,004 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46539', status: init, memory: 0, processing: 0>
2024-02-09 06:36:13,004 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46539
2024-02-09 06:36:13,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58336
2024-02-09 06:36:13,005 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:13,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:13,006 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:13,006 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:13,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:13,008 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38051', status: init, memory: 0, processing: 0>
2024-02-09 06:36:13,009 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38051
2024-02-09 06:36:13,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58350
2024-02-09 06:36:13,010 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:13,010 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:13,010 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:13,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:13,021 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40867', status: init, memory: 0, processing: 0>
2024-02-09 06:36:13,022 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40867
2024-02-09 06:36:13,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58352
2024-02-09 06:36:13,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:13,024 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:13,024 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:13,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:13,047 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,047 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,047 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,047 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,047 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,047 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,048 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,048 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,058 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:13,058 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:13,058 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:13,059 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:13,059 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:13,059 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:13,059 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:13,059 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:36:13,067 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,068 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:13,071 - distributed.scheduler - INFO - Remove client Client-80b3994b-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:13,071 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39762; closing.
2024-02-09 06:36:13,071 - distributed.scheduler - INFO - Remove client Client-80b3994b-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:13,071 - distributed.scheduler - INFO - Close client connection: Client-80b3994b-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:13,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40411'. Reason: nanny-close
2024-02-09 06:36:13,073 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:13,073 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41033'. Reason: nanny-close
2024-02-09 06:36:13,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:13,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36407'. Reason: nanny-close
2024-02-09 06:36:13,074 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45401. Reason: nanny-close
2024-02-09 06:36:13,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:13,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40367'. Reason: nanny-close
2024-02-09 06:36:13,075 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37453. Reason: nanny-close
2024-02-09 06:36:13,075 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:13,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42513'. Reason: nanny-close
2024-02-09 06:36:13,075 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38051. Reason: nanny-close
2024-02-09 06:36:13,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:13,076 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39569. Reason: nanny-close
2024-02-09 06:36:13,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45229'. Reason: nanny-close
2024-02-09 06:36:13,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:13,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41299'. Reason: nanny-close
2024-02-09 06:36:13,077 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40867. Reason: nanny-close
2024-02-09 06:36:13,077 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:13,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:13,077 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33491'. Reason: nanny-close
2024-02-09 06:36:13,077 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58286; closing.
2024-02-09 06:36:13,077 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:13,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:13,077 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37821. Reason: nanny-close
2024-02-09 06:36:13,077 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46539. Reason: nanny-close
2024-02-09 06:36:13,078 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:13,078 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45401', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460573.0780525')
2024-02-09 06:36:13,078 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:13,078 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58334; closing.
2024-02-09 06:36:13,078 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46673. Reason: nanny-close
2024-02-09 06:36:13,079 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:13,079 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:13,079 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39569', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460573.0793345')
2024-02-09 06:36:13,079 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:13,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58350; closing.
2024-02-09 06:36:13,079 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:13,079 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:13,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58320; closing.
2024-02-09 06:36:13,080 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:13,080 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:13,080 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:13,080 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38051', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460573.0808647')
2024-02-09 06:36:13,081 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:13,081 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460573.0812128')
2024-02-09 06:36:13,081 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:13,081 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:13,082 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:13,082 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58334>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-09 06:36:13,083 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58352; closing.
2024-02-09 06:36:13,083 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58336; closing.
2024-02-09 06:36:13,084 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58326; closing.
2024-02-09 06:36:13,084 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460573.0844214')
2024-02-09 06:36:13,084 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46539', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460573.0848398')
2024-02-09 06:36:13,085 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37821', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460573.0851727')
2024-02-09 06:36:13,085 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58310; closing.
2024-02-09 06:36:13,085 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46673', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460573.085843')
2024-02-09 06:36:13,086 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:36:14,189 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:36:14,190 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:36:14,190 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:36:14,192 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:36:14,192 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-09 06:36:16,342 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:16,347 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38103 instead
  warnings.warn(
2024-02-09 06:36:16,351 - distributed.scheduler - INFO - State start
2024-02-09 06:36:16,373 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:16,374 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:36:16,375 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38103/status
2024-02-09 06:36:16,375 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:16,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35973'
2024-02-09 06:36:16,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38581'
2024-02-09 06:36:16,562 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41497'
2024-02-09 06:36:16,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39447'
2024-02-09 06:36:16,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35413'
2024-02-09 06:36:16,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38555'
2024-02-09 06:36:16,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36013'
2024-02-09 06:36:16,607 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36721'
2024-02-09 06:36:18,022 - distributed.scheduler - INFO - Receive client connection: Client-85a66296-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:18,033 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58472
2024-02-09 06:36:18,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:18,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:18,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:18,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:18,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:18,394 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41069
2024-02-09 06:36:18,394 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41069
2024-02-09 06:36:18,394 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34579
2024-02-09 06:36:18,394 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:18,394 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:18,394 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:18,394 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:18,394 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o8k68y7b
2024-02-09 06:36:18,394 - distributed.worker - INFO - Starting Worker plugin PreImport-2cd0dac3-02fa-4ce1-ba2b-d45c0db0f3d8
2024-02-09 06:36:18,394 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65ca87b8-585d-4ebb-8c33-37d46bf76433
2024-02-09 06:36:18,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:18,395 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cfeaafe3-4598-42f5-b3c8-e0b09150b9e5
2024-02-09 06:36:18,396 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32849
2024-02-09 06:36:18,396 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32849
2024-02-09 06:36:18,396 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44311
2024-02-09 06:36:18,396 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:18,396 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:18,396 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:18,396 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:18,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9_pxedys
2024-02-09 06:36:18,396 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89654069-b3b7-41a7-869b-b8df64f36021
2024-02-09 06:36:18,396 - distributed.worker - INFO - Starting Worker plugin PreImport-35cb218d-d232-427f-9254-04d3aa264dd7
2024-02-09 06:36:18,396 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13a8f50c-d6e0-44cf-a130-d59e3daae545
2024-02-09 06:36:18,471 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:18,471 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:18,472 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:18,472 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:18,476 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:18,476 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:18,477 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44097
2024-02-09 06:36:18,477 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44097
2024-02-09 06:36:18,477 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43721
2024-02-09 06:36:18,477 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:18,477 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:18,477 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:18,477 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:18,477 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-obakxpw5
2024-02-09 06:36:18,477 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04d465eb-59d7-4c98-abcc-06e45adee844
2024-02-09 06:36:18,477 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46667
2024-02-09 06:36:18,477 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46667
2024-02-09 06:36:18,477 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33793
2024-02-09 06:36:18,477 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:18,477 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:18,477 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:18,478 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:18,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u3w8bct2
2024-02-09 06:36:18,478 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c743f1c0-cc53-4595-89c3-5da79c7c8001
2024-02-09 06:36:18,479 - distributed.worker - INFO - Starting Worker plugin PreImport-b0b81b69-a9b9-4e77-84b8-0e1d99efcd6b
2024-02-09 06:36:18,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:18,479 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a1fefe8-ec44-4fb3-85bc-33e3907068b9
2024-02-09 06:36:18,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:18,483 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:18,484 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35881
2024-02-09 06:36:18,484 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35881
2024-02-09 06:36:18,484 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46189
2024-02-09 06:36:18,484 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:18,485 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:18,485 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:18,485 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:18,485 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2sz7_v5b
2024-02-09 06:36:18,485 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8a29ca8-741e-48cb-a23e-0cb2cd95ffb8
2024-02-09 06:36:18,487 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:18,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:18,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:18,488 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:18,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:18,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:18,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:18,492 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33255
2024-02-09 06:36:18,492 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33255
2024-02-09 06:36:18,492 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36275
2024-02-09 06:36:18,492 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:18,492 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:18,492 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:18,493 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:18,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:18,493 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v5am35jz
2024-02-09 06:36:18,493 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-19adfc9e-032a-4b6b-9e1e-58d188c609aa
2024-02-09 06:36:18,493 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56e60d6f-8a22-441a-bf7d-78bb1e7b70eb
2024-02-09 06:36:18,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:18,493 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42689
2024-02-09 06:36:18,493 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42689
2024-02-09 06:36:18,493 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41253
2024-02-09 06:36:18,494 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:18,494 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:18,494 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:18,494 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:18,494 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41511
2024-02-09 06:36:18,494 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v3bl6bs0
2024-02-09 06:36:18,494 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41511
2024-02-09 06:36:18,494 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42393
2024-02-09 06:36:18,494 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:18,494 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec450749-fdd3-471e-bb53-d4b3ef6af325
2024-02-09 06:36:18,494 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:18,494 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:18,494 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:18,494 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sji_v0ct
2024-02-09 06:36:18,494 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38f38934-f085-4d9b-a22d-886d2e2de46b
2024-02-09 06:36:18,495 - distributed.worker - INFO - Starting Worker plugin PreImport-d6c7523d-84ea-4b83-85f8-dc942883c1ce
2024-02-09 06:36:18,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d4010f9-f809-4e30-9486-e604a01315ee
2024-02-09 06:36:19,301 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:19,324 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32849', status: init, memory: 0, processing: 0>
2024-02-09 06:36:19,325 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32849
2024-02-09 06:36:19,325 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58492
2024-02-09 06:36:19,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:19,326 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:19,326 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:19,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:19,839 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:19,872 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41069', status: init, memory: 0, processing: 0>
2024-02-09 06:36:19,872 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41069
2024-02-09 06:36:19,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58508
2024-02-09 06:36:19,874 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:19,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:19,875 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:19,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:22,747 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,780 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42689', status: init, memory: 0, processing: 0>
2024-02-09 06:36:22,781 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42689
2024-02-09 06:36:22,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49856
2024-02-09 06:36:22,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:22,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:22,784 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:22,809 - distributed.worker - INFO - Starting Worker plugin PreImport-f21077ec-e553-49b1-8f1b-fb6646a2c447
2024-02-09 06:36:22,810 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,831 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33255', status: init, memory: 0, processing: 0>
2024-02-09 06:36:22,831 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33255
2024-02-09 06:36:22,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49862
2024-02-09 06:36:22,832 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:22,833 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:22,833 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:22,872 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,892 - distributed.worker - INFO - Starting Worker plugin PreImport-29172920-3086-4eb1-a1c4-4ffb77956349
2024-02-09 06:36:22,892 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6aafab7-e9e5-4888-88bc-8e8f77e58711
2024-02-09 06:36:22,893 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,905 - distributed.worker - INFO - Starting Worker plugin PreImport-be8e4633-c3c2-4344-b050-7dbbf486e206
2024-02-09 06:36:22,905 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fad4a0c2-8485-4d63-afb7-efe4a5cc1b1f
2024-02-09 06:36:22,905 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46667', status: init, memory: 0, processing: 0>
2024-02-09 06:36:22,911 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46667
2024-02-09 06:36:22,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49866
2024-02-09 06:36:22,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:22,913 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:22,913 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,914 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44097', status: init, memory: 0, processing: 0>
2024-02-09 06:36:22,914 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44097
2024-02-09 06:36:22,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49880
2024-02-09 06:36:22,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:22,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:22,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:22,916 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:22,925 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41511', status: init, memory: 0, processing: 0>
2024-02-09 06:36:22,926 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41511
2024-02-09 06:36:22,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49886
2024-02-09 06:36:22,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:22,927 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:22,927 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:22,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:23,052 - distributed.worker - INFO - Starting Worker plugin PreImport-8f5b2fb8-97f0-4c76-b393-395c8ade2d9f
2024-02-09 06:36:23,052 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75d2399b-6742-49eb-9773-3a295f950181
2024-02-09 06:36:23,054 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:23,087 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35881', status: init, memory: 0, processing: 0>
2024-02-09 06:36:23,087 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35881
2024-02-09 06:36:23,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49898
2024-02-09 06:36:23,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:23,090 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:23,090 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:23,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:23,141 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:23,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:23,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:23,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:23,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:23,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:23,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:23,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:23,147 - distributed.scheduler - INFO - Remove client Client-85a66296-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:23,147 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58472; closing.
2024-02-09 06:36:23,148 - distributed.scheduler - INFO - Remove client Client-85a66296-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:23,148 - distributed.scheduler - INFO - Close client connection: Client-85a66296-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:23,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35973'. Reason: nanny-close
2024-02-09 06:36:23,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:23,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38581'. Reason: nanny-close
2024-02-09 06:36:23,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:23,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41497'. Reason: nanny-close
2024-02-09 06:36:23,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41069. Reason: nanny-close
2024-02-09 06:36:23,151 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:23,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39447'. Reason: nanny-close
2024-02-09 06:36:23,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35881. Reason: nanny-close
2024-02-09 06:36:23,151 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:23,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35413'. Reason: nanny-close
2024-02-09 06:36:23,152 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32849. Reason: nanny-close
2024-02-09 06:36:23,152 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:23,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38555'. Reason: nanny-close
2024-02-09 06:36:23,152 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44097. Reason: nanny-close
2024-02-09 06:36:23,152 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:23,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36013'. Reason: nanny-close
2024-02-09 06:36:23,153 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46667. Reason: nanny-close
2024-02-09 06:36:23,153 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:23,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36721'. Reason: nanny-close
2024-02-09 06:36:23,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:23,153 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42689. Reason: nanny-close
2024-02-09 06:36:23,153 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:23,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58508; closing.
2024-02-09 06:36:23,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:23,153 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33255. Reason: nanny-close
2024-02-09 06:36:23,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:23,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:23,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41069', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460583.1541698')
2024-02-09 06:36:23,154 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41511. Reason: nanny-close
2024-02-09 06:36:23,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49880; closing.
2024-02-09 06:36:23,155 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:23,155 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:23,155 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:23,155 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:23,155 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:23,155 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:23,155 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44097', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460583.1557639')
2024-02-09 06:36:23,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:23,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58492; closing.
2024-02-09 06:36:23,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:23,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49898; closing.
2024-02-09 06:36:23,157 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:23,157 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:23,157 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:23,158 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32849', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460583.1579294')
2024-02-09 06:36:23,158 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:23,158 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460583.1584136')
2024-02-09 06:36:23,160 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49880>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-09 06:36:23,162 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49866; closing.
2024-02-09 06:36:23,163 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49856; closing.
2024-02-09 06:36:23,163 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49862; closing.
2024-02-09 06:36:23,163 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49886; closing.
2024-02-09 06:36:23,164 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460583.1639664')
2024-02-09 06:36:23,164 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42689', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460583.1645455')
2024-02-09 06:36:23,165 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33255', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460583.1650546')
2024-02-09 06:36:23,165 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41511', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460583.1655498')
2024-02-09 06:36:23,165 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:36:24,215 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:36:24,215 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:36:24,216 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:36:24,217 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:36:24,217 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-09 06:36:26,521 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:26,526 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33519 instead
  warnings.warn(
2024-02-09 06:36:26,530 - distributed.scheduler - INFO - State start
2024-02-09 06:36:26,559 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:26,561 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:36:26,562 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33519/status
2024-02-09 06:36:26,562 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:26,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43183'
2024-02-09 06:36:27,949 - distributed.scheduler - INFO - Receive client connection: Client-8ba14375-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:27,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49968
2024-02-09 06:36:28,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:28,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:29,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:29,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42589
2024-02-09 06:36:29,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42589
2024-02-09 06:36:29,136 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-09 06:36:29,136 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:29,136 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:29,136 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:29,136 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-09 06:36:29,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p91s09x0
2024-02-09 06:36:29,136 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-acca5abd-2135-44e5-8786-f887b3332605
2024-02-09 06:36:29,137 - distributed.worker - INFO - Starting Worker plugin PreImport-3f4a8244-1773-4189-8018-dba1b43633d0
2024-02-09 06:36:29,137 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b63da9a8-663b-4acb-8c1e-24393b931c4a
2024-02-09 06:36:29,137 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:29,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42589', status: init, memory: 0, processing: 0>
2024-02-09 06:36:29,193 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42589
2024-02-09 06:36:29,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49994
2024-02-09 06:36:29,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:29,195 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:29,195 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:29,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:29,198 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:29,201 - distributed.scheduler - INFO - Remove client Client-8ba14375-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:29,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49968; closing.
2024-02-09 06:36:29,201 - distributed.scheduler - INFO - Remove client Client-8ba14375-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:29,202 - distributed.scheduler - INFO - Close client connection: Client-8ba14375-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:29,203 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43183'. Reason: nanny-close
2024-02-09 06:36:29,243 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:29,245 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42589. Reason: nanny-close
2024-02-09 06:36:29,246 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:29,246 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49994; closing.
2024-02-09 06:36:29,247 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42589', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460589.24716')
2024-02-09 06:36:29,247 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:36:29,248 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:29,868 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:36:29,868 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:36:29,869 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:36:29,870 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:36:29,871 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-09 06:36:34,434 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:34,439 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46639 instead
  warnings.warn(
2024-02-09 06:36:34,444 - distributed.scheduler - INFO - State start
2024-02-09 06:36:34,497 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:34,498 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:36:34,499 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46639/status
2024-02-09 06:36:34,499 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:34,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44551'
2024-02-09 06:36:35,160 - distributed.scheduler - INFO - Receive client connection: Client-90683a5c-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:35,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43924
2024-02-09 06:36:36,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:36,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:37,532 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:37,533 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39731
2024-02-09 06:36:37,533 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39731
2024-02-09 06:36:37,533 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44777
2024-02-09 06:36:37,533 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:37,533 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:37,534 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:37,534 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-09 06:36:37,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l5pb9hji
2024-02-09 06:36:37,534 - distributed.worker - INFO - Starting Worker plugin RMMSetup-83706076-c5be-48f3-994b-fab1ae4ff963
2024-02-09 06:36:37,534 - distributed.worker - INFO - Starting Worker plugin PreImport-e3eec394-0671-4a0e-b7ee-505e89ba57ad
2024-02-09 06:36:37,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-36f50000-8a74-43a5-984e-28082b475e9d
2024-02-09 06:36:37,536 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:37,592 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39731', status: init, memory: 0, processing: 0>
2024-02-09 06:36:37,594 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39731
2024-02-09 06:36:37,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43936
2024-02-09 06:36:37,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:37,596 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:37,596 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:37,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:37,654 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:37,657 - distributed.scheduler - INFO - Remove client Client-90683a5c-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:37,657 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43924; closing.
2024-02-09 06:36:37,657 - distributed.scheduler - INFO - Remove client Client-90683a5c-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:37,658 - distributed.scheduler - INFO - Close client connection: Client-90683a5c-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:37,659 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44551'. Reason: nanny-close
2024-02-09 06:36:37,659 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:37,660 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39731. Reason: nanny-close
2024-02-09 06:36:37,662 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:37,662 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43936; closing.
2024-02-09 06:36:37,662 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39731', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460597.6625433')
2024-02-09 06:36:37,663 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:36:37,664 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:38,374 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:36:38,375 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:36:38,376 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:36:38,377 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:36:38,378 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-09 06:36:40,613 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:40,618 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-09 06:36:40,622 - distributed.scheduler - INFO - State start
2024-02-09 06:36:40,643 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:40,644 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:36:40,645 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-09 06:36:40,645 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:42,905 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:45588'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45588>: Stream is closed
2024-02-09 06:36:43,310 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:36:43,310 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:36:43,310 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:36:43,311 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:36:43,312 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-09 06:36:45,528 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:45,533 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36657 instead
  warnings.warn(
2024-02-09 06:36:45,537 - distributed.scheduler - INFO - State start
2024-02-09 06:36:46,013 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:46,014 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-09 06:36:46,016 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36657/status
2024-02-09 06:36:46,016 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:46,096 - distributed.scheduler - INFO - Receive client connection: Client-9703a290-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:46,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44820
2024-02-09 06:36:46,179 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33681'
2024-02-09 06:36:47,947 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:47,947 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:47,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:47,951 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38567
2024-02-09 06:36:47,951 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38567
2024-02-09 06:36:47,951 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42739
2024-02-09 06:36:47,952 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-09 06:36:47,952 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:47,952 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:47,952 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-09 06:36:47,952 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-bqlldtf2
2024-02-09 06:36:47,952 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bea20126-3df0-42d8-b56f-f4e2de8ed7c5
2024-02-09 06:36:47,952 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8b5d908-afc0-4222-b670-af9ec089ea0b
2024-02-09 06:36:47,952 - distributed.worker - INFO - Starting Worker plugin PreImport-3d23fd23-2bdc-41c3-a8a2-f7b3816369ad
2024-02-09 06:36:47,953 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:49,098 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38567', status: init, memory: 0, processing: 0>
2024-02-09 06:36:49,099 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38567
2024-02-09 06:36:49,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44844
2024-02-09 06:36:49,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:49,101 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-09 06:36:49,101 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:49,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-09 06:36:49,114 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:49,116 - distributed.scheduler - INFO - Remove client Client-9703a290-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:49,116 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44820; closing.
2024-02-09 06:36:49,117 - distributed.scheduler - INFO - Remove client Client-9703a290-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:49,117 - distributed.scheduler - INFO - Close client connection: Client-9703a290-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:49,118 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33681'. Reason: nanny-close
2024-02-09 06:36:49,118 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:49,119 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38567. Reason: nanny-close
2024-02-09 06:36:49,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-09 06:36:49,121 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44844; closing.
2024-02-09 06:36:49,121 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460609.1213107')
2024-02-09 06:36:49,121 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:36:49,122 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:49,833 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:36:49,833 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:36:49,834 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:36:49,834 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-09 06:36:49,835 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-09 06:36:52,027 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:52,033 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41463 instead
  warnings.warn(
2024-02-09 06:36:52,037 - distributed.scheduler - INFO - State start
2024-02-09 06:36:52,059 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:52,060 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:36:52,060 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41463/status
2024-02-09 06:36:52,061 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:52,103 - distributed.scheduler - INFO - Receive client connection: Client-9ae321b8-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:52,113 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47868
2024-02-09 06:36:52,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39711'
2024-02-09 06:36:52,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35361'
2024-02-09 06:36:52,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33969'
2024-02-09 06:36:52,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33273'
2024-02-09 06:36:52,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34429'
2024-02-09 06:36:52,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35457'
2024-02-09 06:36:52,325 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40873'
2024-02-09 06:36:52,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34189'
2024-02-09 06:36:54,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:54,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:54,146 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:54,147 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45229
2024-02-09 06:36:54,147 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45229
2024-02-09 06:36:54,147 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45645
2024-02-09 06:36:54,147 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:54,147 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:54,147 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:54,147 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:54,147 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3yzuy4w4
2024-02-09 06:36:54,147 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa026df8-27a7-4716-bd2e-7f4ed42e4e67
2024-02-09 06:36:54,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:54,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:54,155 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:54,156 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46163
2024-02-09 06:36:54,156 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46163
2024-02-09 06:36:54,156 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44005
2024-02-09 06:36:54,156 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:54,156 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:54,156 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:54,156 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:54,157 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ti8p_fwz
2024-02-09 06:36:54,157 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b440a3d8-bd16-4f68-91c5-3a0c4b60627f
2024-02-09 06:36:54,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:54,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:54,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:54,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46835
2024-02-09 06:36:54,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46835
2024-02-09 06:36:54,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35569
2024-02-09 06:36:54,171 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:54,172 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:54,172 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:54,172 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:54,172 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-02es3hj8
2024-02-09 06:36:54,172 - distributed.worker - INFO - Starting Worker plugin PreImport-afa5fa4b-55ca-46c7-8cc0-191d07a289ff
2024-02-09 06:36:54,172 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-948c15b4-7500-4a40-82d5-45c1f82daf58
2024-02-09 06:36:54,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0f08fb3f-db2c-49a5-83af-ca8da2aa3a7d
2024-02-09 06:36:54,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:54,199 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:54,203 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:54,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43411
2024-02-09 06:36:54,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43411
2024-02-09 06:36:54,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45095
2024-02-09 06:36:54,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:54,204 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:54,204 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:54,204 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:54,204 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jkcygajm
2024-02-09 06:36:54,204 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a05250bb-c937-4ae7-a567-b9933d4ed190
2024-02-09 06:36:54,206 - distributed.worker - INFO - Starting Worker plugin PreImport-fdff4725-3bed-458b-aaad-7f897ce4fd54
2024-02-09 06:36:54,206 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17982fe5-64b8-48b0-813a-3c9c6aa5c56e
2024-02-09 06:36:54,216 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:54,216 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:54,220 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:54,221 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44531
2024-02-09 06:36:54,221 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44531
2024-02-09 06:36:54,221 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36531
2024-02-09 06:36:54,221 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:54,221 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:54,221 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:54,221 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:54,221 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uxw7e4li
2024-02-09 06:36:54,221 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b578a83d-a3f2-47d3-a955-29764d4aa9d3
2024-02-09 06:36:54,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a9096d0-51a0-4825-9dac-191e5a39c68e
2024-02-09 06:36:54,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:54,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:54,228 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:54,229 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43025
2024-02-09 06:36:54,229 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43025
2024-02-09 06:36:54,229 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33525
2024-02-09 06:36:54,229 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:54,229 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:54,229 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:54,229 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:54,229 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k6d2sn8c
2024-02-09 06:36:54,229 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96ab93b6-d9b4-432b-8bba-086a0d9b7c52
2024-02-09 06:36:54,229 - distributed.worker - INFO - Starting Worker plugin PreImport-1c0b3446-ef20-4ebb-a69c-f8716e7e1a8e
2024-02-09 06:36:54,229 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b13ae79-3e58-4ece-944e-65f4f5297fcf
2024-02-09 06:36:54,238 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:54,238 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:54,242 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:54,243 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40615
2024-02-09 06:36:54,243 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40615
2024-02-09 06:36:54,243 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37213
2024-02-09 06:36:54,243 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:54,243 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:54,243 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:54,243 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:54,243 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bauu2ky4
2024-02-09 06:36:54,243 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed8a5505-33fc-4fb4-9f9d-dfcd11608a28
2024-02-09 06:36:54,244 - distributed.worker - INFO - Starting Worker plugin PreImport-a2bdf783-96d4-4a2b-8d71-f892611d26ed
2024-02-09 06:36:54,245 - distributed.worker - INFO - Starting Worker plugin RMMSetup-762bcbab-8129-4e50-9384-2cba5010edb1
2024-02-09 06:36:54,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:36:54,448 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:36:54,455 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:36:54,456 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36611
2024-02-09 06:36:54,456 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36611
2024-02-09 06:36:54,456 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40731
2024-02-09 06:36:54,456 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:36:54,456 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:54,456 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:36:54,456 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-09 06:36:54,457 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-npfldnxq
2024-02-09 06:36:54,457 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7fa604af-b026-4f07-856e-ddf273d4505a
2024-02-09 06:36:55,756 - distributed.worker - INFO - Starting Worker plugin PreImport-d16997ef-08f3-4b74-84e7-ae77e31c1e91
2024-02-09 06:36:55,757 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6fe41fda-9538-458c-bdd6-cc358cc5677f
2024-02-09 06:36:55,758 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:55,781 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45229', status: init, memory: 0, processing: 0>
2024-02-09 06:36:55,783 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45229
2024-02-09 06:36:55,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47960
2024-02-09 06:36:55,784 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:55,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:55,785 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:55,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:56,061 - distributed.worker - INFO - Starting Worker plugin PreImport-f21669db-ef6c-48a2-a8c7-6d81b1ddd45a
2024-02-09 06:36:56,062 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-adbe44c1-14e0-41c5-87d9-939cde182a00
2024-02-09 06:36:56,064 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,095 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46163', status: init, memory: 0, processing: 0>
2024-02-09 06:36:56,096 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46163
2024-02-09 06:36:56,096 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47974
2024-02-09 06:36:56,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:56,099 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:56,099 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:56,249 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,274 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,280 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46835', status: init, memory: 0, processing: 0>
2024-02-09 06:36:56,280 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46835
2024-02-09 06:36:56,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47982
2024-02-09 06:36:56,282 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:56,283 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:56,283 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:56,300 - distributed.worker - INFO - Starting Worker plugin PreImport-8f14a2ff-3381-4817-a7ce-06e1584180d9
2024-02-09 06:36:56,300 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,304 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43411', status: init, memory: 0, processing: 0>
2024-02-09 06:36:56,305 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43411
2024-02-09 06:36:56,305 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47996
2024-02-09 06:36:56,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:56,307 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:56,307 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:56,322 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44531', status: init, memory: 0, processing: 0>
2024-02-09 06:36:56,323 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44531
2024-02-09 06:36:56,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48008
2024-02-09 06:36:56,324 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:56,324 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:56,324 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:56,344 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,350 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,360 - distributed.worker - INFO - Starting Worker plugin PreImport-94680985-fdf7-4454-a2da-1f0e63ce8aa2
2024-02-09 06:36:56,361 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e49b2271-a8d7-4956-a03a-fbba11e8999e
2024-02-09 06:36:56,361 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,366 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43025', status: init, memory: 0, processing: 0>
2024-02-09 06:36:56,367 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43025
2024-02-09 06:36:56,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48014
2024-02-09 06:36:56,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:56,369 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:56,369 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:56,382 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36611', status: init, memory: 0, processing: 0>
2024-02-09 06:36:56,383 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36611
2024-02-09 06:36:56,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48024
2024-02-09 06:36:56,384 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:56,384 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40615', status: init, memory: 0, processing: 0>
2024-02-09 06:36:56,384 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40615
2024-02-09 06:36:56,384 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:56,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48016
2024-02-09 06:36:56,384 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,386 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:36:56,386 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:56,387 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:36:56,387 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:36:56,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:36:56,399 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:56,399 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:56,399 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:56,400 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:56,400 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:56,400 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:56,400 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:56,400 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-09 06:36:56,415 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:56,415 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:56,415 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:56,415 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:56,415 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:56,415 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:56,415 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:56,416 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:36:56,421 - distributed.scheduler - INFO - Remove client Client-9ae321b8-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:56,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47868; closing.
2024-02-09 06:36:56,421 - distributed.scheduler - INFO - Remove client Client-9ae321b8-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:56,422 - distributed.scheduler - INFO - Close client connection: Client-9ae321b8-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:36:56,423 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39711'. Reason: nanny-close
2024-02-09 06:36:56,423 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:56,423 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35361'. Reason: nanny-close
2024-02-09 06:36:56,424 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:56,424 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33969'. Reason: nanny-close
2024-02-09 06:36:56,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46835. Reason: nanny-close
2024-02-09 06:36:56,424 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:56,425 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33273'. Reason: nanny-close
2024-02-09 06:36:56,425 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46163. Reason: nanny-close
2024-02-09 06:36:56,425 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:56,425 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34429'. Reason: nanny-close
2024-02-09 06:36:56,425 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43025. Reason: nanny-close
2024-02-09 06:36:56,426 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:56,426 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35457'. Reason: nanny-close
2024-02-09 06:36:56,426 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45229. Reason: nanny-close
2024-02-09 06:36:56,426 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:56,426 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40873'. Reason: nanny-close
2024-02-09 06:36:56,426 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43411. Reason: nanny-close
2024-02-09 06:36:56,426 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:56,427 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34189'. Reason: nanny-close
2024-02-09 06:36:56,427 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40615. Reason: nanny-close
2024-02-09 06:36:56,427 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:36:56,427 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:56,427 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44531. Reason: nanny-close
2024-02-09 06:36:56,427 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:56,427 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:56,427 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47982; closing.
2024-02-09 06:36:56,427 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36611. Reason: nanny-close
2024-02-09 06:36:56,428 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:56,428 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47960; closing.
2024-02-09 06:36:56,428 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460616.4284272')
2024-02-09 06:36:56,428 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:56,429 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:56,429 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:56,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48014; closing.
2024-02-09 06:36:56,429 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:56,429 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:56,429 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:56,429 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:56,429 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:36:56,429 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45229', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460616.4295795')
2024-02-09 06:36:56,430 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:56,430 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43025', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460616.4305098')
2024-02-09 06:36:56,430 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:56,430 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:56,431 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47974; closing.
2024-02-09 06:36:56,431 - distributed.nanny - INFO - Worker closed
2024-02-09 06:36:56,432 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46163', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460616.432234')
2024-02-09 06:36:56,432 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47996; closing.
2024-02-09 06:36:56,433 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48014>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-09 06:36:56,434 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43411', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460616.4348207')
2024-02-09 06:36:56,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48008; closing.
2024-02-09 06:36:56,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48016; closing.
2024-02-09 06:36:56,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48024; closing.
2024-02-09 06:36:56,435 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460616.4358525')
2024-02-09 06:36:56,436 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40615', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460616.4362059')
2024-02-09 06:36:56,436 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36611', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460616.4365456')
2024-02-09 06:36:56,436 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:36:57,338 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:36:57,339 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:36:57,339 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:36:57,340 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:36:57,341 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-09 06:36:59,518 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:59,523 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36325 instead
  warnings.warn(
2024-02-09 06:36:59,527 - distributed.scheduler - INFO - State start
2024-02-09 06:36:59,550 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:36:59,551 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:36:59,552 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36325/status
2024-02-09 06:36:59,552 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:36:59,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33357'
2024-02-09 06:37:01,288 - distributed.scheduler - INFO - Receive client connection: Client-9f6007d8-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:37:01,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43668
2024-02-09 06:37:01,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:37:01,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:37:01,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:37:01,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34727
2024-02-09 06:37:01,548 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34727
2024-02-09 06:37:01,548 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37637
2024-02-09 06:37:01,548 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:37:01,548 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:37:01,548 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:37:01,548 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-09 06:37:01,548 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dlugbk_0
2024-02-09 06:37:01,549 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6f7790b-bf24-447e-bf2f-434d90df7295
2024-02-09 06:37:03,564 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93e0e1d7-0730-485c-9586-025d75d3c1c6
2024-02-09 06:37:03,565 - distributed.worker - INFO - Starting Worker plugin PreImport-fc48dfcb-fc5e-43a6-a2f7-787e4554f006
2024-02-09 06:37:03,566 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:37:03,596 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34727', status: init, memory: 0, processing: 0>
2024-02-09 06:37:03,597 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34727
2024-02-09 06:37:03,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43672
2024-02-09 06:37:03,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:37:03,599 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:37:03,600 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:37:03,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:37:03,645 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:37:03,650 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:37:03,651 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:37:03,654 - distributed.scheduler - INFO - Remove client Client-9f6007d8-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:37:03,654 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43668; closing.
2024-02-09 06:37:03,655 - distributed.scheduler - INFO - Remove client Client-9f6007d8-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:37:03,655 - distributed.scheduler - INFO - Close client connection: Client-9f6007d8-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:37:03,656 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33357'. Reason: nanny-close
2024-02-09 06:37:03,656 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:37:03,657 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34727. Reason: nanny-close
2024-02-09 06:37:03,659 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43672; closing.
2024-02-09 06:37:03,659 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:37:03,660 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460623.6601198')
2024-02-09 06:37:03,660 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:37:03,661 - distributed.nanny - INFO - Worker closed
2024-02-09 06:37:04,372 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:37:04,372 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:37:04,373 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:37:04,374 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:37:04,374 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-09 06:37:06,501 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:37:06,505 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-09 06:37:06,508 - distributed.scheduler - INFO - State start
2024-02-09 06:37:06,529 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-09 06:37:06,530 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-09 06:37:06,531 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-09 06:37:06,531 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-09 06:37:06,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32837'
2024-02-09 06:37:07,120 - distributed.scheduler - INFO - Receive client connection: Client-a3920272-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:37:07,138 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43760
2024-02-09 06:37:08,226 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-09 06:37:08,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-09 06:37:08,230 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-09 06:37:08,231 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34065
2024-02-09 06:37:08,231 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34065
2024-02-09 06:37:08,231 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35807
2024-02-09 06:37:08,231 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-09 06:37:08,231 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:37:08,231 - distributed.worker - INFO -               Threads:                          1
2024-02-09 06:37:08,231 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-09 06:37:08,231 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-br7hpl7j
2024-02-09 06:37:08,231 - distributed.worker - INFO - Starting Worker plugin PreImport-c11ce785-3025-4be2-9191-2261fb96fd39
2024-02-09 06:37:08,232 - distributed.worker - INFO - Starting Worker plugin RMMSetup-416d0add-403e-4521-b235-8247968f931c
2024-02-09 06:37:08,511 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d3ee47a-9143-4eaa-8cb4-3ee17afa8ad8
2024-02-09 06:37:08,511 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:37:08,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34065', status: init, memory: 0, processing: 0>
2024-02-09 06:37:08,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34065
2024-02-09 06:37:08,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43770
2024-02-09 06:37:08,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-09 06:37:08,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-09 06:37:08,569 - distributed.worker - INFO - -------------------------------------------------
2024-02-09 06:37:08,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-09 06:37:08,671 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-09 06:37:08,676 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-09 06:37:08,680 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:37:08,682 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-09 06:37:08,685 - distributed.scheduler - INFO - Remove client Client-a3920272-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:37:08,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43760; closing.
2024-02-09 06:37:08,685 - distributed.scheduler - INFO - Remove client Client-a3920272-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:37:08,686 - distributed.scheduler - INFO - Close client connection: Client-a3920272-c715-11ee-96c1-d8c49764f6bb
2024-02-09 06:37:08,687 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32837'. Reason: nanny-close
2024-02-09 06:37:08,687 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-09 06:37:08,688 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34065. Reason: nanny-close
2024-02-09 06:37:08,690 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-09 06:37:08,690 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43770; closing.
2024-02-09 06:37:08,691 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34065', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707460628.6909516')
2024-02-09 06:37:08,691 - distributed.scheduler - INFO - Lost all workers
2024-02-09 06:37:08,692 - distributed.nanny - INFO - Worker closed
2024-02-09 06:37:09,502 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-09 06:37:09,503 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-09 06:37:09,503 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-09 06:37:09,504 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-09 06:37:09,504 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42641 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39395 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46295 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37515 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] 2024-02-09 06:38:47,660 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39091 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41501 instead
  warnings.warn(
[1707460819.782104] [dgx13:61937:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:40331) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43139 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36271 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46007 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41849 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41423 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45983 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36727 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34241 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41221 instead
  warnings.warn(
[1707461068.374006] [dgx13:64903:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:56668) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43371 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34515 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46425 instead
  warnings.warn(
[1707461160.500780] [dgx13:66455:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:39053) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43567 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45457 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33601 instead
  warnings.warn(
[1707461239.537536] [dgx13:67434:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:54534) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36537 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32865 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42741 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33835 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44913 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43943 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] [1707461528.784950] [dgx13:71727:0]            sock.c:481  UCX  ERROR bind(fd=133 addr=0.0.0.0:45802) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35891 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43157 instead
  warnings.warn(
[1707461582.281506] [dgx13:72458:0]            sock.c:481  UCX  ERROR bind(fd=133 addr=0.0.0.0:43972) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38153 instead
  warnings.warn(
[1707461616.362765] [dgx13:73046:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:39451) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39027 instead
  warnings.warn(
[1707461654.635462] [dgx13:73397:0]            sock.c:481  UCX  ERROR bind(fd=124 addr=0.0.0.0:48118) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45377 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41107 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36357 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34319 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40065 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44283 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34683 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37185 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44005 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35457 instead
  warnings.warn(
2024-02-09 06:57:59,418 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:39686 remote=tcp://127.0.0.1:43339>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39725 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33039 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35233 instead
  warnings.warn(
2024-02-09 06:58:39,541 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40049 instead
  warnings.warn(
[1707461925.950110] [dgx13:77118:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:43339) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37651 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40511 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43423 instead
  warnings.warn(
2024-02-09 06:59:45,017 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-09 06:59:45,025 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-09 06:59:45,057 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:43208'.
2024-02-09 06:59:45,058 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:43208'. Shutting down.
2024-02-09 06:59:45,123 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f73c30e8bb0>>, <Task finished name='Task-18' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-18' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-09 06:59:45,150 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-09 06:59:45,157 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-09 06:59:45,173 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:55828'.
2024-02-09 06:59:45,175 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:55828'. Shutting down.
2024-02-09 06:59:45,237 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f3536c68bb0>>, <Task finished name='Task-21' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-21' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 403, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-09 06:59:47,127 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-02-09 06:59:47,240 - distributed.nanny - ERROR - Worker process died unexpectedly
