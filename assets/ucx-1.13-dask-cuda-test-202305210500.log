============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-21 05:39:01,793 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:01,798 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41977 instead
  warnings.warn(
2023-05-21 05:39:01,802 - distributed.scheduler - INFO - State start
2023-05-21 05:39:01,823 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:01,824 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-21 05:39:01,825 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41977/status
2023-05-21 05:39:01,995 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41043'
2023-05-21 05:39:02,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42461'
2023-05-21 05:39:02,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36093'
2023-05-21 05:39:02,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39275'
2023-05-21 05:39:02,110 - distributed.scheduler - INFO - Receive client connection: Client-c95beb7d-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:02,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59302
2023-05-21 05:39:03,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:03,749 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:03,756 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:03,815 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:03,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:03,816 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:03,816 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:03,823 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:03,823 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:03,863 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:03,863 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:03,871 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-21 05:39:04,015 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39857
2023-05-21 05:39:04,015 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39857
2023-05-21 05:39:04,015 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36863
2023-05-21 05:39:04,015 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:39:04,015 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:04,015 - distributed.worker - INFO -               Threads:                          4
2023-05-21 05:39:04,015 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-21 05:39:04,015 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hc3goc0m
2023-05-21 05:39:04,016 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6aa10116-8698-41c5-ac09-8aaeae2d6b4b
2023-05-21 05:39:04,016 - distributed.worker - INFO - Starting Worker plugin PreImport-ab0fd168-43f2-4d5a-ba42-bfa56ae5041b
2023-05-21 05:39:04,016 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb7ad16f-8f5f-4f8c-9b2b-142f2c154fde
2023-05-21 05:39:04,016 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:04,035 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39857', status: init, memory: 0, processing: 0>
2023-05-21 05:39:04,036 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39857
2023-05-21 05:39:04,036 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59314
2023-05-21 05:39:04,037 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:39:04,037 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:04,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:39:05,008 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45913
2023-05-21 05:39:05,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45913
2023-05-21 05:39:05,008 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41903
2023-05-21 05:39:05,008 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:39:05,008 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,008 - distributed.worker - INFO -               Threads:                          4
2023-05-21 05:39:05,008 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-21 05:39:05,008 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fr5nsewg
2023-05-21 05:39:05,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d83e68da-e126-4e61-838f-0c2aaa10335b
2023-05-21 05:39:05,008 - distributed.worker - INFO - Starting Worker plugin PreImport-67253199-1db0-4851-b9db-f9a3f41a5a04
2023-05-21 05:39:05,009 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5e1d031-9f7e-4074-913d-15ef3e5a0be7
2023-05-21 05:39:05,009 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,033 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45913', status: init, memory: 0, processing: 0>
2023-05-21 05:39:05,034 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45913
2023-05-21 05:39:05,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59322
2023-05-21 05:39:05,034 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:39:05,035 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:39:05,174 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46701
2023-05-21 05:39:05,174 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46701
2023-05-21 05:39:05,174 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36769
2023-05-21 05:39:05,174 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:39:05,174 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,174 - distributed.worker - INFO -               Threads:                          4
2023-05-21 05:39:05,174 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-21 05:39:05,174 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_yqqboyu
2023-05-21 05:39:05,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-059a223e-993d-48d0-a1a1-cf526db47101
2023-05-21 05:39:05,175 - distributed.worker - INFO - Starting Worker plugin PreImport-ea584b7e-9055-473c-bd89-f2106107f7f1
2023-05-21 05:39:05,175 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b148b22-3b3b-4c6f-be34-ffc979b8172c
2023-05-21 05:39:05,175 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,187 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34403
2023-05-21 05:39:05,187 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34403
2023-05-21 05:39:05,187 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41051
2023-05-21 05:39:05,188 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:39:05,188 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,188 - distributed.worker - INFO -               Threads:                          4
2023-05-21 05:39:05,188 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-21 05:39:05,188 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_gspcvqq
2023-05-21 05:39:05,188 - distributed.worker - INFO - Starting Worker plugin RMMSetup-260fc6de-16a5-4c66-9357-a829246dfb3d
2023-05-21 05:39:05,188 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3133afa1-71c6-406b-b4a2-9d188c3750ef
2023-05-21 05:39:05,188 - distributed.worker - INFO - Starting Worker plugin PreImport-2b027685-d69d-4ebc-8a4e-53aee7d98917
2023-05-21 05:39:05,188 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,209 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46701', status: init, memory: 0, processing: 0>
2023-05-21 05:39:05,210 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46701
2023-05-21 05:39:05,210 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59334
2023-05-21 05:39:05,211 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34403', status: init, memory: 0, processing: 0>
2023-05-21 05:39:05,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34403
2023-05-21 05:39:05,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59338
2023-05-21 05:39:05,211 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:39:05,212 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,212 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:39:05,212 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:05,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:39:05,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:39:05,311 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-21 05:39:05,312 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-21 05:39:05,312 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-21 05:39:05,313 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-21 05:39:05,317 - distributed.scheduler - INFO - Remove client Client-c95beb7d-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:05,318 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59302; closing.
2023-05-21 05:39:05,318 - distributed.scheduler - INFO - Remove client Client-c95beb7d-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:05,318 - distributed.scheduler - INFO - Close client connection: Client-c95beb7d-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:05,319 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41043'. Reason: nanny-close
2023-05-21 05:39:05,320 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:05,320 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42461'. Reason: nanny-close
2023-05-21 05:39:05,321 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:05,321 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45913. Reason: nanny-close
2023-05-21 05:39:05,321 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36093'. Reason: nanny-close
2023-05-21 05:39:05,321 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:05,322 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39275'. Reason: nanny-close
2023-05-21 05:39:05,322 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46701. Reason: nanny-close
2023-05-21 05:39:05,322 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:05,322 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34403. Reason: nanny-close
2023-05-21 05:39:05,323 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59322; closing.
2023-05-21 05:39:05,323 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:39:05,323 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45913', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:05,323 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39857. Reason: nanny-close
2023-05-21 05:39:05,323 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45913
2023-05-21 05:39:05,324 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:05,324 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:39:05,324 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45913
2023-05-21 05:39:05,324 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59338; closing.
2023-05-21 05:39:05,324 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45913
2023-05-21 05:39:05,325 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34403', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:05,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:39:05,325 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34403
2023-05-21 05:39:05,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:39:05,325 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:05,325 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59334; closing.
2023-05-21 05:39:05,325 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46701', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:05,326 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46701
2023-05-21 05:39:05,326 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59314; closing.
2023-05-21 05:39:05,326 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:05,326 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:05,326 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39857', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:05,327 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39857
2023-05-21 05:39:05,327 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:06,487 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:39:06,487 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:06,488 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:06,488 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-21 05:39:06,489 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-21 05:39:08,781 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:08,786 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42465 instead
  warnings.warn(
2023-05-21 05:39:08,790 - distributed.scheduler - INFO - State start
2023-05-21 05:39:08,811 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:08,812 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:39:08,813 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42465/status
2023-05-21 05:39:09,223 - distributed.scheduler - INFO - Receive client connection: Client-cd92ce92-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:09,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48338
2023-05-21 05:39:09,257 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34519'
2023-05-21 05:39:09,285 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34079'
2023-05-21 05:39:09,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34673'
2023-05-21 05:39:09,295 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40603'
2023-05-21 05:39:09,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36737'
2023-05-21 05:39:09,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41837'
2023-05-21 05:39:09,321 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39441'
2023-05-21 05:39:09,331 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35287'
2023-05-21 05:39:11,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:11,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:11,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:11,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:11,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:11,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:11,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:11,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:11,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:11,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:11,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:11,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:11,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:11,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:11,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:11,160 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:11,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:11,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:11,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:11,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:11,209 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:11,209 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:11,228 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:11,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:13,374 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32803
2023-05-21 05:39:13,375 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32803
2023-05-21 05:39:13,375 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40779
2023-05-21 05:39:13,375 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:13,375 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:13,375 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:13,376 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:13,376 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g4n985k2
2023-05-21 05:39:13,377 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f103bc93-081d-487e-9dfa-11ae50b307c9
2023-05-21 05:39:13,712 - distributed.worker - INFO - Starting Worker plugin PreImport-d627f843-05bc-4847-8f78-40ead0cd704b
2023-05-21 05:39:13,713 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5586fdb-8955-447d-bd30-1767a2d0f100
2023-05-21 05:39:13,713 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:13,754 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32803', status: init, memory: 0, processing: 0>
2023-05-21 05:39:13,756 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32803
2023-05-21 05:39:13,756 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50798
2023-05-21 05:39:13,757 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:13,757 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:13,760 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:14,526 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42105
2023-05-21 05:39:14,526 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42105
2023-05-21 05:39:14,526 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40259
2023-05-21 05:39:14,526 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,526 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,526 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:14,526 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:14,526 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-af8snlb6
2023-05-21 05:39:14,527 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97d65ea5-e852-47c6-a175-905e4a50c988
2023-05-21 05:39:14,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38697
2023-05-21 05:39:14,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38697
2023-05-21 05:39:14,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45917
2023-05-21 05:39:14,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,534 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,534 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:14,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:14,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-khfiw0k8
2023-05-21 05:39:14,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-277f48dd-f5b3-4046-9647-6226fc9cefcc
2023-05-21 05:39:14,536 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39635
2023-05-21 05:39:14,536 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39635
2023-05-21 05:39:14,536 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38311
2023-05-21 05:39:14,537 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,537 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,537 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:14,537 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:14,537 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g6m4z_hb
2023-05-21 05:39:14,538 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbd6851c-1f80-4171-951e-fbe8f5047589
2023-05-21 05:39:14,539 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39411
2023-05-21 05:39:14,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39411
2023-05-21 05:39:14,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35471
2023-05-21 05:39:14,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,539 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,539 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:14,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:14,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5slqgomi
2023-05-21 05:39:14,540 - distributed.worker - INFO - Starting Worker plugin PreImport-c4c578fa-9f39-4e72-92c2-8f4c30fa1d89
2023-05-21 05:39:14,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54564fa3-8a13-4dbc-84c7-3fc5963dab26
2023-05-21 05:39:14,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42c5c47a-3d25-43dc-bca6-9fdf284e435b
2023-05-21 05:39:14,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37015
2023-05-21 05:39:14,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37015
2023-05-21 05:39:14,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39705
2023-05-21 05:39:14,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,542 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,542 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:14,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:14,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9ymwz233
2023-05-21 05:39:14,543 - distributed.worker - INFO - Starting Worker plugin PreImport-58c999b6-c59f-4d90-8907-c6b58dbbfc0e
2023-05-21 05:39:14,543 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9b16a7f-fa17-4bc7-9d72-8186ba731d1f
2023-05-21 05:39:14,543 - distributed.worker - INFO - Starting Worker plugin RMMSetup-816b8df6-dea6-46b3-838a-26a8dbf256be
2023-05-21 05:39:14,544 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33087
2023-05-21 05:39:14,544 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33087
2023-05-21 05:39:14,544 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40623
2023-05-21 05:39:14,544 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,545 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,545 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:14,545 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:14,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_yoa1gqh
2023-05-21 05:39:14,545 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38345
2023-05-21 05:39:14,545 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38345
2023-05-21 05:39:14,545 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39881
2023-05-21 05:39:14,546 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,546 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b81ebe19-3db0-4d0a-8b21-4ee3380bc76f
2023-05-21 05:39:14,546 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,546 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:14,546 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:14,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jmxkafm1
2023-05-21 05:39:14,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07d29767-d1f2-4f37-b149-44f6161af71d
2023-05-21 05:39:14,700 - distributed.worker - INFO - Starting Worker plugin PreImport-f21258b8-7500-4149-8bed-497c1ce7827e
2023-05-21 05:39:14,700 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,700 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8ff79b6-ee57-49b2-9041-636159d8dd89
2023-05-21 05:39:14,701 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,701 - distributed.worker - INFO - Starting Worker plugin PreImport-ad8f89b2-a822-45ec-8967-4e1e93fdd6bb
2023-05-21 05:39:14,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8776dc27-32af-4cc1-942b-cc3627992eb8
2023-05-21 05:39:14,701 - distributed.worker - INFO - Starting Worker plugin PreImport-0c316b21-4aac-4ac6-b328-a535f4dce077
2023-05-21 05:39:14,701 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,701 - distributed.worker - INFO - Starting Worker plugin PreImport-78f9016c-42fb-4b05-ba72-fb5e9b8cbbab
2023-05-21 05:39:14,701 - distributed.worker - INFO - Starting Worker plugin PreImport-f9ed8045-bf95-4436-a453-03f4f18d2a2a
2023-05-21 05:39:14,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-741c816e-2136-455e-9cad-ff3bd11500cd
2023-05-21 05:39:14,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c394b1c7-a716-4b39-9d4e-bcba94065c0a
2023-05-21 05:39:14,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f176198-bad3-48e0-b6a5-b63bef3c1b1c
2023-05-21 05:39:14,701 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,701 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,701 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,702 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,726 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39411', status: init, memory: 0, processing: 0>
2023-05-21 05:39:14,727 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39411
2023-05-21 05:39:14,727 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50806
2023-05-21 05:39:14,727 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,727 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,727 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42105', status: init, memory: 0, processing: 0>
2023-05-21 05:39:14,728 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42105
2023-05-21 05:39:14,728 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50816
2023-05-21 05:39:14,728 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,729 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:14,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:14,731 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38345', status: init, memory: 0, processing: 0>
2023-05-21 05:39:14,732 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38345
2023-05-21 05:39:14,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50834
2023-05-21 05:39:14,732 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,733 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:14,735 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33087', status: init, memory: 0, processing: 0>
2023-05-21 05:39:14,736 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33087
2023-05-21 05:39:14,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50848
2023-05-21 05:39:14,736 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,737 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,737 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37015', status: init, memory: 0, processing: 0>
2023-05-21 05:39:14,737 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37015
2023-05-21 05:39:14,737 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50818
2023-05-21 05:39:14,738 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,738 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,738 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38697', status: init, memory: 0, processing: 0>
2023-05-21 05:39:14,739 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38697
2023-05-21 05:39:14,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50864
2023-05-21 05:39:14,739 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:14,740 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,741 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39635', status: init, memory: 0, processing: 0>
2023-05-21 05:39:14,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:14,741 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39635
2023-05-21 05:39:14,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50876
2023-05-21 05:39:14,742 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:14,742 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:14,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:14,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:14,767 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:14,767 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:14,767 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:14,767 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:14,767 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:14,767 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:14,767 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:14,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:14,773 - distributed.scheduler - INFO - Remove client Client-cd92ce92-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:14,773 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48338; closing.
2023-05-21 05:39:14,773 - distributed.scheduler - INFO - Remove client Client-cd92ce92-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:14,774 - distributed.scheduler - INFO - Close client connection: Client-cd92ce92-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:14,775 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34673'. Reason: nanny-close
2023-05-21 05:39:14,775 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:14,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40603'. Reason: nanny-close
2023-05-21 05:39:14,777 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:14,777 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39441'. Reason: nanny-close
2023-05-21 05:39:14,777 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37015. Reason: nanny-close
2023-05-21 05:39:14,777 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:14,778 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34519'. Reason: nanny-close
2023-05-21 05:39:14,778 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:14,778 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32803. Reason: nanny-close
2023-05-21 05:39:14,778 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39411. Reason: nanny-close
2023-05-21 05:39:14,778 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34079'. Reason: nanny-close
2023-05-21 05:39:14,779 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:14,779 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36737'. Reason: nanny-close
2023-05-21 05:39:14,779 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39635. Reason: nanny-close
2023-05-21 05:39:14,779 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:14,779 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41837'. Reason: nanny-close
2023-05-21 05:39:14,780 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33087. Reason: nanny-close
2023-05-21 05:39:14,780 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:14,780 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35287'. Reason: nanny-close
2023-05-21 05:39:14,780 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:14,780 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50818; closing.
2023-05-21 05:39:14,780 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:14,780 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:14,780 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38697. Reason: nanny-close
2023-05-21 05:39:14,780 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37015', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:14,781 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37015
2023-05-21 05:39:14,781 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42105. Reason: nanny-close
2023-05-21 05:39:14,781 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:14,781 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50806; closing.
2023-05-21 05:39:14,781 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:14,781 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:14,782 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:14,782 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38345. Reason: nanny-close
2023-05-21 05:39:14,782 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39411', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:14,782 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39411
2023-05-21 05:39:14,782 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:14,782 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:14,782 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50798; closing.
2023-05-21 05:39:14,782 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:14,783 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:14,783 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32803', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:14,783 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32803
2023-05-21 05:39:14,783 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37015
2023-05-21 05:39:14,783 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:14,784 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:14,784 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37015
2023-05-21 05:39:14,784 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50876; closing.
2023-05-21 05:39:14,784 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:14,784 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:14,785 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:14,785 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:14,784 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50806>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-05-21 05:39:14,786 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39635', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:14,786 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39635
2023-05-21 05:39:14,787 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50848; closing.
2023-05-21 05:39:14,787 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50816; closing.
2023-05-21 05:39:14,787 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33087', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:14,788 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33087
2023-05-21 05:39:14,788 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42105', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:14,788 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42105
2023-05-21 05:39:14,788 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50864; closing.
2023-05-21 05:39:14,789 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50834; closing.
2023-05-21 05:39:14,789 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38697', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:14,789 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38697
2023-05-21 05:39:14,789 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38345', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:14,789 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38345
2023-05-21 05:39:14,790 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:16,544 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:39:16,545 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:16,546 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:16,547 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:39:16,547 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-21 05:39:18,859 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:18,864 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37981 instead
  warnings.warn(
2023-05-21 05:39:18,868 - distributed.scheduler - INFO - State start
2023-05-21 05:39:18,893 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:18,894 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:39:18,894 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37981/status
2023-05-21 05:39:19,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44085'
2023-05-21 05:39:19,152 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38265'
2023-05-21 05:39:19,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39949'
2023-05-21 05:39:19,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43543'
2023-05-21 05:39:19,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39853'
2023-05-21 05:39:19,180 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39859'
2023-05-21 05:39:19,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46481'
2023-05-21 05:39:19,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34547'
2023-05-21 05:39:19,314 - distributed.scheduler - INFO - Receive client connection: Client-d3983cfd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:19,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51070
2023-05-21 05:39:20,862 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:20,862 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:20,864 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:20,864 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:20,888 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:20,889 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:20,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:20,898 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:20,914 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:20,914 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:20,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:20,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:20,927 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:20,949 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:20,952 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:20,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:20,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:20,968 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:20,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:20,992 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:21,024 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:21,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:21,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:21,085 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:23,946 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45081
2023-05-21 05:39:23,946 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45081
2023-05-21 05:39:23,946 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39013
2023-05-21 05:39:23,946 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:23,946 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:23,946 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:23,946 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:23,946 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bzwjpgt1
2023-05-21 05:39:23,947 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9fbfe1f-cdad-4e61-b3b6-ab9624dd554d
2023-05-21 05:39:24,001 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44583
2023-05-21 05:39:24,001 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44583
2023-05-21 05:39:24,001 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35013
2023-05-21 05:39:24,001 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,001 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,002 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:24,002 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:24,002 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d894ezes
2023-05-21 05:39:24,002 - distributed.worker - INFO - Starting Worker plugin RMMSetup-607de992-061e-4e27-99d6-400762b6e238
2023-05-21 05:39:24,031 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32825
2023-05-21 05:39:24,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32825
2023-05-21 05:39:24,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45355
2023-05-21 05:39:24,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,031 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,032 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:24,032 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:24,032 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x39jd105
2023-05-21 05:39:24,032 - distributed.worker - INFO - Starting Worker plugin RMMSetup-473cbd2d-55c2-4ba3-a063-bc5ea4521432
2023-05-21 05:39:24,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40921
2023-05-21 05:39:24,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40921
2023-05-21 05:39:24,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41329
2023-05-21 05:39:24,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,035 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,035 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:24,035 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:24,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qcfrsjsw
2023-05-21 05:39:24,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13686a3d-e0b3-4adc-9a48-8e2ed747e387
2023-05-21 05:39:24,037 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42139
2023-05-21 05:39:24,037 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42139
2023-05-21 05:39:24,037 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37707
2023-05-21 05:39:24,037 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,037 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,037 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:24,037 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:24,037 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5nvnpjla
2023-05-21 05:39:24,038 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6240bdf3-b400-4c97-84e4-10327f601e95
2023-05-21 05:39:24,049 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34451
2023-05-21 05:39:24,049 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34451
2023-05-21 05:39:24,049 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34869
2023-05-21 05:39:24,049 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,049 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,049 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:24,049 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:24,049 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nsho73yg
2023-05-21 05:39:24,050 - distributed.worker - INFO - Starting Worker plugin PreImport-5993e7f7-3f59-4e1d-80bf-73ae77d2bd77
2023-05-21 05:39:24,050 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d01909c0-1ace-410f-a193-5e379de61c1d
2023-05-21 05:39:24,050 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b15e891-15ad-40cc-beb2-337eeed47348
2023-05-21 05:39:24,052 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34859
2023-05-21 05:39:24,052 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34859
2023-05-21 05:39:24,053 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36419
2023-05-21 05:39:24,053 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,053 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,053 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:24,053 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:24,053 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4x_h9x25
2023-05-21 05:39:24,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34133
2023-05-21 05:39:24,054 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34133
2023-05-21 05:39:24,054 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39373
2023-05-21 05:39:24,054 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,054 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af7e8102-0884-4ff7-9f30-5aa66ef78731
2023-05-21 05:39:24,054 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,054 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:24,054 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:24,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8petrn6j
2023-05-21 05:39:24,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fcc64d76-410f-495d-8bd5-cadb356a50a6
2023-05-21 05:39:24,065 - distributed.worker - INFO - Starting Worker plugin PreImport-5e7f33a0-fa2f-45c1-9336-1eb4bb9e3e2d
2023-05-21 05:39:24,065 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0105fbb-7df3-4665-a6d4-f0d14b498d8f
2023-05-21 05:39:24,065 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,083 - distributed.worker - INFO - Starting Worker plugin PreImport-f854e7d4-0832-46d0-896a-ed475d9707da
2023-05-21 05:39:24,083 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4eee63eb-03f0-4cbb-92e5-4f0adde4e7cb
2023-05-21 05:39:24,083 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,088 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45081', status: init, memory: 0, processing: 0>
2023-05-21 05:39:24,089 - distributed.worker - INFO - Starting Worker plugin PreImport-4523f42d-9971-471c-bc59-efa98902b147
2023-05-21 05:39:24,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0039fe2e-6309-43e0-a8bc-e61dc5f896bd
2023-05-21 05:39:24,089 - distributed.worker - INFO - Starting Worker plugin PreImport-82e8262f-fc10-4e34-84c3-d3c72b6facc8
2023-05-21 05:39:24,090 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed407289-1a58-452b-b442-03c7ae5d642f
2023-05-21 05:39:24,090 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45081
2023-05-21 05:39:24,090 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,090 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49056
2023-05-21 05:39:24,090 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,090 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:24,093 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,095 - distributed.worker - INFO - Starting Worker plugin PreImport-b4abf4d6-511d-457a-a6e0-07686e966401
2023-05-21 05:39:24,096 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb18ffba-3901-405b-80f3-837efb15188d
2023-05-21 05:39:24,096 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,097 - distributed.worker - INFO - Starting Worker plugin PreImport-8d5924b8-7b16-44df-bdd7-dccd0a173fdb
2023-05-21 05:39:24,098 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d88d21a3-8db1-4df0-985b-33c98d7fb151
2023-05-21 05:39:24,098 - distributed.worker - INFO - Starting Worker plugin PreImport-3436d35f-1916-40a8-85cc-0e388793df74
2023-05-21 05:39:24,098 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c91f161f-cf45-4fdc-a586-18172b47a876
2023-05-21 05:39:24,098 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,099 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,105 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44583', status: init, memory: 0, processing: 0>
2023-05-21 05:39:24,106 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44583
2023-05-21 05:39:24,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49066
2023-05-21 05:39:24,106 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,106 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,108 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:24,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40921', status: init, memory: 0, processing: 0>
2023-05-21 05:39:24,111 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40921
2023-05-21 05:39:24,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49082
2023-05-21 05:39:24,111 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,111 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,113 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:24,120 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32825', status: init, memory: 0, processing: 0>
2023-05-21 05:39:24,121 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32825
2023-05-21 05:39:24,121 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49090
2023-05-21 05:39:24,121 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,121 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,122 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34451', status: init, memory: 0, processing: 0>
2023-05-21 05:39:24,122 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34451
2023-05-21 05:39:24,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49092
2023-05-21 05:39:24,123 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,123 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:24,126 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34133', status: init, memory: 0, processing: 0>
2023-05-21 05:39:24,126 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:24,126 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34133
2023-05-21 05:39:24,126 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49110
2023-05-21 05:39:24,127 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,127 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,128 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42139', status: init, memory: 0, processing: 0>
2023-05-21 05:39:24,129 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42139
2023-05-21 05:39:24,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49100
2023-05-21 05:39:24,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:24,129 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,129 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,130 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34859', status: init, memory: 0, processing: 0>
2023-05-21 05:39:24,131 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34859
2023-05-21 05:39:24,131 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49116
2023-05-21 05:39:24,132 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:24,132 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:24,132 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:24,134 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:24,244 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:24,244 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:24,244 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:24,244 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:24,244 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:24,244 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:24,245 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:24,245 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:24,251 - distributed.scheduler - INFO - Remove client Client-d3983cfd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:24,251 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51070; closing.
2023-05-21 05:39:24,251 - distributed.scheduler - INFO - Remove client Client-d3983cfd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:24,252 - distributed.scheduler - INFO - Close client connection: Client-d3983cfd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:24,253 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43543'. Reason: nanny-close
2023-05-21 05:39:24,254 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:24,254 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44085'. Reason: nanny-close
2023-05-21 05:39:24,255 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:24,256 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34451. Reason: nanny-close
2023-05-21 05:39:24,256 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38265'. Reason: nanny-close
2023-05-21 05:39:24,256 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:24,256 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39949'. Reason: nanny-close
2023-05-21 05:39:24,256 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42139. Reason: nanny-close
2023-05-21 05:39:24,257 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:24,257 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40921. Reason: nanny-close
2023-05-21 05:39:24,257 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39853'. Reason: nanny-close
2023-05-21 05:39:24,257 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:24,258 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45081. Reason: nanny-close
2023-05-21 05:39:24,258 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39859'. Reason: nanny-close
2023-05-21 05:39:24,258 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:24,258 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49092; closing.
2023-05-21 05:39:24,258 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:24,258 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46481'. Reason: nanny-close
2023-05-21 05:39:24,258 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34859. Reason: nanny-close
2023-05-21 05:39:24,258 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34451', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:24,258 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:24,258 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34451
2023-05-21 05:39:24,259 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:24,259 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34547'. Reason: nanny-close
2023-05-21 05:39:24,259 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:24,259 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32825. Reason: nanny-close
2023-05-21 05:39:24,259 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:24,259 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:24,259 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44583. Reason: nanny-close
2023-05-21 05:39:24,260 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:24,260 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34451
2023-05-21 05:39:24,260 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:24,260 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49082; closing.
2023-05-21 05:39:24,260 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:24,260 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49100; closing.
2023-05-21 05:39:24,260 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:24,260 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34451
2023-05-21 05:39:24,261 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34133. Reason: nanny-close
2023-05-21 05:39:24,261 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34451
2023-05-21 05:39:24,261 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34451
2023-05-21 05:39:24,261 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40921', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:24,261 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:24,261 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40921
2023-05-21 05:39:24,261 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:24,261 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42139', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:24,261 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42139
2023-05-21 05:39:24,261 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:24,262 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49056; closing.
2023-05-21 05:39:24,262 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45081', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:24,262 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45081
2023-05-21 05:39:24,262 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:24,262 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:24,262 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:24,263 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49116; closing.
2023-05-21 05:39:24,263 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:24,263 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34859', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:24,263 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34859
2023-05-21 05:39:24,263 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49066; closing.
2023-05-21 05:39:24,264 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:24,264 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49090; closing.
2023-05-21 05:39:24,264 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44583', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:24,264 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44583
2023-05-21 05:39:24,265 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32825', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:24,265 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32825
2023-05-21 05:39:24,265 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49110; closing.
2023-05-21 05:39:24,265 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34133', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:24,265 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34133
2023-05-21 05:39:24,266 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:25,922 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:39:25,922 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:25,923 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:25,926 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:39:25,927 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-21 05:39:28,273 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:28,278 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36713 instead
  warnings.warn(
2023-05-21 05:39:28,282 - distributed.scheduler - INFO - State start
2023-05-21 05:39:28,303 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:28,304 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:39:28,305 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36713/status
2023-05-21 05:39:28,589 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43013'
2023-05-21 05:39:28,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36107'
2023-05-21 05:39:28,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35391'
2023-05-21 05:39:28,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41275'
2023-05-21 05:39:28,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37249'
2023-05-21 05:39:28,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45997'
2023-05-21 05:39:28,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40761'
2023-05-21 05:39:28,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36351'
2023-05-21 05:39:29,842 - distributed.scheduler - INFO - Receive client connection: Client-d9309510-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:29,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49298
2023-05-21 05:39:30,410 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:30,410 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:30,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:30,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:30,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:30,441 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:30,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:30,450 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:30,451 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:30,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:30,501 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:30,501 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:30,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:30,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:30,553 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:30,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:30,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:30,587 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:30,587 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:30,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:30,588 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:30,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:30,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:30,642 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:33,718 - distributed.scheduler - INFO - Receive client connection: Client-dd887957-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:33,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52484
2023-05-21 05:39:33,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34811
2023-05-21 05:39:33,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34811
2023-05-21 05:39:33,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35793
2023-05-21 05:39:33,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:33,765 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:33,765 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:33,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:33,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cbw86f1p
2023-05-21 05:39:33,766 - distributed.worker - INFO - Starting Worker plugin PreImport-57fbe914-7dc7-4f1a-b7de-e175f023590f
2023-05-21 05:39:33,766 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9324f79-e549-4a9c-b41b-229592e95c1c
2023-05-21 05:39:33,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-031063b9-74ec-46e6-96e5-1b6d24322f45
2023-05-21 05:39:34,152 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35763
2023-05-21 05:39:34,152 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35763
2023-05-21 05:39:34,153 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36287
2023-05-21 05:39:34,153 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,153 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,153 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:34,153 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:34,153 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vzd4klek
2023-05-21 05:39:34,154 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81944c7e-fce0-44e8-84a1-0b6029a047bd
2023-05-21 05:39:34,157 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39799
2023-05-21 05:39:34,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39799
2023-05-21 05:39:34,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46335
2023-05-21 05:39:34,158 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,158 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,158 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:34,158 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:34,158 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_l05tzsf
2023-05-21 05:39:34,159 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20caeb14-4500-4890-a03d-1fe6d287c28b
2023-05-21 05:39:34,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34051
2023-05-21 05:39:34,248 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34051
2023-05-21 05:39:34,248 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41043
2023-05-21 05:39:34,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,248 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,248 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:34,248 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:34,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zsoei0gf
2023-05-21 05:39:34,249 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cdd69ae-3ee4-4f48-907b-f356cb92ada6
2023-05-21 05:39:34,249 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40045
2023-05-21 05:39:34,250 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40045
2023-05-21 05:39:34,250 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41515
2023-05-21 05:39:34,250 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,250 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,250 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:34,250 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:34,250 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-178ckwqj
2023-05-21 05:39:34,251 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8a24cf3c-cadc-4b82-8bc7-701f4ec0992d
2023-05-21 05:39:34,253 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41949
2023-05-21 05:39:34,253 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41949
2023-05-21 05:39:34,254 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40001
2023-05-21 05:39:34,254 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,254 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,254 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:34,254 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:34,254 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dbgttnj2
2023-05-21 05:39:34,254 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ece7412-87dd-476f-add0-149cb39e0560
2023-05-21 05:39:34,259 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45219
2023-05-21 05:39:34,259 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45219
2023-05-21 05:39:34,259 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46263
2023-05-21 05:39:34,259 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,259 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,259 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:34,259 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:34,259 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-flwqtq20
2023-05-21 05:39:34,260 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebe58221-21a2-456f-acf2-7edff0ae82af
2023-05-21 05:39:34,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37727
2023-05-21 05:39:34,266 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37727
2023-05-21 05:39:34,266 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35509
2023-05-21 05:39:34,266 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,266 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,266 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:34,267 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:34,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1rmxicg7
2023-05-21 05:39:34,267 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f6df4f2-ec12-41c3-90ac-d9c0bb879597
2023-05-21 05:39:34,274 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,310 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34811', status: init, memory: 0, processing: 0>
2023-05-21 05:39:34,313 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34811
2023-05-21 05:39:34,313 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52512
2023-05-21 05:39:34,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,313 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:34,416 - distributed.worker - INFO - Starting Worker plugin PreImport-0e2c6d61-1a13-4b32-8352-e53ae23ec20a
2023-05-21 05:39:34,416 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4bcfb1e0-2c2a-4c52-883d-64d6c4145a09
2023-05-21 05:39:34,417 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,428 - distributed.worker - INFO - Starting Worker plugin PreImport-11cb125f-76df-46c7-9dc8-b9ff270b3b21
2023-05-21 05:39:34,428 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db9c2279-dfe4-476a-b1a8-8e73d84d8f59
2023-05-21 05:39:34,429 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,453 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35763', status: init, memory: 0, processing: 0>
2023-05-21 05:39:34,454 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35763
2023-05-21 05:39:34,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52522
2023-05-21 05:39:34,454 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,454 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,454 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39799', status: init, memory: 0, processing: 0>
2023-05-21 05:39:34,455 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39799
2023-05-21 05:39:34,455 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52538
2023-05-21 05:39:34,455 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,456 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:34,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:34,479 - distributed.worker - INFO - Starting Worker plugin PreImport-946d3b67-4a30-4937-a627-219f0e7241e9
2023-05-21 05:39:34,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-720997c2-bc9c-4a88-a833-9d35d4016119
2023-05-21 05:39:34,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5dd5036-c5a7-4d49-8bef-f8f5d6777586
2023-05-21 05:39:34,480 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,480 - distributed.worker - INFO - Starting Worker plugin PreImport-f7bddf2b-bc8e-4618-9855-ceceeca6db5f
2023-05-21 05:39:34,480 - distributed.worker - INFO - Starting Worker plugin PreImport-ca39196c-9779-4638-8355-4682d59ab50c
2023-05-21 05:39:34,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb471d26-3a90-452f-bc7d-a98d72b3b16e
2023-05-21 05:39:34,481 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,481 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,481 - distributed.worker - INFO - Starting Worker plugin PreImport-71d47727-518b-485f-b829-0e35ae448a0f
2023-05-21 05:39:34,481 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a99c57e-6c6c-4471-8994-fc8505e4e8c7
2023-05-21 05:39:34,481 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,481 - distributed.worker - INFO - Starting Worker plugin PreImport-3a64637d-b2f0-4aa0-af5b-0c086dd0f3df
2023-05-21 05:39:34,481 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cba94fa5-92ce-4814-93b9-6d43c89aeeeb
2023-05-21 05:39:34,482 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,508 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45219', status: init, memory: 0, processing: 0>
2023-05-21 05:39:34,509 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45219
2023-05-21 05:39:34,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52556
2023-05-21 05:39:34,509 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,509 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,509 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37727', status: init, memory: 0, processing: 0>
2023-05-21 05:39:34,510 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37727
2023-05-21 05:39:34,510 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52582
2023-05-21 05:39:34,510 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,511 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:34,512 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:34,515 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41949', status: init, memory: 0, processing: 0>
2023-05-21 05:39:34,516 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41949
2023-05-21 05:39:34,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52580
2023-05-21 05:39:34,516 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,517 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,517 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34051', status: init, memory: 0, processing: 0>
2023-05-21 05:39:34,517 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34051
2023-05-21 05:39:34,517 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52550
2023-05-21 05:39:34,518 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,518 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,518 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40045', status: init, memory: 0, processing: 0>
2023-05-21 05:39:34,519 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40045
2023-05-21 05:39:34,519 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52568
2023-05-21 05:39:34,519 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:34,519 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:34,520 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:34,521 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:34,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:34,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,533 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,534 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,534 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,534 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:34,556 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:34,556 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:34,556 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:34,557 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:34,557 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:34,557 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:34,557 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:34,557 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:34,561 - distributed.scheduler - INFO - Remove client Client-dd887957-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:34,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52484; closing.
2023-05-21 05:39:34,562 - distributed.scheduler - INFO - Remove client Client-dd887957-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:34,563 - distributed.scheduler - INFO - Close client connection: Client-dd887957-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:34,565 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:39:34,567 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:39:34,570 - distributed.scheduler - INFO - Remove client Client-d9309510-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:34,570 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49298; closing.
2023-05-21 05:39:34,571 - distributed.scheduler - INFO - Remove client Client-d9309510-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:34,571 - distributed.scheduler - INFO - Close client connection: Client-d9309510-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:34,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35391'. Reason: nanny-close
2023-05-21 05:39:34,574 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:34,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45997'. Reason: nanny-close
2023-05-21 05:39:34,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:34,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43013'. Reason: nanny-close
2023-05-21 05:39:34,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41949. Reason: nanny-close
2023-05-21 05:39:34,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:34,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36107'. Reason: nanny-close
2023-05-21 05:39:34,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40045. Reason: nanny-close
2023-05-21 05:39:34,577 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:34,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34811. Reason: nanny-close
2023-05-21 05:39:34,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41275'. Reason: nanny-close
2023-05-21 05:39:34,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:34,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39799. Reason: nanny-close
2023-05-21 05:39:34,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37249'. Reason: nanny-close
2023-05-21 05:39:34,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:34,579 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52580; closing.
2023-05-21 05:39:34,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:34,579 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41949', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:34,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35763. Reason: nanny-close
2023-05-21 05:39:34,579 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40761'. Reason: nanny-close
2023-05-21 05:39:34,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:34,579 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-05-21 05:39:34,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:34,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:34,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:34,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36351'. Reason: nanny-close
2023-05-21 05:39:34,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34051. Reason: nanny-close
2023-05-21 05:39:34,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:34,580 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:34,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52512; closing.
2023-05-21 05:39:34,580 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:34,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37727. Reason: nanny-close
2023-05-21 05:39:34,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52568; closing.
2023-05-21 05:39:34,580 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:34,581 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:34,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45219. Reason: nanny-close
2023-05-21 05:39:34,581 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-05-21 05:39:34,581 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34811', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:34,582 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34811
2023-05-21 05:39:34,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:34,582 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-05-21 05:39:34,582 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40045', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:34,582 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-05-21 05:39:34,582 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40045
2023-05-21 05:39:34,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:34,583 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41949
2023-05-21 05:39:34,583 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52538; closing.
2023-05-21 05:39:34,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:34,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:34,583 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:34,583 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:34,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39799', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:34,584 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39799
2023-05-21 05:39:34,584 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:34,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52522; closing.
2023-05-21 05:39:34,584 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:34,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52582; closing.
2023-05-21 05:39:34,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52550; closing.
2023-05-21 05:39:34,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52556; closing.
2023-05-21 05:39:34,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35763', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:34,585 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35763
2023-05-21 05:39:34,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37727', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:34,586 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37727
2023-05-21 05:39:34,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34051', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:34,586 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34051
2023-05-21 05:39:34,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45219', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:34,587 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45219
2023-05-21 05:39:34,587 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:35,135 - distributed.scheduler - INFO - Receive client connection: Client-de609b6a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:35,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52594
2023-05-21 05:39:36,342 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:39:36,343 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:36,343 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:36,345 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:39:36,346 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-21 05:39:38,552 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:38,556 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40837 instead
  warnings.warn(
2023-05-21 05:39:38,560 - distributed.scheduler - INFO - State start
2023-05-21 05:39:38,581 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:38,582 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:38,582 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:38,583 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-21 05:39:38,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33849'
2023-05-21 05:39:38,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45171'
2023-05-21 05:39:38,936 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40485'
2023-05-21 05:39:38,939 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38667'
2023-05-21 05:39:38,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42701'
2023-05-21 05:39:38,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46749'
2023-05-21 05:39:38,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45021'
2023-05-21 05:39:38,989 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41641'
2023-05-21 05:39:40,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:40,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:40,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:40,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:40,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:40,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:40,706 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:40,706 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:40,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:40,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:40,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:40,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:40,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:40,787 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:40,872 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:40,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:40,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:40,997 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:41,204 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:41,205 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:41,205 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:41,205 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:41,391 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:41,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:44,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33849'. Reason: nanny-close
2023-05-21 05:39:44,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45171'. Reason: nanny-close
2023-05-21 05:39:44,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40485'. Reason: nanny-close
2023-05-21 05:39:44,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38667'. Reason: nanny-close
2023-05-21 05:39:44,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42701'. Reason: nanny-close
2023-05-21 05:39:44,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45021'. Reason: nanny-close
2023-05-21 05:39:44,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41641'. Reason: nanny-close
2023-05-21 05:39:44,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46749'. Reason: nanny-close
2023-05-21 05:39:46,375 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41969
2023-05-21 05:39:46,376 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41969
2023-05-21 05:39:46,376 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42745
2023-05-21 05:39:46,376 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:46,376 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,376 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:46,376 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:46,376 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-erztgxz4
2023-05-21 05:39:46,377 - distributed.worker - INFO - Starting Worker plugin RMMSetup-982c1d5b-b901-4cbb-884e-0fef41bd8740
2023-05-21 05:39:46,400 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32943
2023-05-21 05:39:46,400 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32943
2023-05-21 05:39:46,400 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36735
2023-05-21 05:39:46,400 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:46,400 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,400 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:46,401 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:46,400 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34405
2023-05-21 05:39:46,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ulrgoe2g
2023-05-21 05:39:46,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34405
2023-05-21 05:39:46,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39765
2023-05-21 05:39:46,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:46,401 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,401 - distributed.worker - INFO - Starting Worker plugin RMMSetup-787095d2-570d-427c-8095-4d22650335ca
2023-05-21 05:39:46,401 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:46,401 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:46,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_qy0y_ah
2023-05-21 05:39:46,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff9fbeae-a058-4a92-8284-487be06f63ae
2023-05-21 05:39:46,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42059
2023-05-21 05:39:46,406 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42059
2023-05-21 05:39:46,406 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42773
2023-05-21 05:39:46,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:46,406 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,406 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:46,407 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:46,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bpzh3ve3
2023-05-21 05:39:46,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-341cb294-1b47-4874-bd86-821ba89bcc0b
2023-05-21 05:39:46,461 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43735
2023-05-21 05:39:46,461 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43735
2023-05-21 05:39:46,461 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45221
2023-05-21 05:39:46,461 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:46,462 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,462 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:46,462 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:46,462 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zsluf4nm
2023-05-21 05:39:46,462 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a8548e8-8dc3-4c77-9f7b-a1968b26e33c
2023-05-21 05:39:46,467 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44207
2023-05-21 05:39:46,467 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44207
2023-05-21 05:39:46,467 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40669
2023-05-21 05:39:46,467 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:46,467 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,467 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:46,467 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:46,467 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rurl1_vk
2023-05-21 05:39:46,468 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f84ce741-7147-4377-b183-a46cf7f07e86
2023-05-21 05:39:46,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38313
2023-05-21 05:39:46,468 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38313
2023-05-21 05:39:46,468 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44443
2023-05-21 05:39:46,468 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:46,468 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,468 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:46,469 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:46,469 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z9znzu5h
2023-05-21 05:39:46,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a3292e2-2331-4ed1-9ff4-fc91a7768bd3
2023-05-21 05:39:46,469 - distributed.worker - INFO - Starting Worker plugin PreImport-d1ec6b9e-adf5-4dfa-af42-8ea1d00f1a29
2023-05-21 05:39:46,470 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb10ed83-8db4-40f9-86bc-9a691dcb11f3
2023-05-21 05:39:46,470 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41025
2023-05-21 05:39:46,470 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41025
2023-05-21 05:39:46,471 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41785
2023-05-21 05:39:46,471 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:46,471 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,471 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:46,471 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:46,471 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5jgaxfw7
2023-05-21 05:39:46,472 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b50d5262-cecc-4d57-bd33-ed817955d677
2023-05-21 05:39:46,612 - distributed.worker - INFO - Starting Worker plugin PreImport-6b66cf26-7fb3-4c9e-9a84-b2a7062cabe8
2023-05-21 05:39:46,612 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77d37a51-1e8a-4ba1-86c4-c0957042a79e
2023-05-21 05:39:46,613 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,617 - distributed.worker - INFO - Starting Worker plugin PreImport-86f19478-9da8-485f-a381-369f53615317
2023-05-21 05:39:46,617 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06706603-2004-4b6c-a112-4cee9f0f861d
2023-05-21 05:39:46,617 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin PreImport-938a4d73-f87f-431b-9e07-5bfdb927d332
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41b31c97-514b-4acc-a539-9ba2dd08c30f
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin PreImport-d6679dbf-9b5c-4530-a657-6c5a8ec52dc4
2023-05-21 05:39:46,622 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-536b11a9-6631-4a21-8bb9-65359678f897
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin PreImport-11e096db-e7e1-4dab-832b-dc90de040a18
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin PreImport-280feb12-79b3-4152-a1ea-6d6933c8d27c
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8870478-10d4-4a0e-bc95-a6db0b433749
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e333cac1-ed4b-4785-8350-3621f1dc938f
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin PreImport-e0fe5120-9c58-4ef2-b0df-90f3928091e0
2023-05-21 05:39:46,622 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-603f81c1-041b-45f9-8de6-8974b9b3715c
2023-05-21 05:39:46,622 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,623 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,623 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:46,623 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:48,979 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:48,979 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:48,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:49,019 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:49,022 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38313. Reason: nanny-close
2023-05-21 05:39:49,025 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:49,027 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:49,469 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:49,470 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:49,473 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:49,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:49,478 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41969. Reason: nanny-close
2023-05-21 05:39:49,481 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:49,483 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:49,502 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:49,502 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:49,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:49,528 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:49,529 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34405. Reason: nanny-close
2023-05-21 05:39:49,533 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:49,535 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:49,740 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:49,740 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:49,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:49,768 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:49,769 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41025. Reason: nanny-close
2023-05-21 05:39:49,771 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:49,772 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:49,899 - distributed.nanny - WARNING - Restarting worker
2023-05-21 05:39:49,967 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:49,967 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:49,970 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-21 05:39:50,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:50,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43735. Reason: nanny-close
2023-05-21 05:39:50,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:50,148 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:50,325 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-21 05:39:50,345 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:50,345 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:50,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:50,381 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-21 05:39:50,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:50,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32943. Reason: nanny-close
2023-05-21 05:39:50,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:50,405 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:50,532 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-21 05:39:50,928 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-21 05:39:51,095 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:51,095 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:51,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:51,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:51,134 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44207. Reason: nanny-close
2023-05-21 05:39:51,137 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:51,138 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:51,341 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-21 05:39:51,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:51,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:52,164 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:52,164 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:52,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:52,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:52,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:52,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:52,749 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:52,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:52,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:52,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:52,958 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:52,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:52,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:52,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:53,001 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:53,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42059. Reason: nanny-close
2023-05-21 05:39:53,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:53,007 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:53,013 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:53,123 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-21 05:39:53,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:53,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:53,155 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:53,324 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:53,944 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-05-21 05:39:53,962 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48847 parent=48504 started daemon>
2023-05-21 05:39:53,963 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48838 parent=48504 started daemon>
2023-05-21 05:39:53,963 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48814 parent=48504 started daemon>
2023-05-21 05:39:53,963 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48802 parent=48504 started daemon>
2023-05-21 05:39:53,963 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48798 parent=48504 started daemon>
2023-05-21 05:39:53,963 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48792 parent=48504 started daemon>
2023-05-21 05:39:53,964 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48787 parent=48504 started daemon>
2023-05-21 05:39:53,964 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48777 parent=48504 started daemon>
2023-05-21 05:39:53,989 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 48814 exit status was already read will report exitcode 255
2023-05-21 05:39:54,009 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 48802 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-21 05:39:56,734 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:56,739 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37993 instead
  warnings.warn(
2023-05-21 05:39:56,743 - distributed.scheduler - INFO - State start
2023-05-21 05:39:56,764 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:56,765 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:39:56,765 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37993/status
2023-05-21 05:39:56,772 - distributed.scheduler - INFO - Receive client connection: Client-ea2e20dd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:56,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38792
2023-05-21 05:39:56,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36865'
2023-05-21 05:39:57,188 - distributed.scheduler - INFO - Receive client connection: Client-eb85c914-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:57,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38822
2023-05-21 05:39:58,650 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8o3yhl8h', purging
2023-05-21 05:39:58,650 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-micg_0tg', purging
2023-05-21 05:39:58,650 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x9jpbqi3', purging
2023-05-21 05:39:58,651 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-crvaecss', purging
2023-05-21 05:39:58,651 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gmn_4d23', purging
2023-05-21 05:39:58,651 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xg1oj4fi', purging
2023-05-21 05:39:58,652 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:58,652 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:58,995 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:59,996 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43849
2023-05-21 05:39:59,997 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43849
2023-05-21 05:39:59,997 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-05-21 05:39:59,997 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:59,997 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:59,997 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:59,997 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:39:59,997 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2lejytgx
2023-05-21 05:39:59,997 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31dbf7ef-da87-4ce9-8ad8-ced39e84e8cf
2023-05-21 05:39:59,997 - distributed.worker - INFO - Starting Worker plugin PreImport-ffbb0bfd-c003-4ca0-9bb5-a7884cc3c917
2023-05-21 05:39:59,998 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f80a31af-73b8-4fa2-bfa5-b2e142197a03
2023-05-21 05:40:00,192 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:00,227 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43849', status: init, memory: 0, processing: 0>
2023-05-21 05:40:00,230 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43849
2023-05-21 05:40:00,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48690
2023-05-21 05:40:00,231 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:00,231 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:00,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:00,311 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:00,314 - distributed.scheduler - INFO - Remove client Client-ea2e20dd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:00,314 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38792; closing.
2023-05-21 05:40:00,314 - distributed.scheduler - INFO - Remove client Client-ea2e20dd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:00,315 - distributed.scheduler - INFO - Close client connection: Client-ea2e20dd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:00,316 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36865'. Reason: nanny-close
2023-05-21 05:40:00,316 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:00,317 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43849. Reason: nanny-close
2023-05-21 05:40:00,319 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48690; closing.
2023-05-21 05:40:00,319 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:00,319 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43849', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:00,319 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43849
2023-05-21 05:40:00,320 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:00,320 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:01,332 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:01,333 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:01,333 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:01,335 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:40:01,336 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-21 05:40:06,138 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:06,143 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40967 instead
  warnings.warn(
2023-05-21 05:40:06,147 - distributed.scheduler - INFO - State start
2023-05-21 05:40:06,170 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:06,172 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:40:06,172 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40967/status
2023-05-21 05:40:06,258 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43397', status: init, memory: 0, processing: 0>
2023-05-21 05:40:06,272 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43397
2023-05-21 05:40:06,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49754
2023-05-21 05:40:06,274 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38391', status: init, memory: 0, processing: 0>
2023-05-21 05:40:06,274 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38391
2023-05-21 05:40:06,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49738
2023-05-21 05:40:06,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42259'
2023-05-21 05:40:06,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43245', status: init, memory: 0, processing: 0>
2023-05-21 05:40:06,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43245
2023-05-21 05:40:06,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49778
2023-05-21 05:40:06,719 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35351', status: init, memory: 0, processing: 0>
2023-05-21 05:40:06,720 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35351
2023-05-21 05:40:06,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49788
2023-05-21 05:40:06,784 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36119', status: init, memory: 0, processing: 0>
2023-05-21 05:40:06,785 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36119
2023-05-21 05:40:06,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49802
2023-05-21 05:40:06,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38953', status: init, memory: 0, processing: 0>
2023-05-21 05:40:06,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38953
2023-05-21 05:40:06,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49814
2023-05-21 05:40:06,940 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33509', status: init, memory: 0, processing: 0>
2023-05-21 05:40:06,941 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33509
2023-05-21 05:40:06,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49818
2023-05-21 05:40:06,986 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40923', status: init, memory: 0, processing: 0>
2023-05-21 05:40:06,986 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40923
2023-05-21 05:40:06,987 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49828
2023-05-21 05:40:07,358 - distributed.scheduler - INFO - Receive client connection: Client-eb85c914-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:07,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49848
2023-05-21 05:40:07,573 - distributed.scheduler - INFO - Remove client Client-eb85c914-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:07,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49848; closing.
2023-05-21 05:40:07,574 - distributed.scheduler - INFO - Remove client Client-eb85c914-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:07,574 - distributed.scheduler - INFO - Close client connection: Client-eb85c914-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:07,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49814; closing.
2023-05-21 05:40:07,580 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38953', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:07,580 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38953
2023-05-21 05:40:07,582 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49802; closing.
2023-05-21 05:40:07,582 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49788; closing.
2023-05-21 05:40:07,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36119', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:07,583 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36119
2023-05-21 05:40:07,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35351', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:07,584 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35351
2023-05-21 05:40:07,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49828; closing.
2023-05-21 05:40:07,584 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40923', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:07,584 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40923
2023-05-21 05:40:07,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49818; closing.
2023-05-21 05:40:07,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33509', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:07,585 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33509
2023-05-21 05:40:07,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49754; closing.
2023-05-21 05:40:07,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49738; closing.
2023-05-21 05:40:07,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43397', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:07,587 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43397
2023-05-21 05:40:07,587 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38391', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:07,587 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38391
2023-05-21 05:40:07,587 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49778; closing.
2023-05-21 05:40:07,588 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43245', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:07,588 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43245
2023-05-21 05:40:07,588 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:08,161 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:08,161 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:08,508 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:09,457 - distributed.scheduler - INFO - Receive client connection: Client-f2d5b69e-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:09,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49868
2023-05-21 05:40:09,861 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43195
2023-05-21 05:40:09,861 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43195
2023-05-21 05:40:09,861 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39109
2023-05-21 05:40:09,861 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:09,862 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:09,862 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:09,862 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:40:09,862 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wf_luzs_
2023-05-21 05:40:09,862 - distributed.worker - INFO - Starting Worker plugin RMMSetup-043337f5-0f1d-468f-a749-19332a96a9ee
2023-05-21 05:40:09,862 - distributed.worker - INFO - Starting Worker plugin PreImport-f9536fbf-0d7a-4d84-8917-976898dfbfd4
2023-05-21 05:40:09,863 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-523b8269-22c2-4152-9225-8e933cf519d4
2023-05-21 05:40:09,865 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:09,887 - distributed.scheduler - INFO - Receive client connection: Client-efb72c3c-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:09,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59302
2023-05-21 05:40:09,909 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43195', status: init, memory: 0, processing: 0>
2023-05-21 05:40:09,910 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43195
2023-05-21 05:40:09,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59286
2023-05-21 05:40:09,911 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:09,911 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:09,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:09,977 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:09,982 - distributed.scheduler - INFO - Remove client Client-f2d5b69e-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:09,982 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49868; closing.
2023-05-21 05:40:09,983 - distributed.scheduler - INFO - Remove client Client-f2d5b69e-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:09,983 - distributed.scheduler - INFO - Close client connection: Client-f2d5b69e-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:09,996 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:09,999 - distributed.scheduler - INFO - Remove client Client-efb72c3c-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:09,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59302; closing.
2023-05-21 05:40:09,999 - distributed.scheduler - INFO - Remove client Client-efb72c3c-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:09,999 - distributed.scheduler - INFO - Close client connection: Client-efb72c3c-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:10,000 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42259'. Reason: nanny-close
2023-05-21 05:40:10,001 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:10,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43195. Reason: nanny-close
2023-05-21 05:40:10,005 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59286; closing.
2023-05-21 05:40:10,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:10,005 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43195', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:10,005 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43195
2023-05-21 05:40:10,005 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:10,006 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:11,569 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:11,569 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:11,570 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:11,571 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:40:11,571 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-21 05:40:13,742 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:13,747 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39529 instead
  warnings.warn(
2023-05-21 05:40:13,751 - distributed.scheduler - INFO - State start
2023-05-21 05:40:13,772 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:13,773 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:40:13,773 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39529/status
2023-05-21 05:40:15,743 - distributed.scheduler - INFO - Receive client connection: Client-f4942a94-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:15,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59416
2023-05-21 05:40:17,527 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40331', status: init, memory: 0, processing: 0>
2023-05-21 05:40:17,529 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40331
2023-05-21 05:40:17,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59430
2023-05-21 05:40:17,590 - distributed.scheduler - INFO - Remove client Client-f4942a94-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:17,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59416; closing.
2023-05-21 05:40:17,590 - distributed.scheduler - INFO - Remove client Client-f4942a94-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:17,591 - distributed.scheduler - INFO - Close client connection: Client-f4942a94-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:17,596 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59430; closing.
2023-05-21 05:40:17,597 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40331', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:17,597 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40331
2023-05-21 05:40:17,597 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:17,960 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:17,960 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:17,961 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:17,961 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:40:17,962 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-05-21 05:40:20,364 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:20,368 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35029 instead
  warnings.warn(
2023-05-21 05:40:20,373 - distributed.scheduler - INFO - State start
2023-05-21 05:40:20,394 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:20,395 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-21 05:40:20,396 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35029/status
2023-05-21 05:40:20,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35745'
2023-05-21 05:40:21,078 - distributed.scheduler - INFO - Receive client connection: Client-f82f271d-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:21,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56956
2023-05-21 05:40:22,238 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:22,239 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:22,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:23,158 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43115
2023-05-21 05:40:23,159 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43115
2023-05-21 05:40:23,159 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35959
2023-05-21 05:40:23,159 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:40:23,159 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:23,159 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:23,159 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:40:23,159 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yipd_ekp
2023-05-21 05:40:23,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73d3e12a-9769-4247-be64-0b97c3382267
2023-05-21 05:40:23,160 - distributed.worker - INFO - Starting Worker plugin PreImport-935b679a-0589-4765-9c7d-c2d6711ad676
2023-05-21 05:40:23,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc52e4b0-1de9-4b44-a89f-4245d6a00c73
2023-05-21 05:40:23,160 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:23,259 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43115', status: init, memory: 0, processing: 0>
2023-05-21 05:40:23,261 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43115
2023-05-21 05:40:23,261 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56976
2023-05-21 05:40:23,262 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:40:23,262 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:23,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:40:23,309 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:23,312 - distributed.scheduler - INFO - Remove client Client-f82f271d-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:23,312 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56956; closing.
2023-05-21 05:40:23,313 - distributed.scheduler - INFO - Remove client Client-f82f271d-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:23,313 - distributed.scheduler - INFO - Close client connection: Client-f82f271d-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:23,314 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35745'. Reason: nanny-close
2023-05-21 05:40:23,314 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:23,316 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43115. Reason: nanny-close
2023-05-21 05:40:23,318 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56976; closing.
2023-05-21 05:40:23,318 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:40:23,319 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43115', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:23,319 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43115
2023-05-21 05:40:23,319 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:23,320 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:24,532 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:24,532 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:24,533 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:24,533 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-21 05:40:24,534 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-05-21 05:40:26,872 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:26,877 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41055 instead
  warnings.warn(
2023-05-21 05:40:26,881 - distributed.scheduler - INFO - State start
2023-05-21 05:40:26,901 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:26,902 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:40:26,902 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41055/status
2023-05-21 05:40:27,121 - distributed.scheduler - INFO - Receive client connection: Client-fc2a52b0-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:27,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35970
2023-05-21 05:40:27,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41483'
2023-05-21 05:40:27,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45641'
2023-05-21 05:40:27,180 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37661'
2023-05-21 05:40:27,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44707'
2023-05-21 05:40:27,190 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35491'
2023-05-21 05:40:27,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35087'
2023-05-21 05:40:27,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34721'
2023-05-21 05:40:27,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36217'
2023-05-21 05:40:28,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:28,884 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:28,896 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:28,896 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:28,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:28,908 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:28,909 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:28,914 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:28,914 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:28,919 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:28,919 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:28,919 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:28,919 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:28,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:28,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:28,929 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:28,940 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:28,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:28,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:28,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:28,970 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:29,003 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:29,003 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:29,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:32,196 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44563
2023-05-21 05:40:32,197 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44563
2023-05-21 05:40:32,197 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39807
2023-05-21 05:40:32,197 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,197 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,197 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,197 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:32,197 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ry70p_33
2023-05-21 05:40:32,198 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4525ae8e-fcf4-4fad-ad17-4a6f60a48010
2023-05-21 05:40:32,203 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37223
2023-05-21 05:40:32,203 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37223
2023-05-21 05:40:32,203 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35393
2023-05-21 05:40:32,203 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,203 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,203 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,203 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:32,203 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t4lxkq3l
2023-05-21 05:40:32,204 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f0a0cd5-83d5-4860-a017-6c3a75c82a4e
2023-05-21 05:40:32,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45161
2023-05-21 05:40:32,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45161
2023-05-21 05:40:32,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43701
2023-05-21 05:40:32,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,204 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,204 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,204 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:32,204 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ag37lm0b
2023-05-21 05:40:32,205 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b80cdc5f-cc31-4a8c-ad3f-28088b09c8fc
2023-05-21 05:40:32,212 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40015
2023-05-21 05:40:32,212 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40015
2023-05-21 05:40:32,212 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41313
2023-05-21 05:40:32,212 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,212 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,212 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,212 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:32,212 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-83ha08rr
2023-05-21 05:40:32,213 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41535
2023-05-21 05:40:32,213 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b22c44ca-78fd-48bb-9bf5-82c18951e37c
2023-05-21 05:40:32,213 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41535
2023-05-21 05:40:32,213 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42427
2023-05-21 05:40:32,213 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,213 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,213 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,213 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:32,213 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dfpbyz_l
2023-05-21 05:40:32,214 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3b1a8f5-e4b9-4668-918f-64926f09abd4
2023-05-21 05:40:32,215 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34575
2023-05-21 05:40:32,216 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34575
2023-05-21 05:40:32,216 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34461
2023-05-21 05:40:32,216 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,216 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,216 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,216 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:32,216 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b8n1q3n_
2023-05-21 05:40:32,217 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63e95ab0-f2f3-40c5-81c5-f89d75a2cdce
2023-05-21 05:40:32,218 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36749
2023-05-21 05:40:32,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36749
2023-05-21 05:40:32,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38321
2023-05-21 05:40:32,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,219 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,219 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,219 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:32,219 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ffv9uxnj
2023-05-21 05:40:32,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45105
2023-05-21 05:40:32,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45105
2023-05-21 05:40:32,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34369
2023-05-21 05:40:32,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,220 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,220 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:32,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oxd21hxw
2023-05-21 05:40:32,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-190e9e5b-d067-469c-906b-429e71e93a6d
2023-05-21 05:40:32,220 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e0d1a75-6083-4658-b685-eaaa6b6c781a
2023-05-21 05:40:32,220 - distributed.worker - INFO - Starting Worker plugin PreImport-55677a0f-b6ba-45fb-8173-b9ffedb6f4d9
2023-05-21 05:40:32,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81d18464-39af-4526-8c95-2c2b42943033
2023-05-21 05:40:32,395 - distributed.worker - INFO - Starting Worker plugin PreImport-f268cce5-67b8-4f95-a96e-cee917f56f2d
2023-05-21 05:40:32,395 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c10ae7df-31f1-4d43-b93c-a9e685d675b2
2023-05-21 05:40:32,395 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,413 - distributed.worker - INFO - Starting Worker plugin PreImport-f6aeb293-ab26-438a-908b-e313f1c8392d
2023-05-21 05:40:32,413 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,413 - distributed.worker - INFO - Starting Worker plugin PreImport-c81e1df5-c863-4342-a0c6-a9f2c1043524
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37b10f39-5552-43ea-92c8-ef4efb0eda0f
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin PreImport-61b6f32e-4df9-4020-a7aa-a4bb5c13ddce
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin PreImport-2aadb113-24a4-4a0c-98d2-73a027ab9d31
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa3547c3-151d-4e78-b304-e48285029b83
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45f12af4-a7e4-40a3-99e9-94ca1f0ad250
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc56d1dc-9e61-4872-9574-b943d550d674
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin PreImport-756af2f2-8bc3-4050-a462-77ab1caeef37
2023-05-21 05:40:32,414 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1b37eed-b631-4226-a043-10bc6856c2b0
2023-05-21 05:40:32,414 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,414 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin PreImport-70d27d8d-1fa2-412f-bf5f-5cea8c3903e7
2023-05-21 05:40:32,414 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,414 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-225c322f-93f2-4d48-9270-41a4e4e51d64
2023-05-21 05:40:32,415 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,424 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37223', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,425 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37223
2023-05-21 05:40:32,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34968
2023-05-21 05:40:32,426 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,426 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:32,439 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45161', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,439 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45161
2023-05-21 05:40:32,440 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34970
2023-05-21 05:40:32,440 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,440 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40015', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,441 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40015
2023-05-21 05:40:32,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34978
2023-05-21 05:40:32,441 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,441 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:32,443 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:32,444 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45105', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,445 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45105
2023-05-21 05:40:32,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34986
2023-05-21 05:40:32,445 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,446 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,446 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36749', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,446 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36749
2023-05-21 05:40:32,446 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35002
2023-05-21 05:40:32,447 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41535', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,447 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,447 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,447 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41535
2023-05-21 05:40:32,447 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35010
2023-05-21 05:40:32,448 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,448 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,448 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34575', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:32,449 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34575
2023-05-21 05:40:32,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34994
2023-05-21 05:40:32,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:32,449 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,450 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,450 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44563', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,450 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44563
2023-05-21 05:40:32,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35026
2023-05-21 05:40:32,451 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:32,451 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,451 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:32,452 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:32,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:32,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:32,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:32,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:32,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:32,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:32,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:32,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:32,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:32,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,540 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,544 - distributed.scheduler - INFO - Remove client Client-fc2a52b0-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:32,544 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35970; closing.
2023-05-21 05:40:32,544 - distributed.scheduler - INFO - Remove client Client-fc2a52b0-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:32,545 - distributed.scheduler - INFO - Close client connection: Client-fc2a52b0-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:32,546 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45641'. Reason: nanny-close
2023-05-21 05:40:32,546 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,547 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35491'. Reason: nanny-close
2023-05-21 05:40:32,547 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,548 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35087'. Reason: nanny-close
2023-05-21 05:40:32,548 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45105. Reason: nanny-close
2023-05-21 05:40:32,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,549 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41483'. Reason: nanny-close
2023-05-21 05:40:32,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37223. Reason: nanny-close
2023-05-21 05:40:32,549 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40015. Reason: nanny-close
2023-05-21 05:40:32,549 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37661'. Reason: nanny-close
2023-05-21 05:40:32,549 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,550 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34575. Reason: nanny-close
2023-05-21 05:40:32,550 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44707'. Reason: nanny-close
2023-05-21 05:40:32,550 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34986; closing.
2023-05-21 05:40:32,550 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34721'. Reason: nanny-close
2023-05-21 05:40:32,550 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:32,550 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44563. Reason: nanny-close
2023-05-21 05:40:32,551 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45105', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,551 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,551 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45105
2023-05-21 05:40:32,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:32,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:32,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36217'. Reason: nanny-close
2023-05-21 05:40:32,551 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41535. Reason: nanny-close
2023-05-21 05:40:32,551 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,551 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45161. Reason: nanny-close
2023-05-21 05:40:32,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:32,552 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:32,552 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45105
2023-05-21 05:40:32,552 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:32,552 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34978; closing.
2023-05-21 05:40:32,552 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:32,552 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34968; closing.
2023-05-21 05:40:32,552 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45105
2023-05-21 05:40:32,553 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36749. Reason: nanny-close
2023-05-21 05:40:32,553 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:32,553 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45105
2023-05-21 05:40:32,553 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40015', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:32,553 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45105
2023-05-21 05:40:32,553 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40015
2023-05-21 05:40:32,553 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37223', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:32,553 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37223
2023-05-21 05:40:32,554 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:32,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34994; closing.
2023-05-21 05:40:32,554 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:32,554 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34575', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,554 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34575
2023-05-21 05:40:32,554 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:32,554 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:32,555 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35026; closing.
2023-05-21 05:40:32,555 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:32,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44563', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,555 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44563
2023-05-21 05:40:32,555 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:32,555 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34970; closing.
2023-05-21 05:40:32,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35010; closing.
2023-05-21 05:40:32,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35002; closing.
2023-05-21 05:40:32,556 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45161', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,556 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45161
2023-05-21 05:40:32,557 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41535', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,557 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41535
2023-05-21 05:40:32,557 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36749', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,557 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36749
2023-05-21 05:40:32,557 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:34,028 - distributed.scheduler - INFO - Receive client connection: Client-017afebc-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:34,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35034
2023-05-21 05:40:34,113 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:34,113 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:34,114 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:34,115 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:40:34,116 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-05-21 05:40:36,352 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:36,357 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38849 instead
  warnings.warn(
2023-05-21 05:40:36,361 - distributed.scheduler - INFO - State start
2023-05-21 05:40:36,382 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:36,383 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:36,384 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:36,384 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-21 05:40:36,583 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36891'
2023-05-21 05:40:38,352 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:38,352 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:38,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:41,637 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36891'. Reason: nanny-close
2023-05-21 05:40:41,653 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38169
2023-05-21 05:40:41,653 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38169
2023-05-21 05:40:41,653 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39549
2023-05-21 05:40:41,653 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,653 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,653 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,653 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:40:41,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xl0s6kse
2023-05-21 05:40:41,654 - distributed.worker - INFO - Starting Worker plugin PreImport-815ea931-e4c5-4c47-9dc9-c18b78e896d1
2023-05-21 05:40:41,654 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ceb27b83-3da3-4f72-a556-6e484a715109
2023-05-21 05:40:41,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5cfde2a8-1c02-4570-80a0-70633ab31363
2023-05-21 05:40:41,783 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,822 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,822 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,842 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,844 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38169. Reason: nanny-close
2023-05-21 05:40:41,847 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,848 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-05-21 05:40:45,570 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:45,574 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36651 instead
  warnings.warn(
2023-05-21 05:40:45,578 - distributed.scheduler - INFO - State start
2023-05-21 05:40:45,600 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:45,601 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:40:45,602 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36651/status
2023-05-21 05:40:45,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45345'
2023-05-21 05:40:46,058 - distributed.scheduler - INFO - Receive client connection: Client-076f4f6c-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:46,074 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38844
2023-05-21 05:40:47,371 - distributed.scheduler - INFO - Receive client connection: Client-0732c73e-f79a-11ed-b842-d8c49764f6bb
2023-05-21 05:40:47,372 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38908
2023-05-21 05:40:47,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:47,488 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:47,513 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:48,603 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33685
2023-05-21 05:40:48,603 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33685
2023-05-21 05:40:48,603 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44093
2023-05-21 05:40:48,603 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:48,603 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:48,603 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:48,603 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:40:48,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1o4ouc05
2023-05-21 05:40:48,604 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3aa3197c-6869-4bc2-8edc-be25be7ad480
2023-05-21 05:40:48,723 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12e59a8e-a37d-4009-9cdd-c34c55d0e18a
2023-05-21 05:40:48,724 - distributed.worker - INFO - Starting Worker plugin PreImport-2d3fdcce-289f-4fd6-a8f4-ee0773477d7c
2023-05-21 05:40:48,724 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:48,761 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33685', status: init, memory: 0, processing: 0>
2023-05-21 05:40:48,763 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33685
2023-05-21 05:40:48,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38936
2023-05-21 05:40:48,763 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:48,764 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:48,766 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:48,847 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-05-21 05:40:48,849 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:40:48,853 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:48,854 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:40:48,855 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:48,857 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:48,858 - distributed.scheduler - INFO - Remove client Client-076f4f6c-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:48,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38844; closing.
2023-05-21 05:40:48,859 - distributed.scheduler - INFO - Remove client Client-076f4f6c-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:48,859 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:48,860 - distributed.scheduler - INFO - Close client connection: Client-076f4f6c-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:48,862 - distributed.scheduler - INFO - Remove client Client-0732c73e-f79a-11ed-b842-d8c49764f6bb
2023-05-21 05:40:48,862 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38908; closing.
2023-05-21 05:40:48,862 - distributed.scheduler - INFO - Remove client Client-0732c73e-f79a-11ed-b842-d8c49764f6bb
2023-05-21 05:40:48,862 - distributed.scheduler - INFO - Close client connection: Client-0732c73e-f79a-11ed-b842-d8c49764f6bb
2023-05-21 05:40:48,863 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45345'. Reason: nanny-close
2023-05-21 05:40:48,864 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:48,865 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33685. Reason: nanny-close
2023-05-21 05:40:48,867 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38936; closing.
2023-05-21 05:40:48,867 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:48,867 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33685', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:48,868 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33685
2023-05-21 05:40:48,868 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:48,868 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:49,039 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43659', status: init, memory: 0, processing: 0>
2023-05-21 05:40:49,040 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43659
2023-05-21 05:40:49,040 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38952
2023-05-21 05:40:49,095 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38952; closing.
2023-05-21 05:40:49,095 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43659', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:49,095 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43659
2023-05-21 05:40:49,095 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:50,081 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:50,081 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:50,081 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:50,082 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:40:50,083 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35171 instead
  warnings.warn(
2023-05-21 05:41:00,535 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:00,535 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:00,535 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:00,535 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:00,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:00,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:00,583 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:00,583 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:00,589 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:00,589 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:00,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:00,605 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:00,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:00,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:00,670 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:00,670 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33311 instead
  warnings.warn(
2023-05-21 05:41:13,144 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:13,144 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:13,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:13,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:13,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:13,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:13,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:13,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:13,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:13,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:13,247 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:13,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:13,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:13,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:13,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:13,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43875 instead
  warnings.warn(
2023-05-21 05:41:23,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:23,622 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:23,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:23,622 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:23,624 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:23,624 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:23,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:23,667 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:23,667 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:23,667 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:23,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:23,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:23,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:23,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:23,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:23,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37575 instead
  warnings.warn(
2023-05-21 05:41:36,561 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:36,561 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:36,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:36,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:36,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:36,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:36,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:36,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:36,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:36,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:36,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:36,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:36,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:36,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:36,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:36,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34181 instead
  warnings.warn(
2023-05-21 05:41:49,044 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:49,044 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:49,093 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:49,093 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:49,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:49,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:49,117 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:49,117 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:49,119 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:49,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:49,163 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:49,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:49,275 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:49,275 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:49,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:49,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33021 instead
  warnings.warn(
2023-05-21 05:42:02,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:02,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:02,573 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:02,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:02,579 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:02,579 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:02,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:02,588 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:02,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:02,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:02,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:02,593 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:02,595 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:02,595 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:02,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:02,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41649 instead
  warnings.warn(
2023-05-21 05:42:15,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:15,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:15,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:15,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:15,971 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:15,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:15,976 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:15,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:16,022 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:16,022 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:16,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:16,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:16,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:16,038 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:16,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:16,093 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33971 instead
  warnings.warn(
2023-05-21 05:42:29,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:29,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:29,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:29,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:29,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:29,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:29,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:29,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:29,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:29,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:29,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:29,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:29,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:29,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:29,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:29,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40407 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45879 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43033 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38447 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] 2023-05-21 05:47:23,476 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 21, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 118, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 84, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 435, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-05-21 05:47:23,485 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fb593f6ef40>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 21, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 118, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 84, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 435, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 21, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 118, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 84, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 435, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-05-21 05:47:23,508 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 21, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 118, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 84, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 435, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-05-21 05:47:23,517 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f539b1d0fd0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 21, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 118, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 84, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 435, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 21, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 118, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 84, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 435, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-05-21 05:47:25,489 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-05-21 05:47:25,520 - distributed.nanny - ERROR - Worker process died unexpectedly
