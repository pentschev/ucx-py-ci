============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-10 06:26:51,252 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:51,257 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40843 instead
  warnings.warn(
2024-01-10 06:26:51,261 - distributed.scheduler - INFO - State start
2024-01-10 06:26:51,284 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:51,285 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-10 06:26:51,286 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40843/status
2024-01-10 06:26:51,286 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:26:51,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37889'
2024-01-10 06:26:51,467 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46585'
2024-01-10 06:26:51,472 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43527'
2024-01-10 06:26:51,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44723'
2024-01-10 06:26:53,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:53,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:53,317 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:53,318 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39049
2024-01-10 06:26:53,318 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39049
2024-01-10 06:26:53,318 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41639
2024-01-10 06:26:53,318 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:26:53,318 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:53,318 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:26:53,318 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:26:53,318 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6oc79k5s
2024-01-10 06:26:53,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2eaaca56-ebd8-4313-9e6b-abe192bf3c51
2024-01-10 06:26:53,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b24d5531-45b7-401b-8311-a20f38902aa0
2024-01-10 06:26:53,319 - distributed.worker - INFO - Starting Worker plugin PreImport-38c8a2dc-fb79-4f65-9ffa-d683e506945d
2024-01-10 06:26:53,319 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:53,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:53,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:53,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:53,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:53,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:53,335 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46241
2024-01-10 06:26:53,335 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46241
2024-01-10 06:26:53,335 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33371
2024-01-10 06:26:53,335 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:26:53,335 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:53,335 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:26:53,335 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:26:53,335 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-oqbiisid
2024-01-10 06:26:53,335 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97407198-c47d-4a5c-a098-edab175cfa63
2024-01-10 06:26:53,336 - distributed.worker - INFO - Starting Worker plugin PreImport-2814304b-c3dd-4e82-ab9c-46052cb71fa0
2024-01-10 06:26:53,336 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79c810c2-5b46-43aa-8ab7-5cb406d9da15
2024-01-10 06:26:53,337 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:53,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:53,340 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44791
2024-01-10 06:26:53,340 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44791
2024-01-10 06:26:53,340 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38025
2024-01-10 06:26:53,340 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:26:53,340 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:53,340 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:26:53,340 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:26:53,340 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-_bf8ua72
2024-01-10 06:26:53,340 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba248233-5c6a-4c5b-8cee-f01745b30873
2024-01-10 06:26:53,340 - distributed.worker - INFO - Starting Worker plugin PreImport-b53bb3a6-2c31-4839-814e-e09650df027c
2024-01-10 06:26:53,340 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80aa6888-2dc0-49cc-b85f-7d3bde0a81ce
2024-01-10 06:26:53,341 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:53,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:53,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:53,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:53,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42517
2024-01-10 06:26:53,349 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42517
2024-01-10 06:26:53,349 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39139
2024-01-10 06:26:53,349 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:26:53,349 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:53,349 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:26:53,349 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:26:53,349 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6ja_lmdw
2024-01-10 06:26:53,349 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-46d4c95a-5c4a-407b-adc0-d3a0731b8f6c
2024-01-10 06:26:53,350 - distributed.worker - INFO - Starting Worker plugin PreImport-f5c438ff-9d6d-49f1-81e7-1bf5217c6a7e
2024-01-10 06:26:53,350 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9c5126a-f430-489b-ac9e-48c3901fef28
2024-01-10 06:26:53,350 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,571 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42517', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,586 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42517
2024-01-10 06:26:54,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37118
2024-01-10 06:26:54,587 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,588 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:26:54,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,588 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44791', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,589 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44791
2024-01-10 06:26:54,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37112
2024-01-10 06:26:54,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:26:54,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,591 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:26:54,591 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:26:54,592 - distributed.scheduler - INFO - Receive client connection: Client-3c64ffbb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:26:54,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37130
2024-01-10 06:26:54,616 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46241', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,616 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46241
2024-01-10 06:26:54,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37106
2024-01-10 06:26:54,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,618 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:26:54,618 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,620 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:26:54,623 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39049', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,624 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39049
2024-01-10 06:26:54,624 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37090
2024-01-10 06:26:54,625 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,626 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:26:54,626 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:26:54,704 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:26:54,704 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:26:54,704 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:26:54,704 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:26:54,709 - distributed.scheduler - INFO - Remove client Client-3c64ffbb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:26:54,709 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37130; closing.
2024-01-10 06:26:54,710 - distributed.scheduler - INFO - Remove client Client-3c64ffbb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:26:54,710 - distributed.scheduler - INFO - Close client connection: Client-3c64ffbb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:26:54,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37889'. Reason: nanny-close
2024-01-10 06:26:54,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,712 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46585'. Reason: nanny-close
2024-01-10 06:26:54,712 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43527'. Reason: nanny-close
2024-01-10 06:26:54,713 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39049. Reason: nanny-close
2024-01-10 06:26:54,713 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,713 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44723'. Reason: nanny-close
2024-01-10 06:26:54,713 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44791. Reason: nanny-close
2024-01-10 06:26:54,714 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,714 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46241. Reason: nanny-close
2024-01-10 06:26:54,714 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42517. Reason: nanny-close
2024-01-10 06:26:54,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37090; closing.
2024-01-10 06:26:54,715 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:26:54,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39049', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.7153208')
2024-01-10 06:26:54,715 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:26:54,716 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:26:54,716 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:26:54,716 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,716 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,716 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37106; closing.
2024-01-10 06:26:54,717 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37118; closing.
2024-01-10 06:26:54,717 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37112; closing.
2024-01-10 06:26:54,717 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,717 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,717 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.71767')
2024-01-10 06:26:54,718 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.7179768')
2024-01-10 06:26:54,718 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44791', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.718344')
2024-01-10 06:26:54,718 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:26:55,527 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:26:55,527 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:26:55,528 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:26:55,529 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-10 06:26:55,529 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-10 06:26:57,813 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:57,818 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:26:57,821 - distributed.scheduler - INFO - State start
2024-01-10 06:26:57,843 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:57,844 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:26:57,845 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:26:57,845 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:26:57,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45725'
2024-01-10 06:26:58,000 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32961'
2024-01-10 06:26:58,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42383'
2024-01-10 06:26:58,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33463'
2024-01-10 06:26:58,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46281'
2024-01-10 06:26:58,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37533'
2024-01-10 06:26:58,050 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44437'
2024-01-10 06:26:58,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42223'
2024-01-10 06:26:58,101 - distributed.scheduler - INFO - Receive client connection: Client-404fca6f-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:26:58,122 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42484
2024-01-10 06:26:59,587 - distributed.scheduler - INFO - Receive client connection: Client-4070de9b-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:59,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42582
2024-01-10 06:26:59,870 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:59,871 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:59,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:59,874 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:59,875 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:59,876 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36367
2024-01-10 06:26:59,876 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36367
2024-01-10 06:26:59,876 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45671
2024-01-10 06:26:59,876 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:59,876 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:59,876 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:59,876 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:59,876 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_c9xx7n7
2024-01-10 06:26:59,876 - distributed.worker - INFO - Starting Worker plugin PreImport-44a06b9d-c72c-461a-b1da-2c448d8b8da3
2024-01-10 06:26:59,876 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-594a1a36-d07c-4f51-a324-30f9c6a8a482
2024-01-10 06:26:59,877 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd2d8fc1-f7e3-4197-ae27-f51d57752e2c
2024-01-10 06:26:59,878 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:59,879 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41639
2024-01-10 06:26:59,879 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41639
2024-01-10 06:26:59,879 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46613
2024-01-10 06:26:59,879 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:59,879 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:59,879 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:59,879 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:59,879 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-etc7m91a
2024-01-10 06:26:59,879 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b75c1ecc-8ba9-4b46-ac43-740e18940f12
2024-01-10 06:26:59,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:59,882 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:59,883 - distributed.worker - INFO - Starting Worker plugin PreImport-d920e468-743b-44cb-a159-900865f1ec8f
2024-01-10 06:26:59,884 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a2e6089-0ae9-43ba-980b-7e528c4b27fc
2024-01-10 06:26:59,886 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:59,887 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45081
2024-01-10 06:26:59,887 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45081
2024-01-10 06:26:59,887 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36113
2024-01-10 06:26:59,887 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:59,887 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:59,887 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:59,887 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:59,887 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hdxhpzb0
2024-01-10 06:26:59,887 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfe78c63-0d28-4b55-8381-5932639452bf
2024-01-10 06:26:59,953 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:59,953 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:59,954 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:59,954 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:59,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:59,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:59,959 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46237
2024-01-10 06:26:59,959 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41927
2024-01-10 06:26:59,959 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46237
2024-01-10 06:26:59,959 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41927
2024-01-10 06:26:59,959 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34109
2024-01-10 06:26:59,959 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44479
2024-01-10 06:26:59,959 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:59,959 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:59,959 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:59,959 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:59,959 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:59,959 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:59,959 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:59,959 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-thj1xqlh
2024-01-10 06:26:59,959 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:59,959 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-frot_uwe
2024-01-10 06:26:59,959 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a598524b-293d-4b13-8a13-3f126b13e5a3
2024-01-10 06:26:59,959 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73939b42-e343-4416-8342-39f10142405c
2024-01-10 06:26:59,959 - distributed.worker - INFO - Starting Worker plugin PreImport-9cf0c041-6c34-4eaf-bea6-754a531bed46
2024-01-10 06:26:59,959 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af443973-a4a3-4b19-8c3b-a7932e337f1c
2024-01-10 06:26:59,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:59,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:59,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:59,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:59,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:59,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:59,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:59,965 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:59,965 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33533
2024-01-10 06:26:59,965 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33533
2024-01-10 06:26:59,965 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33509
2024-01-10 06:26:59,965 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:59,965 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:59,965 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:59,966 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:59,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-26hrw591
2024-01-10 06:26:59,966 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39577
2024-01-10 06:26:59,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5fb6a8b5-2b76-473e-833a-e73d15ad81ce
2024-01-10 06:26:59,966 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39577
2024-01-10 06:26:59,966 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40959
2024-01-10 06:26:59,966 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:59,966 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:59,966 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:59,966 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:59,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ajs8tfbf
2024-01-10 06:26:59,966 - distributed.worker - INFO - Starting Worker plugin PreImport-8e2e25dc-873f-4ac2-908d-0738b2ae0dae
2024-01-10 06:26:59,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06eb7ca1-efd7-4ae1-a459-4a7bd6e35bab
2024-01-10 06:26:59,966 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfde5b6a-5945-44b4-bd8a-e2fb14ad4d80
2024-01-10 06:26:59,966 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:59,967 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44057
2024-01-10 06:26:59,967 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44057
2024-01-10 06:26:59,967 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39965
2024-01-10 06:26:59,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:59,967 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:59,967 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:59,967 - distributed.worker - INFO - Starting Worker plugin PreImport-a254ffb1-438a-4d70-8639-98c3a436cac6
2024-01-10 06:26:59,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:59,968 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6_szx064
2024-01-10 06:26:59,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-34c4b797-4565-4342-bb5c-2a2f6b30652f
2024-01-10 06:26:59,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bcda7ea3-8a37-4331-90fb-5abb6704f0c4
2024-01-10 06:26:59,968 - distributed.worker - INFO - Starting Worker plugin PreImport-f2a4251e-6ca1-4a02-a4b6-ececa7d58872
2024-01-10 06:26:59,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-43b5e303-8a3c-44a0-975c-987c1d7d5bd4
2024-01-10 06:27:03,518 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,551 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36367', status: init, memory: 0, processing: 0>
2024-01-10 06:27:03,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36367
2024-01-10 06:27:03,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52648
2024-01-10 06:27:03,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:03,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:03,556 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:03,661 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41639', status: init, memory: 0, processing: 0>
2024-01-10 06:27:03,695 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41639
2024-01-10 06:27:03,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52662
2024-01-10 06:27:03,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:03,698 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:03,698 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:03,712 - distributed.worker - INFO - Starting Worker plugin PreImport-defd4fc4-3f06-44be-8c89-7c59d3575a96
2024-01-10 06:27:03,713 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02a87342-128d-4e52-8930-b9b3892b59a1
2024-01-10 06:27:03,713 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,736 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45081', status: init, memory: 0, processing: 0>
2024-01-10 06:27:03,737 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45081
2024-01-10 06:27:03,737 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52664
2024-01-10 06:27:03,738 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:03,739 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:03,739 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:03,852 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,885 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39577', status: init, memory: 0, processing: 0>
2024-01-10 06:27:03,886 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39577
2024-01-10 06:27:03,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52678
2024-01-10 06:27:03,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:03,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:03,889 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:03,944 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,959 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,959 - distributed.worker - INFO - Starting Worker plugin PreImport-b372b1ae-d095-4c19-9764-c264256f9619
2024-01-10 06:27:03,960 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eeefe466-7236-4e15-88ea-9e2c67d2a942
2024-01-10 06:27:03,960 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,967 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44057', status: init, memory: 0, processing: 0>
2024-01-10 06:27:03,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44057
2024-01-10 06:27:03,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52694
2024-01-10 06:27:03,968 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:03,969 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:03,970 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,972 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:03,982 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46237', status: init, memory: 0, processing: 0>
2024-01-10 06:27:03,983 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46237
2024-01-10 06:27:03,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52710
2024-01-10 06:27:03,983 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41927', status: init, memory: 0, processing: 0>
2024-01-10 06:27:03,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:03,984 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41927
2024-01-10 06:27:03,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52720
2024-01-10 06:27:03,984 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:03,985 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:03,986 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:03,986 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:03,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:03,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,092 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,128 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33533', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,129 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33533
2024-01-10 06:27:04,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52728
2024-01-10 06:27:04,131 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,132 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,133 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,160 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:04,160 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:04,160 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:04,160 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:04,160 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:04,161 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:04,161 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:04,161 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:04,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:04,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:04,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:04,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:04,164 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:04,165 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:04,165 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:04,165 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:04,170 - distributed.scheduler - INFO - Remove client Client-4070de9b-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:04,170 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42582; closing.
2024-01-10 06:27:04,171 - distributed.scheduler - INFO - Remove client Client-4070de9b-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:04,171 - distributed.scheduler - INFO - Close client connection: Client-4070de9b-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:04,172 - distributed.scheduler - INFO - Remove client Client-404fca6f-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:04,172 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42484; closing.
2024-01-10 06:27:04,172 - distributed.scheduler - INFO - Remove client Client-404fca6f-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:04,172 - distributed.scheduler - INFO - Close client connection: Client-404fca6f-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:04,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45725'. Reason: nanny-close
2024-01-10 06:27:04,174 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,174 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32961'. Reason: nanny-close
2024-01-10 06:27:04,174 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,175 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42383'. Reason: nanny-close
2024-01-10 06:27:04,175 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36367. Reason: nanny-close
2024-01-10 06:27:04,175 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,175 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33463'. Reason: nanny-close
2024-01-10 06:27:04,175 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,175 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41639. Reason: nanny-close
2024-01-10 06:27:04,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46281'. Reason: nanny-close
2024-01-10 06:27:04,176 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45081. Reason: nanny-close
2024-01-10 06:27:04,176 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37533'. Reason: nanny-close
2024-01-10 06:27:04,176 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44057. Reason: nanny-close
2024-01-10 06:27:04,176 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,176 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44437'. Reason: nanny-close
2024-01-10 06:27:04,177 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39577. Reason: nanny-close
2024-01-10 06:27:04,177 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,177 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42223'. Reason: nanny-close
2024-01-10 06:27:04,177 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,177 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33533. Reason: nanny-close
2024-01-10 06:27:04,177 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,178 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41927. Reason: nanny-close
2024-01-10 06:27:04,178 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,178 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46237. Reason: nanny-close
2024-01-10 06:27:04,178 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,178 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52648; closing.
2024-01-10 06:27:04,178 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,179 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52694; closing.
2024-01-10 06:27:04,179 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,179 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36367', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.179427')
2024-01-10 06:27:04,179 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,179 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,179 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,180 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,180 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,180 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,181 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44057', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.1810186')
2024-01-10 06:27:04,181 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,181 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,181 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,181 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52678; closing.
2024-01-10 06:27:04,182 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52664; closing.
2024-01-10 06:27:04,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52662; closing.
2024-01-10 06:27:04,184 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39577', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.1841228')
2024-01-10 06:27:04,184 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45081', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.1846592')
2024-01-10 06:27:04,185 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52728; closing.
2024-01-10 06:27:04,185 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41639', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.1854475')
2024-01-10 06:27:04,186 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52720; closing.
2024-01-10 06:27:04,186 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52678>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52678>: Stream is closed
2024-01-10 06:27:04,189 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33533', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.189191')
2024-01-10 06:27:04,189 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.1898103')
2024-01-10 06:27:04,190 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52710; closing.
2024-01-10 06:27:04,190 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46237', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.1908755')
2024-01-10 06:27:04,191 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:04,200 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38703', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,200 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38703
2024-01-10 06:27:04,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52742
2024-01-10 06:27:04,227 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52742; closing.
2024-01-10 06:27:04,228 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38703', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.228088')
2024-01-10 06:27:04,228 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:04,248 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41025', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,249 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41025
2024-01-10 06:27:04,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52752
2024-01-10 06:27:04,261 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37265', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37265
2024-01-10 06:27:04,262 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52776
2024-01-10 06:27:04,268 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44121', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,269 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44121
2024-01-10 06:27:04,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52768
2024-01-10 06:27:04,270 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46091', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,271 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46091
2024-01-10 06:27:04,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52788
2024-01-10 06:27:04,282 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52776; closing.
2024-01-10 06:27:04,282 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37265', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.2827735')
2024-01-10 06:27:04,284 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52752; closing.
2024-01-10 06:27:04,285 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41025', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.285133')
2024-01-10 06:27:04,286 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34683', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,287 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34683
2024-01-10 06:27:04,287 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52798
2024-01-10 06:27:04,288 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40261', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,289 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40261
2024-01-10 06:27:04,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52814
2024-01-10 06:27:04,292 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33105', status: init, memory: 0, processing: 0>
2024-01-10 06:27:04,293 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33105
2024-01-10 06:27:04,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52810
2024-01-10 06:27:04,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52788; closing.
2024-01-10 06:27:04,330 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52810; closing.
2024-01-10 06:27:04,330 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46091', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.3305259')
2024-01-10 06:27:04,331 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52814; closing.
2024-01-10 06:27:04,331 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33105', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.3314805')
2024-01-10 06:27:04,332 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40261', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.3322377')
2024-01-10 06:27:04,332 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52768; closing.
2024-01-10 06:27:04,333 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.333156')
2024-01-10 06:27:04,333 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52814>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:27:04,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52798; closing.
2024-01-10 06:27:04,334 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34683', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868024.334362')
2024-01-10 06:27:04,334 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:05,240 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:27:05,240 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:27:05,241 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:05,242 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:27:05,242 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-10 06:27:07,442 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:07,446 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:27:07,450 - distributed.scheduler - INFO - State start
2024-01-10 06:27:07,472 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:07,473 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:27:07,474 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:27:07,474 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:27:07,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42537'
2024-01-10 06:27:07,706 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39763'
2024-01-10 06:27:07,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40423'
2024-01-10 06:27:07,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35671'
2024-01-10 06:27:07,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37969'
2024-01-10 06:27:07,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45803'
2024-01-10 06:27:07,752 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46861'
2024-01-10 06:27:07,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45077'
2024-01-10 06:27:08,910 - distributed.scheduler - INFO - Receive client connection: Client-461287fb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:08,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53180
2024-01-10 06:27:09,091 - distributed.scheduler - INFO - Receive client connection: Client-463b8aa6-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:09,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53202
2024-01-10 06:27:09,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:09,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:09,550 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:09,550 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:09,552 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:09,553 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41725
2024-01-10 06:27:09,553 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41725
2024-01-10 06:27:09,553 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38121
2024-01-10 06:27:09,553 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:09,553 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:09,553 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:09,553 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:09,553 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-593p0al9
2024-01-10 06:27:09,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7379cac-ca0c-457f-8741-cad91f7e5f9a
2024-01-10 06:27:09,553 - distributed.worker - INFO - Starting Worker plugin PreImport-d32b162d-81ba-4d47-b1db-2006f11ab03b
2024-01-10 06:27:09,554 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc7c6808-72bf-4843-9b78-abd8343774df
2024-01-10 06:27:09,555 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:09,556 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46745
2024-01-10 06:27:09,556 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46745
2024-01-10 06:27:09,556 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33307
2024-01-10 06:27:09,556 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:09,556 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:09,556 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:09,556 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:09,556 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2la7n1vn
2024-01-10 06:27:09,556 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b50c1d0-ba65-4391-a7e4-b81a4e9bae95
2024-01-10 06:27:09,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:09,622 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:09,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:09,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44267
2024-01-10 06:27:09,628 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44267
2024-01-10 06:27:09,628 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36627
2024-01-10 06:27:09,628 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:09,628 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:09,628 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:09,628 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:09,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hz4p5v1f
2024-01-10 06:27:09,628 - distributed.worker - INFO - Starting Worker plugin PreImport-c75b0e10-e552-47f9-8d4d-561e310e45a8
2024-01-10 06:27:09,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ddbaca2-d139-4240-ba3a-dd7b7c1fc849
2024-01-10 06:27:09,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8ce977b-8405-4e16-a77e-1927ff028a12
2024-01-10 06:27:09,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:09,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:09,822 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:09,823 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41513
2024-01-10 06:27:09,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41513
2024-01-10 06:27:09,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37587
2024-01-10 06:27:09,823 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:09,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:09,823 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:09,824 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:09,824 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6twj1kha
2024-01-10 06:27:09,824 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9033b4c-c3a9-47e0-904b-7e5b3a4022f8
2024-01-10 06:27:09,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:09,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:09,996 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:09,997 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36929
2024-01-10 06:27:09,997 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36929
2024-01-10 06:27:09,997 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33931
2024-01-10 06:27:09,997 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:09,997 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:09,997 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:09,997 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:09,997 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dkm8cnog
2024-01-10 06:27:09,997 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd9fb186-dbf4-44a5-8665-a96f412930d5
2024-01-10 06:27:09,998 - distributed.worker - INFO - Starting Worker plugin PreImport-d4b6c307-bd23-4b52-b927-1b1be77f1e61
2024-01-10 06:27:09,998 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a39136f7-828c-48a8-b9c3-053861e0e6f1
2024-01-10 06:27:10,016 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,016 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,021 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,022 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39387
2024-01-10 06:27:10,022 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39387
2024-01-10 06:27:10,022 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36309
2024-01-10 06:27:10,022 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,022 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,022 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,022 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,022 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mto4hktc
2024-01-10 06:27:10,022 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b683a74b-a384-4a73-bab4-ddac124b6b85
2024-01-10 06:27:10,022 - distributed.worker - INFO - Starting Worker plugin PreImport-d0e4efd9-2ddb-403e-9e7b-4bb25f4703b7
2024-01-10 06:27:10,022 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a037b966-609c-4801-bfc2-1bbf79c0570c
2024-01-10 06:27:10,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,037 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,038 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43295
2024-01-10 06:27:10,038 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43295
2024-01-10 06:27:10,038 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43407
2024-01-10 06:27:10,038 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,038 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,038 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,039 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,039 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jvutnujn
2024-01-10 06:27:10,039 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89e44895-ae31-49d8-9d16-96324ee35914
2024-01-10 06:27:10,039 - distributed.worker - INFO - Starting Worker plugin PreImport-a9d323cb-de99-49ba-bdd7-0c7af4354720
2024-01-10 06:27:10,039 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f19aa375-3c1e-47e2-b6b1-b81077526301
2024-01-10 06:27:10,040 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41541
2024-01-10 06:27:10,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41541
2024-01-10 06:27:10,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33667
2024-01-10 06:27:10,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,042 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,042 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,042 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,042 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uqoq4vcr
2024-01-10 06:27:10,042 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ef614ef-ba2f-4dff-99e6-5eef8d87a063
2024-01-10 06:27:10,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8786e644-3876-467b-bb4f-80aff085f395
2024-01-10 06:27:10,329 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,355 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41725', status: init, memory: 0, processing: 0>
2024-01-10 06:27:10,356 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41725
2024-01-10 06:27:10,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51652
2024-01-10 06:27:10,357 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:10,357 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,357 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:10,387 - distributed.worker - INFO - Starting Worker plugin PreImport-b8b43d7c-de62-4e9b-9522-edc55d390b85
2024-01-10 06:27:10,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09771b62-cd80-4d6a-ad71-9cca2cf0e9f5
2024-01-10 06:27:10,392 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,419 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46745', status: init, memory: 0, processing: 0>
2024-01-10 06:27:10,419 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46745
2024-01-10 06:27:10,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51658
2024-01-10 06:27:10,420 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:10,421 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,421 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,353 - distributed.worker - INFO - Starting Worker plugin PreImport-754b8615-0975-4861-b5cf-c460a8ec2cf5
2024-01-10 06:27:13,354 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4287bdc8-62c7-4990-9c41-72e7dcb6349d
2024-01-10 06:27:13,356 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,377 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,391 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41513', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,392 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41513
2024-01-10 06:27:13,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51676
2024-01-10 06:27:13,394 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,395 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,395 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,397 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,401 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44267', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,402 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44267
2024-01-10 06:27:13,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51680
2024-01-10 06:27:13,403 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,404 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,404 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,405 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,407 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,430 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36929', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,431 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36929
2024-01-10 06:27:13,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51696
2024-01-10 06:27:13,432 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,432 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,432 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,522 - distributed.worker - INFO - Starting Worker plugin PreImport-9137bd85-31fd-4a63-b4bb-7e32c7fb062d
2024-01-10 06:27:13,525 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,555 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41541', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41541
2024-01-10 06:27:13,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51700
2024-01-10 06:27:13,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,558 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,573 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,577 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35131', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,577 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35131
2024-01-10 06:27:13,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51716
2024-01-10 06:27:13,594 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39387', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,595 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39387
2024-01-10 06:27:13,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51724
2024-01-10 06:27:13,596 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,597 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,597 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,639 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35677', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,640 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35677
2024-01-10 06:27:13,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51732
2024-01-10 06:27:13,666 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32807', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,667 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32807
2024-01-10 06:27:13,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51736
2024-01-10 06:27:13,670 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38185', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,671 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38185
2024-01-10 06:27:13,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51748
2024-01-10 06:27:13,672 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,699 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40143', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,699 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40143
2024-01-10 06:27:13,699 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51764
2024-01-10 06:27:13,704 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42865', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,705 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42865
2024-01-10 06:27:13,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51776
2024-01-10 06:27:13,705 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43295', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,706 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43295
2024-01-10 06:27:13,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51766
2024-01-10 06:27:13,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,709 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,709 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,720 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37281', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,720 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37281
2024-01-10 06:27:13,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51790
2024-01-10 06:27:13,727 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35605', status: init, memory: 0, processing: 0>
2024-01-10 06:27:13,727 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35605
2024-01-10 06:27:13,727 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51792
2024-01-10 06:27:25,015 - distributed.scheduler - INFO - Remove client Client-461287fb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:25,016 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53180; closing.
2024-01-10 06:27:25,016 - distributed.scheduler - INFO - Remove client Client-461287fb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:25,017 - distributed.scheduler - INFO - Close client connection: Client-461287fb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:25,018 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42537'. Reason: nanny-close
2024-01-10 06:27:25,018 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,019 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39763'. Reason: nanny-close
2024-01-10 06:27:25,019 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,019 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40423'. Reason: nanny-close
2024-01-10 06:27:25,019 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44267. Reason: nanny-close
2024-01-10 06:27:25,020 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,020 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35671'. Reason: nanny-close
2024-01-10 06:27:25,020 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,020 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37969'. Reason: nanny-close
2024-01-10 06:27:25,020 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46745. Reason: nanny-close
2024-01-10 06:27:25,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45803'. Reason: nanny-close
2024-01-10 06:27:25,021 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43295. Reason: nanny-close
2024-01-10 06:27:25,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,021 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41513. Reason: nanny-close
2024-01-10 06:27:25,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46861'. Reason: nanny-close
2024-01-10 06:27:25,021 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39387. Reason: nanny-close
2024-01-10 06:27:25,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45077'. Reason: nanny-close
2024-01-10 06:27:25,022 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,022 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51680; closing.
2024-01-10 06:27:25,022 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,022 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36929. Reason: nanny-close
2024-01-10 06:27:25,022 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44267', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.0225437')
2024-01-10 06:27:25,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,023 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41541. Reason: nanny-close
2024-01-10 06:27:25,023 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,023 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41725. Reason: nanny-close
2024-01-10 06:27:25,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,024 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,024 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,024 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51766; closing.
2024-01-10 06:27:25,025 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,025 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,026 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,026 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,026 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,026 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43295', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.0269122')
2024-01-10 06:27:25,027 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51658; closing.
2024-01-10 06:27:25,028 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,028 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,029 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51766>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51766>: Stream is closed
2024-01-10 06:27:25,033 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51676; closing.
2024-01-10 06:27:25,033 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51724; closing.
2024-01-10 06:27:25,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51696; closing.
2024-01-10 06:27:25,034 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46745', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.0342393')
2024-01-10 06:27:25,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.0352232')
2024-01-10 06:27:25,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39387', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.0356383')
2024-01-10 06:27:25,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.0360398')
2024-01-10 06:27:25,036 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51700; closing.
2024-01-10 06:27:25,036 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51652; closing.
2024-01-10 06:27:25,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41541', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.037256')
2024-01-10 06:27:25,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41725', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.0376537')
2024-01-10 06:27:25,132 - distributed.scheduler - INFO - Remove client Client-463b8aa6-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:25,132 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53202; closing.
2024-01-10 06:27:25,132 - distributed.scheduler - INFO - Remove client Client-463b8aa6-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:25,133 - distributed.scheduler - INFO - Close client connection: Client-463b8aa6-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:25,139 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51732; closing.
2024-01-10 06:27:25,139 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35677', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.1396296')
2024-01-10 06:27:25,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51736; closing.
2024-01-10 06:27:25,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51792; closing.
2024-01-10 06:27:25,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51776; closing.
2024-01-10 06:27:25,143 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.1436315')
2024-01-10 06:27:25,144 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35605', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.1440082')
2024-01-10 06:27:25,144 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51764; closing.
2024-01-10 06:27:25,144 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42865', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.1445732')
2024-01-10 06:27:25,145 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40143', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.1456213')
2024-01-10 06:27:25,146 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51748; closing.
2024-01-10 06:27:25,146 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51716; closing.
2024-01-10 06:27:25,146 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38185', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.1467848')
2024-01-10 06:27:25,147 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.1471727')
2024-01-10 06:27:25,147 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51790; closing.
2024-01-10 06:27:25,147 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37281', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868045.147935')
2024-01-10 06:27:25,148 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:26,034 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:27:26,034 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:27:26,035 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:26,037 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:27:26,038 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-10 06:27:28,552 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:28,557 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40105 instead
  warnings.warn(
2024-01-10 06:27:28,561 - distributed.scheduler - INFO - State start
2024-01-10 06:27:28,584 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:28,585 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-10 06:27:28,585 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:28,586 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-10 06:27:28,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46679'
2024-01-10 06:27:28,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43759'
2024-01-10 06:27:28,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36059'
2024-01-10 06:27:28,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46089'
2024-01-10 06:27:28,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45719'
2024-01-10 06:27:28,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45381'
2024-01-10 06:27:28,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40231'
2024-01-10 06:27:28,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42243'
2024-01-10 06:27:30,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,759 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,764 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42049
2024-01-10 06:27:30,764 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42049
2024-01-10 06:27:30,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44871
2024-01-10 06:27:30,764 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,764 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,764 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,764 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:30,764 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36943
2024-01-10 06:27:30,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vio1qfqb
2024-01-10 06:27:30,764 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36943
2024-01-10 06:27:30,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44631
2024-01-10 06:27:30,764 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,764 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-932ba303-a2cb-4bdf-824f-00bfa3f5110b
2024-01-10 06:27:30,764 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:30,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kaj67hhd
2024-01-10 06:27:30,765 - distributed.worker - INFO - Starting Worker plugin PreImport-f47c5ed2-3dc9-4cf5-a4ba-2da56b667748
2024-01-10 06:27:30,765 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9b68651-0e60-459b-9ee1-cd042cc70d32
2024-01-10 06:27:30,766 - distributed.worker - INFO - Starting Worker plugin PreImport-3b0a0353-0aed-41f7-a860-15ff26f7c783
2024-01-10 06:27:30,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-317edbb8-7ee1-47c2-922f-8d21f4ec05f4
2024-01-10 06:27:30,767 - distributed.worker - INFO - Starting Worker plugin RMMSetup-195d3b5c-120e-45be-8a31-3147b7895d94
2024-01-10 06:27:30,775 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,775 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,779 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,780 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34607
2024-01-10 06:27:30,780 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34607
2024-01-10 06:27:30,780 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36685
2024-01-10 06:27:30,780 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,780 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,780 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:30,780 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_3htaagd
2024-01-10 06:27:30,781 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ace1e0a-e1d3-4f09-a042-b6d820a71ab6
2024-01-10 06:27:30,783 - distributed.worker - INFO - Starting Worker plugin PreImport-46a8b3a8-abdc-4631-af82-966d98d03049
2024-01-10 06:27:30,784 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b607e74d-66df-4d93-b92d-a3293168f81f
2024-01-10 06:27:30,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,803 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,807 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33971
2024-01-10 06:27:30,807 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33971
2024-01-10 06:27:30,807 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44751
2024-01-10 06:27:30,807 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,807 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,807 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,808 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:30,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bu0o2dep
2024-01-10 06:27:30,808 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc8019b7-414b-422b-98a9-16a4a3451bea
2024-01-10 06:27:30,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,809 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46669
2024-01-10 06:27:30,809 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46669
2024-01-10 06:27:30,809 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40989
2024-01-10 06:27:30,809 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,809 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,809 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,809 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:30,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qu6ihfuf
2024-01-10 06:27:30,809 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bca6b914-145a-4238-a1d6-923a84524454
2024-01-10 06:27:30,810 - distributed.worker - INFO - Starting Worker plugin PreImport-279918ca-c1fd-45c9-be97-6a42836b3577
2024-01-10 06:27:30,810 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50b37c48-32e0-4bd9-b4ed-39c831c60e05
2024-01-10 06:27:30,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,822 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,823 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38651
2024-01-10 06:27:30,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38651
2024-01-10 06:27:30,824 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39015
2024-01-10 06:27:30,824 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,824 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,824 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,824 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:30,824 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uxpeqmdj
2024-01-10 06:27:30,824 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6bbba8b3-2e5d-4bac-b8d0-b13000bed43f
2024-01-10 06:27:30,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,832 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33293
2024-01-10 06:27:30,833 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33293
2024-01-10 06:27:30,833 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36243
2024-01-10 06:27:30,833 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,833 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,833 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,833 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:30,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-omnqs12v
2024-01-10 06:27:30,833 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54ff0ef9-6b29-419c-ab4b-cb9b6e21b197
2024-01-10 06:27:30,833 - distributed.worker - INFO - Starting Worker plugin PreImport-57893789-4c8d-49fd-9baa-7357c61dbf11
2024-01-10 06:27:30,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5c73427-a815-4b61-9aba-f919f0c9cd75
2024-01-10 06:27:30,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,891 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,896 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32809
2024-01-10 06:27:30,897 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32809
2024-01-10 06:27:30,897 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39341
2024-01-10 06:27:30,897 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,897 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,897 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,897 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:30,897 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-46hrzmnl
2024-01-10 06:27:30,897 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fad6972f-a316-4890-b22f-66b7e434dd59
2024-01-10 06:27:30,897 - distributed.worker - INFO - Starting Worker plugin PreImport-8e5c0959-5d5e-484b-87f0-4d5a54411a5a
2024-01-10 06:27:30,898 - distributed.worker - INFO - Starting Worker plugin RMMSetup-585a840d-73eb-4e84-aa36-7d82435ff4d5
2024-01-10 06:27:32,945 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:32,949 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:32,982 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:32,985 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:32,987 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:32,989 - distributed.worker - INFO - Starting Worker plugin PreImport-adc8d899-a846-445d-a6d1-38da69f96e4c
2024-01-10 06:27:32,990 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea88d70d-40ec-4577-b800-be81d3924dcc
2024-01-10 06:27:32,991 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:32,999 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:33,011 - distributed.worker - INFO - Starting Worker plugin PreImport-1ac549c1-a782-416a-8210-b9278d613214
2024-01-10 06:27:33,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b532b288-74bc-4c1f-88ff-bde1c5a2378d
2024-01-10 06:27:33,014 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:33,018 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:33,044 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:37,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:37,540 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:37,540 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:37,542 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:37,565 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:37,590 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36059'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:37,591 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:37,592 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38651. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:37,594 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:37,596 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:60900 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-10 06:27:37,973 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65916 parent=65719 started daemon>
2024-01-10 06:27:37,973 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65912 parent=65719 started daemon>
2024-01-10 06:27:37,974 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65908 parent=65719 started daemon>
2024-01-10 06:27:37,974 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65904 parent=65719 started daemon>
2024-01-10 06:27:37,974 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65899 parent=65719 started daemon>
2024-01-10 06:27:37,974 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65889 parent=65719 started daemon>
2024-01-10 06:27:37,974 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65885 parent=65719 started daemon>
2024-01-10 06:27:38,255 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 65904 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-10 06:27:46,542 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:46,546 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:27:46,550 - distributed.scheduler - INFO - State start
2024-01-10 06:27:46,551 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_3htaagd', purging
2024-01-10 06:27:46,552 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-bu0o2dep', purging
2024-01-10 06:27:46,552 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-46hrzmnl', purging
2024-01-10 06:27:46,552 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-omnqs12v', purging
2024-01-10 06:27:46,553 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-kaj67hhd', purging
2024-01-10 06:27:46,553 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qu6ihfuf', purging
2024-01-10 06:27:46,554 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vio1qfqb', purging
2024-01-10 06:27:46,575 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:46,576 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:27:46,577 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:27:46,577 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:27:46,612 - distributed.scheduler - INFO - Receive client connection: Client-5d65a9b4-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:46,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40484
2024-01-10 06:27:46,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39775'
2024-01-10 06:27:46,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33519'
2024-01-10 06:27:46,861 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37883'
2024-01-10 06:27:46,871 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44381'
2024-01-10 06:27:46,879 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45049'
2024-01-10 06:27:46,889 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46567'
2024-01-10 06:27:46,900 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35239'
2024-01-10 06:27:46,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46281'
2024-01-10 06:27:48,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,769 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,772 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,772 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,773 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,773 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38197
2024-01-10 06:27:48,773 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38197
2024-01-10 06:27:48,773 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39743
2024-01-10 06:27:48,773 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:48,773 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,773 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,773 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:48,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kprdw0_q
2024-01-10 06:27:48,773 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42143
2024-01-10 06:27:48,773 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42143
2024-01-10 06:27:48,773 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37135
2024-01-10 06:27:48,774 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:48,774 - distributed.worker - INFO - Starting Worker plugin PreImport-c0ab1f4a-0b2b-4d1e-86bd-22996370a326
2024-01-10 06:27:48,774 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,774 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,774 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b78953c7-2b5a-4e87-a9ec-6a64ecd155a8
2024-01-10 06:27:48,774 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:48,774 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33733
2024-01-10 06:27:48,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oejf0umg
2024-01-10 06:27:48,774 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33733
2024-01-10 06:27:48,774 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46357
2024-01-10 06:27:48,774 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:48,774 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-420bf472-5169-4eda-a17c-18c7641256b8
2024-01-10 06:27:48,774 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,774 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,774 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:48,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r4frtnzs
2024-01-10 06:27:48,774 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8de41cb4-599b-4c8c-a15a-2e9b9232470a
2024-01-10 06:27:48,774 - distributed.worker - INFO - Starting Worker plugin PreImport-55af84e8-6038-455e-b4b9-8bbff1aca7e6
2024-01-10 06:27:48,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9e466cb-f078-4c68-8612-2d4ca6c887e3
2024-01-10 06:27:48,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5bce693b-6e54-4def-8993-24faab002fe5
2024-01-10 06:27:48,775 - distributed.worker - INFO - Starting Worker plugin PreImport-8c78ca8b-a696-48c7-8db1-af88d8f860fe
2024-01-10 06:27:48,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2aba2c8-1786-4a0a-89f6-3f959c6c86ec
2024-01-10 06:27:48,817 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,817 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,819 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,819 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,822 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37523
2024-01-10 06:27:48,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37523
2024-01-10 06:27:48,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41269
2024-01-10 06:27:48,823 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:48,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,823 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,823 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:48,823 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vwf1yon0
2024-01-10 06:27:48,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57e9b566-c976-4af2-ae52-b8d50f6590cb
2024-01-10 06:27:48,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,825 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,825 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46051
2024-01-10 06:27:48,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35583
2024-01-10 06:27:48,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46051
2024-01-10 06:27:48,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35583
2024-01-10 06:27:48,825 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40629
2024-01-10 06:27:48,825 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38539
2024-01-10 06:27:48,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:48,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:48,825 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,825 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,825 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,825 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,825 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:48,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mxpdw3sg
2024-01-10 06:27:48,825 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:48,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-utrqzr2a
2024-01-10 06:27:48,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16efc646-c04e-4948-b0e9-043a1699f069
2024-01-10 06:27:48,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63b7da25-2f5b-461f-a889-72130f7f6e9a
2024-01-10 06:27:48,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33697
2024-01-10 06:27:48,825 - distributed.worker - INFO - Starting Worker plugin PreImport-a49b1c39-021e-439c-bfa4-3a2d80b00479
2024-01-10 06:27:48,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33697
2024-01-10 06:27:48,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42753
2024-01-10 06:27:48,825 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b321140-1886-4281-bd0f-60aec36ba383
2024-01-10 06:27:48,825 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44691
2024-01-10 06:27:48,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42753
2024-01-10 06:27:48,826 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:48,826 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33351
2024-01-10 06:27:48,826 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,826 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:48,826 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,826 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,826 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:48,826 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ybilm9gx
2024-01-10 06:27:48,826 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:48,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t1ujv90p
2024-01-10 06:27:48,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa802423-7c2f-4b7a-b312-711e2b5354b3
2024-01-10 06:27:48,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0636c93d-320b-4aa5-afab-8a9391ef6f54
2024-01-10 06:27:48,827 - distributed.worker - INFO - Starting Worker plugin PreImport-3e78fdde-fb75-44d0-9653-1a111d14dd0a
2024-01-10 06:27:48,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a2a91cc-8c09-433a-a581-0ef63be4ad0e
2024-01-10 06:27:48,827 - distributed.worker - INFO - Starting Worker plugin PreImport-d309a0d9-ad59-40bc-9d7b-f514663d8f5d
2024-01-10 06:27:48,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba00ca32-e349-4f2e-81cf-2d399f7da594
2024-01-10 06:27:50,805 - distributed.scheduler - INFO - Receive client connection: Client-60fe3e38-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:50,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45944
2024-01-10 06:27:51,002 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,003 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,031 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,037 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38197', status: init, memory: 0, processing: 0>
2024-01-10 06:27:51,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38197
2024-01-10 06:27:51,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45964
2024-01-10 06:27:51,040 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42143', status: init, memory: 0, processing: 0>
2024-01-10 06:27:51,040 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:51,041 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42143
2024-01-10 06:27:51,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45978
2024-01-10 06:27:51,042 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:51,042 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,043 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:51,044 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:51,044 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:51,046 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:51,049 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,056 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33733', status: init, memory: 0, processing: 0>
2024-01-10 06:27:51,057 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33733
2024-01-10 06:27:51,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45986
2024-01-10 06:27:51,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:51,059 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:51,059 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:51,061 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,072 - distributed.worker - INFO - Starting Worker plugin PreImport-44db3e59-d5d8-428d-a6cf-b49e1cfb6eeb
2024-01-10 06:27:51,072 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-396a5d58-9d71-45af-bf48-ccb3b5f98a61
2024-01-10 06:27:51,073 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,073 - distributed.worker - INFO - Starting Worker plugin PreImport-ba82b8c2-162f-47c5-9039-33bd3ed9b675
2024-01-10 06:27:51,074 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9c1d7bf-9a24-4383-aecf-784b40cc1eef
2024-01-10 06:27:51,074 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46051', status: init, memory: 0, processing: 0>
2024-01-10 06:27:51,074 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,075 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46051
2024-01-10 06:27:51,075 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46000
2024-01-10 06:27:51,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:51,077 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:51,077 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:51,095 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42753', status: init, memory: 0, processing: 0>
2024-01-10 06:27:51,096 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42753
2024-01-10 06:27:51,096 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46014
2024-01-10 06:27:51,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:51,099 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:51,099 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,100 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33697', status: init, memory: 0, processing: 0>
2024-01-10 06:27:51,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:51,101 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33697
2024-01-10 06:27:51,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46044
2024-01-10 06:27:51,102 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:51,102 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37523', status: init, memory: 0, processing: 0>
2024-01-10 06:27:51,103 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:51,103 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,103 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37523
2024-01-10 06:27:51,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46034
2024-01-10 06:27:51,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:51,104 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:51,105 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35583', status: init, memory: 0, processing: 0>
2024-01-10 06:27:51,105 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:51,105 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,105 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35583
2024-01-10 06:27:51,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46020
2024-01-10 06:27:51,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:51,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:51,108 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:51,108 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:51,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:51,127 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:51,127 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:51,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:51,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:51,129 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:51,129 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:51,129 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:51,133 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:51,137 - distributed.scheduler - INFO - Remove client Client-60fe3e38-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:51,138 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45944; closing.
2024-01-10 06:27:51,138 - distributed.scheduler - INFO - Remove client Client-60fe3e38-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:51,138 - distributed.scheduler - INFO - Close client connection: Client-60fe3e38-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:51,194 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,196 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,196 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,208 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:51,208 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:51,208 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:51,209 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:51,209 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:51,209 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:51,209 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:51,213 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:51,223 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,225 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:51,228 - distributed.scheduler - INFO - Remove client Client-5d65a9b4-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:51,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40484; closing.
2024-01-10 06:27:51,228 - distributed.scheduler - INFO - Remove client Client-5d65a9b4-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:51,229 - distributed.scheduler - INFO - Close client connection: Client-5d65a9b4-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:51,230 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33519'. Reason: nanny-close
2024-01-10 06:27:51,230 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:51,230 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37883'. Reason: nanny-close
2024-01-10 06:27:51,231 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:51,231 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44381'. Reason: nanny-close
2024-01-10 06:27:51,231 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:51,231 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38197. Reason: nanny-close
2024-01-10 06:27:51,232 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45049'. Reason: nanny-close
2024-01-10 06:27:51,232 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:51,232 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33697. Reason: nanny-close
2024-01-10 06:27:51,232 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46567'. Reason: nanny-close
2024-01-10 06:27:51,232 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42143. Reason: nanny-close
2024-01-10 06:27:51,233 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:51,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35239'. Reason: nanny-close
2024-01-10 06:27:51,233 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46051. Reason: nanny-close
2024-01-10 06:27:51,233 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:51,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46281'. Reason: nanny-close
2024-01-10 06:27:51,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:51,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42753. Reason: nanny-close
2024-01-10 06:27:51,234 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39775'. Reason: nanny-close
2024-01-10 06:27:51,234 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46044; closing.
2024-01-10 06:27:51,234 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:51,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:51,234 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868071.2346094')
2024-01-10 06:27:51,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37523. Reason: nanny-close
2024-01-10 06:27:51,235 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33733. Reason: nanny-close
2024-01-10 06:27:51,235 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:51,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:51,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:51,236 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46000; closing.
2024-01-10 06:27:51,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:51,237 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35583. Reason: nanny-close
2024-01-10 06:27:51,237 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46051', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868071.2372544')
2024-01-10 06:27:51,237 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45978; closing.
2024-01-10 06:27:51,237 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45964; closing.
2024-01-10 06:27:51,237 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:51,238 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:51,238 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:51,238 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42143', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868071.2382412')
2024-01-10 06:27:51,238 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46034; closing.
2024-01-10 06:27:51,238 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868071.238786')
2024-01-10 06:27:51,239 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:51,239 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37523', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868071.23953')
2024-01-10 06:27:51,239 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:51,239 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46014; closing.
2024-01-10 06:27:51,240 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868071.2404442')
2024-01-10 06:27:51,240 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:51,240 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45986; closing.
2024-01-10 06:27:51,241 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33733', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868071.241225')
2024-01-10 06:27:51,241 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:51,241 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:51,241 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46020; closing.
2024-01-10 06:27:51,242 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35583', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868071.2422147')
2024-01-10 06:27:51,242 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:51,242 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:51,242 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:51,242 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:51,508 - distributed.scheduler - INFO - Receive client connection: Client-61697a4a-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:51,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46058
2024-01-10 06:27:52,246 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:27:52,246 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:27:52,247 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:52,248 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:27:52,249 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-10 06:27:54,647 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:54,651 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46141 instead
  warnings.warn(
2024-01-10 06:27:54,656 - distributed.scheduler - INFO - State start
2024-01-10 06:27:54,681 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:54,682 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-10 06:27:54,683 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:54,684 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-10 06:27:54,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40801'
2024-01-10 06:27:54,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37185'
2024-01-10 06:27:54,815 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34711'
2024-01-10 06:27:54,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41377'
2024-01-10 06:27:54,829 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43923'
2024-01-10 06:27:54,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41447'
2024-01-10 06:27:54,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44195'
2024-01-10 06:27:54,860 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44965'
2024-01-10 06:27:56,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:56,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:56,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:56,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:56,747 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:56,747 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:56,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:56,748 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:56,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:56,748 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:56,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:56,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:56,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:56,751 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:56,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:56,751 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:56,751 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40247
2024-01-10 06:27:56,751 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40247
2024-01-10 06:27:56,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37437
2024-01-10 06:27:56,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:56,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:56,752 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:56,752 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:56,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:56,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gfc0uqc7
2024-01-10 06:27:56,752 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43255
2024-01-10 06:27:56,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43255
2024-01-10 06:27:56,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43313
2024-01-10 06:27:56,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60378ce7-73cb-4964-9929-7288dcde69b3
2024-01-10 06:27:56,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:56,752 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:56,752 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:56,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:56,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-962mn0n6
2024-01-10 06:27:56,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:56,752 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38757
2024-01-10 06:27:56,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:56,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38757
2024-01-10 06:27:56,753 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4db5fae3-c7d3-4865-8388-241d21ddb4be
2024-01-10 06:27:56,753 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39741
2024-01-10 06:27:56,753 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:56,753 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:56,753 - distributed.worker - INFO - Starting Worker plugin PreImport-b26c7d0b-71fd-49b1-a99f-9be78b0a7478
2024-01-10 06:27:56,753 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:56,753 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:56,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14ebfaa9-37f8-4176-a630-4a3e761f98c5
2024-01-10 06:27:56,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kuj2iivw
2024-01-10 06:27:56,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c1a9f14-0cd8-4451-a9e7-e40d916c6308
2024-01-10 06:27:56,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35099
2024-01-10 06:27:56,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42587
2024-01-10 06:27:56,753 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35099
2024-01-10 06:27:56,753 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42587
2024-01-10 06:27:56,753 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32973
2024-01-10 06:27:56,753 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36513
2024-01-10 06:27:56,753 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:56,753 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:56,753 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:56,753 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:56,754 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:56,754 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:56,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:56,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:56,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p8d55_17
2024-01-10 06:27:56,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3fpx3tv4
2024-01-10 06:27:56,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77336d12-82b5-4a15-8594-3e9ffb44843b
2024-01-10 06:27:56,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b98ec74e-3079-460f-b869-16b1e65b4133
2024-01-10 06:27:56,754 - distributed.worker - INFO - Starting Worker plugin PreImport-fe600e46-9577-4304-9c62-6a34a1412740
2024-01-10 06:27:56,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7b4b0ae-89a7-40fa-8c8d-d94b2ca699f1
2024-01-10 06:27:56,755 - distributed.worker - INFO - Starting Worker plugin PreImport-0abdc23f-85d7-4368-8762-dc8e93505f60
2024-01-10 06:27:56,755 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a96614da-a695-4ed6-a98f-bfe5e54d70e3
2024-01-10 06:27:56,755 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:56,756 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:56,756 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38119
2024-01-10 06:27:56,756 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38119
2024-01-10 06:27:56,756 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40373
2024-01-10 06:27:56,756 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:56,757 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:56,757 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:56,757 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:56,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w0_f60mj
2024-01-10 06:27:56,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44435
2024-01-10 06:27:56,757 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44435
2024-01-10 06:27:56,757 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42229
2024-01-10 06:27:56,757 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d1fd8505-dc6b-416f-b151-1c1f51e26021
2024-01-10 06:27:56,757 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:56,757 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:56,757 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:56,757 - distributed.worker - INFO - Starting Worker plugin PreImport-4b3ef221-8313-4043-8d58-8155fd7fe342
2024-01-10 06:27:56,757 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:56,757 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97811354-769c-4205-8e23-4385d19cc289
2024-01-10 06:27:56,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cmv7zfvs
2024-01-10 06:27:56,757 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ac7bacd-bf48-4b8e-b359-c485286a9a39
2024-01-10 06:27:56,758 - distributed.worker - INFO - Starting Worker plugin PreImport-86f11d82-c17b-4a6f-a7b0-b74a998f4c92
2024-01-10 06:27:56,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-704175a4-8efc-4cf4-9dbc-d867b1fb1b2b
2024-01-10 06:27:56,894 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:56,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:56,899 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:56,901 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44609
2024-01-10 06:27:56,901 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44609
2024-01-10 06:27:56,901 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44811
2024-01-10 06:27:56,901 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:56,901 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:56,901 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:56,901 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:56,901 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iu9juep2
2024-01-10 06:27:56,901 - distributed.worker - INFO - Starting Worker plugin RMMSetup-209f160d-d219-4315-9424-b790b779899b
2024-01-10 06:27:58,765 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,859 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,870 - distributed.worker - INFO - Starting Worker plugin PreImport-0ac4fc28-20e8-40a2-bc85-6c1a5f61b498
2024-01-10 06:27:58,870 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c303e93-755e-444e-9f3e-f5d7d94b73bf
2024-01-10 06:27:58,872 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,891 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,897 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,905 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb1f1145-31d3-44c5-8582-cedd2b211907
2024-01-10 06:27:58,907 - distributed.worker - INFO - Starting Worker plugin PreImport-f6e83630-41df-4330-82b8-788c79fecd19
2024-01-10 06:27:58,908 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,912 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,919 - distributed.worker - INFO - Starting Worker plugin PreImport-62e0af4e-387c-4acc-97bf-cc1e9abada8c
2024-01-10 06:27:58,920 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d8f9baf-e6d8-4401-8a64-0edaf025098f
2024-01-10 06:27:58,923 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:58,938 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:58,938 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:58,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:58,940 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:58,940 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,941 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:58,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:58,942 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:58,942 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,944 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:58,945 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:58,946 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:58,946 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,946 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44965'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:58,948 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38119. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:58,951 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:58,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:58,953 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:58,953 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:58,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:58,956 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:58,956 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:58,971 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:58,972 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:58,972 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:58,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:58,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37185'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41377'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35099. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43923'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,991 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43255. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41447'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,992 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38757. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,992 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44195'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,992 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44609. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,993 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:58,993 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44435. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,993 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:58,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:58,995 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:58,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:58,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:58,996 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:58,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40801'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,997 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:58,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:58,997 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:58,997 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:58,998 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40247. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:59,000 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:59,002 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:59,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:59,036 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:59,036 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:59,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:59,041 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34711'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:59,041 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:59,042 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42587. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-10 06:27:59,044 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:59,045 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:46360 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-10 06:27:59,350 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66460 parent=66267 started daemon>
2024-01-10 06:27:59,350 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66456 parent=66267 started daemon>
2024-01-10 06:27:59,350 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66452 parent=66267 started daemon>
2024-01-10 06:27:59,351 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66448 parent=66267 started daemon>
2024-01-10 06:27:59,351 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66442 parent=66267 started daemon>
2024-01-10 06:27:59,351 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66437 parent=66267 started daemon>
2024-01-10 06:27:59,351 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66433 parent=66267 started daemon>
2024-01-10 06:27:59,609 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 66437 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-10 06:28:33,945 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:33,950 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:28:33,954 - distributed.scheduler - INFO - State start
2024-01-10 06:28:33,977 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:33,978 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:28:33,979 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:28:33,979 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:28:34,038 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40551'
2024-01-10 06:28:35,378 - distributed.scheduler - INFO - Receive client connection: Client-79a1d431-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:35,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50062
2024-01-10 06:28:35,941 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:28:35,941 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:28:36,598 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:28:36,598 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45141
2024-01-10 06:28:36,599 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45141
2024-01-10 06:28:36,599 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-10 06:28:36,599 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:28:36,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:36,599 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:28:36,599 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:28:36,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tf3k3r2k
2024-01-10 06:28:36,599 - distributed.worker - INFO - Starting Worker plugin PreImport-fb6d6f67-d5b2-4d9f-b7ec-083495caf30f
2024-01-10 06:28:36,599 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9eadb80-73c8-489a-85ea-ee58973a0200
2024-01-10 06:28:36,599 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb5732ce-9963-4203-85c2-3fd9e3d9bfa7
2024-01-10 06:28:36,600 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:36,764 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45141', status: init, memory: 0, processing: 0>
2024-01-10 06:28:36,765 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45141
2024-01-10 06:28:36,765 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50086
2024-01-10 06:28:36,766 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:28:36,767 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:28:36,767 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:36,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:28:36,824 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:28:36,827 - distributed.scheduler - INFO - Remove client Client-79a1d431-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:36,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50062; closing.
2024-01-10 06:28:36,827 - distributed.scheduler - INFO - Remove client Client-79a1d431-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:36,828 - distributed.scheduler - INFO - Close client connection: Client-79a1d431-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:36,829 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40551'. Reason: nanny-close
2024-01-10 06:28:36,829 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:28:36,830 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45141. Reason: nanny-close
2024-01-10 06:28:36,832 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:28:36,832 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50086; closing.
2024-01-10 06:28:36,832 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45141', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868116.8328552')
2024-01-10 06:28:36,833 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:28:36,834 - distributed.nanny - INFO - Worker closed
2024-01-10 06:28:37,594 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:28:37,595 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:28:37,595 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:28:37,596 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:28:37,597 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-10 06:28:41,847 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:41,851 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35451 instead
  warnings.warn(
2024-01-10 06:28:41,855 - distributed.scheduler - INFO - State start
2024-01-10 06:28:41,877 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:41,878 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:28:41,878 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35451/status
2024-01-10 06:28:41,879 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:28:42,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43917'
2024-01-10 06:28:42,729 - distributed.scheduler - INFO - Receive client connection: Client-7e5f9967-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:42,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59250
2024-01-10 06:28:44,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:28:44,449 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:28:45,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:28:45,033 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37873
2024-01-10 06:28:45,033 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37873
2024-01-10 06:28:45,033 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45493
2024-01-10 06:28:45,033 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:28:45,033 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:45,033 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:28:45,033 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:28:45,033 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4nb8x5at
2024-01-10 06:28:45,034 - distributed.worker - INFO - Starting Worker plugin PreImport-834f0d9d-affb-49de-9475-c4e3d410140f
2024-01-10 06:28:45,035 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30d189ac-13f3-4272-a10b-960448a2d0d0
2024-01-10 06:28:45,035 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0fd1855-af20-4876-b38d-155bf6bfa24f
2024-01-10 06:28:45,035 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:45,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37873', status: init, memory: 0, processing: 0>
2024-01-10 06:28:45,239 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37873
2024-01-10 06:28:45,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59268
2024-01-10 06:28:45,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:28:45,241 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:28:45,241 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:45,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:28:45,296 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:28:45,299 - distributed.scheduler - INFO - Remove client Client-7e5f9967-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:45,299 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59250; closing.
2024-01-10 06:28:45,300 - distributed.scheduler - INFO - Remove client Client-7e5f9967-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:45,300 - distributed.scheduler - INFO - Close client connection: Client-7e5f9967-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:45,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43917'. Reason: nanny-close
2024-01-10 06:28:45,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:28:45,302 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37873. Reason: nanny-close
2024-01-10 06:28:45,304 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:28:45,304 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59268; closing.
2024-01-10 06:28:45,305 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37873', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868125.3050327')
2024-01-10 06:28:45,305 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:28:45,306 - distributed.nanny - INFO - Worker closed
2024-01-10 06:28:45,966 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:28:45,967 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:28:45,967 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:28:45,968 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:28:45,969 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-10 06:28:48,243 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:48,248 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:28:48,251 - distributed.scheduler - INFO - State start
2024-01-10 06:28:48,274 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:48,275 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:28:48,276 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:28:48,276 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:28:50,717 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:59284'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59284>: Stream is closed
2024-01-10 06:28:51,056 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:28:51,057 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:28:51,057 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:28:51,058 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:28:51,058 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-10 06:28:53,361 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:53,366 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45445 instead
  warnings.warn(
2024-01-10 06:28:53,370 - distributed.scheduler - INFO - State start
2024-01-10 06:28:53,393 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:53,394 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-10 06:28:53,395 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45445/status
2024-01-10 06:28:53,395 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:28:53,438 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39071'
2024-01-10 06:28:53,443 - distributed.scheduler - INFO - Receive client connection: Client-851e3aeb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:53,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37666
2024-01-10 06:28:55,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:28:55,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:28:55,284 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:28:55,285 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45073
2024-01-10 06:28:55,285 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45073
2024-01-10 06:28:55,285 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43183
2024-01-10 06:28:55,285 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:28:55,285 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:55,285 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:28:55,285 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:28:55,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-fsule79j
2024-01-10 06:28:55,285 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0a500c6-bfa7-4e65-8053-7d0f660c8c0a
2024-01-10 06:28:55,285 - distributed.worker - INFO - Starting Worker plugin PreImport-794a1377-32c6-4707-89d1-7b61d22d5572
2024-01-10 06:28:55,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17bdd5d6-7e1e-45fe-99b0-7b7f09e08906
2024-01-10 06:28:55,286 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:55,973 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45073', status: init, memory: 0, processing: 0>
2024-01-10 06:28:55,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45073
2024-01-10 06:28:55,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37684
2024-01-10 06:28:55,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:28:55,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:28:55,977 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:55,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:28:55,998 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:28:56,001 - distributed.scheduler - INFO - Remove client Client-851e3aeb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:56,001 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37666; closing.
2024-01-10 06:28:56,001 - distributed.scheduler - INFO - Remove client Client-851e3aeb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:56,002 - distributed.scheduler - INFO - Close client connection: Client-851e3aeb-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:56,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39071'. Reason: nanny-close
2024-01-10 06:28:56,024 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:28:56,025 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45073. Reason: nanny-close
2024-01-10 06:28:56,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:28:56,027 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37684; closing.
2024-01-10 06:28:56,027 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868136.0273006')
2024-01-10 06:28:56,027 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:28:56,028 - distributed.nanny - INFO - Worker closed
2024-01-10 06:28:56,668 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:28:56,668 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:28:56,668 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:28:56,669 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-10 06:28:56,670 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-10 06:28:58,919 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:58,924 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46069 instead
  warnings.warn(
2024-01-10 06:28:58,928 - distributed.scheduler - INFO - State start
2024-01-10 06:28:58,953 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:28:58,954 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:28:58,955 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46069/status
2024-01-10 06:28:58,955 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:28:59,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41515'
2024-01-10 06:28:59,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43571'
2024-01-10 06:28:59,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34431'
2024-01-10 06:28:59,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33107'
2024-01-10 06:28:59,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39703'
2024-01-10 06:28:59,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39959'
2024-01-10 06:28:59,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45175'
2024-01-10 06:28:59,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40119'
2024-01-10 06:28:59,625 - distributed.scheduler - INFO - Receive client connection: Client-88837a9c-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:59,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59390
2024-01-10 06:29:01,278 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:01,278 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:01,283 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:01,284 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39923
2024-01-10 06:29:01,284 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39923
2024-01-10 06:29:01,284 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33273
2024-01-10 06:29:01,284 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,284 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,284 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:01,284 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:29:01,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-da33khdr
2024-01-10 06:29:01,284 - distributed.worker - INFO - Starting Worker plugin PreImport-aefe4dd3-3649-485e-b92a-05881d09c7e6
2024-01-10 06:29:01,285 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9c8bb1c-9058-43d7-bf31-181ae30187c3
2024-01-10 06:29:01,285 - distributed.worker - INFO - Starting Worker plugin RMMSetup-346bbeca-fabe-4308-a226-9bf50994cdb0
2024-01-10 06:29:01,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:01,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:01,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:01,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:01,535 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:01,536 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35597
2024-01-10 06:29:01,536 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35597
2024-01-10 06:29:01,536 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34493
2024-01-10 06:29:01,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,536 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,536 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:01,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:29:01,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a21dkd6h
2024-01-10 06:29:01,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84279c1d-7b48-4b3a-9140-0980a4f6b1de
2024-01-10 06:29:01,538 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:01,539 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40263
2024-01-10 06:29:01,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40263
2024-01-10 06:29:01,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37547
2024-01-10 06:29:01,540 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,540 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,540 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:01,540 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:29:01,540 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v92ts_01
2024-01-10 06:29:01,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df1b6418-39d3-4173-99b4-b201d5e4e476
2024-01-10 06:29:01,540 - distributed.worker - INFO - Starting Worker plugin PreImport-eb5fbb2a-b3e6-4d87-813d-ac6f1fd194ab
2024-01-10 06:29:01,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8060d0fd-4264-4e69-a2df-b42d04c82117
2024-01-10 06:29:01,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:01,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:01,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:01,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:01,545 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:01,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:01,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:01,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:01,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:01,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:01,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:01,549 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:01,550 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35203
2024-01-10 06:29:01,550 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35203
2024-01-10 06:29:01,550 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37303
2024-01-10 06:29:01,550 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,550 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,550 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40943
2024-01-10 06:29:01,550 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:01,550 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40943
2024-01-10 06:29:01,550 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:29:01,550 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c45rvu5g
2024-01-10 06:29:01,550 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43825
2024-01-10 06:29:01,550 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,550 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,550 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:01,550 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:29:01,550 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o9t55_i3
2024-01-10 06:29:01,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d70ac99f-dc40-40e2-935c-faeac16a1612
2024-01-10 06:29:01,551 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57cbf567-8894-4c5f-b6d0-d27a5838b1f6
2024-01-10 06:29:01,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:01,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:01,551 - distributed.worker - INFO - Starting Worker plugin PreImport-fb8e466f-26f4-4dbd-84c5-e00117b217ea
2024-01-10 06:29:01,551 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d14e180e-0932-4a51-bffe-8f73d41a7efb
2024-01-10 06:29:01,551 - distributed.worker - INFO - Starting Worker plugin PreImport-bb09ee9e-8ce1-4c1f-be0f-7aa37c3e94d7
2024-01-10 06:29:01,552 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43469
2024-01-10 06:29:01,552 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94ed18b0-3c72-4dca-9a52-1695e2ae6c39
2024-01-10 06:29:01,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43469
2024-01-10 06:29:01,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41139
2024-01-10 06:29:01,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,552 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46811
2024-01-10 06:29:01,552 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:01,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46811
2024-01-10 06:29:01,552 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:29:01,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34063
2024-01-10 06:29:01,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c2b0d1up
2024-01-10 06:29:01,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,552 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:01,552 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:29:01,552 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3db0c4a6-3682-4345-94e2-fbd09cedac97
2024-01-10 06:29:01,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lrrud_9s
2024-01-10 06:29:01,552 - distributed.worker - INFO - Starting Worker plugin PreImport-752f7c93-5776-4e04-b826-00a9cd524257
2024-01-10 06:29:01,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d98e109d-5b71-4afe-a76b-62125d649300
2024-01-10 06:29:01,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bdf8930-28a2-48c7-bd26-ebda4f1eac3e
2024-01-10 06:29:01,553 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:01,554 - distributed.worker - INFO - Starting Worker plugin PreImport-281c06df-b1fa-4f68-9249-0f02afda177b
2024-01-10 06:29:01,554 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f699e25-a666-412f-94f1-52f60075b21e
2024-01-10 06:29:01,555 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34099
2024-01-10 06:29:01,555 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34099
2024-01-10 06:29:01,555 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38915
2024-01-10 06:29:01,555 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,555 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,555 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:01,555 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:29:01,555 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-agkc6sqx
2024-01-10 06:29:01,556 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fa441c2-4164-4460-8052-4c1026d76798
2024-01-10 06:29:01,785 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,809 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39923', status: init, memory: 0, processing: 0>
2024-01-10 06:29:01,810 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39923
2024-01-10 06:29:01,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34908
2024-01-10 06:29:01,811 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:01,812 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:01,812 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:01,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:03,319 - distributed.worker - INFO - Starting Worker plugin PreImport-5c73fdc9-e5d5-4fb6-aafb-c67e6101e513
2024-01-10 06:29:03,319 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22f3af7d-adf5-4c62-a1c9-c514871e87fd
2024-01-10 06:29:03,320 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,344 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35597', status: init, memory: 0, processing: 0>
2024-01-10 06:29:03,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35597
2024-01-10 06:29:03,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34928
2024-01-10 06:29:03,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:03,346 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:03,346 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:03,377 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,400 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40263', status: init, memory: 0, processing: 0>
2024-01-10 06:29:03,400 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40263
2024-01-10 06:29:03,400 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34934
2024-01-10 06:29:03,401 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:03,402 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:03,402 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:03,405 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,418 - distributed.worker - INFO - Starting Worker plugin PreImport-8231be94-d9af-41bc-9fe4-6d466d5cd3e8
2024-01-10 06:29:03,419 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b35977f9-65e8-49fd-ac48-09b3a423c9e0
2024-01-10 06:29:03,420 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,428 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43469', status: init, memory: 0, processing: 0>
2024-01-10 06:29:03,428 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,429 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43469
2024-01-10 06:29:03,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34940
2024-01-10 06:29:03,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:03,431 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:03,431 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:03,436 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,444 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,445 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34099', status: init, memory: 0, processing: 0>
2024-01-10 06:29:03,446 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34099
2024-01-10 06:29:03,446 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34942
2024-01-10 06:29:03,447 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:03,448 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:03,448 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:03,464 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40943', status: init, memory: 0, processing: 0>
2024-01-10 06:29:03,465 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40943
2024-01-10 06:29:03,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34954
2024-01-10 06:29:03,466 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:03,468 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:03,468 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:03,473 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46811', status: init, memory: 0, processing: 0>
2024-01-10 06:29:03,473 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46811
2024-01-10 06:29:03,473 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34970
2024-01-10 06:29:03,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:03,476 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:03,476 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:03,478 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35203', status: init, memory: 0, processing: 0>
2024-01-10 06:29:03,478 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35203
2024-01-10 06:29:03,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34976
2024-01-10 06:29:03,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:03,481 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:03,481 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:03,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:03,551 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:29:03,551 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:29:03,551 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:29:03,551 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:29:03,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:29:03,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:29:03,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:29:03,552 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:29:03,565 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:03,565 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:03,566 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:03,566 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:03,566 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:03,566 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:03,566 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:03,566 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:03,570 - distributed.scheduler - INFO - Remove client Client-88837a9c-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:03,570 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59390; closing.
2024-01-10 06:29:03,571 - distributed.scheduler - INFO - Remove client Client-88837a9c-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:03,571 - distributed.scheduler - INFO - Close client connection: Client-88837a9c-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:03,572 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41515'. Reason: nanny-close
2024-01-10 06:29:03,572 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:03,572 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43571'. Reason: nanny-close
2024-01-10 06:29:03,573 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:03,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34431'. Reason: nanny-close
2024-01-10 06:29:03,573 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39923. Reason: nanny-close
2024-01-10 06:29:03,574 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:03,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33107'. Reason: nanny-close
2024-01-10 06:29:03,574 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:03,574 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40943. Reason: nanny-close
2024-01-10 06:29:03,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39703'. Reason: nanny-close
2024-01-10 06:29:03,574 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:03,575 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34099. Reason: nanny-close
2024-01-10 06:29:03,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39959'. Reason: nanny-close
2024-01-10 06:29:03,575 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40263. Reason: nanny-close
2024-01-10 06:29:03,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:03,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45175'. Reason: nanny-close
2024-01-10 06:29:03,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:03,575 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46811. Reason: nanny-close
2024-01-10 06:29:03,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40119'. Reason: nanny-close
2024-01-10 06:29:03,576 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:03,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35203. Reason: nanny-close
2024-01-10 06:29:03,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:03,576 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34908; closing.
2024-01-10 06:29:03,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35597. Reason: nanny-close
2024-01-10 06:29:03,576 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34942; closing.
2024-01-10 06:29:03,576 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:03,576 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:03,576 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:03,577 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39923', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868143.5770676')
2024-01-10 06:29:03,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43469. Reason: nanny-close
2024-01-10 06:29:03,577 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:03,577 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34934; closing.
2024-01-10 06:29:03,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:03,578 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34099', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868143.5780036')
2024-01-10 06:29:03,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:03,578 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:03,578 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:03,578 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40263', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868143.5786862')
2024-01-10 06:29:03,578 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:03,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:03,579 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34954; closing.
2024-01-10 06:29:03,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:03,579 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:03,580 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868143.5801208')
2024-01-10 06:29:03,580 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:03,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34970; closing.
2024-01-10 06:29:03,580 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:03,581 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:03,581 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34934>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:29:03,582 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868143.5827365')
2024-01-10 06:29:03,583 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34976; closing.
2024-01-10 06:29:03,583 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34928; closing.
2024-01-10 06:29:03,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35203', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868143.583784')
2024-01-10 06:29:03,584 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35597', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868143.584138')
2024-01-10 06:29:03,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34940; closing.
2024-01-10 06:29:03,584 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43469', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868143.5848167')
2024-01-10 06:29:03,585 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:29:04,538 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:29:04,538 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:29:04,539 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:29:04,540 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:29:04,541 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-10 06:29:06,838 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:29:06,843 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38985 instead
  warnings.warn(
2024-01-10 06:29:06,847 - distributed.scheduler - INFO - State start
2024-01-10 06:29:06,869 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:29:06,870 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:29:06,871 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38985/status
2024-01-10 06:29:06,871 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:29:07,119 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46647'
2024-01-10 06:29:08,879 - distributed.scheduler - INFO - Receive client connection: Client-8d4a252e-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:08,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35068
2024-01-10 06:29:09,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:09,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:09,620 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:09,621 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37139
2024-01-10 06:29:09,621 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37139
2024-01-10 06:29:09,621 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42941
2024-01-10 06:29:09,621 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:09,621 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:09,622 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:09,622 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:29:09,622 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0ffzdnx9
2024-01-10 06:29:09,622 - distributed.worker - INFO - Starting Worker plugin PreImport-72df6515-17a5-4e19-b541-ed77a0944466
2024-01-10 06:29:09,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-293bc113-6b1d-4bfe-b5d2-10e5943b123f
2024-01-10 06:29:09,622 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44cf949f-6dde-436e-a0a3-e79d1e4ad5bc
2024-01-10 06:29:10,291 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:10,359 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37139', status: init, memory: 0, processing: 0>
2024-01-10 06:29:10,361 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37139
2024-01-10 06:29:10,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36878
2024-01-10 06:29:10,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:10,363 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:10,363 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:10,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:10,432 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:29:10,436 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:10,438 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:10,440 - distributed.scheduler - INFO - Remove client Client-8d4a252e-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:10,440 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35068; closing.
2024-01-10 06:29:10,441 - distributed.scheduler - INFO - Remove client Client-8d4a252e-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:10,441 - distributed.scheduler - INFO - Close client connection: Client-8d4a252e-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:10,442 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46647'. Reason: nanny-close
2024-01-10 06:29:10,442 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:10,443 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37139. Reason: nanny-close
2024-01-10 06:29:10,445 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:10,445 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36878; closing.
2024-01-10 06:29:10,445 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868150.4456978')
2024-01-10 06:29:10,445 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:29:10,446 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:11,108 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:29:11,108 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:29:11,108 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:29:11,109 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:29:11,110 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-10 06:29:13,394 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:29:13,402 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:29:13,408 - distributed.scheduler - INFO - State start
2024-01-10 06:29:13,432 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:29:13,434 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:29:13,434 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:29:13,434 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:29:13,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35539'
2024-01-10 06:29:15,555 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:29:15,555 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:29:15,559 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:29:15,560 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43123
2024-01-10 06:29:15,560 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43123
2024-01-10 06:29:15,560 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38529
2024-01-10 06:29:15,560 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:29:15,560 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:15,560 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:29:15,561 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:29:15,561 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-46eog6dt
2024-01-10 06:29:15,561 - distributed.worker - INFO - Starting Worker plugin PreImport-1a86abad-06c1-4ba3-a028-fc44f046480f
2024-01-10 06:29:15,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45433981-02a5-45d8-be4b-0abe193570e3
2024-01-10 06:29:15,633 - distributed.scheduler - INFO - Receive client connection: Client-91221d25-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:15,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36970
2024-01-10 06:29:16,101 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe84cc5e-4c86-4514-8ec5-1496f9211489
2024-01-10 06:29:16,102 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:16,174 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43123', status: init, memory: 0, processing: 0>
2024-01-10 06:29:16,176 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43123
2024-01-10 06:29:16,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36992
2024-01-10 06:29:16,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:29:16,178 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:29:16,178 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:29:16,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:29:16,183 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-10 06:29:16,188 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:29:16,192 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:16,193 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:29:16,196 - distributed.scheduler - INFO - Remove client Client-91221d25-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:16,196 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36970; closing.
2024-01-10 06:29:16,196 - distributed.scheduler - INFO - Remove client Client-91221d25-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:16,197 - distributed.scheduler - INFO - Close client connection: Client-91221d25-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:29:16,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35539'. Reason: nanny-close
2024-01-10 06:29:16,198 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:29:16,199 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43123. Reason: nanny-close
2024-01-10 06:29:16,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:29:16,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36992; closing.
2024-01-10 06:29:16,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43123', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868156.201911')
2024-01-10 06:29:16,202 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:29:16,203 - distributed.nanny - INFO - Worker closed
2024-01-10 06:29:16,913 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:29:16,914 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:29:16,914 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:29:16,915 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:29:16,916 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43917 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43557 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43423 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37169 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33051 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46433 instead
  warnings.warn(
[1704868311.628720] [dgx13:70949:0]            sock.c:470  UCX  ERROR bind(fd=173 addr=0.0.0.0:33462) failed: Address already in use
[1704868311.822762] [dgx13:70949:0]            sock.c:470  UCX  ERROR bind(fd=178 addr=0.0.0.0:33215) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37177 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40719 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33647 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44269 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46361 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40371 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42243 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34801 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37655 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38589 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] [1704868509.928070] [dgx13:74547:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:36031) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34119 instead
  warnings.warn(
[1704868531.199352] [dgx13:74959:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:34956) failed: Address already in use
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7fe3d0000140, tag: 0x4593e733a910636>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7fe3d0000140, tag: 0x4593e733a910636>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-3950' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43731 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40115 instead
  warnings.warn(
[1704868612.927093] [dgx13:76182:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:39760) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] [1704868666.499123] [dgx13:76748:0]            sock.c:470  UCX  ERROR bind(fd=126 addr=0.0.0.0:33612) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33931 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35881 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39865 instead
  warnings.warn(
2024-01-10 06:42:11,630 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:60496 remote=tcp://127.0.0.1:45529>: Stream is closed
2024-01-10 06:42:11,630 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1590, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-01-10 06:42:11,635 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:35317', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1590, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39903 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36721 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7fd290659140, tag: 0x49336ce7639637dc>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7fd290659140, tag: 0x49336ce7639637dc>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-4901' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45439 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38801 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43187 instead
  warnings.warn(
[1704869200.388568] [dgx13:84837:0]            sock.c:470  UCX  ERROR bind(fd=159 addr=0.0.0.0:35837) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41501 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33731 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40009 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43067 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45017 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35031 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35357 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35719 instead
  warnings.warn(
[1704869323.412696] [dgx13:86996:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:52942) failed: Address already in use
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-1001' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39909 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45313 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39365 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36655 instead
  warnings.warn(
[1704869382.916556] [dgx13:87882:0]            sock.c:470  UCX  ERROR bind(fd=123 addr=0.0.0.0:39041) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37223 instead
  warnings.warn(
2024-01-10 06:49:53,274 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-10 06:49:53,276 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
[1704869394.104899] [dgx13:87901] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
[1704869394.104997] [dgx13:87901] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43791 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38529 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41487 instead
  warnings.warn(
[1704869443.709181] [dgx13:88829:0]            sock.c:470  UCX  ERROR bind(fd=123 addr=0.0.0.0:44488) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45535 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] [1704869525.530977] [dgx13:64745:0]            sock.c:470  UCX  ERROR bind(fd=257 addr=0.0.0.0:37378) failed: Address already in use
[1704869532.171790] [dgx13:90018:0]            sock.c:470  UCX  ERROR bind(fd=163 addr=0.0.0.0:33575) failed: Address already in use
2024-01-10 06:52:14,009 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 738, in wait
  File "libucxx.pyx", line 723, in wait_yield
  File "libucxx.pyx", line 718, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34349 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42447 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33367 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45873 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44019 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38775 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1704869614.161646] [dgx13:64745:1]            sock.c:470  UCX  ERROR bind(fd=251 addr=0.0.0.0:39354) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-10 06:54:07,877 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:07,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:07,879 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:07,879 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:07,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:07,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:07,943 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:07,943 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:07,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:07,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:08,001 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:08,001 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:08,045 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:08,045 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:08,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:08,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:08,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:08,494 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41289
2024-01-10 06:54:08,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41289
2024-01-10 06:54:08,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40481
2024-01-10 06:54:08,495 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,495 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,495 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:08,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qyq6xex0
2024-01-10 06:54:08,495 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9b2df22-e430-4844-868f-2468d6c21ab3
2024-01-10 06:54:08,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a393c42-7e2c-46c7-aab8-1c5201709e65
2024-01-10 06:54:08,496 - distributed.worker - INFO - Starting Worker plugin PreImport-6fd8d3f8-59f6-4df2-b421-e202b8317e50
2024-01-10 06:54:08,496 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:08,504 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46797
2024-01-10 06:54:08,505 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46797
2024-01-10 06:54:08,505 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37871
2024-01-10 06:54:08,505 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,505 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,505 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:08,505 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-27wtp6de
2024-01-10 06:54:08,505 - distributed.worker - INFO - Starting Worker plugin PreImport-2ffa2c12-5196-400f-a53f-a3f88fcf9742
2024-01-10 06:54:08,505 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd648a7b-7951-410b-b191-0b4f07e23be2
2024-01-10 06:54:08,505 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e2b0c07-798a-4478-8f3e-e95dc8e8553d
2024-01-10 06:54:08,505 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:08,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39803
2024-01-10 06:54:08,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39803
2024-01-10 06:54:08,583 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34427
2024-01-10 06:54:08,583 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,583 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,583 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:08,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y_s7ttmx
2024-01-10 06:54:08,583 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d302411e-7b57-4ce6-8faa-ff1d0d9635d1
2024-01-10 06:54:08,583 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80af6f4a-f177-4437-b6a9-4eef01f80e3f
2024-01-10 06:54:08,583 - distributed.worker - INFO - Starting Worker plugin PreImport-fe5e7e00-bb05-4f7f-af23-bd890de52add
2024-01-10 06:54:08,584 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:08,588 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42123
2024-01-10 06:54:08,593 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:08,593 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,593 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42123
2024-01-10 06:54:08,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:08,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36847
2024-01-10 06:54:08,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36847
2024-01-10 06:54:08,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36267
2024-01-10 06:54:08,597 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,597 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,598 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:08,598 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b50o_u94
2024-01-10 06:54:08,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed5a7002-3d90-44cd-81a2-9cc90e6e8785
2024-01-10 06:54:08,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b8107ff-5819-4af9-9444-26853fd739cf
2024-01-10 06:54:08,598 - distributed.worker - INFO - Starting Worker plugin PreImport-537525a2-b245-439e-80e7-f7bdeed82884
2024-01-10 06:54:08,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:08,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42997
2024-01-10 06:54:08,644 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42997
2024-01-10 06:54:08,644 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45975
2024-01-10 06:54:08,644 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,644 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,644 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:08,644 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bi7lzyap
2024-01-10 06:54:08,644 - distributed.worker - INFO - Starting Worker plugin PreImport-52829c51-956b-4ec9-884d-801342abf14d
2024-01-10 06:54:08,645 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c50cdb2c-9676-471c-887c-b91c20bf4fa4
2024-01-10 06:54:08,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-615ba63d-a9ca-4c93-98af-d47132f24b1a
2024-01-10 06:54:08,645 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,655 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:08,656 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37127
2024-01-10 06:54:08,656 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37127
2024-01-10 06:54:08,656 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38073
2024-01-10 06:54:08,656 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,656 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,656 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:08,656 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d_61gsn3
2024-01-10 06:54:08,657 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b95be542-f3cc-4cf6-b694-ce0929f6d59c
2024-01-10 06:54:08,657 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01681a86-eda8-4237-be0e-3c7bd41cffd3
2024-01-10 06:54:08,657 - distributed.worker - INFO - Starting Worker plugin PreImport-c9873fba-4c0a-43cd-88d8-fa5998def45b
2024-01-10 06:54:08,657 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,660 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:08,661 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40407
2024-01-10 06:54:08,661 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40407
2024-01-10 06:54:08,661 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36603
2024-01-10 06:54:08,661 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,661 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,661 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:08,661 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n05bur_m
2024-01-10 06:54:08,661 - distributed.worker - INFO - Starting Worker plugin PreImport-f1f178bc-2b57-4b34-8e4e-38d832c16b38
2024-01-10 06:54:08,661 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be26bb0c-a42d-4b05-9887-281a84a85f02
2024-01-10 06:54:08,662 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ecdf0d1-75e6-4d38-8589-01b532dfa519
2024-01-10 06:54:08,662 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,662 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:08,663 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,663 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42123
2024-01-10 06:54:08,698 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:08,699 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42123
2024-01-10 06:54:08,815 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:08,816 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,816 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42123
2024-01-10 06:54:08,825 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:08,826 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,826 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42123
2024-01-10 06:54:08,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:08,830 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,830 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42123
2024-01-10 06:54:08,904 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:08,905 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38233
2024-01-10 06:54:08,905 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38233
2024-01-10 06:54:08,905 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37711
2024-01-10 06:54:08,905 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,905 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,905 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:08,905 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a82qk70y
2024-01-10 06:54:08,906 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb930357-d42c-421a-8566-67803b56a026
2024-01-10 06:54:08,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ebc9834-6859-4dec-8faa-4f9ad3f8ff17
2024-01-10 06:54:08,906 - distributed.worker - INFO - Starting Worker plugin PreImport-01d04d45-d33c-4c9a-9806-4204462051b6
2024-01-10 06:54:08,907 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,974 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:08,975 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42123
2024-01-10 06:54:08,975 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:08,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42123
2024-01-10 06:54:08,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:54:08,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:54:08,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:54:08,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:54:08,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:54:08,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:54:08,993 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:54:08,993 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:54:08,998 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42997. Reason: nanny-close
2024-01-10 06:54:08,999 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46797. Reason: nanny-close
2024-01-10 06:54:09,000 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41289. Reason: nanny-close
2024-01-10 06:54:09,001 - distributed.core - INFO - Connection to tcp://127.0.0.1:42123 has been closed.
2024-01-10 06:54:09,001 - distributed.core - INFO - Connection to tcp://127.0.0.1:42123 has been closed.
2024-01-10 06:54:09,001 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36847. Reason: nanny-close
2024-01-10 06:54:09,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37127. Reason: nanny-close
2024-01-10 06:54:09,002 - distributed.core - INFO - Connection to tcp://127.0.0.1:42123 has been closed.
2024-01-10 06:54:09,002 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:09,002 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:09,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39803. Reason: nanny-close
2024-01-10 06:54:09,003 - distributed.core - INFO - Connection to tcp://127.0.0.1:42123 has been closed.
2024-01-10 06:54:09,003 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:09,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40407. Reason: nanny-close
2024-01-10 06:54:09,004 - distributed.core - INFO - Connection to tcp://127.0.0.1:42123 has been closed.
2024-01-10 06:54:09,004 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38233. Reason: nanny-close
2024-01-10 06:54:09,004 - distributed.core - INFO - Connection to tcp://127.0.0.1:42123 has been closed.
2024-01-10 06:54:09,005 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:09,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:42123 has been closed.
2024-01-10 06:54:09,005 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:09,005 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:09,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:42123 has been closed.
2024-01-10 06:54:09,006 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:09,008 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-10 06:54:43,355 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:43,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:43,360 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:43,360 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40691
2024-01-10 06:54:43,361 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40691
2024-01-10 06:54:43,361 - distributed.worker - INFO -           Worker name:                          0
2024-01-10 06:54:43,361 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37069
2024-01-10 06:54:43,361 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36977
2024-01-10 06:54:43,361 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:43,361 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:43,361 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:54:43,361 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8ao5pvr2
2024-01-10 06:54:43,361 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3f5278c-ee50-4628-8f00-14e19a5fc200
2024-01-10 06:54:43,361 - distributed.worker - INFO - Starting Worker plugin PreImport-da1f5b42-35d8-4d8f-8381-4ebb01f4292f
2024-01-10 06:54:43,377 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-10 06:54:43,378 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52c3e143-93d1-49ea-bbd5-586e8db31c9b
2024-01-10 06:54:43,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40691. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-10 06:54:43,378 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-10 06:54:43,381 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-10 06:54:47,946 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:47,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:47,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:47,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:48,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:48,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:48,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:48,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:48,148 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:48,148 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:48,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:48,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:48,191 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:48,191 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:48,208 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:54:48,208 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:54:48,569 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:48,570 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44141
2024-01-10 06:54:48,570 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44141
2024-01-10 06:54:48,570 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38485
2024-01-10 06:54:48,570 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,570 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,570 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:48,570 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:54:48,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-erlzo2en
2024-01-10 06:54:48,571 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5124ac5-4676-4f55-ac5c-453f4731d044
2024-01-10 06:54:48,571 - distributed.worker - INFO - Starting Worker plugin RMMSetup-980ead45-f036-49a5-86f6-9b838631f851
2024-01-10 06:54:48,571 - distributed.worker - INFO - Starting Worker plugin PreImport-44d5736c-b51b-4ec5-a404-ea0b7493ab30
2024-01-10 06:54:48,571 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,614 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:48,614 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34785
2024-01-10 06:54:48,614 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34785
2024-01-10 06:54:48,615 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35105
2024-01-10 06:54:48,615 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,615 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,615 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:48,615 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:54:48,615 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9eio8p14
2024-01-10 06:54:48,615 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-295fc1be-35cb-4788-aef8-a8ae40ed98c7
2024-01-10 06:54:48,615 - distributed.worker - INFO - Starting Worker plugin PreImport-bf7496e3-1f00-4c9d-be9c-e4dedea589a3
2024-01-10 06:54:48,615 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8af63b4-7669-495a-983c-c9b4606e90bf
2024-01-10 06:54:48,615 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:48,635 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44671
2024-01-10 06:54:48,679 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:48,680 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,680 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44671
2024-01-10 06:54:48,721 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:48,722 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40595
2024-01-10 06:54:48,722 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40595
2024-01-10 06:54:48,723 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39337
2024-01-10 06:54:48,723 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,723 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,723 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:48,723 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:54:48,723 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_jlsa230
2024-01-10 06:54:48,723 - distributed.worker - INFO - Starting Worker plugin PreImport-31579c67-3de0-405a-ac8e-5ac36b22d07d
2024-01-10 06:54:48,723 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6f4d2de-caa6-4eae-ab10-df1254e8571e
2024-01-10 06:54:48,724 - distributed.worker - INFO - Starting Worker plugin RMMSetup-768e3758-437a-46de-83eb-31827e14e8d5
2024-01-10 06:54:48,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,733 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:48,734 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46541
2024-01-10 06:54:48,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46541
2024-01-10 06:54:48,734 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41897
2024-01-10 06:54:48,734 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,735 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,735 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:48,735 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:54:48,735 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qphsrka0
2024-01-10 06:54:48,735 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d1d4562-66eb-485a-a52c-f035f0500ef2
2024-01-10 06:54:48,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2752652d-f63d-4f05-9783-5272d5517246
2024-01-10 06:54:48,735 - distributed.worker - INFO - Starting Worker plugin PreImport-0af6110a-4688-4799-be49-ac33f81a4c4e
2024-01-10 06:54:48,736 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,777 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:48,778 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33559
2024-01-10 06:54:48,778 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33559
2024-01-10 06:54:48,778 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33187
2024-01-10 06:54:48,778 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,778 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,778 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:48,778 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:54:48,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iljvhhnr
2024-01-10 06:54:48,779 - distributed.worker - INFO - Starting Worker plugin PreImport-9b02b5e4-063f-44b0-a6c9-1b32f69325c5
2024-01-10 06:54:48,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94f25c52-47b0-4bb6-859b-8ad9f5569c32
2024-01-10 06:54:48,779 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f8d943e-074d-4036-a51a-771db5eed722
2024-01-10 06:54:48,779 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:48,779 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,780 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34409
2024-01-10 06:54:48,780 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34409
2024-01-10 06:54:48,780 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41999
2024-01-10 06:54:48,780 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,780 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:48,780 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:54:48,780 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kcgspk5o
2024-01-10 06:54:48,781 - distributed.worker - INFO - Starting Worker plugin PreImport-f4f90dd8-2449-4846-bad5-a65ecf9a44f3
2024-01-10 06:54:48,781 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e31e356c-6c8f-4ec0-bb4d-9b875a37957e
2024-01-10 06:54:48,781 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d591d36-dc0a-4077-874b-f9e4a01b69f7
2024-01-10 06:54:48,781 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,826 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:48,827 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45461
2024-01-10 06:54:48,827 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45461
2024-01-10 06:54:48,827 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43153
2024-01-10 06:54:48,827 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,827 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,827 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:48,827 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:54:48,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2df3eg78
2024-01-10 06:54:48,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1de01359-fa96-41f9-92b8-1f6bad9f82ba
2024-01-10 06:54:48,828 - distributed.worker - INFO - Starting Worker plugin PreImport-88ddc5c5-6c6d-46eb-be58-c3fc5cff637b
2024-01-10 06:54:48,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-181674ab-ef0c-4b1c-8352-555d27b35937
2024-01-10 06:54:48,828 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:48,833 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,834 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44671
2024-01-10 06:54:48,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:54:48,841 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44159
2024-01-10 06:54:48,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44159
2024-01-10 06:54:48,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43325
2024-01-10 06:54:48,841 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,841 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,841 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:54:48,841 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:54:48,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ycvusjch
2024-01-10 06:54:48,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f2667da-f76c-4463-a8fb-6c16495498ad
2024-01-10 06:54:48,842 - distributed.worker - INFO - Starting Worker plugin PreImport-c6966106-ed63-46f1-8093-43c6379c73e2
2024-01-10 06:54:48,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f3bca042-3a5a-4308-92f0-66db85c811c6
2024-01-10 06:54:48,842 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:48,919 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,919 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44671
2024-01-10 06:54:48,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:48,984 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,984 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,985 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44671
2024-01-10 06:54:48,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:48,987 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,987 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44671
2024-01-10 06:54:48,996 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:48,997 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44671
2024-01-10 06:54:48,997 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:48,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44671
2024-01-10 06:54:49,000 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:54:49,000 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44671
2024-01-10 06:54:49,000 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:54:49,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44671
2024-01-10 06:54:49,009 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44141. Reason: nanny-close
2024-01-10 06:54:49,010 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34785. Reason: nanny-close
2024-01-10 06:54:49,011 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40595. Reason: nanny-close
2024-01-10 06:54:49,011 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46541. Reason: nanny-close
2024-01-10 06:54:49,012 - distributed.core - INFO - Connection to tcp://127.0.0.1:44671 has been closed.
2024-01-10 06:54:49,012 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34409. Reason: nanny-close
2024-01-10 06:54:49,012 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33559. Reason: nanny-close
2024-01-10 06:54:49,012 - distributed.core - INFO - Connection to tcp://127.0.0.1:44671 has been closed.
2024-01-10 06:54:49,013 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44159. Reason: nanny-close
2024-01-10 06:54:49,013 - distributed.core - INFO - Connection to tcp://127.0.0.1:44671 has been closed.
2024-01-10 06:54:49,013 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:49,013 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45461. Reason: nanny-close
2024-01-10 06:54:49,014 - distributed.core - INFO - Connection to tcp://127.0.0.1:44671 has been closed.
2024-01-10 06:54:49,014 - distributed.core - INFO - Connection to tcp://127.0.0.1:44671 has been closed.
2024-01-10 06:54:49,014 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:49,014 - distributed.core - INFO - Connection to tcp://127.0.0.1:44671 has been closed.
2024-01-10 06:54:49,015 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:49,015 - distributed.core - INFO - Connection to tcp://127.0.0.1:44671 has been closed.
2024-01-10 06:54:49,015 - distributed.core - INFO - Connection to tcp://127.0.0.1:44671 has been closed.
2024-01-10 06:54:49,015 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:49,015 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:49,016 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:49,016 - distributed.nanny - INFO - Worker closed
2024-01-10 06:54:49,016 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk PASSED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-10 06:56:10,244 - distributed.worker - WARNING - RMM allocation of 1.00 MiB failed, spill-on-demand couldn't find any device memory to spill.
RMM allocs: 1.00 MiB, <ProxyManager dev_limit=25.60 GiB host_limit=0.98 TiB disk=0 B(0) host=0 B(0) dev=0 B(0)>, traceback:
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/threadpoolexecutor.py", line 57, in _worker
    task.run()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py", line 65, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1541, in <lambda>
    executor, lambda: context.run(func, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2954, in apply_function
    msg = apply_function_simple(function, args, kwargs, time_delay)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2990, in apply_function_simple
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_proxify_host_file.py", line 467, in task
    rmm.DeviceBuffer(size=rmm_pool_size),  # Trigger OOM
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/proxify_host_file.py", line 617, in oom
    traceback.print_stack(file=f)


2024-01-10 06:56:10,473 - distributed.worker - WARNING - Compute Failed
Key:       task-5c302f54392a5ca98adb3c55c8357b0e
Function:  task
args:      ()
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_serializer PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[numpy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[cupy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_name PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj0] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj1] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[dask] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[pickle] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[disk] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-send_serializers1] /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
