============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-01 06:39:16,689 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:16,694 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36405 instead
  warnings.warn(
2023-12-01 06:39:16,698 - distributed.scheduler - INFO - State start
2023-12-01 06:39:16,722 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:16,723 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-01 06:39:16,724 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36405/status
2023-12-01 06:39:16,724 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:39:16,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46215'
2023-12-01 06:39:16,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45317'
2023-12-01 06:39:16,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38415'
2023-12-01 06:39:16,807 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45145'
2023-12-01 06:39:18,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:18,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:18,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:18,492 - distributed.scheduler - INFO - Receive client connection: Client-58465be9-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:18,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:18,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:18,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:18,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:18,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:18,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:18,507 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57484
2023-12-01 06:39:18,528 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:18,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:18,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-01 06:39:18,552 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36645
2023-12-01 06:39:18,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36645
2023-12-01 06:39:18,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42701
2023-12-01 06:39:18,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:39:18,552 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:18,552 - distributed.worker - INFO -               Threads:                          4
2023-12-01 06:39:18,553 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-01 06:39:18,553 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-685186xz
2023-12-01 06:39:18,553 - distributed.worker - INFO - Starting Worker plugin PreImport-aa1e2a2c-ba16-4e56-a5af-758c7bbc2bf6
2023-12-01 06:39:18,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e03e20c-d051-4e89-8eae-689f2e50a1a8
2023-12-01 06:39:18,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44e03c9b-2dbd-4f69-94d6-f0d20f39396c
2023-12-01 06:39:18,553 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:19,270 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36645', status: init, memory: 0, processing: 0>
2023-12-01 06:39:19,271 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36645
2023-12-01 06:39:19,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57510
2023-12-01 06:39:19,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:19,273 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:39:19,273 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:19,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:39:19,836 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33589
2023-12-01 06:39:19,837 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33589
2023-12-01 06:39:19,837 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38591
2023-12-01 06:39:19,837 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:39:19,837 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:19,837 - distributed.worker - INFO -               Threads:                          4
2023-12-01 06:39:19,838 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-01 06:39:19,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38589
2023-12-01 06:39:19,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-edk24ylb
2023-12-01 06:39:19,838 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38589
2023-12-01 06:39:19,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37131
2023-12-01 06:39:19,838 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33315
2023-12-01 06:39:19,838 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37131
2023-12-01 06:39:19,838 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:39:19,838 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44473
2023-12-01 06:39:19,838 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:39:19,838 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:19,838 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6eca6a90-29d7-4a13-8c6d-6db7ed3d345e
2023-12-01 06:39:19,838 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:19,838 - distributed.worker - INFO -               Threads:                          4
2023-12-01 06:39:19,838 - distributed.worker - INFO - Starting Worker plugin RMMSetup-622ca6e8-8c8a-4b4e-9452-3c4d76e8adba
2023-12-01 06:39:19,838 - distributed.worker - INFO -               Threads:                          4
2023-12-01 06:39:19,838 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-01 06:39:19,838 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-01 06:39:19,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zamihdad
2023-12-01 06:39:19,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-xtzz5boh
2023-12-01 06:39:19,838 - distributed.worker - INFO - Starting Worker plugin PreImport-03165b75-26f3-4e48-812d-91d3532f3f67
2023-12-01 06:39:19,839 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:19,839 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eff9f782-1646-497f-a7d4-6ee917d9fa48
2023-12-01 06:39:19,839 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc9ffbde-d8f5-44c7-aa9a-a23d01abe297
2023-12-01 06:39:19,839 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65b22ed0-a2b8-4603-9747-19e9335db67c
2023-12-01 06:39:19,840 - distributed.worker - INFO - Starting Worker plugin PreImport-c92af03f-5790-4c2b-9c63-023e346a01e5
2023-12-01 06:39:19,840 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:19,845 - distributed.worker - INFO - Starting Worker plugin PreImport-9c8395c7-9e31-472b-b51a-4be7012867f1
2023-12-01 06:39:19,846 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad8e095b-e75e-417e-b6ff-8cceed97f7d1
2023-12-01 06:39:19,848 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:20,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37131', status: init, memory: 0, processing: 0>
2023-12-01 06:39:20,216 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37131
2023-12-01 06:39:20,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57520
2023-12-01 06:39:20,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:20,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:39:20,219 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:20,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:39:20,366 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33589', status: init, memory: 0, processing: 0>
2023-12-01 06:39:20,367 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33589
2023-12-01 06:39:20,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57512
2023-12-01 06:39:20,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:20,369 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:39:20,369 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:20,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:39:20,537 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38589', status: init, memory: 0, processing: 0>
2023-12-01 06:39:20,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38589
2023-12-01 06:39:20,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57532
2023-12-01 06:39:20,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:20,541 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:39:20,541 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:20,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:39:20,574 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-01 06:39:20,575 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-01 06:39:20,575 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-01 06:39:20,782 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-01 06:39:20,787 - distributed.scheduler - INFO - Remove client Client-58465be9-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:20,787 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57484; closing.
2023-12-01 06:39:20,788 - distributed.scheduler - INFO - Remove client Client-58465be9-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:20,788 - distributed.scheduler - INFO - Close client connection: Client-58465be9-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:20,789 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45317'. Reason: nanny-close
2023-12-01 06:39:20,790 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:20,791 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38415'. Reason: nanny-close
2023-12-01 06:39:20,791 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:20,791 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38589. Reason: nanny-close
2023-12-01 06:39:20,792 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33589. Reason: nanny-close
2023-12-01 06:39:20,794 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:39:20,794 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57512; closing.
2023-12-01 06:39:20,794 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:39:20,794 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33589', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412760.7946036')
2023-12-01 06:39:20,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45145'. Reason: nanny-close
2023-12-01 06:39:20,795 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:20,795 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57532; closing.
2023-12-01 06:39:20,795 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:20,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46215'. Reason: nanny-close
2023-12-01 06:39:20,796 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38589', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412760.7961867')
2023-12-01 06:39:20,796 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:20,796 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:20,797 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36645. Reason: nanny-close
2023-12-01 06:39:20,797 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37131. Reason: nanny-close
2023-12-01 06:39:20,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57520; closing.
2023-12-01 06:39:20,799 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:39:20,799 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412760.799545')
2023-12-01 06:39:20,799 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:39:20,800 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57510; closing.
2023-12-01 06:39:20,800 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36645', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412760.800325')
2023-12-01 06:39:20,800 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:39:20,800 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:20,801 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:22,157 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:39:22,157 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:39:22,158 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:39:22,159 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-01 06:39:22,159 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-01 06:39:24,383 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:24,389 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43821 instead
  warnings.warn(
2023-12-01 06:39:24,393 - distributed.scheduler - INFO - State start
2023-12-01 06:39:24,417 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:24,418 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:39:24,418 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43821/status
2023-12-01 06:39:24,419 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:39:24,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34551'
2023-12-01 06:39:24,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44825'
2023-12-01 06:39:24,617 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37435'
2023-12-01 06:39:24,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36159'
2023-12-01 06:39:24,632 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44011'
2023-12-01 06:39:24,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45093'
2023-12-01 06:39:24,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33381'
2023-12-01 06:39:24,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46739'
2023-12-01 06:39:24,966 - distributed.scheduler - INFO - Receive client connection: Client-5cd76f4c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:24,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58670
2023-12-01 06:39:26,683 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:26,683 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:26,687 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:26,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:26,832 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:26,837 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:26,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:26,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:26,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:26,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:26,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:26,851 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:26,854 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:26,854 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:26,859 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:26,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:26,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:26,897 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:26,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:26,898 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:26,899 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:26,899 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:26,902 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:26,904 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:32,007 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40461
2023-12-01 06:39:32,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40461
2023-12-01 06:39:32,008 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36397
2023-12-01 06:39:32,009 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,009 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,009 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:32,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:32,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ctbe9z6w
2023-12-01 06:39:32,010 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05763b69-7ce9-4760-9ba6-2160e908accd
2023-12-01 06:39:32,021 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45595
2023-12-01 06:39:32,022 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45595
2023-12-01 06:39:32,022 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36577
2023-12-01 06:39:32,022 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,022 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,022 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:32,022 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:32,022 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ec50oslt
2023-12-01 06:39:32,023 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72877e16-43ee-4b23-98b2-446a8f468963
2023-12-01 06:39:32,023 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79a25b33-651b-487c-b753-2469e51c7810
2023-12-01 06:39:32,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37837
2023-12-01 06:39:32,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37837
2023-12-01 06:39:32,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40641
2023-12-01 06:39:32,024 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,024 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,024 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:32,024 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:32,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y4fjzc46
2023-12-01 06:39:32,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-833a9159-dbd3-4f2f-81fe-f67b328607d2
2023-12-01 06:39:32,222 - distributed.worker - INFO - Starting Worker plugin PreImport-7dcd82b7-7630-426c-a716-896d6ce01d4c
2023-12-01 06:39:32,223 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,224 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aaa960ae-1cf2-45a3-95af-73aa3820666b
2023-12-01 06:39:32,224 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58b99f93-357b-4405-8eb2-2a77ae9b1813
2023-12-01 06:39:32,225 - distributed.worker - INFO - Starting Worker plugin PreImport-41744176-fcb1-4a4c-bc01-edb8e5ca6287
2023-12-01 06:39:32,225 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,225 - distributed.worker - INFO - Starting Worker plugin PreImport-1f2c9d9d-1547-4d07-a1fb-c83ed2a76120
2023-12-01 06:39:32,225 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,268 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36927
2023-12-01 06:39:32,269 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36927
2023-12-01 06:39:32,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34363
2023-12-01 06:39:32,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,269 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,269 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:32,269 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:32,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gnzabl3e
2023-12-01 06:39:32,270 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b309916b-e86b-4709-8982-88069092fb49
2023-12-01 06:39:32,270 - distributed.worker - INFO - Starting Worker plugin RMMSetup-539a0cbe-9cb8-46a5-9fe7-053ac26ec7fa
2023-12-01 06:39:32,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40461', status: init, memory: 0, processing: 0>
2023-12-01 06:39:32,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40461
2023-12-01 06:39:32,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43488
2023-12-01 06:39:32,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:32,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,275 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,278 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36293
2023-12-01 06:39:32,279 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36293
2023-12-01 06:39:32,279 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33583
2023-12-01 06:39:32,279 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,279 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,279 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:32,280 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:32,280 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dwvl0z4j
2023-12-01 06:39:32,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:32,280 - distributed.worker - INFO - Starting Worker plugin PreImport-4d56c9b5-41d3-4673-b60d-210b4f80fe88
2023-12-01 06:39:32,280 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8dd3b9f7-6a33-46cd-adf2-3d55870998bf
2023-12-01 06:39:32,282 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45595', status: init, memory: 0, processing: 0>
2023-12-01 06:39:32,283 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45595
2023-12-01 06:39:32,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43476
2023-12-01 06:39:32,283 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34437
2023-12-01 06:39:32,284 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34437
2023-12-01 06:39:32,284 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39909
2023-12-01 06:39:32,284 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,284 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,284 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:32,284 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:32,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ttt819c4
2023-12-01 06:39:32,284 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37837', status: init, memory: 0, processing: 0>
2023-12-01 06:39:32,284 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:32,285 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d33d1673-2c1a-4287-ac4e-95832dde5843
2023-12-01 06:39:32,285 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37837
2023-12-01 06:39:32,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43500
2023-12-01 06:39:32,285 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,285 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46097
2023-12-01 06:39:32,286 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:32,287 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46097
2023-12-01 06:39:32,287 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41583
2023-12-01 06:39:32,287 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,287 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,287 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:32,287 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:32,287 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z8vnvibp
2023-12-01 06:39:32,287 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,287 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,287 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29bf05f8-26b2-4020-ab0e-0053e8974698
2023-12-01 06:39:32,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38239
2023-12-01 06:39:32,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38239
2023-12-01 06:39:32,290 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39363
2023-12-01 06:39:32,290 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,290 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,290 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:32,290 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:32,290 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dlnl7grd
2023-12-01 06:39:32,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f63c0723-f6a6-4a7e-9b4c-5a79e9218763
2023-12-01 06:39:32,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:32,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:32,732 - distributed.worker - INFO - Starting Worker plugin PreImport-80bceab1-c452-47d5-bea3-9ec19ffc8e91
2023-12-01 06:39:32,732 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,760 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23383c53-32ca-4c95-a4e1-66c80a5cd0e5
2023-12-01 06:39:32,760 - distributed.worker - INFO - Starting Worker plugin PreImport-8da0f5e4-08b8-4163-9371-1bf8b276bc0f
2023-12-01 06:39:32,761 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,763 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36927', status: init, memory: 0, processing: 0>
2023-12-01 06:39:32,764 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36927
2023-12-01 06:39:32,764 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43512
2023-12-01 06:39:32,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:32,766 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,766 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,771 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:32,786 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca2c3166-bd34-47e3-91d7-cf72ef619f4d
2023-12-01 06:39:32,787 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce522561-4c64-454b-9c6d-9adc6f2a3c86
2023-12-01 06:39:32,793 - distributed.worker - INFO - Starting Worker plugin PreImport-7512d43f-6b09-4c79-bb98-b4fef63bc5b1
2023-12-01 06:39:32,794 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79e5b08b-03b5-4a03-8fbe-be1e9c76a4ed
2023-12-01 06:39:32,794 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,795 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38239', status: init, memory: 0, processing: 0>
2023-12-01 06:39:32,795 - distributed.worker - INFO - Starting Worker plugin PreImport-3e0c47c6-26e5-4fa3-b8a5-88a06a2b7727
2023-12-01 06:39:32,796 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38239
2023-12-01 06:39:32,796 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43520
2023-12-01 06:39:32,796 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,797 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:32,798 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,798 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,802 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:32,818 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46097', status: init, memory: 0, processing: 0>
2023-12-01 06:39:32,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46097
2023-12-01 06:39:32,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43540
2023-12-01 06:39:32,820 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:32,821 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,821 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:32,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36293', status: init, memory: 0, processing: 0>
2023-12-01 06:39:32,829 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36293
2023-12-01 06:39:32,829 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43526
2023-12-01 06:39:32,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:32,832 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,832 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,841 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:32,851 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34437', status: init, memory: 0, processing: 0>
2023-12-01 06:39:32,851 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34437
2023-12-01 06:39:32,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43542
2023-12-01 06:39:32,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:32,854 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:32,854 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:32,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:32,927 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:32,927 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:32,927 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:32,927 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:32,928 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:32,928 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:32,928 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:32,928 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:32,933 - distributed.scheduler - INFO - Remove client Client-5cd76f4c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:32,933 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58670; closing.
2023-12-01 06:39:32,933 - distributed.scheduler - INFO - Remove client Client-5cd76f4c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:32,934 - distributed.scheduler - INFO - Close client connection: Client-5cd76f4c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:32,934 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34551'. Reason: nanny-close
2023-12-01 06:39:32,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:32,936 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44825'. Reason: nanny-close
2023-12-01 06:39:32,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:32,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36293. Reason: nanny-close
2023-12-01 06:39:32,936 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37435'. Reason: nanny-close
2023-12-01 06:39:32,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:32,937 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36159'. Reason: nanny-close
2023-12-01 06:39:32,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37837. Reason: nanny-close
2023-12-01 06:39:32,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:32,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38239. Reason: nanny-close
2023-12-01 06:39:32,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44011'. Reason: nanny-close
2023-12-01 06:39:32,938 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:32,938 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40461. Reason: nanny-close
2023-12-01 06:39:32,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45093'. Reason: nanny-close
2023-12-01 06:39:32,938 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:32,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33381'. Reason: nanny-close
2023-12-01 06:39:32,939 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43526; closing.
2023-12-01 06:39:32,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:32,939 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:32,939 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34437. Reason: nanny-close
2023-12-01 06:39:32,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:32,939 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36293', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412772.939752')
2023-12-01 06:39:32,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46739'. Reason: nanny-close
2023-12-01 06:39:32,939 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45595. Reason: nanny-close
2023-12-01 06:39:32,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:32,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:32,940 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46097. Reason: nanny-close
2023-12-01 06:39:32,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:32,940 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36927. Reason: nanny-close
2023-12-01 06:39:32,940 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43488; closing.
2023-12-01 06:39:32,941 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43520; closing.
2023-12-01 06:39:32,941 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:32,941 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:32,941 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:32,941 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:32,942 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412772.9419496')
2023-12-01 06:39:32,942 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:32,942 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:32,942 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38239', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412772.9423149')
2023-12-01 06:39:32,942 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43500; closing.
2023-12-01 06:39:32,942 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:32,943 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:32,943 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:32,944 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:32,944 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:32,943 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43488>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43488>: Stream is closed
2023-12-01 06:39:32,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37837', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412772.9450557')
2023-12-01 06:39:32,945 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:32,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43542; closing.
2023-12-01 06:39:32,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43540; closing.
2023-12-01 06:39:32,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43476; closing.
2023-12-01 06:39:32,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34437', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412772.9469')
2023-12-01 06:39:32,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46097', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412772.9473758')
2023-12-01 06:39:32,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412772.9477344')
2023-12-01 06:39:32,948 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43512; closing.
2023-12-01 06:39:32,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412772.9486313')
2023-12-01 06:39:32,948 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:39:32,949 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43512>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-01 06:39:34,502 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:39:34,503 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:39:34,503 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:39:34,504 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:39:34,505 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-01 06:39:36,601 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:36,605 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34197 instead
  warnings.warn(
2023-12-01 06:39:36,608 - distributed.scheduler - INFO - State start
2023-12-01 06:39:36,631 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:36,632 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:39:36,632 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34197/status
2023-12-01 06:39:36,633 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:39:36,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36307'
2023-12-01 06:39:36,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46113'
2023-12-01 06:39:36,675 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39321'
2023-12-01 06:39:36,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40985'
2023-12-01 06:39:36,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44785'
2023-12-01 06:39:36,702 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42013'
2023-12-01 06:39:36,711 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39839'
2023-12-01 06:39:36,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32881'
2023-12-01 06:39:38,078 - distributed.scheduler - INFO - Receive client connection: Client-6427260c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:38,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43666
2023-12-01 06:39:38,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:38,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:38,532 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:38,534 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:38,534 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:38,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:38,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:38,539 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:38,541 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:38,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:38,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:38,576 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:38,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:38,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:38,624 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:38,624 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:38,624 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:38,624 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:38,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:38,628 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:38,628 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:38,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:38,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:38,726 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:41,405 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39461
2023-12-01 06:39:41,405 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39461
2023-12-01 06:39:41,406 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43215
2023-12-01 06:39:41,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:41,406 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:41,406 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:41,406 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:41,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3iu0gg2w
2023-12-01 06:39:41,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c11a045a-be16-4b52-9fec-a780a6beb93d
2023-12-01 06:39:41,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33749
2023-12-01 06:39:41,406 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33749
2023-12-01 06:39:41,407 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42021
2023-12-01 06:39:41,407 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:41,407 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:41,407 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:41,407 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:41,407 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-byzy6vf4
2023-12-01 06:39:41,407 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5359f307-ccb7-410d-acb0-e9d0c7c09dec
2023-12-01 06:39:41,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1086525-4e5e-40b2-bdc6-75ffdf49cdce
2023-12-01 06:39:41,770 - distributed.worker - INFO - Starting Worker plugin PreImport-26619942-60c7-49d8-94c3-0d73f202bcd5
2023-12-01 06:39:41,771 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:41,778 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b460013d-40c9-4006-84de-3ed9baa6abbf
2023-12-01 06:39:41,778 - distributed.worker - INFO - Starting Worker plugin PreImport-0165c046-d6f7-4da4-90df-a346a1c8f5f0
2023-12-01 06:39:41,778 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:41,809 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39461', status: init, memory: 0, processing: 0>
2023-12-01 06:39:41,811 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39461
2023-12-01 06:39:41,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37324
2023-12-01 06:39:41,812 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33749', status: init, memory: 0, processing: 0>
2023-12-01 06:39:41,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:41,813 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33749
2023-12-01 06:39:41,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37316
2023-12-01 06:39:41,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:41,816 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:41,816 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:41,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:41,821 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:41,821 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:41,823 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:42,768 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36761
2023-12-01 06:39:42,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36761
2023-12-01 06:39:42,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35829
2023-12-01 06:39:42,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,770 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,770 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:42,770 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:42,770 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1tya17sb
2023-12-01 06:39:42,770 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f15c7d30-4a3b-4455-924b-614ee5bd0a0b
2023-12-01 06:39:42,781 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35943
2023-12-01 06:39:42,782 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35943
2023-12-01 06:39:42,782 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41349
2023-12-01 06:39:42,782 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,782 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,782 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:42,783 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:42,783 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w21kmgu4
2023-12-01 06:39:42,783 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ee4b7f4-a5a6-41b3-b484-e16a08f9207c
2023-12-01 06:39:42,786 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33495
2023-12-01 06:39:42,787 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33495
2023-12-01 06:39:42,787 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37629
2023-12-01 06:39:42,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,787 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,787 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:42,787 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:42,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8dydstca
2023-12-01 06:39:42,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a3fc3f5-4790-4264-aa3c-08a788dac9db
2023-12-01 06:39:42,789 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45631
2023-12-01 06:39:42,790 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45631
2023-12-01 06:39:42,790 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33733
2023-12-01 06:39:42,790 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,791 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,791 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:42,791 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:42,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1e2kikbr
2023-12-01 06:39:42,791 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df69a684-f40b-4cb8-96ef-c38337622001
2023-12-01 06:39:42,797 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38189
2023-12-01 06:39:42,797 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38189
2023-12-01 06:39:42,797 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42391
2023-12-01 06:39:42,798 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,798 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,798 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:42,798 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:42,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h2xczhzk
2023-12-01 06:39:42,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b743352-de5a-4a5a-a439-373ed42f9e1c
2023-12-01 06:39:42,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0156885-518f-47dc-b8a2-f12d7bd02cc1
2023-12-01 06:39:42,917 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42323
2023-12-01 06:39:42,918 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42323
2023-12-01 06:39:42,918 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39859
2023-12-01 06:39:42,918 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,918 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,918 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:42,918 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:42,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0dfi1x79
2023-12-01 06:39:42,919 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d8ce4471-0610-4441-980d-967b31001b96
2023-12-01 06:39:42,919 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9595bdd3-92ba-4425-91f3-95c3d04f3106
2023-12-01 06:39:42,929 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7bbfd61-a19c-41af-acfc-45a822e3a32f
2023-12-01 06:39:42,930 - distributed.worker - INFO - Starting Worker plugin PreImport-cbb23ad1-cb82-4e66-aa4b-6cd5e5a4c211
2023-12-01 06:39:42,930 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,934 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de7fad73-e404-4414-b93a-bcc9a9719211
2023-12-01 06:39:42,934 - distributed.worker - INFO - Starting Worker plugin PreImport-5c7b3627-4c13-4dbe-9c4f-54a222dc9836
2023-12-01 06:39:42,934 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,937 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d344693-71aa-4e3e-9d9c-d7667b63a1d2
2023-12-01 06:39:42,937 - distributed.worker - INFO - Starting Worker plugin PreImport-2ecc5312-d304-4102-8391-1b375951bee0
2023-12-01 06:39:42,937 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,938 - distributed.worker - INFO - Starting Worker plugin PreImport-62eaeedb-acd2-449d-baef-2d16551e1e62
2023-12-01 06:39:42,938 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4fec635-af7c-4237-b5b4-c0be89a5c2c2
2023-12-01 06:39:42,940 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,951 - distributed.worker - INFO - Starting Worker plugin PreImport-58e546b9-b973-42b3-a6ea-297b567b3166
2023-12-01 06:39:42,952 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,963 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35943', status: init, memory: 0, processing: 0>
2023-12-01 06:39:42,963 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35943
2023-12-01 06:39:42,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37350
2023-12-01 06:39:42,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:42,968 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33495', status: init, memory: 0, processing: 0>
2023-12-01 06:39:42,968 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,968 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,968 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33495
2023-12-01 06:39:42,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37352
2023-12-01 06:39:42,969 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36761', status: init, memory: 0, processing: 0>
2023-12-01 06:39:42,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:42,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:42,970 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36761
2023-12-01 06:39:42,970 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37344
2023-12-01 06:39:42,971 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:42,973 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,973 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:42,981 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,981 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,982 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45631', status: init, memory: 0, processing: 0>
2023-12-01 06:39:42,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:42,983 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45631
2023-12-01 06:39:42,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37362
2023-12-01 06:39:42,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:42,988 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38189', status: init, memory: 0, processing: 0>
2023-12-01 06:39:42,988 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38189
2023-12-01 06:39:42,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37372
2023-12-01 06:39:42,990 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:42,994 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,994 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:42,996 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:42,999 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:42,999 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:43,001 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:43,015 - distributed.worker - INFO - Starting Worker plugin PreImport-bb41aff0-5d21-4e47-92f1-d5a18c290175
2023-12-01 06:39:43,015 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:43,042 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42323', status: init, memory: 0, processing: 0>
2023-12-01 06:39:43,043 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42323
2023-12-01 06:39:43,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37382
2023-12-01 06:39:43,044 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:43,044 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:43,044 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:43,049 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:43,117 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:43,117 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:43,118 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:43,118 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:43,118 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:43,118 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:43,118 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:43,118 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:43,123 - distributed.scheduler - INFO - Remove client Client-6427260c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:43,123 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43666; closing.
2023-12-01 06:39:43,123 - distributed.scheduler - INFO - Remove client Client-6427260c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:43,123 - distributed.scheduler - INFO - Close client connection: Client-6427260c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:43,125 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36307'. Reason: nanny-close
2023-12-01 06:39:43,125 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:43,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46113'. Reason: nanny-close
2023-12-01 06:39:43,126 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:43,126 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45631. Reason: nanny-close
2023-12-01 06:39:43,126 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39321'. Reason: nanny-close
2023-12-01 06:39:43,126 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:43,127 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33749. Reason: nanny-close
2023-12-01 06:39:43,127 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40985'. Reason: nanny-close
2023-12-01 06:39:43,127 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:43,127 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39461. Reason: nanny-close
2023-12-01 06:39:43,128 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44785'. Reason: nanny-close
2023-12-01 06:39:43,128 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:43,128 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33495. Reason: nanny-close
2023-12-01 06:39:43,128 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42013'. Reason: nanny-close
2023-12-01 06:39:43,128 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:43,129 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36761. Reason: nanny-close
2023-12-01 06:39:43,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39839'. Reason: nanny-close
2023-12-01 06:39:43,129 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:43,129 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37362; closing.
2023-12-01 06:39:43,129 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:43,129 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:43,129 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38189. Reason: nanny-close
2023-12-01 06:39:43,129 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32881'. Reason: nanny-close
2023-12-01 06:39:43,129 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45631', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412783.1298323')
2023-12-01 06:39:43,129 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:43,129 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:43,130 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:43,130 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35943. Reason: nanny-close
2023-12-01 06:39:43,130 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37352; closing.
2023-12-01 06:39:43,130 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42323. Reason: nanny-close
2023-12-01 06:39:43,131 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33495', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412783.1309857')
2023-12-01 06:39:43,131 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:43,131 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:43,131 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37324; closing.
2023-12-01 06:39:43,131 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:43,131 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37316; closing.
2023-12-01 06:39:43,131 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:43,131 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:43,131 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:43,131 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:43,132 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:43,132 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412783.1323702')
2023-12-01 06:39:43,132 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33749', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412783.1327295')
2023-12-01 06:39:43,133 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:43,133 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:43,133 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:43,133 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:43,133 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37352>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-01 06:39:43,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37344; closing.
2023-12-01 06:39:43,135 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36761', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412783.135899')
2023-12-01 06:39:43,136 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37372; closing.
2023-12-01 06:39:43,136 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37350; closing.
2023-12-01 06:39:43,136 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37382; closing.
2023-12-01 06:39:43,142 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38189', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412783.142637')
2023-12-01 06:39:43,143 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412783.143117')
2023-12-01 06:39:43,143 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42323', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412783.1435633')
2023-12-01 06:39:43,143 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:39:44,842 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:39:44,843 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:39:44,843 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:39:44,844 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:39:44,845 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-01 06:39:46,965 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:46,969 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:39:46,972 - distributed.scheduler - INFO - State start
2023-12-01 06:39:46,992 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:46,993 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:39:46,994 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:39:46,994 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:39:47,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43669'
2023-12-01 06:39:47,377 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33211'
2023-12-01 06:39:47,389 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35791'
2023-12-01 06:39:47,404 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34175'
2023-12-01 06:39:47,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39113'
2023-12-01 06:39:47,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35563'
2023-12-01 06:39:47,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34641'
2023-12-01 06:39:47,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46613'
2023-12-01 06:39:47,561 - distributed.scheduler - INFO - Receive client connection: Client-6a5f22cd-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:47,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37544
2023-12-01 06:39:49,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:49,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:49,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:49,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:49,219 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:49,219 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:49,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:49,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:49,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:49,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:49,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:49,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:49,277 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:49,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:49,282 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:49,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:49,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:49,287 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:49,289 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:49,289 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:49,294 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:49,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:49,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:49,313 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:51,826 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34687
2023-12-01 06:39:51,826 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34687
2023-12-01 06:39:51,827 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40619
2023-12-01 06:39:51,827 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:51,827 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:51,827 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:51,827 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:51,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dusassc8
2023-12-01 06:39:51,827 - distributed.worker - INFO - Starting Worker plugin PreImport-8e50b53f-e4c1-4bfc-b68e-8b62d993fc29
2023-12-01 06:39:51,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8c6772e-3a4c-4529-a3e9-f6faa396bd3e
2023-12-01 06:39:51,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35999f7b-e77b-49d9-8f5b-d4ff372c6cef
2023-12-01 06:39:51,939 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35525
2023-12-01 06:39:51,940 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35525
2023-12-01 06:39:51,940 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38015
2023-12-01 06:39:51,940 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:51,940 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:51,940 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:51,940 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:51,940 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tqsb7gh3
2023-12-01 06:39:51,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7bf0300d-05e5-48ad-9a6e-6834637606e6
2023-12-01 06:39:53,169 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33513
2023-12-01 06:39:53,170 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33513
2023-12-01 06:39:53,170 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37769
2023-12-01 06:39:53,170 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:53,170 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,171 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:53,171 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:53,171 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gebipcq5
2023-12-01 06:39:53,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-75bb2592-d9af-492c-b6a6-e0685c622880
2023-12-01 06:39:53,233 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37735
2023-12-01 06:39:53,234 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37735
2023-12-01 06:39:53,234 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39135
2023-12-01 06:39:53,234 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:53,235 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,235 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:53,235 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:53,235 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mqejoyhf
2023-12-01 06:39:53,235 - distributed.worker - INFO - Starting Worker plugin PreImport-bc81cfcb-1400-4006-968b-231317868499
2023-12-01 06:39:53,235 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d2dae85-b8b4-4233-9519-154ac8f299db
2023-12-01 06:39:53,240 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33631
2023-12-01 06:39:53,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33631
2023-12-01 06:39:53,241 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44945
2023-12-01 06:39:53,241 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:53,241 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,241 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:53,241 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:53,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lkbyorog
2023-12-01 06:39:53,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29466e5a-2321-48da-b0e7-cd70083d2320
2023-12-01 06:39:53,242 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e88b779-1915-4ad5-a686-693bcc3ab4e7
2023-12-01 06:39:53,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46643
2023-12-01 06:39:53,249 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46643
2023-12-01 06:39:53,249 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41619
2023-12-01 06:39:53,249 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:53,249 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,249 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:53,249 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:53,249 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jzmsbub8
2023-12-01 06:39:53,250 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-655fc1a5-d46c-4137-adf3-878d0f37b053
2023-12-01 06:39:53,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81a54d35-6bcd-4f24-a729-c9c246717057
2023-12-01 06:39:53,251 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39875
2023-12-01 06:39:53,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39875
2023-12-01 06:39:53,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36143
2023-12-01 06:39:53,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:53,257 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,257 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:53,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:53,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wztftjyy
2023-12-01 06:39:53,258 - distributed.worker - INFO - Starting Worker plugin PreImport-a0b681d7-077b-475b-b424-9a55a1224287
2023-12-01 06:39:53,258 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac05effb-783c-4a6c-a160-15a66bbdf0d6
2023-12-01 06:39:53,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f42121d-edbd-4ee5-905c-5fa80d2ed676
2023-12-01 06:39:53,409 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45947
2023-12-01 06:39:53,410 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45947
2023-12-01 06:39:53,410 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35635
2023-12-01 06:39:53,410 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:53,410 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,410 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:53,411 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:53,411 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x_d1h7k8
2023-12-01 06:39:53,411 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5b3bcac-5d3a-4e6a-89f9-2f3221ddd6b6
2023-12-01 06:39:53,428 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,468 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34687', status: init, memory: 0, processing: 0>
2023-12-01 06:39:53,470 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34687
2023-12-01 06:39:53,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46210
2023-12-01 06:39:53,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:53,474 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:53,475 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,476 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:53,744 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff44480b-74ad-4c23-ac0a-f79e21fd1cbf
2023-12-01 06:39:53,746 - distributed.worker - INFO - Starting Worker plugin PreImport-ebf6815d-020f-4b8f-8a7b-632fee0132db
2023-12-01 06:39:53,746 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,788 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35525', status: init, memory: 0, processing: 0>
2023-12-01 06:39:53,788 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35525
2023-12-01 06:39:53,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46212
2023-12-01 06:39:53,790 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:53,796 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:53,796 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:53,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:53,821 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c238f2fa-bf01-42c4-adfc-323fd36168f1
2023-12-01 06:39:53,822 - distributed.worker - INFO - Starting Worker plugin PreImport-a0b535d5-c539-4303-a9ab-00e22ff55810
2023-12-01 06:39:53,822 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,003 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33513', status: init, memory: 0, processing: 0>
2023-12-01 06:39:54,004 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33513
2023-12-01 06:39:54,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46214
2023-12-01 06:39:54,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:54,015 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:54,015 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:54,062 - distributed.worker - INFO - Starting Worker plugin PreImport-f518942c-9935-4629-9398-f3fb643707f2
2023-12-01 06:39:54,063 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,098 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46643', status: init, memory: 0, processing: 0>
2023-12-01 06:39:54,099 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46643
2023-12-01 06:39:54,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46220
2023-12-01 06:39:54,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:54,103 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:54,104 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:54,108 - distributed.worker - INFO - Starting Worker plugin PreImport-e97b7f90-0743-4ed6-81c9-d4df8abc41df
2023-12-01 06:39:54,109 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,116 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6233bd16-51d6-463d-a799-165f1bed0ae1
2023-12-01 06:39:54,117 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,125 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-935e086f-51ce-44f4-9a08-238f07579a62
2023-12-01 06:39:54,125 - distributed.worker - INFO - Starting Worker plugin PreImport-d987308f-6963-4b14-803d-6712369d2f06
2023-12-01 06:39:54,126 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,146 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,155 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33631', status: init, memory: 0, processing: 0>
2023-12-01 06:39:54,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33631
2023-12-01 06:39:54,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46224
2023-12-01 06:39:54,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:54,162 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37735', status: init, memory: 0, processing: 0>
2023-12-01 06:39:54,163 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37735
2023-12-01 06:39:54,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46228
2023-12-01 06:39:54,164 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:54,167 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:54,167 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:54,173 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:54,173 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,173 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45947', status: init, memory: 0, processing: 0>
2023-12-01 06:39:54,174 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45947
2023-12-01 06:39:54,174 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46244
2023-12-01 06:39:54,175 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:54,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:54,177 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39875', status: init, memory: 0, processing: 0>
2023-12-01 06:39:54,177 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39875
2023-12-01 06:39:54,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46254
2023-12-01 06:39:54,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:54,179 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:54,179 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:54,183 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:54,183 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:54,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:54,192 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:54,192 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:54,192 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:54,192 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:54,193 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:54,193 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:54,193 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:54,193 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:54,203 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:54,203 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:54,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:54,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:54,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:54,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:54,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:54,204 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:54,211 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:54,212 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:54,215 - distributed.scheduler - INFO - Remove client Client-6a5f22cd-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:54,215 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37544; closing.
2023-12-01 06:39:54,215 - distributed.scheduler - INFO - Remove client Client-6a5f22cd-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:54,216 - distributed.scheduler - INFO - Close client connection: Client-6a5f22cd-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:39:54,218 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43669'. Reason: nanny-close
2023-12-01 06:39:54,219 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33211'. Reason: nanny-close
2023-12-01 06:39:54,219 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:54,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35791'. Reason: nanny-close
2023-12-01 06:39:54,220 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:54,220 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35525. Reason: nanny-close
2023-12-01 06:39:54,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34175'. Reason: nanny-close
2023-12-01 06:39:54,220 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:54,221 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34687. Reason: nanny-close
2023-12-01 06:39:54,221 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39113'. Reason: nanny-close
2023-12-01 06:39:54,221 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:54,221 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39875. Reason: nanny-close
2023-12-01 06:39:54,221 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35563'. Reason: nanny-close
2023-12-01 06:39:54,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:54,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34641'. Reason: nanny-close
2023-12-01 06:39:54,222 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33631. Reason: nanny-close
2023-12-01 06:39:54,222 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46210; closing.
2023-12-01 06:39:54,222 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:54,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:54,222 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:54,222 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33513. Reason: nanny-close
2023-12-01 06:39:54,223 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34687', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412794.2229216')
2023-12-01 06:39:54,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46613'. Reason: nanny-close
2023-12-01 06:39:54,223 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:54,223 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:54,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46643. Reason: nanny-close
2023-12-01 06:39:54,223 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46212; closing.
2023-12-01 06:39:54,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45947. Reason: nanny-close
2023-12-01 06:39:54,223 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35525', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412794.2239175')
2023-12-01 06:39:54,224 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:54,224 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:54,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46254; closing.
2023-12-01 06:39:54,225 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:54,225 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:54,225 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:54,225 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:54,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:54,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:54,226 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37735. Reason: nanny-close
2023-12-01 06:39:54,225 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46212>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-01 06:39:54,226 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:54,226 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412794.2268274')
2023-12-01 06:39:54,227 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:54,227 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:54,227 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46224; closing.
2023-12-01 06:39:54,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46214; closing.
2023-12-01 06:39:54,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46220; closing.
2023-12-01 06:39:54,228 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:54,228 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:54,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33631', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412794.228983')
2023-12-01 06:39:54,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412794.2292814')
2023-12-01 06:39:54,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46643', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412794.2296097')
2023-12-01 06:39:54,229 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46244; closing.
2023-12-01 06:39:54,230 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:54,230 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45947', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412794.2304385')
2023-12-01 06:39:54,230 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46228; closing.
2023-12-01 06:39:54,231 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37735', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412794.2311718')
2023-12-01 06:39:54,231 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:39:54,231 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46228>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-01 06:39:56,085 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:39:56,085 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:39:56,086 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:39:56,087 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:39:56,087 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-12-01 06:39:58,228 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:58,232 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:39:58,236 - distributed.scheduler - INFO - State start
2023-12-01 06:39:58,257 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:58,258 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:39:58,259 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:39:58,259 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:39:58,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39441'
2023-12-01 06:39:58,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36567'
2023-12-01 06:39:58,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46335'
2023-12-01 06:39:58,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45633'
2023-12-01 06:39:58,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35647'
2023-12-01 06:39:58,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35283'
2023-12-01 06:39:58,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33357'
2023-12-01 06:39:58,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33193'
2023-12-01 06:40:00,358 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:00,358 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:00,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:00,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:00,362 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:00,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:00,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:00,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:00,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:00,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:00,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:00,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:00,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:00,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:00,397 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:00,397 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:00,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:00,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:00,400 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:00,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:00,405 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:00,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:00,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:00,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:01,515 - distributed.scheduler - INFO - Receive client connection: Client-710bacd4-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:01,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33754
2023-12-01 06:40:03,108 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40327
2023-12-01 06:40:03,109 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40327
2023-12-01 06:40:03,109 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39167
2023-12-01 06:40:03,109 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,109 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,109 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:03,109 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:03,109 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bf48h4uv
2023-12-01 06:40:03,110 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a4d8cb6-89b4-4402-b14f-e732ed4a336b
2023-12-01 06:40:03,115 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46835
2023-12-01 06:40:03,116 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46835
2023-12-01 06:40:03,116 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43139
2023-12-01 06:40:03,116 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,116 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,116 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:03,116 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:03,116 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7saixodm
2023-12-01 06:40:03,117 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c32431ca-3713-4fc9-b7c9-0e660fa5f2a1
2023-12-01 06:40:03,157 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33721
2023-12-01 06:40:03,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33721
2023-12-01 06:40:03,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43109
2023-12-01 06:40:03,158 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,158 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,158 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:03,158 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:03,158 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tf8d3tqh
2023-12-01 06:40:03,159 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6b43902-b44e-4485-bab1-79bbd8e9716a
2023-12-01 06:40:03,247 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38771
2023-12-01 06:40:03,247 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38771
2023-12-01 06:40:03,248 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33125
2023-12-01 06:40:03,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,248 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,248 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:03,248 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:03,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aimlie0o
2023-12-01 06:40:03,248 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ae33f38-aee2-4872-81b2-8ca7b13d1271
2023-12-01 06:40:03,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34273
2023-12-01 06:40:03,249 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34273
2023-12-01 06:40:03,249 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42665
2023-12-01 06:40:03,249 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,249 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,249 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:03,249 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:03,249 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_lcoshb0
2023-12-01 06:40:03,250 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e0c8aaf-a1d4-45f0-904f-e50280ce50f8
2023-12-01 06:40:03,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4ab5679-35a2-4fc8-b8b3-a74ea9bc5bda
2023-12-01 06:40:03,261 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39993
2023-12-01 06:40:03,262 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39993
2023-12-01 06:40:03,262 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38243
2023-12-01 06:40:03,262 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,262 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,262 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:03,263 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:03,263 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l04hwav1
2023-12-01 06:40:03,263 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f313961-50ea-4019-879b-fcc304b87371
2023-12-01 06:40:03,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40043
2023-12-01 06:40:03,267 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40043
2023-12-01 06:40:03,267 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34067
2023-12-01 06:40:03,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,267 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45343
2023-12-01 06:40:03,267 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45343
2023-12-01 06:40:03,267 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:03,267 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44421
2023-12-01 06:40:03,267 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:03,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-94_20top
2023-12-01 06:40:03,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,267 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,267 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:03,268 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:03,268 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rycjw05k
2023-12-01 06:40:03,268 - distributed.worker - INFO - Starting Worker plugin PreImport-86a10d6b-9cbe-42b6-a909-21d64667bb33
2023-12-01 06:40:03,268 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c60ac641-4d71-49da-99ee-2b1f5f1edd47
2023-12-01 06:40:03,268 - distributed.worker - INFO - Starting Worker plugin RMMSetup-793bb7f2-8db7-4c62-a45a-362b1de13d2b
2023-12-01 06:40:03,268 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae6f6d64-a6a5-4289-9d61-198c9f4b0872
2023-12-01 06:40:03,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-930e14bf-efc5-4bea-bf0d-a15a5d0f72c6
2023-12-01 06:40:03,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bbc2a10a-2ad0-4b1a-89d6-c7f6e4682b6b
2023-12-01 06:40:03,343 - distributed.worker - INFO - Starting Worker plugin PreImport-dc7fa83d-c6a1-4222-b895-ca664ac70445
2023-12-01 06:40:03,343 - distributed.worker - INFO - Starting Worker plugin PreImport-4dc9786c-c460-4ce4-babd-2df6c93482c4
2023-12-01 06:40:03,343 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,343 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,348 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-702b3c7b-1c7b-40e5-8262-a17b5bd5db48
2023-12-01 06:40:03,349 - distributed.worker - INFO - Starting Worker plugin PreImport-bc7f4560-45cb-4b20-a09e-6f2f38a5cf9d
2023-12-01 06:40:03,349 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,377 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46835', status: init, memory: 0, processing: 0>
2023-12-01 06:40:03,379 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46835
2023-12-01 06:40:03,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33780
2023-12-01 06:40:03,380 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40327', status: init, memory: 0, processing: 0>
2023-12-01 06:40:03,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:03,381 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40327
2023-12-01 06:40:03,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33782
2023-12-01 06:40:03,381 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,381 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:03,386 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:03,386 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,386 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:03,400 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33721', status: init, memory: 0, processing: 0>
2023-12-01 06:40:03,400 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33721
2023-12-01 06:40:03,400 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33794
2023-12-01 06:40:03,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:03,411 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,411 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:03,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81a07699-17c0-419a-8954-e2223226350e
2023-12-01 06:40:03,441 - distributed.worker - INFO - Starting Worker plugin PreImport-876a353a-91de-457d-965e-cd1703d3e136
2023-12-01 06:40:03,441 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,456 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11657959-ddc3-4609-b466-e1ae65123c79
2023-12-01 06:40:03,457 - distributed.worker - INFO - Starting Worker plugin PreImport-282aebf2-5457-405b-8bcf-0635899dbd63
2023-12-01 06:40:03,457 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,462 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,462 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca4fbf06-899a-44ad-b08c-b6c1c8b8285c
2023-12-01 06:40:03,463 - distributed.worker - INFO - Starting Worker plugin PreImport-8b0a626c-e2b5-458e-9475-b23aff48934b
2023-12-01 06:40:03,463 - distributed.worker - INFO - Starting Worker plugin PreImport-84ee070f-295f-4d30-b4a9-692bed5de536
2023-12-01 06:40:03,463 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,464 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,486 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38771', status: init, memory: 0, processing: 0>
2023-12-01 06:40:03,486 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38771
2023-12-01 06:40:03,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33806
2023-12-01 06:40:03,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:03,490 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39993', status: init, memory: 0, processing: 0>
2023-12-01 06:40:03,491 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39993
2023-12-01 06:40:03,491 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33816
2023-12-01 06:40:03,492 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:03,493 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40043', status: init, memory: 0, processing: 0>
2023-12-01 06:40:03,493 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40043
2023-12-01 06:40:03,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33830
2023-12-01 06:40:03,494 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,494 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,494 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:03,495 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,495 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,496 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:03,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:03,498 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,498 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,499 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:03,503 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34273', status: init, memory: 0, processing: 0>
2023-12-01 06:40:03,503 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34273
2023-12-01 06:40:03,503 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33846
2023-12-01 06:40:03,505 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:03,510 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45343', status: init, memory: 0, processing: 0>
2023-12-01 06:40:03,510 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45343
2023-12-01 06:40:03,510 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33860
2023-12-01 06:40:03,512 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,512 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:03,514 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:03,520 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:03,520 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:03,523 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:03,538 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,539 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,552 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:40:03,552 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:40:03,553 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:40:03,553 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:40:03,553 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:40:03,553 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:40:03,553 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:40:03,553 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:40:03,561 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,563 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:03,566 - distributed.scheduler - INFO - Remove client Client-710bacd4-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:03,566 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33754; closing.
2023-12-01 06:40:03,567 - distributed.scheduler - INFO - Remove client Client-710bacd4-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:03,567 - distributed.scheduler - INFO - Close client connection: Client-710bacd4-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:03,568 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39441'. Reason: nanny-close
2023-12-01 06:40:03,569 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:03,569 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36567'. Reason: nanny-close
2023-12-01 06:40:03,569 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:03,570 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46335'. Reason: nanny-close
2023-12-01 06:40:03,570 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45343. Reason: nanny-close
2023-12-01 06:40:03,570 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:03,570 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34273. Reason: nanny-close
2023-12-01 06:40:03,570 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45633'. Reason: nanny-close
2023-12-01 06:40:03,571 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:03,571 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40327. Reason: nanny-close
2023-12-01 06:40:03,571 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35647'. Reason: nanny-close
2023-12-01 06:40:03,571 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:03,572 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46835. Reason: nanny-close
2023-12-01 06:40:03,572 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35283'. Reason: nanny-close
2023-12-01 06:40:03,572 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:03,572 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38771. Reason: nanny-close
2023-12-01 06:40:03,572 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33357'. Reason: nanny-close
2023-12-01 06:40:03,573 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:03,573 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:03,573 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33721. Reason: nanny-close
2023-12-01 06:40:03,573 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:03,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33782; closing.
2023-12-01 06:40:03,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33193'. Reason: nanny-close
2023-12-01 06:40:03,573 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:03,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33846; closing.
2023-12-01 06:40:03,573 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:03,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33860; closing.
2023-12-01 06:40:03,573 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:03,573 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40043. Reason: nanny-close
2023-12-01 06:40:03,573 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412803.5738952')
2023-12-01 06:40:03,574 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39993. Reason: nanny-close
2023-12-01 06:40:03,574 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34273', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412803.5744932')
2023-12-01 06:40:03,574 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:03,574 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:03,574 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45343', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412803.5748756')
2023-12-01 06:40:03,575 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:03,575 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:03,575 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:03,575 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:03,575 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:03,575 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33780; closing.
2023-12-01 06:40:03,576 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:03,576 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:03,576 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33806; closing.
2023-12-01 06:40:03,576 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412803.5769103')
2023-12-01 06:40:03,577 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:03,577 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:03,577 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:03,577 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38771', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412803.5779068')
2023-12-01 06:40:03,578 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33794; closing.
2023-12-01 06:40:03,578 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33830; closing.
2023-12-01 06:40:03,578 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33816; closing.
2023-12-01 06:40:03,578 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412803.5789225')
2023-12-01 06:40:03,579 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412803.579271')
2023-12-01 06:40:03,579 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412803.5796115')
2023-12-01 06:40:03,579 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:40:06,639 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:40:06,639 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:40:06,640 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:40:06,641 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:40:06,641 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-01 06:40:08,985 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:08,991 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:40:08,996 - distributed.scheduler - INFO - State start
2023-12-01 06:40:09,020 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:09,022 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:40:09,023 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:40:09,023 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:40:09,198 - distributed.scheduler - INFO - Receive client connection: Client-775370e1-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:09,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33964
2023-12-01 06:40:09,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42065'
2023-12-01 06:40:09,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41453'
2023-12-01 06:40:09,315 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43747'
2023-12-01 06:40:09,336 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38679'
2023-12-01 06:40:09,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35851'
2023-12-01 06:40:09,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45541'
2023-12-01 06:40:09,365 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32937'
2023-12-01 06:40:09,379 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40041'
2023-12-01 06:40:11,182 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:11,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:11,186 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:11,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:11,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:11,225 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:11,225 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:11,227 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:11,227 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:11,227 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:11,229 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:11,231 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:11,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:11,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:11,291 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:11,291 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:11,292 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:11,295 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:11,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:11,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:11,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:11,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:11,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:11,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:13,864 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34271
2023-12-01 06:40:13,865 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34271
2023-12-01 06:40:13,865 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34623
2023-12-01 06:40:13,865 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:13,865 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:13,865 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:13,865 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:13,865 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q1m5rtqm
2023-12-01 06:40:13,866 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ab440d1-e8ea-4d5a-aa78-7aadb8fb7180
2023-12-01 06:40:14,002 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53c8ca20-b61a-4c01-89a3-7b975929be3e
2023-12-01 06:40:14,002 - distributed.worker - INFO - Starting Worker plugin PreImport-8930584c-14d5-455c-89a9-4843d75913e6
2023-12-01 06:40:14,002 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,033 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34271', status: init, memory: 0, processing: 0>
2023-12-01 06:40:14,035 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34271
2023-12-01 06:40:14,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55108
2023-12-01 06:40:14,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:14,036 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,037 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:14,270 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33385
2023-12-01 06:40:14,271 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33385
2023-12-01 06:40:14,271 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32925
2023-12-01 06:40:14,271 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,271 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,271 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:14,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:14,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8wl01mjj
2023-12-01 06:40:14,272 - distributed.worker - INFO - Starting Worker plugin PreImport-14619999-6aa8-47b3-b680-6d3e18245baa
2023-12-01 06:40:14,272 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2bf104c4-10b5-4eaf-a96b-e0aa370cefd2
2023-12-01 06:40:14,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-578cb6b6-546f-44e0-9bef-da00fdc4f35f
2023-12-01 06:40:14,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41641
2023-12-01 06:40:14,273 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41641
2023-12-01 06:40:14,273 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36857
2023-12-01 06:40:14,273 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,273 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,273 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:14,273 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:14,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4hya6kc9
2023-12-01 06:40:14,274 - distributed.worker - INFO - Starting Worker plugin PreImport-834e723c-7e90-400f-b9bf-b98bc5dc0f07
2023-12-01 06:40:14,274 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1894fab5-327d-4054-bf90-794712610da0
2023-12-01 06:40:14,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8300e0f1-e20b-4189-8915-59173c7a4aff
2023-12-01 06:40:14,302 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43309
2023-12-01 06:40:14,303 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43309
2023-12-01 06:40:14,303 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38935
2023-12-01 06:40:14,303 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,303 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,303 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:14,303 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:14,303 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d4v09hzc
2023-12-01 06:40:14,304 - distributed.worker - INFO - Starting Worker plugin PreImport-aa320918-afbe-4c7d-8928-06652fbbbe4a
2023-12-01 06:40:14,304 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53a978a3-3747-4a1d-9e2d-dd516dffe4dc
2023-12-01 06:40:14,305 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34515
2023-12-01 06:40:14,306 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34515
2023-12-01 06:40:14,306 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42911
2023-12-01 06:40:14,306 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,306 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,306 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:14,306 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:14,306 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-10ts7w1k
2023-12-01 06:40:14,307 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5121a0d-254f-48c0-95bc-a5aa310c97bd
2023-12-01 06:40:14,307 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbbc3c2c-ee96-419d-b4d3-a0174a4b807f
2023-12-01 06:40:14,315 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43351
2023-12-01 06:40:14,316 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43351
2023-12-01 06:40:14,316 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34441
2023-12-01 06:40:14,316 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,316 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,316 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:14,317 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:14,317 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6dtjvu9f
2023-12-01 06:40:14,317 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5f70be2-a0fb-4ed0-87d2-effbc0560d51
2023-12-01 06:40:14,355 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37567
2023-12-01 06:40:14,355 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37567
2023-12-01 06:40:14,356 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33023
2023-12-01 06:40:14,356 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,356 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,356 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:14,356 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:14,356 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n6tj6djg
2023-12-01 06:40:14,356 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8bd0779d-0842-4954-a26f-fc1d287d5860
2023-12-01 06:40:14,361 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44307
2023-12-01 06:40:14,362 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44307
2023-12-01 06:40:14,362 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34287
2023-12-01 06:40:14,362 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,362 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,362 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:14,362 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:14,363 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qohuyx4f
2023-12-01 06:40:14,363 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7092f4f-0a4f-44f0-beab-9f3e6f22d512
2023-12-01 06:40:14,365 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b1603cb-6544-43db-93f7-352b5923a41d
2023-12-01 06:40:14,490 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f07447c5-a82d-46f6-b5de-811a848a84c1
2023-12-01 06:40:14,491 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,491 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,493 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35a43fe6-198a-4731-843a-a79ca0b14ffb
2023-12-01 06:40:14,493 - distributed.worker - INFO - Starting Worker plugin PreImport-b136ca54-689f-48e1-84ec-64ea9d8462e0
2023-12-01 06:40:14,493 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,493 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,493 - distributed.worker - INFO - Starting Worker plugin PreImport-f38ea989-7b56-49df-a9ae-b81b1a90b459
2023-12-01 06:40:14,494 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,494 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4cd707df-f312-4291-b0c3-dcd8c46188a2
2023-12-01 06:40:14,494 - distributed.worker - INFO - Starting Worker plugin PreImport-bcbaa4cd-4e8f-441a-9a32-68637b887804
2023-12-01 06:40:14,495 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,495 - distributed.worker - INFO - Starting Worker plugin PreImport-fe3695fc-b6a0-4fbf-b9bf-2f9ee82b4d71
2023-12-01 06:40:14,496 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,521 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43309', status: init, memory: 0, processing: 0>
2023-12-01 06:40:14,522 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43309
2023-12-01 06:40:14,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55112
2023-12-01 06:40:14,523 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43351', status: init, memory: 0, processing: 0>
2023-12-01 06:40:14,523 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:14,523 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43351
2023-12-01 06:40:14,523 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55128
2023-12-01 06:40:14,524 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,524 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,524 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:14,525 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34515', status: init, memory: 0, processing: 0>
2023-12-01 06:40:14,525 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,525 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,525 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34515
2023-12-01 06:40:14,525 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55136
2023-12-01 06:40:14,526 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:14,527 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,527 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:14,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:14,530 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33385', status: init, memory: 0, processing: 0>
2023-12-01 06:40:14,530 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33385
2023-12-01 06:40:14,530 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55124
2023-12-01 06:40:14,531 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:14,532 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:14,532 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44307', status: init, memory: 0, processing: 0>
2023-12-01 06:40:14,533 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44307
2023-12-01 06:40:14,533 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55148
2023-12-01 06:40:14,534 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37567', status: init, memory: 0, processing: 0>
2023-12-01 06:40:14,534 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37567
2023-12-01 06:40:14,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55162
2023-12-01 06:40:14,536 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:14,536 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41641', status: init, memory: 0, processing: 0>
2023-12-01 06:40:14,537 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41641
2023-12-01 06:40:14,537 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55138
2023-12-01 06:40:14,538 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,538 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:14,539 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,539 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,540 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:14,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:14,541 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:14,541 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:14,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:14,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:14,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:14,564 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:40:14,564 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:40:14,564 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:40:14,564 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:40:14,565 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:40:14,565 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:40:14,565 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:40:14,565 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:40:14,569 - distributed.scheduler - INFO - Remove client Client-775370e1-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:14,569 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33964; closing.
2023-12-01 06:40:14,570 - distributed.scheduler - INFO - Remove client Client-775370e1-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:14,570 - distributed.scheduler - INFO - Close client connection: Client-775370e1-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:14,571 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42065'. Reason: nanny-close
2023-12-01 06:40:14,571 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41453'. Reason: nanny-close
2023-12-01 06:40:14,571 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:14,572 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43747'. Reason: nanny-close
2023-12-01 06:40:14,573 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34271. Reason: nanny-close
2023-12-01 06:40:14,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38679'. Reason: nanny-close
2023-12-01 06:40:14,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35851'. Reason: nanny-close
2023-12-01 06:40:14,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45541'. Reason: nanny-close
2023-12-01 06:40:14,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32937'. Reason: nanny-close
2023-12-01 06:40:14,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40041'. Reason: nanny-close
2023-12-01 06:40:14,575 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:14,575 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55108; closing.
2023-12-01 06:40:14,575 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412814.5757108')
2023-12-01 06:40:14,577 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:14,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:14,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:14,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43309. Reason: nanny-close
2023-12-01 06:40:14,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:14,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41641. Reason: nanny-close
2023-12-01 06:40:14,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:14,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33385. Reason: nanny-close
2023-12-01 06:40:14,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:14,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:14,581 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55112; closing.
2023-12-01 06:40:14,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34515. Reason: nanny-close
2023-12-01 06:40:14,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:14,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43351. Reason: nanny-close
2023-12-01 06:40:14,581 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43309', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412814.5816467')
2023-12-01 06:40:14,582 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55138; closing.
2023-12-01 06:40:14,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:14,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44307. Reason: nanny-close
2023-12-01 06:40:14,582 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41641', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412814.5824428')
2023-12-01 06:40:14,582 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:14,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:14,582 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:14,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:14,584 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:14,584 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:14,584 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:14,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55124; closing.
2023-12-01 06:40:14,584 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:14,584 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:14,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55136; closing.
2023-12-01 06:40:14,584 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:14,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55128; closing.
2023-12-01 06:40:14,585 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37567. Reason: nanny-close
2023-12-01 06:40:14,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412814.5855236')
2023-12-01 06:40:14,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34515', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412814.58587')
2023-12-01 06:40:14,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43351', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412814.5862937')
2023-12-01 06:40:14,586 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:14,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55148; closing.
2023-12-01 06:40:14,587 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:14,587 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44307', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412814.587505')
2023-12-01 06:40:14,588 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55162; closing.
2023-12-01 06:40:14,588 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412814.5885482')
2023-12-01 06:40:14,588 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:40:14,589 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:14,589 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55162>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-01 06:40:16,289 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:40:16,290 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:40:16,290 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:40:16,292 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:40:16,293 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-12-01 06:40:18,631 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:18,635 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:40:18,639 - distributed.scheduler - INFO - State start
2023-12-01 06:40:19,044 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:19,046 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:40:19,046 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:40:19,047 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:40:19,151 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43543'
2023-12-01 06:40:19,220 - distributed.scheduler - INFO - Receive client connection: Client-7d255fb9-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:19,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55258
2023-12-01 06:40:21,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:21,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:21,728 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:22,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40361
2023-12-01 06:40:22,678 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40361
2023-12-01 06:40:22,678 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-12-01 06:40:22,678 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:22,678 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:22,678 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:22,678 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:40:22,678 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nbrxz6jv
2023-12-01 06:40:22,679 - distributed.worker - INFO - Starting Worker plugin PreImport-08aac7d8-567f-4e4b-9498-2c2280285f3f
2023-12-01 06:40:22,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e8f09e8-2917-456d-81b1-3534fa4cf845
2023-12-01 06:40:22,679 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed67a534-cc87-48e8-a468-c7c856d32a8e
2023-12-01 06:40:22,680 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:22,711 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40361', status: init, memory: 0, processing: 0>
2023-12-01 06:40:22,712 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40361
2023-12-01 06:40:22,712 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41448
2023-12-01 06:40:22,713 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:22,714 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:22,714 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:22,716 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:22,723 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:22,726 - distributed.scheduler - INFO - Remove client Client-7d255fb9-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:22,727 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55258; closing.
2023-12-01 06:40:22,727 - distributed.scheduler - INFO - Remove client Client-7d255fb9-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:22,727 - distributed.scheduler - INFO - Close client connection: Client-7d255fb9-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:22,728 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43543'. Reason: nanny-close
2023-12-01 06:40:22,746 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:22,747 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40361. Reason: nanny-close
2023-12-01 06:40:22,749 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:22,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41448; closing.
2023-12-01 06:40:22,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412822.749539')
2023-12-01 06:40:22,749 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:40:22,750 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:23,995 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:40:23,996 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:40:23,996 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:40:23,997 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:40:23,998 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-01 06:40:28,653 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:28,658 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43835 instead
  warnings.warn(
2023-12-01 06:40:28,663 - distributed.scheduler - INFO - State start
2023-12-01 06:40:28,685 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:28,686 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:40:28,686 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43835/status
2023-12-01 06:40:28,687 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:40:28,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35827'
2023-12-01 06:40:30,527 - distributed.scheduler - INFO - Receive client connection: Client-8324042d-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:30,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53386
2023-12-01 06:40:31,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:31,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:31,906 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:35,788 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45421
2023-12-01 06:40:35,789 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45421
2023-12-01 06:40:35,789 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40955
2023-12-01 06:40:35,790 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:35,790 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:35,790 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:35,790 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:40:35,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-shp9klmz
2023-12-01 06:40:35,791 - distributed.worker - INFO - Starting Worker plugin PreImport-f98b764e-aa99-43eb-903e-1a5232826908
2023-12-01 06:40:35,793 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5f89f1a-ba78-4c2b-a66b-79fc854bde33
2023-12-01 06:40:35,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c90d087-4659-4c70-91be-849b8105d3ba
2023-12-01 06:40:35,794 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:35,823 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45421', status: init, memory: 0, processing: 0>
2023-12-01 06:40:35,824 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45421
2023-12-01 06:40:35,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53404
2023-12-01 06:40:35,825 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:35,826 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:40:35,826 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:35,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:40:35,885 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:35,888 - distributed.scheduler - INFO - Remove client Client-8324042d-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:35,888 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53386; closing.
2023-12-01 06:40:35,888 - distributed.scheduler - INFO - Remove client Client-8324042d-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:35,889 - distributed.scheduler - INFO - Close client connection: Client-8324042d-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:35,889 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35827'. Reason: nanny-close
2023-12-01 06:40:35,890 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:35,891 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45421. Reason: nanny-close
2023-12-01 06:40:35,893 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:40:35,893 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53404; closing.
2023-12-01 06:40:35,893 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45421', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412835.893741')
2023-12-01 06:40:35,894 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:40:35,895 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:37,407 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:40:37,407 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:40:37,408 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:40:37,409 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:40:37,410 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-12-01 06:40:39,745 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:39,749 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:40:39,753 - distributed.scheduler - INFO - State start
2023-12-01 06:40:39,776 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:39,778 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:40:39,778 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:40:39,779 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:40:43,673 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53408'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53408>: Stream is closed
2023-12-01 06:40:44,078 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:40:44,078 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:40:44,079 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:40:44,079 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:40:44,080 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-01 06:40:46,692 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:46,697 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:40:46,701 - distributed.scheduler - INFO - State start
2023-12-01 06:40:46,730 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:46,732 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-01 06:40:46,733 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:40:46,733 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:40:46,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43547'
2023-12-01 06:40:48,729 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:48,729 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:48,733 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:49,840 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44733
2023-12-01 06:40:49,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44733
2023-12-01 06:40:49,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40911
2023-12-01 06:40:49,841 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:40:49,841 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:49,841 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:49,842 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:40:49,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-_uxtuwhd
2023-12-01 06:40:49,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d973013-bfcf-453b-ae37-3b660876b146
2023-12-01 06:40:49,842 - distributed.worker - INFO - Starting Worker plugin PreImport-7c7c07bf-c8aa-450f-ad4d-0f9737039328
2023-12-01 06:40:49,843 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-072f1abf-afce-4ddb-9d36-3c53fc70959a
2023-12-01 06:40:49,843 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:49,895 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44733', status: init, memory: 0, processing: 0>
2023-12-01 06:40:49,908 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44733
2023-12-01 06:40:49,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38252
2023-12-01 06:40:49,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:40:49,910 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:40:49,910 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:49,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:40:50,265 - distributed.scheduler - INFO - Receive client connection: Client-8ddcefe8-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:50,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32916
2023-12-01 06:40:50,272 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:40:50,278 - distributed.scheduler - INFO - Remove client Client-8ddcefe8-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:50,278 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32916; closing.
2023-12-01 06:40:50,278 - distributed.scheduler - INFO - Remove client Client-8ddcefe8-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:50,279 - distributed.scheduler - INFO - Close client connection: Client-8ddcefe8-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:50,282 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43547'. Reason: nanny-close
2023-12-01 06:40:50,282 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:40:50,284 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44733. Reason: nanny-close
2023-12-01 06:40:50,286 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38252; closing.
2023-12-01 06:40:50,286 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:40:50,286 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44733', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412850.2864418')
2023-12-01 06:40:50,286 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:40:50,288 - distributed.nanny - INFO - Worker closed
2023-12-01 06:40:51,346 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:40:51,346 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:40:51,347 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:40:51,348 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-01 06:40:51,348 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-01 06:40:53,398 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:53,402 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35255 instead
  warnings.warn(
2023-12-01 06:40:53,405 - distributed.scheduler - INFO - State start
2023-12-01 06:40:53,427 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:40:53,428 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:40:53,428 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35255/status
2023-12-01 06:40:53,429 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:40:53,580 - distributed.scheduler - INFO - Receive client connection: Client-9202eb6c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:40:53,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60782
2023-12-01 06:40:53,770 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32843'
2023-12-01 06:40:53,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38929'
2023-12-01 06:40:53,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33201'
2023-12-01 06:40:53,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37319'
2023-12-01 06:40:53,821 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43891'
2023-12-01 06:40:53,831 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38069'
2023-12-01 06:40:53,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38521'
2023-12-01 06:40:53,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34827'
2023-12-01 06:40:55,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:55,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:55,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:55,696 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:55,696 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:55,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:55,706 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:55,706 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:55,710 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:55,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:55,737 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:55,741 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:55,756 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:55,756 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:55,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:55,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:55,761 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:55,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:55,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:55,795 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:55,799 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:40:55,799 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:40:55,799 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:55,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:40:59,951 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35663
2023-12-01 06:40:59,952 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35663
2023-12-01 06:40:59,952 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43829
2023-12-01 06:40:59,952 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:40:59,952 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:40:59,952 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:40:59,952 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:40:59,952 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-syb8n65e
2023-12-01 06:40:59,953 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbf7dd63-f872-4fcf-9c95-346076f26291
2023-12-01 06:40:59,953 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08595400-f88f-4c96-bca9-35a896bb3b67
2023-12-01 06:41:00,133 - distributed.worker - INFO - Starting Worker plugin PreImport-8e10b4d0-a3c3-45e0-ac77-457b8a9fc008
2023-12-01 06:41:00,133 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,333 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35663', status: init, memory: 0, processing: 0>
2023-12-01 06:41:00,334 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35663
2023-12-01 06:41:00,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54678
2023-12-01 06:41:00,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:00,337 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:00,338 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:00,804 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34293
2023-12-01 06:41:00,805 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34293
2023-12-01 06:41:00,805 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38185
2023-12-01 06:41:00,805 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:00,805 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,805 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:00,805 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:41:00,805 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_9nxjik0
2023-12-01 06:41:00,806 - distributed.worker - INFO - Starting Worker plugin PreImport-39d6ff02-73e3-4403-96e5-942aae7760e5
2023-12-01 06:41:00,806 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da243392-60e9-4a2b-8939-44db99bb5e44
2023-12-01 06:41:00,806 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c539b2a3-786e-4156-b588-2ccfd41c9b60
2023-12-01 06:41:00,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40833
2023-12-01 06:41:00,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40833
2023-12-01 06:41:00,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42025
2023-12-01 06:41:00,823 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:00,823 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,823 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:00,823 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:41:00,824 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7x3nx1xs
2023-12-01 06:41:00,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a97331bd-ea2e-4f4b-a2f9-58b382a5805c
2023-12-01 06:41:00,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-046e4780-454e-4996-a65b-6f9c7082a040
2023-12-01 06:41:00,856 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34925
2023-12-01 06:41:00,857 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34925
2023-12-01 06:41:00,857 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37879
2023-12-01 06:41:00,857 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:00,857 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,857 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:00,857 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:41:00,857 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hm0mymez
2023-12-01 06:41:00,858 - distributed.worker - INFO - Starting Worker plugin PreImport-55eb9f32-8755-4853-93a2-4346dceb2551
2023-12-01 06:41:00,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e59e8fa1-8b44-4600-90a2-94e87638bb60
2023-12-01 06:41:00,858 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ccb29f3-84fb-4018-bb25-b8af10a5fd9d
2023-12-01 06:41:00,860 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45109
2023-12-01 06:41:00,861 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45109
2023-12-01 06:41:00,861 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37615
2023-12-01 06:41:00,861 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:00,861 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,861 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:00,862 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:41:00,862 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-58oz41vb
2023-12-01 06:41:00,862 - distributed.worker - INFO - Starting Worker plugin RMMSetup-269536fb-5e59-4d8f-8a85-99c8010af38d
2023-12-01 06:41:00,922 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37967
2023-12-01 06:41:00,923 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37967
2023-12-01 06:41:00,923 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43361
2023-12-01 06:41:00,923 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:00,923 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,923 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:00,924 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:41:00,924 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wb1gmnii
2023-12-01 06:41:00,924 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2007ceeb-b16c-4f08-8039-bb7de76d8688
2023-12-01 06:41:00,924 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45103
2023-12-01 06:41:00,925 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45103
2023-12-01 06:41:00,925 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39465
2023-12-01 06:41:00,925 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:00,925 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,925 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:00,925 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:41:00,925 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4c6xod97
2023-12-01 06:41:00,926 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e0c3aca-1756-47bf-9cbf-27d5bddc4547
2023-12-01 06:41:00,990 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38297
2023-12-01 06:41:00,991 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38297
2023-12-01 06:41:00,991 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39795
2023-12-01 06:41:00,991 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:00,991 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:00,991 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:00,991 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:41:00,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-79cd7aw0
2023-12-01 06:41:00,992 - distributed.worker - INFO - Starting Worker plugin PreImport-59745891-9371-4071-8f7c-dc5c37a5cb3d
2023-12-01 06:41:00,992 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54223041-bb29-4bf7-b887-27a819dff6be
2023-12-01 06:41:01,151 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6e987c3-c9d5-428f-99cd-76afbcf00ef5
2023-12-01 06:41:01,176 - distributed.worker - INFO - Starting Worker plugin PreImport-e0857c0d-56bb-464a-a680-8c59b18597e8
2023-12-01 06:41:01,177 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,177 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,178 - distributed.worker - INFO - Starting Worker plugin PreImport-2aab295a-e054-499c-bc4a-dbfdb39bf013
2023-12-01 06:41:01,179 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,193 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34293', status: init, memory: 0, processing: 0>
2023-12-01 06:41:01,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34293
2023-12-01 06:41:01,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54680
2023-12-01 06:41:01,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:01,199 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:01,199 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:01,215 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b7315c66-1081-42c5-b125-444d7a870eeb
2023-12-01 06:41:01,215 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-811a275a-125c-40ff-a27c-6f56fc49e266
2023-12-01 06:41:01,215 - distributed.worker - INFO - Starting Worker plugin PreImport-ef66c172-ffd1-4a04-b0f4-e566efa19e38
2023-12-01 06:41:01,215 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75802f21-6b98-402c-9b0b-ee95f769f9de
2023-12-01 06:41:01,216 - distributed.worker - INFO - Starting Worker plugin PreImport-10db5c60-9da6-4264-84f9-6412c26886fd
2023-12-01 06:41:01,216 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,216 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,220 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34925', status: init, memory: 0, processing: 0>
2023-12-01 06:41:01,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34925
2023-12-01 06:41:01,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54696
2023-12-01 06:41:01,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:01,226 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:01,226 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40833', status: init, memory: 0, processing: 0>
2023-12-01 06:41:01,226 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,227 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40833
2023-12-01 06:41:01,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54718
2023-12-01 06:41:01,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:01,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:01,236 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45109', status: init, memory: 0, processing: 0>
2023-12-01 06:41:01,236 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45109
2023-12-01 06:41:01,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54710
2023-12-01 06:41:01,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:01,240 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:01,240 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,243 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:01,243 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:01,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37967', status: init, memory: 0, processing: 0>
2023-12-01 06:41:01,248 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37967
2023-12-01 06:41:01,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54720
2023-12-01 06:41:01,249 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:01,250 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:01,250 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:01,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:01,258 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38297', status: init, memory: 0, processing: 0>
2023-12-01 06:41:01,259 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38297
2023-12-01 06:41:01,259 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54732
2023-12-01 06:41:01,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:01,262 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:01,262 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,270 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:01,274 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45103', status: init, memory: 0, processing: 0>
2023-12-01 06:41:01,275 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45103
2023-12-01 06:41:01,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54726
2023-12-01 06:41:01,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:01,278 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:01,278 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:01,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:01,375 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:41:01,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:41:01,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:41:01,376 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:41:01,377 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:41:01,377 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:41:01,377 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:41:01,378 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:41:01,390 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:01,391 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:01,391 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:01,391 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:01,391 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:01,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:01,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:01,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:01,396 - distributed.scheduler - INFO - Remove client Client-9202eb6c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:01,396 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60782; closing.
2023-12-01 06:41:01,396 - distributed.scheduler - INFO - Remove client Client-9202eb6c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:01,396 - distributed.scheduler - INFO - Close client connection: Client-9202eb6c-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:01,398 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32843'. Reason: nanny-close
2023-12-01 06:41:01,398 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:01,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38929'. Reason: nanny-close
2023-12-01 06:41:01,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:01,399 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38297. Reason: nanny-close
2023-12-01 06:41:01,400 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33201'. Reason: nanny-close
2023-12-01 06:41:01,400 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:01,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45109. Reason: nanny-close
2023-12-01 06:41:01,401 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37319'. Reason: nanny-close
2023-12-01 06:41:01,401 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:01,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34925. Reason: nanny-close
2023-12-01 06:41:01,401 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43891'. Reason: nanny-close
2023-12-01 06:41:01,402 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:01,402 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34293. Reason: nanny-close
2023-12-01 06:41:01,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38069'. Reason: nanny-close
2023-12-01 06:41:01,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:01,402 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:01,402 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54732; closing.
2023-12-01 06:41:01,402 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40833. Reason: nanny-close
2023-12-01 06:41:01,403 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38521'. Reason: nanny-close
2023-12-01 06:41:01,403 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:01,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:01,403 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38297', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412861.4033825')
2023-12-01 06:41:01,403 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34827'. Reason: nanny-close
2023-12-01 06:41:01,403 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45103. Reason: nanny-close
2023-12-01 06:41:01,403 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:01,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:01,404 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35663. Reason: nanny-close
2023-12-01 06:41:01,404 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54696; closing.
2023-12-01 06:41:01,404 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:01,404 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:01,404 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37967. Reason: nanny-close
2023-12-01 06:41:01,405 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412861.4049375')
2023-12-01 06:41:01,405 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:01,405 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54710; closing.
2023-12-01 06:41:01,405 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:01,405 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:01,406 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:01,406 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:01,406 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:01,406 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:01,406 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54680; closing.
2023-12-01 06:41:01,407 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45109', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412861.407209')
2023-12-01 06:41:01,408 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:01,408 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:01,408 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:01,409 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:01,408 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54696>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-01 06:41:01,410 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34293', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412861.4104807')
2023-12-01 06:41:01,410 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54718; closing.
2023-12-01 06:41:01,411 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412861.4117367')
2023-12-01 06:41:01,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54678; closing.
2023-12-01 06:41:01,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54726; closing.
2023-12-01 06:41:01,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54720; closing.
2023-12-01 06:41:01,412 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35663', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412861.4129188')
2023-12-01 06:41:01,413 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45103', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412861.4132726')
2023-12-01 06:41:01,413 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412861.413603')
2023-12-01 06:41:01,413 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:41:03,116 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:41:03,116 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:41:03,117 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:41:03,118 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:41:03,118 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-01 06:41:05,286 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:41:05,290 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36947 instead
  warnings.warn(
2023-12-01 06:41:05,293 - distributed.scheduler - INFO - State start
2023-12-01 06:41:05,314 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:41:05,315 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:41:05,316 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36947/status
2023-12-01 06:41:05,316 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:41:05,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42369'
2023-12-01 06:41:06,344 - distributed.scheduler - INFO - Receive client connection: Client-9909c809-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:06,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54820
2023-12-01 06:41:07,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:41:07,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:41:07,325 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:41:08,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36393
2023-12-01 06:41:08,191 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36393
2023-12-01 06:41:08,191 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43605
2023-12-01 06:41:08,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:08,191 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:08,191 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:08,191 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:41:08,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f4e2l7hp
2023-12-01 06:41:08,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31e743ae-a6ec-4cc4-b4f1-dcfbba562d9d
2023-12-01 06:41:08,299 - distributed.worker - INFO - Starting Worker plugin PreImport-7494f43e-ee78-4c86-b2aa-9e50eedcf765
2023-12-01 06:41:08,299 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23c8888e-0b53-424b-b4e9-878c56799422
2023-12-01 06:41:08,299 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:08,334 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36393', status: init, memory: 0, processing: 0>
2023-12-01 06:41:08,336 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36393
2023-12-01 06:41:08,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54840
2023-12-01 06:41:08,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:08,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:08,339 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:08,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:08,402 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:41:08,406 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:08,408 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:08,410 - distributed.scheduler - INFO - Remove client Client-9909c809-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:08,411 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54820; closing.
2023-12-01 06:41:08,411 - distributed.scheduler - INFO - Remove client Client-9909c809-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:08,411 - distributed.scheduler - INFO - Close client connection: Client-9909c809-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:08,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42369'. Reason: nanny-close
2023-12-01 06:41:08,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:08,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36393. Reason: nanny-close
2023-12-01 06:41:08,416 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54840; closing.
2023-12-01 06:41:08,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:08,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36393', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412868.4163833')
2023-12-01 06:41:08,416 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:41:08,417 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:09,629 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:41:09,629 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:41:09,630 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:41:09,630 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:41:09,631 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-01 06:41:11,924 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:41:11,928 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40561 instead
  warnings.warn(
2023-12-01 06:41:11,932 - distributed.scheduler - INFO - State start
2023-12-01 06:41:11,953 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:41:11,954 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:41:11,955 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40561/status
2023-12-01 06:41:11,955 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:41:12,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35797'
2023-12-01 06:41:12,305 - distributed.scheduler - INFO - Receive client connection: Client-9ce5b2a8-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:12,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45602
2023-12-01 06:41:13,866 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:41:13,867 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:41:13,871 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:41:14,773 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39393
2023-12-01 06:41:14,774 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39393
2023-12-01 06:41:14,774 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46849
2023-12-01 06:41:14,774 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:41:14,774 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:14,774 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:41:14,775 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:41:14,775 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6tq4whwx
2023-12-01 06:41:14,775 - distributed.worker - INFO - Starting Worker plugin PreImport-4925bd96-4b4a-4c9a-80fe-689010bc9789
2023-12-01 06:41:14,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-630675c5-3543-4f2f-aee9-9df6266280c1
2023-12-01 06:41:14,893 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c6950e9-29a8-4527-8a3b-433fcacd843a
2023-12-01 06:41:14,893 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:14,935 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39393', status: init, memory: 0, processing: 0>
2023-12-01 06:41:14,936 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39393
2023-12-01 06:41:14,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45628
2023-12-01 06:41:14,938 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:41:14,939 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:41:14,939 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:41:14,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:41:14,975 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-01 06:41:15,141 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:41:15,144 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:15,146 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:41:15,149 - distributed.scheduler - INFO - Remove client Client-9ce5b2a8-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:15,149 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45602; closing.
2023-12-01 06:41:15,149 - distributed.scheduler - INFO - Remove client Client-9ce5b2a8-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:15,150 - distributed.scheduler - INFO - Close client connection: Client-9ce5b2a8-9014-11ee-b275-d8c49764f6bb
2023-12-01 06:41:15,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35797'. Reason: nanny-close
2023-12-01 06:41:15,151 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:41:15,152 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39393. Reason: nanny-close
2023-12-01 06:41:15,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:41:15,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45628; closing.
2023-12-01 06:41:15,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39393', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412875.1543932')
2023-12-01 06:41:15,154 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:41:15,155 - distributed.nanny - INFO - Worker closed
2023-12-01 06:41:16,217 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:41:16,218 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:41:16,218 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:41:16,219 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:41:16,220 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40293 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34977 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38793 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45021 instead
  warnings.warn(
2023-12-01 06:42:19,514 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #006] ep: 0x7f6962dc3140, tag: 0xe2cc1d8489986165, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #006] ep: 0x7f6962dc3140, tag: 0xe2cc1d8489986165, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] 2023-12-01 06:42:48,128 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-01 06:42:48,132 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://10.33.225.163:46902', name: 2, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44937 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43753 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39955 instead
  warnings.warn(
[1701412994.010418] [dgx13:67535:0]            sock.c:470  UCX  ERROR bind(fd=135 addr=0.0.0.0:43552) failed: Address already in use
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f7610d0e140, tag: 0x54c53c47b4728972>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f7610d0e140, tag: 0x54c53c47b4728972>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-590' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37769 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44039 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33067 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39773 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33437 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41599 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38623 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37495 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40491 instead
  warnings.warn(
[1701413161.881685] [dgx13:70657:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:41070) failed: Address already in use
[1701413161.881756] [dgx13:70657:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:52870) failed: Address already in use
[1701413163.175593] [dgx13:70743:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:55284) failed: Address already in use
[1701413166.180760] [dgx13:70750:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:44278) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36445 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40055 instead
  warnings.warn(
[1701413202.264229] [dgx13:71562:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:54886) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34899 instead
  warnings.warn(
[1701413226.488475] [dgx13:71966:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:51368) failed: Address already in use
[1701413228.588063] [dgx13:71966:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:33224) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46477 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39045 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41277 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38743 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33169 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44197 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34155 instead
  warnings.warn(
[1701413392.669292] [dgx13:74799:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:33956) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42685 instead
  warnings.warn(
[1701413404.832066] [dgx13:75038:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:34150) failed: Address already in use
[1701413404.832139] [dgx13:75038:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:34212) failed: Address already in use
[1701413404.832199] [dgx13:75038:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:50728) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40999 instead
  warnings.warn(
[1701413424.563921] [dgx13:75650:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:51190) failed: Address already in use
[1701413425.817552] [dgx13:75650:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:52122) failed: Address already in use
[1701413425.817661] [dgx13:75650:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:52146) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44607 instead
  warnings.warn(
2023-12-01 06:50:53,406 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-01 06:50:53,594 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-01 06:50:53,621 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:46435'.
2023-12-01 06:50:53,635 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:46435'. Shutting down.
2023-12-01 06:50:53,647 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f56e66e3e50>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-01 06:50:55,650 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
