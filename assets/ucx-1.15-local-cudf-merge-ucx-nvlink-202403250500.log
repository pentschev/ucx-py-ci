[dgx13:61220:0:61220] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  61220) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fb11c11607d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fb11c116274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fb11c11643a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fb1479e9420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fb11c1956b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fb11c1be839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fb11c0d03df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fb11c0d3838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fb11c11f4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fb11c0d25dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fb11c1928da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fb11c24b06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x56439476a516]
13  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564394765184]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564394776559]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564394766606]
16  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564394776802]
17  /opt/conda/envs/gdf/bin/python(+0x14cbb3) [0x564394783bb3]
18  /opt/conda/envs/gdf/bin/python(+0x25842c) [0x56439488f42c]
19  /opt/conda/envs/gdf/bin/python(+0xf266c) [0x56439472966c]
20  /opt/conda/envs/gdf/bin/python(+0x136833) [0x56439476d833]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x58b4) [0x56439476b824]
22  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564394776802]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564394766606]
24  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564394776802]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564394766606]
26  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564394776802]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564394766606]
28  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564394776802]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564394766606]
30  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564394765184]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564394776559]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x564394767162]
33  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564394765184]
34  /opt/conda/envs/gdf/bin/python(+0x14c9fb) [0x5643947839fb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x56439478417c]
36  /opt/conda/envs/gdf/bin/python(+0x2109ce) [0x5643948479ce]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x56439476e7ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x56439476a516]
39  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564394776802]
40  /opt/conda/envs/gdf/bin/python(+0x14cadc) [0x564394783adc]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x56439476a516]
42  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564394776802]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564394766606]
44  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564394765184]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564394776559]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564394766606]
47  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564394776802]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x564394766352]
49  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564394765184]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564394776559]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x564394767162]
52  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564394765184]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x564394764e58]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x564394764e09]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5643948124ab]
56  /opt/conda/envs/gdf/bin/python(+0x20887a) [0x56439483f87a]
57  /opt/conda/envs/gdf/bin/python(+0x204c03) [0x56439483bc03]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x564394833a1a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x56439483390c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x564394832b47]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5643948062e7]
=================================
[dgx13:61211:0:61211] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
[dgx13:61206:0:61206] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  61211) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f9c1c9a407d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7f9c1c9a4274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7f9c1c9a443a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f9c56280420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f9c1ca236b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f9c1ca4c839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7f9c1c95e3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7f9c1c961838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f9c1c9ad4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f9c1c9605dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f9c1ca208da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f9c1cad906a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x562253bc0516]
13  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x562253bbb184]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x562253bcc559]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562253bbc606]
16  /opt/conda/envs/gdf/bin/python(+0x1e28a2) [0x562253c6f8a2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f9c403d91e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x562253bc47ac]
19  /opt/conda/envs/gdf/bin/python(+0xf266c) [0x562253b7f66c]
20  /opt/conda/envs/gdf/bin/python(+0x136833) [0x562253bc3833]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x58b4) [0x562253bc1824]
22  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x562253bcc802]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562253bbc606]
24  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x562253bcc802]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562253bbc606]
26  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x562253bcc802]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562253bbc606]
28  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x562253bcc802]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562253bbc606]
30  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x562253bbb184]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x562253bcc559]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x562253bbd162]
33  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x562253bbb184]
34  /opt/conda/envs/gdf/bin/python(+0x14c9fb) [0x562253bd99fb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x562253bda17c]
36  /opt/conda/envs/gdf/bin/python(+0x2109ce) [0x562253c9d9ce]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x562253bc47ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x562253bc0516]
39  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x562253bcc802]
40  /opt/conda/envs/gdf/bin/python(+0x14cadc) [0x562253bd9adc]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x562253bc0516]
42  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x562253bcc802]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562253bbc606]
44  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x562253bbb184]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x562253bcc559]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562253bbc606]
47  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x562253bcc802]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x562253bbc352]
49  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x562253bbb184]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x562253bcc559]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x562253bbd162]
52  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x562253bbb184]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x562253bbae58]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x562253bbae09]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x562253c684ab]
56  /opt/conda/envs/gdf/bin/python(+0x20887a) [0x562253c9587a]
57  /opt/conda/envs/gdf/bin/python(+0x204c03) [0x562253c91c03]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x562253c89a1a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x562253c8990c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x562253c88b47]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x562253c5c2e7]
=================================
==== backtrace (tid:  61206) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fcc5d4e307d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fcc5d4e3274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fcc5d4e343a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fcc9ae12420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fcc5d5626b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fcc5d58b839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fcc7002e3df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fcc70031838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fcc5d4ec4a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fcc700305dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fcc5d55f8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fcc5d61806a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564434b15516]
13  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564434b10184]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564434b21559]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564434b11606]
16  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564434b21802]
17  /opt/conda/envs/gdf/bin/python(+0x14cbb3) [0x564434b2ebb3]
18  /opt/conda/envs/gdf/bin/python(+0x25842c) [0x564434c3a42c]
19  /opt/conda/envs/gdf/bin/python(+0xf266c) [0x564434ad466c]
20  /opt/conda/envs/gdf/bin/python(+0x136833) [0x564434b18833]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x58b4) [0x564434b16824]
22  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564434b21802]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564434b11606]
24  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564434b21802]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564434b11606]
26  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564434b21802]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564434b11606]
28  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564434b21802]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564434b11606]
30  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564434b10184]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564434b21559]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x564434b12162]
33  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564434b10184]
34  /opt/conda/envs/gdf/bin/python(+0x14c9fb) [0x564434b2e9fb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x564434b2f17c]
36  /opt/conda/envs/gdf/bin/python(+0x2109ce) [0x564434bf29ce]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x564434b197ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564434b15516]
39  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564434b21802]
40  /opt/conda/envs/gdf/bin/python(+0x14cadc) [0x564434b2eadc]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564434b15516]
42  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564434b21802]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564434b11606]
44  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564434b10184]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564434b21559]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564434b11606]
47  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x564434b21802]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x564434b11352]
49  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564434b10184]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564434b21559]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x564434b12162]
52  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x564434b10184]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x564434b0fe58]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x564434b0fe09]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x564434bbd4ab]
56  /opt/conda/envs/gdf/bin/python(+0x20887a) [0x564434bea87a]
57  /opt/conda/envs/gdf/bin/python(+0x204c03) [0x564434be6c03]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x564434bdea1a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x564434bde90c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x564434bddb47]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x564434bb12e7]
=================================
[dgx13:61215:0:61215] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  61215) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fa31013f07d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a274) [0x7fa31013f274]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2a43a) [0x7fa31013f43a]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fa33ba0d420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fa3101be6b4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fa3101e7839]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x203df) [0x7fa3100f93df]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23838) [0x7fa3100fc838]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fa3101484a9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fa3100fb5dd]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fa3101bb8da]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fa31027406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x565381424516]
13  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x56538141f184]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x565381430559]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x565381420606]
16  /opt/conda/envs/gdf/bin/python(+0x1e28a2) [0x5653814d38a2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fa325b661e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5653814287ac]
19  /opt/conda/envs/gdf/bin/python(+0xf266c) [0x5653813e366c]
20  /opt/conda/envs/gdf/bin/python(+0x136833) [0x565381427833]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x58b4) [0x565381425824]
22  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x565381430802]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x565381420606]
24  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x565381430802]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x565381420606]
26  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x565381430802]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x565381420606]
28  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x565381430802]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x565381420606]
30  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x56538141f184]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x565381430559]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x565381421162]
33  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x56538141f184]
34  /opt/conda/envs/gdf/bin/python(+0x14c9fb) [0x56538143d9fb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x56538143e17c]
36  /opt/conda/envs/gdf/bin/python(+0x2109ce) [0x5653815019ce]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5653814287ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x565381424516]
39  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x565381430802]
40  /opt/conda/envs/gdf/bin/python(+0x14cadc) [0x56538143dadc]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x565381424516]
42  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x565381430802]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x565381420606]
44  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x56538141f184]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x565381430559]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x565381420606]
47  /opt/conda/envs/gdf/bin/python(+0x13f802) [0x565381430802]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x565381420352]
49  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x56538141f184]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x565381430559]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x565381421162]
52  /opt/conda/envs/gdf/bin/python(+0x12e184) [0x56538141f184]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x56538141ee58]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x56538141ee09]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5653814cc4ab]
56  /opt/conda/envs/gdf/bin/python(+0x20887a) [0x5653814f987a]
57  /opt/conda/envs/gdf/bin/python(+0x204c03) [0x5653814f5c03]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5653814eda1a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5653814ed90c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5653814ecb47]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5653814c02e7]
=================================
Task exception was never retrieved
future: <Task finished name='Task-1154' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1160' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2024-03-25 06:34:59,545 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34081 -> ucx://127.0.0.1:56593
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7ff292ce82c0, tag: 0x4e398851c35f6eb, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-03-25 06:34:59,546 - distributed.scheduler - WARNING - Removing worker 'ucx://127.0.0.1:56593' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('generate-data-8272b35ed56eebf02983170a8d0abff0', 0), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 6, 0), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 1, 0), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 2, 0), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 3, 0), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 4, 0), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 7, 0), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 5, 0), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 0, 0)} (stimulus_id='handle-worker-cleanup-1711348499.546275')
2024-03-25 06:34:59,545 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56593
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f12c1538240, tag: 0x9a6226bcfaab2088, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f12c1538240, tag: 0x9a6226bcfaab2088, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-03-25 06:34:59,546 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56593
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f18f9673100, tag: 0x72022bcd0e4ffc21, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f18f9673100, tag: 0x72022bcd0e4ffc21, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-03-25 06:34:59,546 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56593
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7ff292ce8180, tag: 0x7d793dbb41360a7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7ff292ce8180, tag: 0x7d793dbb41360a7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-03-25 06:34:59,552 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56593
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 467, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1016, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 328, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 60, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXNotConnected: <stream_recv>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 469, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2024-03-25 06:34:59,663 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54845
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f12c15381c0, tag: 0x3c89e9586e0f00ab, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f12c15381c0, tag: 0x3c89e9586e0f00ab, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-03-25 06:34:59,663 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42687 -> ucx://127.0.0.1:54845
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f18f9673380, tag: 0xa66456df5772109d, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-03-25 06:34:59,663 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54845
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f07f0e55140, tag: 0xd41c5997b3913343, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f07f0e55140, tag: 0xd41c5997b3913343, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-03-25 06:34:59,663 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54845
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f18f9673280, tag: 0xbed842317af7f856, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f18f9673280, tag: 0xbed842317af7f856, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-03-25 06:34:59,664 - distributed.scheduler - WARNING - Removing worker 'ucx://127.0.0.1:54845' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 0, 1), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 7, 1), ('generate-data-8272b35ed56eebf02983170a8d0abff0', 2), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 6, 1), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 1, 1), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 2, 1), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 3, 1), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 4, 1), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 5, 1)} (stimulus_id='handle-worker-cleanup-1711348499.664262')
2024-03-25 06:34:59,676 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54845
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: CancelledError()
2024-03-25 06:34:59,691 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55443
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7ff292ce8200, tag: 0xeb1850ae3ae6f10c, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7ff292ce8200, tag: 0xeb1850ae3ae6f10c, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-03-25 06:34:59,691 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55443
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f07f0e55100, tag: 0xf20094718806c7ff, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f07f0e55100, tag: 0xf20094718806c7ff, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-03-25 06:34:59,692 - distributed.scheduler - WARNING - Removing worker 'ucx://127.0.0.1:55443' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 6, 6), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 1, 6), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 2, 6), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 3, 6), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 4, 6), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 5, 6), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 0, 6), ('generate-data-8272b35ed56eebf02983170a8d0abff0', 1), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 7, 6)} (stimulus_id='handle-worker-cleanup-1711348499.6922638')
2024-03-25 06:34:59,702 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55443
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f12c1538140, tag: 0xf7913e822a464e42, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f12c1538140, tag: 0xf7913e822a464e42, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-03-25 06:34:59,695 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42687 -> ucx://127.0.0.1:55443
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f18f9673400, tag: 0x41a91474ad4c4ec, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-03-25 06:34:59,744 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55443
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f18f9673240, tag: 0xfc5ce8304e3d35d9, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f18f9673240, tag: 0xfc5ce8304e3d35d9, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-03-25 06:34:59,914 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37919
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f07f0e551c0, tag: 0x5cc9bdfd6b4fa7f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f07f0e551c0, tag: 0x5cc9bdfd6b4fa7f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-03-25 06:34:59,914 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37919
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7f12c1538280, tag: 0xd4c521d7fd216a7f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7f12c1538280, tag: 0xd4c521d7fd216a7f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-03-25 06:34:59,914 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37919
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f18f9673180, tag: 0x27dc9b0542186e71, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f18f9673180, tag: 0x27dc9b0542186e71, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-03-25 06:34:59,915 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37919
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7ff292ce8100, tag: 0xad8abf7c52583c48, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7ff292ce8100, tag: 0xad8abf7c52583c48, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-03-25 06:34:59,915 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:42045 -> ucx://127.0.0.1:37919
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f07f0e553c0, tag: 0xb4549b8aa2b5b21e, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-03-25 06:34:59,916 - distributed.scheduler - WARNING - Removing worker 'ucx://127.0.0.1:37919' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('generate-data-8272b35ed56eebf02983170a8d0abff0', 6), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 2, 3), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 3, 3), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 4, 3), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 5, 3), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 0, 3), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 7, 3), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 6, 3), ('split-simple-shuffle-2458cef5ac6baf4e0e743a53a45427b4', 1, 3)} (stimulus_id='handle-worker-cleanup-1711348499.9157648')
2024-03-25 06:35:01,262 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,262 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,616 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,616 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,743 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-03749b6a67db4b27e3fb569a3f534480', 0)
Function:  _concat
args:      ([               key   payload  _partitions
shuffle                                  
0          1124055  49886782            0
0          1140973  22617269            0
0           728462  18423782            0
0           686190  73762796            0
0           571040  13165107            0
...            ...       ...          ...
0        799753843  92845390            0
0        799909672  53650324            0
0        799882074  94170008            0
0        799900628  63691376            0
0        799728867  76125485            0

[12497200 rows x 3 columns],                key   payload  _partitions
shuffle                                  
1           752316  67726807            0
1           498199  44796780            0
1          1044077  56391598            0
1           774942  85364164            0
1           963013  28020726            0
...            ...       ...          ...
1        799825666  53532490            0
1        799718932  67977532            0
1 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded')"

2024-03-25 06:35:01,774 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,774 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,786 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,786 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,789 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:01,789 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2863, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-25 06:35:02,049 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-03749b6a67db4b27e3fb569a3f534480', 3)
Function:  _concat
args:      ([               key   payload  _partitions
shuffle                                  
0          1081577  31958053            3
0           820642  93138224            3
0           659756  70847880            3
0           624875  88203527            3
0          1024077  65546293            3
...            ...       ...          ...
0        799796236  16819713            3
0        799906792  25913786            3
0        799756516  40987409            3
0        799810061  56370741            3
0        799704658  78491132            3

[12499485 rows x 3 columns],                key   payload  _partitions
shuffle                                  
1           756287  12159065            3
1           515217  78758673            3
1          1043601  72198144            3
1           823087  88191544            3
1           938760  72579730            3
...            ...       ...          ...
1        799630338  35760708            3
1        799642750  85870519            3
1 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded')"

2024-03-25 06:35:02,289 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-03749b6a67db4b27e3fb569a3f534480', 1)
Function:  _concat
args:      ([               key   payload  _partitions
shuffle                                  
0          1035195  34334258            1
0          1138748   1348073            1
0           699831  20068023            1
0           626509  13352381            1
0           454039  40186296            1
...            ...       ...          ...
0        799753846   5300274            1
0        799747229  41133478            1
0        799873898  25320437            1
0        799822918  80149632            1
0        799883709  89741676            1

[12502439 rows x 3 columns],                key   payload  _partitions
shuffle                                  
1           804433  24157064            1
1           343022  23546192            1
1          1045152  33496523            1
1           776626  40795889            1
1           873741  58556662            1
...            ...       ...          ...
1        799718003  86326558            1
1        799709735  42492088            1
1 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded')"

2024-03-25 06:35:02,357 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-542a0e70d67f6aa727ec7e82a5ed2545', 4)
Function:  _concat
args:      ([                key   payload  _partitions
52422     605620829  90513769            4
103120    815129926  12017563            4
52426     817164261  11331448            4
103122    809585666  13478834            4
101098    604107445  26644203            4
...             ...       ...          ...
99994565  801106210  24257965            4
99994568  710242712   8385102            4
99994569  868549163  75284564            4
99994581  851127797  80355257            4
99994588  109834032  74841718            4

[12499664 rows x 3 columns],                 key   payload  _partitions
41680     923937131  11014874            4
41683     903243810  14627405            4
69798     902293681  60082477            4
104068    941432964  58254438            4
62178     122735051   8361858            4
...             ...       ...          ...
99989990  943872806   9960991            4
99989998  514737916  46607784            4
99990015  918657174  17349971            4
99990484  615179282  8
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded')"

2024-03-25 06:35:02,406 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-86db226a6a1c1400112a4980f3937126', 6)
Function:  subgraph_callable-86c2ba46cf21a7a064f5dd3a290da228
args:      ('drop_by_shallow_copy-7cfabaa1e0541cd256f8f6fa293b613b', 'drop_by_shallow_copy-593ebae6b6f8aa2e896c0b9e9b55b607',                key   payload  _partitions
shuffle                                  
0          1130540  51740335            6
0          1019385  28919146            6
0           899081  48295985            6
0           551821   2456729            6
0           491271  59325745            6
...            ...       ...          ...
7        799932460  48355732            6
7        799947832  90511480            6
7        799970910   8167472            6
7        799917218  96990250            6
7        799977214  21801662            6

[99987674 rows x 3 columns], ['_partitions'], 'simple-shuffle-03749b6a67db4b27e3fb569a3f534480',                  key   payload  _partitions
52421      305605255  37465068            6
103106     844703540   8142847            6
52427      853690976  35207279            6
103109     109374980  53858105            6
101091     610024897 
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded')"

2024-03-25 06:35:02,423 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-542a0e70d67f6aa727ec7e82a5ed2545', 5)
Function:  _concat
args:      ([                key   payload  _partitions
52430     862502872  49017592            5
103107    701872031  96691046            5
52432     821821524  67802104            5
103118    827775558   9925304            5
101110    210904417  86191773            5
...             ...       ...          ...
99994550  806101360  99567971            5
99994552  839344692  98013410            5
99956588  812664605  77664029            5
99956601  511973689  14280010            5
99994574  867583240  38826289            5

[12501172 rows x 3 columns],                 key   payload  _partitions
41667     955377252   2317409            5
41668     967582002   4374775            5
69792     957985181  21634132            5
104071    319276104   9030873            5
62186     943280365  57534230            5
...             ...       ...          ...
99990477  968965455  16598610            5
99990485  905791077  59481648            5
99990491  953396861  50279936            5
99990493  911485373  9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded')"

2024-03-25 06:35:10,467 - distributed.nanny - WARNING - Restarting worker
2024-03-25 06:35:11,954 - distributed.nanny - WARNING - Restarting worker
2024-03-25 06:35:12,131 - distributed.nanny - WARNING - Restarting worker
2024-03-25 06:35:12,265 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
