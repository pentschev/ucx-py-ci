============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-29 06:08:30,187 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:08:30,191 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37685 instead
  warnings.warn(
2023-05-29 06:08:30,195 - distributed.scheduler - INFO - State start
2023-05-29 06:08:30,213 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:08:30,214 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:08:30,215 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:08:30,215 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:08:30,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33931'
2023-05-29 06:08:30,438 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34451'
2023-05-29 06:08:30,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35993'
2023-05-29 06:08:30,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37873'
2023-05-29 06:08:31,944 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8_gjdlx8', purging
2023-05-29 06:08:31,946 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:31,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:31,946 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:31,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:31,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:31,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:31,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:31,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:31,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:31,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:31,956 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:31,957 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-29 06:08:32,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44811
2023-05-29 06:08:32,053 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44811
2023-05-29 06:08:32,053 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33237
2023-05-29 06:08:32,053 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-29 06:08:32,053 - distributed.worker - INFO - -------------------------------------------------
2023-05-29 06:08:32,053 - distributed.worker - INFO -               Threads:                          4
2023-05-29 06:08:32,054 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-29 06:08:32,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b3zex8tl
2023-05-29 06:08:32,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec4a0dee-6d25-498a-acce-4da0209f462e
2023-05-29 06:08:32,054 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3210bce-e345-4895-a723-a50c6ab31bc7
2023-05-29 06:08:32,054 - distributed.worker - INFO - Starting Worker plugin PreImport-03af4e39-f367-4753-9ff2-b6b238f89121
2023-05-29 06:08:32,054 - distributed.worker - INFO - -------------------------------------------------
2023-05-29 06:08:32,068 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-29 06:08:32,068 - distributed.worker - INFO - -------------------------------------------------
2023-05-29 06:08:32,070 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:08:32,862 - distributed.nanny - INFO - Worker process 26375 exited with status 127
2023-05-29 06:08:32,863 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:08:32,886 - distributed.nanny - INFO - Worker process 26382 exited with status 127
2023-05-29 06:08:32,887 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:08:32,908 - distributed.nanny - INFO - Worker process 26378 exited with status 127
2023-05-29 06:08:32,909 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:08:34,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-47ys4b02', purging
2023-05-29 06:08:34,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qz74upg7', purging
2023-05-29 06:08:34,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bodxrbfx', purging
2023-05-29 06:08:34,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:34,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:34,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:34,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:34,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:34,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:34,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:34,314 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:34,315 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:08:34,959 - distributed.nanny - INFO - Worker process 26417 exited with status 127
2023-05-29 06:08:34,959 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:08:35,056 - distributed.nanny - INFO - Worker process 26420 exited with status 127
2023-05-29 06:08:35,057 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:08:35,075 - distributed.nanny - INFO - Worker process 26423 exited with status 127
2023-05-29 06:08:35,075 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:08:36,365 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2spt2djy', purging
2023-05-29 06:08:36,365 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o_kxy2zp', purging
2023-05-29 06:08:36,365 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w5ye5db4', purging
2023-05-29 06:08:36,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:36,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:36,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:36,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:36,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:36,434 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:36,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:36,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:36,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:08:37,040 - distributed.nanny - INFO - Worker process 26445 exited with status 127
2023-05-29 06:08:37,041 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:08:37,161 - distributed.nanny - INFO - Worker process 26453 exited with status 127
2023-05-29 06:08:37,162 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:08:37,182 - distributed.nanny - INFO - Worker process 26450 exited with status 127
2023-05-29 06:08:37,182 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:08:38,406 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wgef23wa', purging
2023-05-29 06:08:38,406 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-necemv2m', purging
2023-05-29 06:08:38,407 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o7537lzk', purging
2023-05-29 06:08:38,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:38,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:38,414 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:38,550 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:38,550 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:38,551 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:08:38,551 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:08:38,557 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:38,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:08:38,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34451'. Reason: nanny-close
2023-05-29 06:08:38,811 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33931'. Reason: nanny-close
2023-05-29 06:08:38,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35993'. Reason: nanny-close
2023-05-29 06:08:38,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37873'. Reason: nanny-close
2023-05-29 06:08:38,812 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-29 06:08:38,813 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44811. Reason: nanny-close
2023-05-29 06:08:38,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-29 06:08:38,816 - distributed.nanny - INFO - Worker closed
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:08:39,153 - distributed.nanny - INFO - Worker process 26475 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:08:39,288 - distributed.nanny - INFO - Worker process 26480 exited with status 127
2023-05-29 06:08:39,306 - distributed.nanny - INFO - Worker process 26483 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-29 06:09:10,989 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:09:10,993 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46451 instead
  warnings.warn(
2023-05-29 06:09:10,997 - distributed.scheduler - INFO - State start
2023-05-29 06:09:11,016 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:09:11,017 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:09:11,017 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:09:11,018 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:09:11,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42981'
2023-05-29 06:09:11,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39167'
2023-05-29 06:09:11,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36845'
2023-05-29 06:09:11,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34841'
2023-05-29 06:09:11,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43627'
2023-05-29 06:09:11,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41957'
2023-05-29 06:09:11,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32815'
2023-05-29 06:09:11,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35089'
2023-05-29 06:09:13,225 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xup4skex', purging
2023-05-29 06:09:13,225 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nyack6l0', purging
2023-05-29 06:09:13,225 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4f9yqnon', purging
2023-05-29 06:09:13,226 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:13,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:13,227 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:13,227 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:13,277 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:13,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:13,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:13,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:13,281 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:13,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:13,286 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:13,286 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:13,287 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:13,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:13,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:13,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:13,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:13,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:13,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:13,371 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:13,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:13,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:13,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:13,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:16,209 - distributed.nanny - INFO - Worker process 26692 exited with status 127
2023-05-29 06:09:16,210 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:16,242 - distributed.nanny - INFO - Worker process 26684 exited with status 127
2023-05-29 06:09:16,243 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:16,331 - distributed.nanny - INFO - Worker process 26701 exited with status 127
2023-05-29 06:09:16,331 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:16,387 - distributed.nanny - INFO - Worker process 26677 exited with status 127
2023-05-29 06:09:16,388 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:16,417 - distributed.nanny - INFO - Worker process 26698 exited with status 127
2023-05-29 06:09:16,418 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:17,621 - distributed.nanny - INFO - Worker process 26695 exited with status 127
2023-05-29 06:09:17,622 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:17,647 - distributed.nanny - INFO - Worker process 26680 exited with status 127
2023-05-29 06:09:17,648 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:17,671 - distributed.nanny - INFO - Worker process 26688 exited with status 127
2023-05-29 06:09:17,672 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:17,805 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e0qha8tf', purging
2023-05-29 06:09:17,805 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mzcrmink', purging
2023-05-29 06:09:17,805 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rndnaug8', purging
2023-05-29 06:09:17,806 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-56vjpy3d', purging
2023-05-29 06:09:17,806 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-onfl6p42', purging
2023-05-29 06:09:17,806 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-90ltfusx', purging
2023-05-29 06:09:17,807 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1wyihggg', purging
2023-05-29 06:09:17,807 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g8h07orr', purging
2023-05-29 06:09:17,808 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:17,808 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:17,812 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:17,812 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:17,910 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:17,913 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:17,941 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:17,941 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:17,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:17,984 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:18,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:18,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:18,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:18,081 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:18,112 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:19,140 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:19,140 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:19,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:19,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:19,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:19,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:19,399 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:19,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:19,456 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:19,791 - distributed.nanny - INFO - Worker process 26760 exited with status 127
2023-05-29 06:09:19,792 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:19,813 - distributed.nanny - INFO - Worker process 26763 exited with status 127
2023-05-29 06:09:19,814 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:19,876 - distributed.nanny - INFO - Worker process 26766 exited with status 127
2023-05-29 06:09:19,877 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:20,224 - distributed.nanny - INFO - Worker process 26769 exited with status 127
2023-05-29 06:09:20,225 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:20,247 - distributed.nanny - INFO - Worker process 26772 exited with status 127
2023-05-29 06:09:20,248 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:20,653 - distributed.nanny - INFO - Worker process 26783 exited with status 127
2023-05-29 06:09:20,654 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:20,695 - distributed.nanny - INFO - Worker process 26780 exited with status 127
2023-05-29 06:09:20,696 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:20,721 - distributed.nanny - INFO - Worker process 26786 exited with status 127
2023-05-29 06:09:20,722 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:21,202 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1w7xfx0h', purging
2023-05-29 06:09:21,202 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-965jr9v6', purging
2023-05-29 06:09:21,203 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2n_s6d4n', purging
2023-05-29 06:09:21,203 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gazmsbdi', purging
2023-05-29 06:09:21,203 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8pbi7clc', purging
2023-05-29 06:09:21,204 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ip5ty_9m', purging
2023-05-29 06:09:21,204 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wid7pk5k', purging
2023-05-29 06:09:21,205 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-644vij61', purging
2023-05-29 06:09:21,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:21,205 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:21,228 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:21,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:21,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:21,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:21,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:21,469 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:21,481 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:21,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:21,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:21,796 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b_p78ltb', purging
2023-05-29 06:09:21,797 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:21,797 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:21,957 - distributed.nanny - INFO - Worker process 26831 exited with status 127
2023-05-29 06:09:21,958 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:21,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:21,985 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:22,162 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fu6x4d1v', purging
2023-05-29 06:09:22,163 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:22,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:22,181 - distributed.nanny - INFO - Worker process 26834 exited with status 127
2023-05-29 06:09:22,182 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:22,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:22,200 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:22,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:22,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:22,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:22,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:22,540 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:22,568 - distributed.nanny - INFO - Worker process 26837 exited with status 127
2023-05-29 06:09:22,569 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:23,275 - distributed.nanny - INFO - Worker process 26846 exited with status 127
2023-05-29 06:09:23,276 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:23,302 - distributed.nanny - INFO - Worker process 26849 exited with status 127
2023-05-29 06:09:23,302 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:23,515 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m418qbhn', purging
2023-05-29 06:09:23,516 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-80j8hj2z', purging
2023-05-29 06:09:23,516 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d5gcih3g', purging
2023-05-29 06:09:23,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:23,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:23,546 - distributed.nanny - INFO - Worker process 26864 exited with status 127
2023-05-29 06:09:23,546 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:23,585 - distributed.nanny - INFO - Worker process 26861 exited with status 127
2023-05-29 06:09:23,586 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:23,619 - distributed.nanny - INFO - Worker process 26858 exited with status 127
2023-05-29 06:09:23,620 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:23,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:23,698 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-354cox97', purging
2023-05-29 06:09:23,699 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2kngbsp8', purging
2023-05-29 06:09:23,699 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xb07qggq', purging
2023-05-29 06:09:23,700 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:23,700 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:23,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:24,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:24,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:24,210 - distributed.nanny - INFO - Worker process 26890 exited with status 127
2023-05-29 06:09:24,211 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:24,217 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:24,521 - distributed.nanny - INFO - Worker process 26904 exited with status 127
2023-05-29 06:09:24,522 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:24,663 - distributed.nanny - INFO - Worker process 26916 exited with status 127
2023-05-29 06:09:24,663 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:24,760 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a1pvrxbe', purging
2023-05-29 06:09:24,761 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j9d47mo6', purging
2023-05-29 06:09:24,761 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3ypda5__', purging
2023-05-29 06:09:24,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:24,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:24,785 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:24,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:24,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:24,852 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:25,070 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:25,070 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:25,078 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:25,078 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:25,103 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:25,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:25,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:25,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:25,313 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:25,398 - distributed.nanny - INFO - Worker process 26932 exited with status 127
2023-05-29 06:09:25,399 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:09:25,440 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42981'. Reason: nanny-close
2023-05-29 06:09:25,440 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39167'. Reason: nanny-close
2023-05-29 06:09:25,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36845'. Reason: nanny-close
2023-05-29 06:09:25,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43627'. Reason: nanny-close
2023-05-29 06:09:25,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41957'. Reason: nanny-close
2023-05-29 06:09:25,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34841'. Reason: nanny-close
2023-05-29 06:09:25,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32815'. Reason: nanny-close
2023-05-29 06:09:25,441 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35089'. Reason: nanny-close
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:25,761 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o85f45lt', purging
2023-05-29 06:09:25,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:25,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:25,837 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:25,878 - distributed.nanny - INFO - Worker process 26929 exited with status 127
2023-05-29 06:09:26,010 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w6q9cvnj', purging
2023-05-29 06:09:26,011 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:26,011 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:09:26,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:26,163 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:26,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:26,216 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:26,275 - distributed.nanny - INFO - Worker process 26939 exited with status 127
2023-05-29 06:09:26,479 - distributed.nanny - INFO - Worker process 26942 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:26,599 - distributed.nanny - INFO - Worker process 26947 exited with status 127
2023-05-29 06:09:26,831 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lnwakvm0', purging
2023-05-29 06:09:26,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-24ge7toy', purging
2023-05-29 06:09:26,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0xa2ahh4', purging
2023-05-29 06:09:26,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:09:26,832 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:26,969 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:09:27,033 - distributed.nanny - INFO - Worker process 26965 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:27,117 - distributed.nanny - INFO - Worker process 26972 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:27,306 - distributed.nanny - INFO - Worker process 26976 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:09:27,444 - distributed.nanny - INFO - Worker process 27002 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-29 06:09:57,574 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:09:57,578 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40605 instead
  warnings.warn(
2023-05-29 06:09:57,581 - distributed.scheduler - INFO - State start
2023-05-29 06:09:57,815 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:09:57,816 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:09:57,816 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:09:57,816 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:09:58,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38749'
2023-05-29 06:09:58,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41775'
2023-05-29 06:09:58,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46595'
2023-05-29 06:09:58,611 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36451'
2023-05-29 06:09:58,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46307'
2023-05-29 06:09:58,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37639'
2023-05-29 06:09:58,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38871'
2023-05-29 06:09:58,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37585'
2023-05-29 06:10:00,116 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-psehdm33', purging
2023-05-29 06:10:00,116 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-so77mj3v', purging
2023-05-29 06:10:00,116 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-iy_hg0nh', purging
2023-05-29 06:10:00,117 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3na0ra6t', purging
2023-05-29 06:10:00,117 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:00,117 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:00,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:00,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:00,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:00,229 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:00,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:00,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:00,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:00,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:00,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:00,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:00,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:00,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:00,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:00,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:00,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:00,351 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:00,475 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:00,475 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:00,480 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:00,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:00,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:00,483 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:00,998 - distributed.nanny - INFO - Worker process 27207 exited with status 127
2023-05-29 06:10:00,999 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:02,491 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-evehq64s', purging
2023-05-29 06:10:02,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:02,492 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:02,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:03,964 - distributed.nanny - INFO - Worker process 27210 exited with status 127
2023-05-29 06:10:03,965 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:04,599 - distributed.nanny - INFO - Worker process 27214 exited with status 127
2023-05-29 06:10:04,599 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:04,626 - distributed.nanny - INFO - Worker process 27226 exited with status 127
2023-05-29 06:10:04,627 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:04,658 - distributed.nanny - INFO - Worker process 27218 exited with status 127
2023-05-29 06:10:04,659 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:04,716 - distributed.nanny - INFO - Worker process 27231 exited with status 127
2023-05-29 06:10:04,718 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:04,777 - distributed.nanny - INFO - Worker process 27228 exited with status 127
2023-05-29 06:10:04,778 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:04,830 - distributed.nanny - INFO - Worker process 27223 exited with status 127
2023-05-29 06:10:04,831 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:05,270 - distributed.nanny - INFO - Worker process 27271 exited with status 127
2023-05-29 06:10:05,271 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:05,641 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-23mgjd_w', purging
2023-05-29 06:10:05,641 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qcl58msf', purging
2023-05-29 06:10:05,642 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pr_lq7ps', purging
2023-05-29 06:10:05,642 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2cbnmmwp', purging
2023-05-29 06:10:05,642 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bsz24lvl', purging
2023-05-29 06:10:05,642 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z9wldvfd', purging
2023-05-29 06:10:05,643 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9873z3_9', purging
2023-05-29 06:10:05,643 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xd74gq2w', purging
2023-05-29 06:10:05,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:05,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:05,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:06,089 - distributed.nanny - INFO - Worker process 27291 exited with status 127
2023-05-29 06:10:06,090 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:06,208 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cckszgo4', purging
2023-05-29 06:10:06,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:06,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:06,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:06,333 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:06,333 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:06,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:06,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:06,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:06,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:06,485 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:06,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:06,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:06,535 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:06,535 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:06,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:06,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:06,590 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:06,595 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:06,953 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:06,953 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:07,040 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:07,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:07,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:07,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:07,983 - distributed.nanny - INFO - Worker process 27310 exited with status 127
2023-05-29 06:10:07,984 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:09,511 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n37qzs8t', purging
2023-05-29 06:10:09,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:09,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:09,868 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:10,085 - distributed.nanny - INFO - Worker process 27307 exited with status 127
2023-05-29 06:10:10,086 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:10,153 - distributed.nanny - INFO - Worker process 27301 exited with status 127
2023-05-29 06:10:10,154 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:10,369 - distributed.nanny - INFO - Worker process 27304 exited with status 127
2023-05-29 06:10:10,370 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:10,677 - distributed.nanny - INFO - Worker process 27316 exited with status 127
2023-05-29 06:10:10,677 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:10,703 - distributed.nanny - INFO - Worker process 27313 exited with status 127
2023-05-29 06:10:10,704 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:10,920 - distributed.nanny - INFO - Worker process 27323 exited with status 127
2023-05-29 06:10:10,921 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:10,949 - distributed.nanny - INFO - Worker process 27338 exited with status 127
2023-05-29 06:10:10,949 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:11,258 - distributed.nanny - INFO - Worker process 27375 exited with status 127
2023-05-29 06:10:11,258 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:11,547 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-78bb7ssb', purging
2023-05-29 06:10:11,547 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qev4y70h', purging
2023-05-29 06:10:11,547 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u02m4sdj', purging
2023-05-29 06:10:11,548 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7_gb4a_0', purging
2023-05-29 06:10:11,548 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w7trwilm', purging
2023-05-29 06:10:11,548 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hxnjngtu', purging
2023-05-29 06:10:11,548 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ko7qcpt2', purging
2023-05-29 06:10:11,549 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ibp8jddl', purging
2023-05-29 06:10:11,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:11,549 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:11,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:11,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:11,737 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:11,831 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:11,925 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:11,925 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:12,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38749'. Reason: nanny-close
2023-05-29 06:10:12,099 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46595'. Reason: nanny-close
2023-05-29 06:10:12,099 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36451'. Reason: nanny-close
2023-05-29 06:10:12,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46307'. Reason: nanny-close
2023-05-29 06:10:12,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37639'. Reason: nanny-close
2023-05-29 06:10:12,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38871'. Reason: nanny-close
2023-05-29 06:10:12,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41775'. Reason: nanny-close
2023-05-29 06:10:12,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37585'. Reason: nanny-close
2023-05-29 06:10:12,140 - distributed.nanny - INFO - Worker process 27393 exited with status 127
2023-05-29 06:10:12,146 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:12,205 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mhgvdeae', purging
2023-05-29 06:10:12,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:12,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:12,233 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:12,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:12,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:12,430 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b4efmpg7', purging
2023-05-29 06:10:12,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:12,431 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:12,440 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:12,440 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:12,467 - distributed.nanny - INFO - Worker process 27396 exited with status 127
2023-05-29 06:10:12,475 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:12,497 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:12,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:12,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:12,773 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:12,821 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:12,851 - distributed.nanny - INFO - Worker process 27399 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:13,472 - distributed.nanny - INFO - Worker process 27404 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:13,748 - distributed.nanny - INFO - Worker process 27408 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:13,794 - distributed.nanny - INFO - Worker process 27416 exited with status 127
2023-05-29 06:10:13,833 - distributed.nanny - INFO - Worker process 27419 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:13,941 - distributed.nanny - INFO - Worker process 27424 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-29 06:10:44,469 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:10:44,473 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38977 instead
  warnings.warn(
2023-05-29 06:10:44,476 - distributed.scheduler - INFO - State start
2023-05-29 06:10:44,558 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:10:44,559 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:10:44,560 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:10:44,560 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:10:44,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42081'
2023-05-29 06:10:44,982 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36705'
2023-05-29 06:10:44,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46731'
2023-05-29 06:10:44,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44205'
2023-05-29 06:10:45,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45093'
2023-05-29 06:10:45,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39713'
2023-05-29 06:10:45,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38873'
2023-05-29 06:10:45,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42565'
2023-05-29 06:10:46,602 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:46,602 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v63mgvad', purging
2023-05-29 06:10:46,602 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:46,602 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sg3ih310', purging
2023-05-29 06:10:46,603 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xalv5oar', purging
2023-05-29 06:10:46,603 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qr5m84in', purging
2023-05-29 06:10:46,603 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9eeofaqs', purging
2023-05-29 06:10:46,604 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5j2ewiya', purging
2023-05-29 06:10:46,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:46,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:46,674 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:46,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:46,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:46,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:46,714 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:46,714 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:46,714 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:46,714 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:46,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:46,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:46,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:46,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:46,884 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:46,894 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:46,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:46,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:46,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:46,916 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:46,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:46,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:50,044 - distributed.nanny - INFO - Worker process 27654 exited with status 127
2023-05-29 06:10:50,045 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:50,071 - distributed.nanny - INFO - Worker process 27671 exited with status 127
2023-05-29 06:10:50,072 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:50,101 - distributed.nanny - INFO - Worker process 27665 exited with status 127
2023-05-29 06:10:50,102 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:50,660 - distributed.nanny - INFO - Worker process 27668 exited with status 127
2023-05-29 06:10:50,661 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:50,686 - distributed.nanny - INFO - Worker process 27662 exited with status 127
2023-05-29 06:10:50,687 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:50,714 - distributed.nanny - INFO - Worker process 27658 exited with status 127
2023-05-29 06:10:50,715 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:50,740 - distributed.nanny - INFO - Worker process 27650 exited with status 127
2023-05-29 06:10:50,741 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:50,776 - distributed.nanny - INFO - Worker process 27647 exited with status 127
2023-05-29 06:10:50,776 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:51,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-devyquzq', purging
2023-05-29 06:10:51,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ibydubo2', purging
2023-05-29 06:10:51,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g_pxzti7', purging
2023-05-29 06:10:51,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mwhmjjcl', purging
2023-05-29 06:10:51,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tsuneduq', purging
2023-05-29 06:10:51,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m300hk4y', purging
2023-05-29 06:10:51,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e6nxle_q', purging
2023-05-29 06:10:51,737 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nxxplv9n', purging
2023-05-29 06:10:51,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:51,737 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:51,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:51,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:51,769 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:51,769 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:51,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:51,796 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:51,998 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:52,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:52,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:52,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:52,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:52,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:52,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:52,420 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:52,420 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:52,499 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:52,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:52,519 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:52,522 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:52,522 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:52,523 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:52,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:54,978 - distributed.nanny - INFO - Worker process 27732 exited with status 127
2023-05-29 06:10:54,979 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:55,020 - distributed.nanny - INFO - Worker process 27729 exited with status 127
2023-05-29 06:10:55,021 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:55,050 - distributed.nanny - INFO - Worker process 27735 exited with status 127
2023-05-29 06:10:55,051 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:55,685 - distributed.nanny - INFO - Worker process 27739 exited with status 127
2023-05-29 06:10:55,686 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:55,772 - distributed.nanny - INFO - Worker process 27751 exited with status 127
2023-05-29 06:10:55,773 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:56,493 - distributed.nanny - INFO - Worker process 27754 exited with status 127
2023-05-29 06:10:56,494 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:56,533 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ncm_d7ti', purging
2023-05-29 06:10:56,534 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bbo8r28_', purging
2023-05-29 06:10:56,534 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kaja3vwv', purging
2023-05-29 06:10:56,534 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0wt6bny2', purging
2023-05-29 06:10:56,535 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s92pa3mm', purging
2023-05-29 06:10:56,535 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_wz55db_', purging
2023-05-29 06:10:56,535 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lk9rko4g', purging
2023-05-29 06:10:56,536 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ywrtw1bv', purging
2023-05-29 06:10:56,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:56,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:56,581 - distributed.nanny - INFO - Worker process 27744 exited with status 127
2023-05-29 06:10:56,582 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:56,640 - distributed.nanny - INFO - Worker process 27748 exited with status 127
2023-05-29 06:10:56,641 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:56,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:56,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:56,656 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:56,656 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:56,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:56,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:56,869 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:57,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:57,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:57,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:57,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:57,522 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:57,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:58,050 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:58,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:58,094 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:58,125 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gz1xvrga', purging
2023-05-29 06:10:58,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:58,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:58,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:58,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:58,280 - distributed.nanny - INFO - Worker process 27805 exited with status 127
2023-05-29 06:10:58,281 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:58,466 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:58,502 - distributed.nanny - INFO - Worker process 27808 exited with status 127
2023-05-29 06:10:58,503 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:10:58,511 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:58,537 - distributed.nanny - INFO - Worker process 27811 exited with status 127
2023-05-29 06:10:58,538 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:58,971 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46731'. Reason: nanny-close
2023-05-29 06:10:58,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39713'. Reason: nanny-close
2023-05-29 06:10:58,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38873'. Reason: nanny-close
2023-05-29 06:10:58,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42081'. Reason: nanny-close
2023-05-29 06:10:58,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36705'. Reason: nanny-close
2023-05-29 06:10:58,972 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44205'. Reason: nanny-close
2023-05-29 06:10:58,973 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45093'. Reason: nanny-close
2023-05-29 06:10:58,973 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42565'. Reason: nanny-close
2023-05-29 06:10:59,083 - distributed.nanny - INFO - Worker process 27825 exited with status 127
2023-05-29 06:10:59,117 - distributed.nanny - INFO - Worker process 27821 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:10:59,495 - distributed.nanny - INFO - Worker process 27830 exited with status 127
2023-05-29 06:10:59,531 - distributed.nanny - INFO - Worker process 27837 exited with status 127
2023-05-29 06:10:59,554 - distributed.nanny - INFO - Worker process 27834 exited with status 127
2023-05-29 06:10:59,766 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nmoo0aot', purging
2023-05-29 06:10:59,767 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9o6qmkm4', purging
2023-05-29 06:10:59,767 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-peywitw_', purging
2023-05-29 06:10:59,768 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2km_etrk', purging
2023-05-29 06:10:59,768 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cv8tqqmo', purging
2023-05-29 06:10:59,768 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kxhu1tnp', purging
2023-05-29 06:10:59,768 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lrk41flb', purging
2023-05-29 06:10:59,769 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:59,769 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:59,793 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:10:59,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:59,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:10:59,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:10:59,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:00,044 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:00,044 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:00,351 - distributed.nanny - INFO - Worker process 27875 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:00,678 - distributed.nanny - INFO - Worker process 27882 exited with status 127
2023-05-29 06:11:00,700 - distributed.nanny - INFO - Worker process 27885 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-29 06:11:31,000 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:11:31,004 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33411 instead
  warnings.warn(
2023-05-29 06:11:31,007 - distributed.scheduler - INFO - State start
2023-05-29 06:11:31,691 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:11:31,693 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:11:31,693 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:11:31,694 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:11:31,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46307'
2023-05-29 06:11:31,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39559'
2023-05-29 06:11:31,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42173'
2023-05-29 06:11:31,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38795'
2023-05-29 06:11:31,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37853'
2023-05-29 06:11:32,007 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44579'
2023-05-29 06:11:32,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32979'
2023-05-29 06:11:32,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39181'
2023-05-29 06:11:33,610 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fsf784s_', purging
2023-05-29 06:11:33,610 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rm4ty056', purging
2023-05-29 06:11:33,610 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-893fik33', purging
2023-05-29 06:11:33,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:33,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:33,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:33,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:33,614 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:33,614 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:33,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:33,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:33,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:33,639 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:33,640 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:33,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:33,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:33,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:33,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:33,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:33,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:33,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:33,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:33,684 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:33,685 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:33,728 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:33,728 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:33,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:35,945 - distributed.nanny - INFO - Worker process 28090 exited with status 127
2023-05-29 06:11:35,946 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:36,014 - distributed.nanny - INFO - Worker process 28094 exited with status 127
2023-05-29 06:11:36,015 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:36,048 - distributed.nanny - INFO - Worker process 28098 exited with status 127
2023-05-29 06:11:36,048 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:37,281 - distributed.nanny - INFO - Worker process 28087 exited with status 127
2023-05-29 06:11:37,282 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:37,325 - distributed.nanny - INFO - Worker process 28103 exited with status 127
2023-05-29 06:11:37,326 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:37,351 - distributed.nanny - INFO - Worker process 28111 exited with status 127
2023-05-29 06:11:37,352 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:37,454 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dclco7k2', purging
2023-05-29 06:11:37,454 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-03sr77m7', purging
2023-05-29 06:11:37,454 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oo54y_cp', purging
2023-05-29 06:11:37,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s9hcwzle', purging
2023-05-29 06:11:37,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e8uogcfg', purging
2023-05-29 06:11:37,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nz249rq6', purging
2023-05-29 06:11:37,456 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i2cbp58s', purging
2023-05-29 06:11:37,456 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g9z30u3c', purging
2023-05-29 06:11:37,457 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:37,457 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:37,539 - distributed.nanny - INFO - Worker process 28108 exited with status 127
2023-05-29 06:11:37,540 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:37,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:37,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:37,550 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:37,550 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:37,575 - distributed.nanny - INFO - Worker process 28105 exited with status 127
2023-05-29 06:11:37,575 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:37,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:37,678 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:37,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:38,870 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:38,870 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:38,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:38,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:38,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:38,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:38,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:38,950 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:39,008 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:39,138 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:39,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:39,161 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:39,161 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:39,180 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:39,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:39,575 - distributed.nanny - INFO - Worker process 28172 exited with status 127
2023-05-29 06:11:39,576 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:39,601 - distributed.nanny - INFO - Worker process 28166 exited with status 127
2023-05-29 06:11:39,601 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:39,807 - distributed.nanny - INFO - Worker process 28175 exited with status 127
2023-05-29 06:11:39,808 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:40,613 - distributed.nanny - INFO - Worker process 28182 exited with status 127
2023-05-29 06:11:40,614 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:40,641 - distributed.nanny - INFO - Worker process 28185 exited with status 127
2023-05-29 06:11:40,642 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:40,714 - distributed.nanny - INFO - Worker process 28188 exited with status 127
2023-05-29 06:11:40,715 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:41,160 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ejhhtcvw', purging
2023-05-29 06:11:41,160 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hh05b0hv', purging
2023-05-29 06:11:41,160 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-03elvtoy', purging
2023-05-29 06:11:41,161 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a5q6puk3', purging
2023-05-29 06:11:41,161 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sncwkyz4', purging
2023-05-29 06:11:41,161 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x_8pb_vo', purging
2023-05-29 06:11:41,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:41,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:41,191 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:41,191 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:41,387 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:41,387 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:41,570 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:41,575 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:41,760 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:42,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:42,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:42,163 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:42,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:42,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:42,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:42,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:42,425 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:42,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:42,906 - distributed.nanny - INFO - Worker process 28197 exited with status 127
2023-05-29 06:11:42,907 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:42,977 - distributed.nanny - INFO - Worker process 28194 exited with status 127
2023-05-29 06:11:42,978 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:44,501 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sselhmvz', purging
2023-05-29 06:11:44,501 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2tlekz2z', purging
2023-05-29 06:11:44,502 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:44,502 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:44,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:44,503 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:44,598 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:44,607 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:44,878 - distributed.nanny - INFO - Worker process 28239 exited with status 127
2023-05-29 06:11:44,878 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:44,924 - distributed.nanny - INFO - Worker process 28247 exited with status 127
2023-05-29 06:11:44,925 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:44,970 - distributed.nanny - INFO - Worker process 28244 exited with status 127
2023-05-29 06:11:44,970 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:45,008 - distributed.nanny - INFO - Worker process 28260 exited with status 127
2023-05-29 06:11:45,008 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:45,035 - distributed.nanny - INFO - Worker process 28263 exited with status 127
2023-05-29 06:11:45,036 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:45,062 - distributed.nanny - INFO - Worker process 28266 exited with status 127
2023-05-29 06:11:45,063 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:11:45,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46307'. Reason: nanny-close
2023-05-29 06:11:45,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42173'. Reason: nanny-close
2023-05-29 06:11:45,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38795'. Reason: nanny-close
2023-05-29 06:11:45,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37853'. Reason: nanny-close
2023-05-29 06:11:45,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44579'. Reason: nanny-close
2023-05-29 06:11:45,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32979'. Reason: nanny-close
2023-05-29 06:11:45,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39559'. Reason: nanny-close
2023-05-29 06:11:45,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39181'. Reason: nanny-close
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:46,453 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1p38fs7i', purging
2023-05-29 06:11:46,454 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0dbtxwem', purging
2023-05-29 06:11:46,454 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3o85s_10', purging
2023-05-29 06:11:46,454 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4f4anqv3', purging
2023-05-29 06:11:46,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b3i_kh6b', purging
2023-05-29 06:11:46,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b_2c73__', purging
2023-05-29 06:11:46,456 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:46,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:46,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:46,513 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:46,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:46,563 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mfk4wx4g', purging
2023-05-29 06:11:46,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:46,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:46,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:46,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:46,579 - distributed.nanny - INFO - Worker process 28292 exited with status 127
2023-05-29 06:11:46,617 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ysbvnqmh', purging
2023-05-29 06:11:46,618 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:46,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:46,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:46,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:11:46,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:11:46,654 - distributed.nanny - INFO - Worker process 28295 exited with status 127
2023-05-29 06:11:46,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:46,667 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:46,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:11:46,713 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:11:48,274 - distributed.nanny - INFO - Worker process 28324 exited with status 127
2023-05-29 06:11:48,302 - distributed.nanny - INFO - Worker process 28327 exited with status 127
2023-05-29 06:11:48,335 - distributed.nanny - INFO - Worker process 28333 exited with status 127
2023-05-29 06:11:48,362 - distributed.nanny - INFO - Worker process 28330 exited with status 127
2023-05-29 06:11:48,390 - distributed.nanny - INFO - Worker process 28336 exited with status 127
2023-05-29 06:11:48,411 - distributed.nanny - INFO - Worker process 28339 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-29 06:12:17,369 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:12:17,373 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45243 instead
  warnings.warn(
2023-05-29 06:12:17,376 - distributed.scheduler - INFO - State start
2023-05-29 06:12:17,413 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:12:17,414 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:12:17,414 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:12:17,415 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:12:17,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45485'
2023-05-29 06:12:18,849 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mf3eeh3x', purging
2023-05-29 06:12:18,850 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jz3dkmub', purging
2023-05-29 06:12:18,851 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xofod4hg', purging
2023-05-29 06:12:18,851 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w2yf526e', purging
2023-05-29 06:12:18,851 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-napyb6qw', purging
2023-05-29 06:12:18,851 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l81yaulv', purging
2023-05-29 06:12:18,852 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:18,852 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:19,106 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:19,510 - distributed.nanny - INFO - Worker process 28557 exited with status 127
2023-05-29 06:12:19,511 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:20,826 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vblou9ez', purging
2023-05-29 06:12:20,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:20,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:21,076 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:21,431 - distributed.nanny - INFO - Worker process 28567 exited with status 127
2023-05-29 06:12:21,432 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:22,772 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5j3_mgnw', purging
2023-05-29 06:12:22,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:22,773 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:23,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:23,401 - distributed.nanny - INFO - Worker process 28577 exited with status 127
2023-05-29 06:12:23,402 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:24,693 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tpmik81k', purging
2023-05-29 06:12:24,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:24,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:24,950 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:25,339 - distributed.nanny - INFO - Worker process 28587 exited with status 127
2023-05-29 06:12:25,340 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:12:25,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45485'. Reason: nanny-close
2023-05-29 06:12:26,623 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rhoos82z', purging
2023-05-29 06:12:26,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:12:26,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:12:26,870 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:12:27,226 - distributed.nanny - INFO - Worker process 28597 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-29 06:12:59,365 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:12:59,369 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-05-29 06:12:59,372 - distributed.scheduler - INFO - State start
2023-05-29 06:12:59,391 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:12:59,391 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-29 06:12:59,392 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-05-29 06:12:59,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42555'
2023-05-29 06:12:59,583 - distributed.scheduler - INFO - Receive client connection: Client-db95c8e4-fde7-11ed-a665-d8c49764f6bb
2023-05-29 06:12:59,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40474
2023-05-29 06:13:00,119 - distributed.scheduler - INFO - Receive client connection: Client-db63134c-fde7-11ed-a590-d8c49764f6bb
2023-05-29 06:13:00,120 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47964
2023-05-29 06:13:00,878 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ki0vcolc', purging
2023-05-29 06:13:00,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:00,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:01,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:01,509 - distributed.nanny - INFO - Worker process 28855 exited with status 127
2023-05-29 06:13:01,510 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:02,300 - distributed.scheduler - INFO - Receive client connection: Client-d4ff867a-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:02,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47980
2023-05-29 06:13:02,852 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-covruyud', purging
2023-05-29 06:13:02,853 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:02,853 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:03,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:03,489 - distributed.nanny - INFO - Worker process 28866 exited with status 127
2023-05-29 06:13:03,490 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:04,836 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tmoxrmgz', purging
2023-05-29 06:13:04,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:04,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:05,125 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:05,500 - distributed.nanny - INFO - Worker process 28876 exited with status 127
2023-05-29 06:13:05,501 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:06,849 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1ml_hej3', purging
2023-05-29 06:13:06,850 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:06,850 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:06,916 - distributed.scheduler - INFO - Remove client Client-d4ff867a-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:06,916 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47980; closing.
2023-05-29 06:13:06,917 - distributed.scheduler - INFO - Remove client Client-d4ff867a-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:06,917 - distributed.scheduler - INFO - Close client connection: Client-d4ff867a-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:07,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:07,478 - distributed.nanny - INFO - Worker process 28886 exited with status 127
2023-05-29 06:13:07,479 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:08,812 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-09ssd24x', purging
2023-05-29 06:13:08,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:08,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:09,068 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:09,437 - distributed.nanny - INFO - Worker process 28896 exited with status 127
2023-05-29 06:13:09,437 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:13:09,683 - distributed.scheduler - INFO - Remove client Client-db95c8e4-fde7-11ed-a665-d8c49764f6bb
2023-05-29 06:13:09,684 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40474; closing.
2023-05-29 06:13:09,684 - distributed.scheduler - INFO - Remove client Client-db95c8e4-fde7-11ed-a665-d8c49764f6bb
2023-05-29 06:13:09,684 - distributed.scheduler - INFO - Close client connection: Client-db95c8e4-fde7-11ed-a665-d8c49764f6bb
2023-05-29 06:13:10,190 - distributed.scheduler - INFO - Remove client Client-db63134c-fde7-11ed-a590-d8c49764f6bb
2023-05-29 06:13:10,190 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47964; closing.
2023-05-29 06:13:10,190 - distributed.scheduler - INFO - Remove client Client-db63134c-fde7-11ed-a590-d8c49764f6bb
2023-05-29 06:13:10,190 - distributed.scheduler - INFO - Close client connection: Client-db63134c-fde7-11ed-a590-d8c49764f6bb
2023-05-29 06:13:10,191 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42555'. Reason: nanny-close
2023-05-29 06:13:10,742 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_kxpb2is', purging
2023-05-29 06:13:10,742 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:13:10,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:13:10,993 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:13:11,353 - distributed.nanny - INFO - Worker process 28906 exited with status 127
2023-05-29 06:13:38,777 - distributed.scheduler - INFO - Receive client connection: Client-f3c9bfe8-fde7-11ed-a53e-d8c49764f6bb
2023-05-29 06:13:38,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56622
2023-05-29 06:13:40,223 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-29 06:13:40,224 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:13:40,225 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:13:40,226 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-29 06:13:40,227 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-29 06:13:42,314 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:13:42,318 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35057 instead
  warnings.warn(
2023-05-29 06:13:42,321 - distributed.scheduler - INFO - State start
2023-05-29 06:13:42,452 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:13:42,452 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:13:42,453 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:13:42,453 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-05-29 06:28:35,465 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:28:35,469 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34889 instead
  warnings.warn(
2023-05-29 06:28:35,472 - distributed.scheduler - INFO - State start
2023-05-29 06:28:35,579 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:28:35,580 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-29 06:28:35,580 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34889/status
2023-05-29 06:28:35,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33975'
2023-05-29 06:28:36,965 - distributed.scheduler - INFO - Receive client connection: Client-095bdffc-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:28:36,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45110
2023-05-29 06:28:37,078 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_hedqng8', purging
2023-05-29 06:28:37,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:28:37,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:28:37,085 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:28:37,427 - distributed.nanny - INFO - Worker process 33519 exited with status 127
2023-05-29 06:28:37,428 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:28:38,822 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-thr1ukt7', purging
2023-05-29 06:28:38,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:28:38,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:28:38,829 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:28:39,177 - distributed.nanny - INFO - Worker process 33530 exited with status 127
2023-05-29 06:28:39,177 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:28:40,549 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rgl3v_6q', purging
2023-05-29 06:28:40,550 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:28:40,550 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:28:40,556 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:28:40,956 - distributed.nanny - INFO - Worker process 33540 exited with status 127
2023-05-29 06:28:40,957 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:28:42,343 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cwbs31zi', purging
2023-05-29 06:28:42,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:28:42,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:28:42,350 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:28:42,693 - distributed.nanny - INFO - Worker process 33550 exited with status 127
2023-05-29 06:28:42,694 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:28:44,072 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-951f4k97', purging
2023-05-29 06:28:44,073 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:28:44,073 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:28:44,079 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:28:44,483 - distributed.nanny - INFO - Worker process 33560 exited with status 127
2023-05-29 06:28:44,483 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:28:45,817 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-376d9sxy', purging
2023-05-29 06:28:45,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:28:45,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:28:45,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:28:46,169 - distributed.nanny - INFO - Worker process 33570 exited with status 127
2023-05-29 06:28:46,170 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:28:47,040 - distributed.scheduler - INFO - Remove client Client-095bdffc-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:28:47,040 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45110; closing.
2023-05-29 06:28:47,041 - distributed.scheduler - INFO - Remove client Client-095bdffc-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:28:47,041 - distributed.scheduler - INFO - Close client connection: Client-095bdffc-fdea-11ed-a590-d8c49764f6bb
2023-05-29 06:28:47,042 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33975'. Reason: nanny-close
2023-05-29 06:28:47,590 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1e48fv2e', purging
2023-05-29 06:28:47,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:28:47,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:28:47,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:28:47,950 - distributed.nanny - INFO - Worker process 33580 exited with status 127
2023-05-29 06:29:17,074 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-29 06:29:17,074 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:29:17,075 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:29:17,075 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-29 06:29:17,076 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-05-29 06:29:19,133 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:29:19,137 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35383 instead
  warnings.warn(
2023-05-29 06:29:19,141 - distributed.scheduler - INFO - State start
2023-05-29 06:29:19,161 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:29:19,162 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:29:19,162 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:29:19,163 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:29:19,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42193'
2023-05-29 06:29:19,357 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44973'
2023-05-29 06:29:19,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37139'
2023-05-29 06:29:19,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35287'
2023-05-29 06:29:19,374 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34175'
2023-05-29 06:29:19,382 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41949'
2023-05-29 06:29:19,391 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42243'
2023-05-29 06:29:19,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41745'
2023-05-29 06:29:20,852 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wrdn65hv', purging
2023-05-29 06:29:20,853 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:20,853 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:20,879 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:20,956 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:20,956 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:20,992 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:20,992 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:20,992 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:21,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:21,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:21,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:21,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:21,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:21,038 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:21,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:21,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:21,113 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:21,113 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:21,161 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:21,186 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:21,187 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:21,189 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:21,189 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:21,192 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:21,782 - distributed.nanny - INFO - Worker process 33757 exited with status 127
2023-05-29 06:29:21,783 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:23,056 - distributed.nanny - INFO - Worker process 33768 exited with status 127
2023-05-29 06:29:23,057 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:23,099 - distributed.nanny - INFO - Worker process 33775 exited with status 127
2023-05-29 06:29:23,100 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:23,132 - distributed.nanny - INFO - Worker process 33778 exited with status 127
2023-05-29 06:29:23,133 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:23,171 - distributed.nanny - INFO - Worker process 33760 exited with status 127
2023-05-29 06:29:23,172 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:23,199 - distributed.nanny - INFO - Worker process 33781 exited with status 127
2023-05-29 06:29:23,199 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:23,226 - distributed.nanny - INFO - Worker process 33764 exited with status 127
2023-05-29 06:29:23,226 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:23,251 - distributed.nanny - INFO - Worker process 33772 exited with status 127
2023-05-29 06:29:23,252 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:23,276 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r_8km77s', purging
2023-05-29 06:29:23,277 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zen7nb8p', purging
2023-05-29 06:29:23,277 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mhwx6kef', purging
2023-05-29 06:29:23,277 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rjj0spcz', purging
2023-05-29 06:29:23,278 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s710ykog', purging
2023-05-29 06:29:23,278 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1459vo15', purging
2023-05-29 06:29:23,278 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6mz65w2m', purging
2023-05-29 06:29:23,279 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-34tesbgz', purging
2023-05-29 06:29:23,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:23,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:23,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:24,123 - distributed.nanny - INFO - Worker process 33824 exited with status 127
2023-05-29 06:29:24,124 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:24,459 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5z9n9o5_', purging
2023-05-29 06:29:24,460 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:24,460 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:24,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:24,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:24,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:24,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:24,719 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:24,739 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:24,750 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:24,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:24,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:24,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:24,826 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:24,860 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:24,861 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:24,891 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u6vavkb_', purging
2023-05-29 06:29:24,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:24,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:24,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:24,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:25,047 - distributed.nanny - INFO - Worker process 33844 exited with status 127
2023-05-29 06:29:25,048 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:25,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:25,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:25,783 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:25,783 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:25,874 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:26,079 - distributed.nanny - INFO - Worker process 33850 exited with status 127
2023-05-29 06:29:26,080 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:26,262 - distributed.nanny - INFO - Worker process 33847 exited with status 127
2023-05-29 06:29:26,263 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:26,596 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1ysvu1ww', purging
2023-05-29 06:29:26,596 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-efjfcrk2', purging
2023-05-29 06:29:26,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:26,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:26,781 - distributed.nanny - INFO - Worker process 33853 exited with status 127
2023-05-29 06:29:26,782 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:26,952 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:26,978 - distributed.nanny - INFO - Worker process 33856 exited with status 127
2023-05-29 06:29:26,979 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:27,186 - distributed.nanny - INFO - Worker process 33862 exited with status 127
2023-05-29 06:29:27,187 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:27,225 - distributed.nanny - INFO - Worker process 33859 exited with status 127
2023-05-29 06:29:27,225 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:27,632 - distributed.nanny - INFO - Worker process 33878 exited with status 127
2023-05-29 06:29:27,633 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:27,711 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-op6iandc', purging
2023-05-29 06:29:27,711 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qf_bv6pw', purging
2023-05-29 06:29:27,712 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_wb2rdi6', purging
2023-05-29 06:29:27,712 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s63x6k6o', purging
2023-05-29 06:29:27,712 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7h665785', purging
2023-05-29 06:29:27,713 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:27,713 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:27,723 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:27,723 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:27,736 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:27,751 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:27,813 - distributed.nanny - INFO - Worker process 33904 exited with status 127
2023-05-29 06:29:27,814 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:28,366 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7cye29hw', purging
2023-05-29 06:29:28,366 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nn5xftan', purging
2023-05-29 06:29:28,367 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:28,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:28,383 - distributed.nanny - INFO - Worker process 33930 exited with status 127
2023-05-29 06:29:28,384 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:28,420 - distributed.nanny - INFO - Worker process 33927 exited with status 127
2023-05-29 06:29:28,420 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:28,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:28,440 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b52ko7rb', purging
2023-05-29 06:29:28,441 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:28,441 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:28,466 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:28,839 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:28,840 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:28,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:28,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:28,935 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:28,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:29,058 - distributed.nanny - INFO - Worker process 33941 exited with status 127
2023-05-29 06:29:29,059 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:29,147 - distributed.nanny - INFO - Worker process 33947 exited with status 127
2023-05-29 06:29:29,148 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:29,265 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nskdm779', purging
2023-05-29 06:29:29,265 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c4zj176z', purging
2023-05-29 06:29:29,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:29,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:29,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:29,351 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:29,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:29,519 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:29,893 - distributed.nanny - INFO - Worker process 33953 exited with status 127
2023-05-29 06:29:29,894 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:29,924 - distributed.nanny - INFO - Worker process 33950 exited with status 127
2023-05-29 06:29:29,925 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:30,019 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jje_y0gq', purging
2023-05-29 06:29:30,020 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yxla639l', purging
2023-05-29 06:29:30,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:30,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:30,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:30,052 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:30,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:30,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:30,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:30,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:30,700 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:30,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:30,731 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:30,939 - distributed.nanny - INFO - Worker process 33971 exited with status 127
2023-05-29 06:29:30,940 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:30,963 - distributed.nanny - INFO - Worker process 33961 exited with status 127
2023-05-29 06:29:30,964 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:31,000 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:31,438 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ri7lb1ij', purging
2023-05-29 06:29:31,438 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fbicetv_', purging
2023-05-29 06:29:31,439 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:31,439 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:31,478 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:31,478 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:31,527 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:31,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:31,569 - distributed.nanny - INFO - Worker process 33984 exited with status 127
2023-05-29 06:29:31,570 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:31,803 - distributed.nanny - INFO - Worker process 33989 exited with status 127
2023-05-29 06:29:31,804 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:32,327 - distributed.nanny - INFO - Worker process 34009 exited with status 127
2023-05-29 06:29:32,328 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:32,367 - distributed.nanny - INFO - Worker process 34017 exited with status 127
2023-05-29 06:29:32,368 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:32,436 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kio00rqx', purging
2023-05-29 06:29:32,436 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-121pfsuv', purging
2023-05-29 06:29:32,437 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j5bmw0zs', purging
2023-05-29 06:29:32,438 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-555_8lru', purging
2023-05-29 06:29:32,438 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:32,438 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:32,463 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:32,528 - distributed.nanny - INFO - Worker process 34036 exited with status 127
2023-05-29 06:29:32,529 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:32,554 - distributed.nanny - INFO - Worker process 34030 exited with status 127
2023-05-29 06:29:32,555 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:32,569 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t8o91z4j', purging
2023-05-29 06:29:32,570 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w6k5e0r1', purging
2023-05-29 06:29:32,570 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:32,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:32,598 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:33,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:33,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:33,275 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:33,292 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jn2kif0b', purging
2023-05-29 06:29:33,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:33,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:33,300 - distributed.nanny - INFO - Worker process 34059 exited with status 127
2023-05-29 06:29:33,301 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:29:33,335 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:33,730 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37139'. Reason: nanny-close
2023-05-29 06:29:33,730 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42193'. Reason: nanny-close
2023-05-29 06:29:33,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44973'. Reason: nanny-close
2023-05-29 06:29:33,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35287'. Reason: nanny-close
2023-05-29 06:29:33,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34175'. Reason: nanny-close
2023-05-29 06:29:33,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41949'. Reason: nanny-close
2023-05-29 06:29:33,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42243'. Reason: nanny-close
2023-05-29 06:29:33,731 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41745'. Reason: nanny-close
2023-05-29 06:29:33,886 - distributed.nanny - INFO - Worker process 34056 exited with status 127
2023-05-29 06:29:33,925 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-olqcmzx0', purging
2023-05-29 06:29:33,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:33,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:33,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:33,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:33,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:34,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:34,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:34,086 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:34,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:34,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:34,246 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:34,248 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-29 06:29:34,281 - distributed.nanny - INFO - Worker process 34079 exited with status 127
2023-05-29 06:29:34,356 - distributed.nanny - INFO - Worker process 34083 exited with status 127
2023-05-29 06:29:34,846 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8hrjns_a', purging
2023-05-29 06:29:34,847 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zr6hgadx', purging
2023-05-29 06:29:34,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:29:34,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:29:34,895 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:35,360 - distributed.nanny - INFO - Worker process 34093 exited with status 127
2023-05-29 06:29:35,413 - distributed.nanny - INFO - Worker process 34096 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:35,637 - distributed.nanny - INFO - Worker process 34105 exited with status 127
2023-05-29 06:29:35,660 - distributed.nanny - INFO - Worker process 34108 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:29:35,934 - distributed.nanny - INFO - Worker process 34127 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-05-29 06:30:05,706 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:30:05,709 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34773 instead
  warnings.warn(
2023-05-29 06:30:05,713 - distributed.scheduler - INFO - State start
2023-05-29 06:30:05,735 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:30:05,736 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:30:05,736 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:30:05,737 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:30:05,996 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40931'
2023-05-29 06:30:07,343 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x1_uk3vb', purging
2023-05-29 06:30:07,344 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ti4qu6vl', purging
2023-05-29 06:30:07,344 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lsilvajr', purging
2023-05-29 06:30:07,345 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pguxw1n_', purging
2023-05-29 06:30:07,345 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z2c722pn', purging
2023-05-29 06:30:07,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:07,346 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:07,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:07,731 - distributed.nanny - INFO - Worker process 34337 exited with status 127
2023-05-29 06:30:07,732 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:30:09,054 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w467n0vy', purging
2023-05-29 06:30:09,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:09,055 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:09,183 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:09,585 - distributed.nanny - INFO - Worker process 34347 exited with status 127
2023-05-29 06:30:09,586 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:30:10,945 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ers23x24', purging
2023-05-29 06:30:10,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:10,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:10,967 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:11,520 - distributed.nanny - INFO - Worker process 34357 exited with status 127
2023-05-29 06:30:11,521 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:30:12,913 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1ez2mibq', purging
2023-05-29 06:30:12,914 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:12,914 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:12,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:13,311 - distributed.nanny - INFO - Worker process 34367 exited with status 127
2023-05-29 06:30:13,312 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:30:14,194 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40931'. Reason: nanny-close
2023-05-29 06:30:14,765 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7pt01tky', purging
2023-05-29 06:30:14,766 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:14,766 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:14,789 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:15,176 - distributed.nanny - INFO - Worker process 34377 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-05-29 06:30:46,185 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:30:46,189 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42845 instead
  warnings.warn(
2023-05-29 06:30:46,193 - distributed.scheduler - INFO - State start
2023-05-29 06:30:46,212 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-29 06:30:46,213 - distributed.scheduler - INFO - Scheduler closing...
2023-05-29 06:30:46,213 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-29 06:30:46,213 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-29 06:30:46,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36719'
2023-05-29 06:30:47,773 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gt9czxg7', purging
2023-05-29 06:30:47,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:47,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:47,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:48,202 - distributed.nanny - INFO - Worker process 34554 exited with status 127
2023-05-29 06:30:48,203 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:30:49,625 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fmb08ay2', purging
2023-05-29 06:30:49,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:49,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:49,650 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:50,081 - distributed.nanny - INFO - Worker process 34564 exited with status 127
2023-05-29 06:30:50,082 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:30:51,514 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rprg4jcf', purging
2023-05-29 06:30:51,515 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:51,515 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:51,608 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:52,054 - distributed.nanny - INFO - Worker process 34574 exited with status 127
2023-05-29 06:30:52,054 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:30:53,511 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7rhnnspm', purging
2023-05-29 06:30:53,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:53,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:53,546 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:53,995 - distributed.nanny - INFO - Worker process 34584 exited with status 127
2023-05-29 06:30:53,996 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:30:54,679 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36719'. Reason: nanny-close
2023-05-29 06:30:55,402 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cj52580j', purging
2023-05-29 06:30:55,403 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:30:55,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:30:55,425 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:30:56,007 - distributed.nanny - INFO - Worker process 34594 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34779 instead
  warnings.warn(
2023-05-29 06:31:33,351 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m0099l5m', purging
2023-05-29 06:31:33,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:33,351 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:33,358 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:33,358 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:33,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:33,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:33,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:33,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:33,442 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:33,442 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:33,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:33,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:33,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:33,455 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:33,469 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:33,469 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:35,336 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:35,425 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:35,521 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:35,552 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:35,607 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:35,670 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:35,695 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:35,717 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:36,830 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hugz5nma', purging
2023-05-29 06:31:36,831 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j1c_h6ab', purging
2023-05-29 06:31:36,831 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6yqqas25', purging
2023-05-29 06:31:36,831 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zhwtm60c', purging
2023-05-29 06:31:36,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jbjp2dho', purging
2023-05-29 06:31:36,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gpmu5pnp', purging
2023-05-29 06:31:36,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qycl2zsh', purging
2023-05-29 06:31:36,833 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ciegge4x', purging
2023-05-29 06:31:36,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:36,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:36,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:36,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:37,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:37,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:37,112 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:37,112 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:37,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:37,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:37,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:37,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:37,264 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:37,264 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:37,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:37,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:37,460 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:38,237 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:38,970 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-99mnnf9c', purging
2023-05-29 06:31:38,971 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l2jmsfuk', purging
2023-05-29 06:31:38,971 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:38,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:39,060 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:39,109 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:39,140 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:39,704 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:39,731 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:39,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j6s9dsn_', purging
2023-05-29 06:31:39,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2pjmxwir', purging
2023-05-29 06:31:39,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-aimugqwu', purging
2023-05-29 06:31:39,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gstvp42m', purging
2023-05-29 06:31:39,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mjxqpgjy', purging
2023-05-29 06:31:39,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r1hrn6k3', purging
2023-05-29 06:31:39,742 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:39,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:39,757 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:40,520 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:40,660 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-95a7nuec', purging
2023-05-29 06:31:40,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:40,660 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:40,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:40,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:40,706 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:40,745 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qk0ce43h', purging
2023-05-29 06:31:40,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:40,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:41,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:41,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:41,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:41,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:41,363 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:41,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:41,852 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:41,903 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:42,109 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pkexpn2t', purging
2023-05-29 06:31:42,109 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ar8_9n7q', purging
2023-05-29 06:31:42,110 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:42,110 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:42,274 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:42,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:42,561 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:42,967 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:43,395 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nn1gsrur', purging
2023-05-29 06:31:43,396 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mkzp5j4e', purging
2023-05-29 06:31:43,397 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:43,397 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:43,567 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hzhz8ol0', purging
2023-05-29 06:31:43,568 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d0r9pbex', purging
2023-05-29 06:31:43,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:43,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:43,753 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:43,794 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:44,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:44,134 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:44,194 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:44,392 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:44,543 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0q8md9f7', purging
2023-05-29 06:31:44,544 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pkqe25w9', purging
2023-05-29 06:31:44,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:44,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:44,934 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:44,990 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:45,306 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:45,321 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cehzzr90', purging
2023-05-29 06:31:45,322 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l_b7qqop', purging
2023-05-29 06:31:45,322 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-02gfmf25', purging
2023-05-29 06:31:45,323 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6x9dypvy', purging
2023-05-29 06:31:45,323 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:45,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:45,340 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:45,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:45,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:45,815 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:45,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:45,957 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ih3tlg14', purging
2023-05-29 06:31:45,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:45,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:45,958 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:46,270 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:46,568 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_omqxfiv', purging
2023-05-29 06:31:46,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:46,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:46,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:46,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:46,940 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2h2uikzg', purging
2023-05-29 06:31:46,941 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:46,941 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:46,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:46,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:47,006 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:47,436 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:47,584 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5f9vxvpw', purging
2023-05-29 06:31:47,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:47,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:47,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:47,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-05-29 06:31:48,478 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:48,549 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:48,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4cmy8ubp', purging
2023-05-29 06:31:48,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-toxmtiw_', purging
2023-05-29 06:31:48,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jiszfa5v', purging
2023-05-29 06:31:48,577 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ijys3oba', purging
2023-05-29 06:31:48,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:48,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:48,581 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:48,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:48,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:49,111 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:49,162 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:49,682 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:49,848 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:50,045 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dzxkmlz9', purging
2023-05-29 06:31:50,045 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3x5b5xfu', purging
2023-05-29 06:31:50,046 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_4ht9le7', purging
2023-05-29 06:31:50,046 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y1q33a5i', purging
2023-05-29 06:31:50,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:50,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:50,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:50,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:50,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:50,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:50,690 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:50,706 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-45f7kvuj', purging
2023-05-29 06:31:50,707 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:50,707 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:50,740 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:50,740 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:51,048 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:51,094 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:51,206 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0qu4eegx', purging
2023-05-29 06:31:51,207 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0hk10k35', purging
2023-05-29 06:31:51,207 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:51,207 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:51,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:51,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:52,022 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:52,056 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:52,278 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-aaiyh6rp', purging
2023-05-29 06:31:52,279 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bfdo8oag', purging
2023-05-29 06:31:52,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:52,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:52,424 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:52,449 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:52,628 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ka0zesuk', purging
2023-05-29 06:31:52,628 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nhq6ionk', purging
2023-05-29 06:31:52,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:52,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:52,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:52,643 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:53,464 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:53,626 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1dtgn__i', purging
2023-05-29 06:31:53,626 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9l_g9db7', purging
2023-05-29 06:31:53,626 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ti74vj4k', purging
2023-05-29 06:31:53,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:53,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:53,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:53,631 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:53,650 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:53,675 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:53,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:53,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:53,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:53,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:54,744 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:54,772 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:55,025 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q0xuwj3b', purging
2023-05-29 06:31:55,026 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2d7fhf3w', purging
2023-05-29 06:31:55,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:55,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:55,149 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:55,174 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:55,192 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qf3ttrq0', purging
2023-05-29 06:31:55,193 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pny1lxxc', purging
2023-05-29 06:31:55,194 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:55,194 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:55,249 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:55,249 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:56,205 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:56,238 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b1lukbkp', purging
2023-05-29 06:31:56,239 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:56,239 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:56,334 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q5heoxme', purging
2023-05-29 06:31:56,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:56,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:56,356 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:56,394 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:56,724 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hsr5tkyi', purging
2023-05-29 06:31:56,724 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:56,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:56,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:56,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:56,922 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:57,361 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:57,565 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:57,595 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:57,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7budfv11', purging
2023-05-29 06:31:57,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8990z175', purging
2023-05-29 06:31:57,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xc7pmwi9', purging
2023-05-29 06:31:57,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b4h1g8uy', purging
2023-05-29 06:31:57,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:57,737 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:57,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:57,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:57,935 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:57,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:58,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:58,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:58,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:58,972 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:59,097 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:59,218 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lc_obpt2', purging
2023-05-29 06:31:59,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:59,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:59,231 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:31:59,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:31:59,382 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:31:59,409 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:31:59,771 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:00,217 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:00,330 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:00,352 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:00,670 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ejzkaw3q', purging
2023-05-29 06:32:00,671 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bfospp8t', purging
2023-05-29 06:32:00,671 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d1g4ov0f', purging
2023-05-29 06:32:00,671 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xusno49v', purging
2023-05-29 06:32:00,672 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9rp_04k3', purging
2023-05-29 06:32:00,672 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6d5r34bc', purging
2023-05-29 06:32:00,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:00,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:00,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:00,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:00,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:00,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:01,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:01,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:01,347 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:01,710 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wkz2d2r7', purging
2023-05-29 06:32:01,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:01,711 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:01,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:01,929 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:01,964 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:01,964 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:02,177 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:02,394 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:02,753 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:02,912 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xv9ekjrh', purging
2023-05-29 06:32:02,913 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8zrlzc3u', purging
2023-05-29 06:32:02,913 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3nvuok79', purging
2023-05-29 06:32:02,914 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:02,914 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:02,983 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:03,341 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:03,387 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:03,680 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4ttpf8ye', purging
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:03,680 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bnbi4eq4', purging
2023-05-29 06:32:03,681 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-25lly0_c', purging
2023-05-29 06:32:03,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:03,681 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:03,761 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:04,090 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f68owog8', purging
2023-05-29 06:32:04,091 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:04,091 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:04,277 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:04,384 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n_jsu78y', purging
2023-05-29 06:32:04,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:04,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:04,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:04,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:04,851 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:04,991 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2aovm2bw', purging
2023-05-29 06:32:04,992 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:04,992 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:05,066 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:05,066 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:05,171 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:05,436 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-khtnkap_', purging
2023-05-29 06:32:05,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:05,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:05,669 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:05,814 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z94m7n1h', purging
2023-05-29 06:32:05,815 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:05,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:06,087 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:06,113 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:06,347 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6gbn0kak', purging
2023-05-29 06:32:06,348 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7vo2i3mo', purging
2023-05-29 06:32:06,349 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:06,349 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:06,715 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:06,765 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vd2fn12s', purging
2023-05-29 06:32:06,766 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:06,766 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:06,971 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:07,264 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_dt0wppe', purging
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:07,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:07,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:07,485 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:07,594 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_rqnaemk', purging
2023-05-29 06:32:07,595 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:07,595 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:07,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:07,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:07,840 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:08,170 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:08,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7zp67ix5', purging
2023-05-29 06:32:08,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ieievcht', purging
2023-05-29 06:32:08,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:08,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:08,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:08,448 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:08,555 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:08,762 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:09,086 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-635sjdwb', purging
2023-05-29 06:32:09,087 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-at_rl66s', purging
2023-05-29 06:32:09,088 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:09,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:09,278 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:09,364 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c73gm1ms', purging
2023-05-29 06:32:09,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:09,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:09,566 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:09,744 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uzpf50dm', purging
2023-05-29 06:32:09,745 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:09,745 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:09,897 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:10,185 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nyb4wre_', purging
2023-05-29 06:32:10,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:10,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:10,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:10,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:10,437 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:10,912 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b5awfs7k', purging
2023-05-29 06:32:10,913 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r3u8_7ja', purging
2023-05-29 06:32:10,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:10,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:10,924 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:11,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:11,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:11,316 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:11,343 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:11,381 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ve1vos6t', purging
2023-05-29 06:32:11,381 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8ssr223c', purging
2023-05-29 06:32:11,382 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:11,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:11,650 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:12,035 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bs3udvy0', purging
2023-05-29 06:32:12,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:12,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:12,256 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:12,481 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qe7s_xnz', purging
2023-05-29 06:32:12,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:12,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:12,535 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:12,810 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_ht8jmkj', purging
2023-05-29 06:32:12,811 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:12,811 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:12,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:12,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:13,076 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:13,254 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1fzgj560', purging
2023-05-29 06:32:13,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:13,255 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:13,588 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:13,871 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-giy3r2m5', purging
2023-05-29 06:32:13,871 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7lqs0dim', purging
2023-05-29 06:32:13,872 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:13,872 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:13,907 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:13,949 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:14,085 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-knivnwby', purging
2023-05-29 06:32:14,087 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:14,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:14,257 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:14,679 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nqi5_hen', purging
2023-05-29 06:32:14,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:14,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:14,766 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:15,081 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mi2__bfg', purging
2023-05-29 06:32:15,082 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:15,082 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:15,144 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:15,470 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:15,479 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ik_8mxau', purging
2023-05-29 06:32:15,480 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-umu52ew5', purging
2023-05-29 06:32:15,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:15,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:15,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:15,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:15,822 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:15,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:16,044 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:16,317 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-aiee7byk', purging
2023-05-29 06:32:16,318 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:16,318 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:16,642 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:16,673 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:16,729 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jht2rim0', purging
2023-05-29 06:32:16,729 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vosvv_py', purging
2023-05-29 06:32:16,730 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:16,730 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:16,946 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:17,025 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qzu0cwa7', purging
2023-05-29 06:32:17,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:17,026 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:17,253 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:17,644 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d22831p0', purging
2023-05-29 06:32:17,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:17,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:17,751 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:18,159 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xi3_44ki', purging
2023-05-29 06:32:18,160 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:18,160 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:18,280 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sthy60sx', purging
2023-05-29 06:32:18,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:18,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:18,292 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:18,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:18,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:18,801 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_d_t80lz', purging
2023-05-29 06:32:18,801 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:18,801 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:18,879 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:19,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:19,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:19,445 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:19,485 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:19,808 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w__zw8i8', purging
2023-05-29 06:32:19,809 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fug71vl5', purging
2023-05-29 06:32:19,809 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-snjxnlrb', purging
2023-05-29 06:32:19,809 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mcn5ql27', purging
2023-05-29 06:32:19,810 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:19,810 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:19,839 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:19,864 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:20,145 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:20,416 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d4olk7b5', purging
2023-05-29 06:32:20,417 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:20,417 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:20,550 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:21,003 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vn5ull1g', purging
2023-05-29 06:32:21,003 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d2rpkxab', purging
2023-05-29 06:32:21,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:21,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:21,032 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:21,040 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:21,040 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:21,454 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:21,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:21,466 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:21,466 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:21,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:21,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:21,772 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:21,831 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:22,149 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h_d05j1g', purging
2023-05-29 06:32:22,150 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f6df2uwq', purging
2023-05-29 06:32:22,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:22,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:22,634 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n9_yj8ph', purging
2023-05-29 06:32:22,634 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x2923im_', purging
2023-05-29 06:32:22,635 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:22,635 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:22,783 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:22,811 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:23,100 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:23,355 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:23,356 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q89byomp', purging
2023-05-29 06:32:23,357 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zz1t5hy9', purging
2023-05-29 06:32:23,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:23,358 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:23,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:23,400 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:23,724 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:24,294 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:24,363 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:24,397 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w9r9hsey', purging
2023-05-29 06:32:24,397 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7cbwmjc6', purging
2023-05-29 06:32:24,397 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r16o2v0d', purging
2023-05-29 06:32:24,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:24,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:24,417 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:24,417 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:24,674 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:24,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:24,920 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:24,920 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:25,266 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:25,320 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:25,322 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r0ddfqj6', purging
2023-05-29 06:32:25,322 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bg1g3tvb', purging
2023-05-29 06:32:25,323 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:25,323 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:25,640 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:25,860 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-083shte6', purging
2023-05-29 06:32:25,861 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:25,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:25,896 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:25,896 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:26,079 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:26,283 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:26,850 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nx194aa0', purging
2023-05-29 06:32:26,850 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_qt21du1', purging
2023-05-29 06:32:26,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:26,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:26,901 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:26,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:27,087 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:27,165 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f0_gm82e', purging
2023-05-29 06:32:27,165 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tuxheo03', purging
2023-05-29 06:32:27,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:27,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:27,297 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:27,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:27,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:27,868 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:27,878 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ugmr0l94', purging
2023-05-29 06:32:27,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:27,879 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:27,921 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:28,236 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:28,496 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:28,687 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o9d4lnjr', purging
2023-05-29 06:32:28,688 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t1he31hq', purging
2023-05-29 06:32:28,688 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o_54et8a', purging
2023-05-29 06:32:28,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:28,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:28,797 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:28,797 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:28,870 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:29,466 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sb4k_7vp', purging
2023-05-29 06:32:29,466 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:29,467 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:29,523 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:29,523 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:29,586 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:29,761 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:29,776 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nudnivis', purging
2023-05-29 06:32:29,777 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v9i5d0_s', purging
2023-05-29 06:32:29,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:29,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:30,081 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:30,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:30,454 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:30,455 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:30,732 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:30,758 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:31,112 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:31,137 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:31,183 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hq6nau88', purging
2023-05-29 06:32:31,183 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jiswdk9x', purging
2023-05-29 06:32:31,184 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-506k0_7l', purging
2023-05-29 06:32:31,184 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lug6l81v', purging
2023-05-29 06:32:31,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:31,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:31,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:31,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:31,472 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:32,112 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:32,153 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:32,343 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-74joewf7', purging
2023-05-29 06:32:32,344 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xuulkct7', purging
2023-05-29 06:32:32,344 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o_mpl1pn', purging
2023-05-29 06:32:32,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:32,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:32,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:32,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:32,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:32,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:32,769 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:32,769 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:32,994 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:32,994 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:33,447 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:33,487 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:33,702 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yt_g68x7', purging
2023-05-29 06:32:33,703 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vkvuiyue', purging
2023-05-29 06:32:33,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:33,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:33,722 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:33,722 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:33,884 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:33,913 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:34,399 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:34,968 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:35,013 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:35,060 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d3xs9oms', purging
2023-05-29 06:32:35,061 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hrtgs0b1', purging
2023-05-29 06:32:35,061 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8jjvx2dw', purging
2023-05-29 06:32:35,061 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-shim54gr', purging
2023-05-29 06:32:35,062 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-45cpc5x5', purging
2023-05-29 06:32:35,062 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:35,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:35,078 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:35,078 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:35,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:35,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:35,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:35,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:35,765 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:35,815 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:35,958 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-95c935gf', purging
2023-05-29 06:32:35,958 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ou8bu4zd', purging
2023-05-29 06:32:35,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:35,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:36,538 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gx_24zrl', purging
2023-05-29 06:32:36,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:36,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:36,558 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-81ynarbe', purging
2023-05-29 06:32:36,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:36,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:36,572 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:36,602 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:36,957 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:37,295 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0hy99wru', purging
2023-05-29 06:32:37,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:37,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:37,349 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:37,349 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:37,919 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:37,945 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:38,148 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yto43btj', purging
2023-05-29 06:32:38,148 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-58c93r23', purging
2023-05-29 06:32:38,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:38,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:38,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:38,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:38,546 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:38,551 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1ilfaj6d', purging
2023-05-29 06:32:38,552 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:38,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:38,587 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:39,130 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:39,174 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:39,304 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:39,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r8dqn_i0', purging
2023-05-29 06:32:39,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mhfkhkqv', purging
2023-05-29 06:32:39,470 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7epxozf1', purging
2023-05-29 06:32:39,470 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-avrf_rf1', purging
2023-05-29 06:32:39,471 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:39,471 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:39,526 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:39,526 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:40,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:40,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:40,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:40,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:40,703 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-20lqmsqj', purging
2023-05-29 06:32:40,704 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7snx4qit', purging
2023-05-29 06:32:40,705 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:40,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:40,707 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:40,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:40,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:40,731 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:40,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:40,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:41,114 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:41,142 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:42,021 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:42,056 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:42,176 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wkr2bqnj', purging
2023-05-29 06:32:42,177 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9_d7m0hx', purging
2023-05-29 06:32:42,177 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o90g1pem', purging
2023-05-29 06:32:42,178 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wmaf4niy', purging
2023-05-29 06:32:42,179 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:42,179 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:42,255 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:42,319 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cdrw7qv2', purging
2023-05-29 06:32:42,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:42,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:42,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:42,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:42,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:42,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:42,906 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:43,369 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:43,569 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:43,595 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:43,615 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3fkyfj0l', purging
2023-05-29 06:32:43,616 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-buqy47tb', purging
2023-05-29 06:32:43,616 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vy1ugma0', purging
2023-05-29 06:32:43,617 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wfe2u02o', purging
2023-05-29 06:32:43,617 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:43,617 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:43,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:43,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:43,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:43,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:44,507 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ru2vht59', purging
2023-05-29 06:32:44,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:44,508 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:44,678 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:44,713 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:44,970 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o9d1h6ui', purging
2023-05-29 06:32:44,971 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:44,971 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:45,108 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:45,147 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vcy39yr8', purging
2023-05-29 06:32:45,148 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:45,148 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:45,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:45,174 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:45,736 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:46,204 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:46,260 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bely7foe', purging
2023-05-29 06:32:46,261 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t2u_mbvx', purging
2023-05-29 06:32:46,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:46,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:46,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:46,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:46,339 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:46,365 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:46,666 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4vqtaa5k', purging
2023-05-29 06:32:46,666 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-muloijvk', purging
2023-05-29 06:32:46,667 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:46,667 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:47,185 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:47,210 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:47,364 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_xpqm6kr', purging
2023-05-29 06:32:47,364 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z6q1u61c', purging
2023-05-29 06:32:47,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:47,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:47,793 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:47,817 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ephv__ow', purging
2023-05-29 06:32:47,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:47,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:47,864 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:47,864 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:47,919 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:47,919 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:48,587 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:48,783 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-066ekzoe', purging
2023-05-29 06:32:48,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:48,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:48,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:48,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:49,033 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:49,081 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:49,271 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:49,364 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-72wz5nu5', purging
2023-05-29 06:32:49,364 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q2cgd3d1', purging
2023-05-29 06:32:49,365 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7xem1c6c', purging
2023-05-29 06:32:49,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:49,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:49,814 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:49,837 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:50,141 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:50,165 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a9bg5qx1', purging
2023-05-29 06:32:50,165 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l0ov0gh5', purging
2023-05-29 06:32:50,166 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4ub5h16l', purging
2023-05-29 06:32:50,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:50,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:50,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:50,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:50,646 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:50,646 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:50,839 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:50,839 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:51,195 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:51,355 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4d0bqkjj', purging
2023-05-29 06:32:51,356 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:51,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:51,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:51,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:51,523 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:51,585 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:51,722 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-10cakai6', purging
2023-05-29 06:32:51,723 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-de0lrstp', purging
2023-05-29 06:32:51,723 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:51,723 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:52,064 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:52,418 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:52,445 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:52,762 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-moli0ghy', purging
2023-05-29 06:32:52,762 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ivjv2yfl', purging
2023-05-29 06:32:52,763 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-crfhrq4c', purging
2023-05-29 06:32:52,763 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mjuw_oyo', purging
2023-05-29 06:32:52,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:52,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:52,766 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:53,122 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:53,122 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:53,122 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:53,122 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:53,482 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:53,585 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3fux114l', purging
2023-05-29 06:32:53,586 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:53,586 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:53,967 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:53,967 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:54,007 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:54,007 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:54,191 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:54,216 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:54,322 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u8upjgc1', purging
2023-05-29 06:32:54,322 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_4dfcdxb', purging
2023-05-29 06:32:54,323 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:54,323 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:54,721 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:55,026 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yqk1gm2k', purging
2023-05-29 06:32:55,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:55,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:55,122 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:55,154 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:55,423 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:55,717 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ainidc6h', purging
2023-05-29 06:32:55,718 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fcml9pck', purging
2023-05-29 06:32:55,718 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a5ldcfsf', purging
2023-05-29 06:32:55,719 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:55,719 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:55,722 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:55,723 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:56,013 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:56,239 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6v4fmasq', purging
2023-05-29 06:32:56,240 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:56,240 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:56,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:56,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:56,705 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:56,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:56,800 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:56,828 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:57,029 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h0x4lid1', purging
2023-05-29 06:32:57,029 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a0rnmo99', purging
2023-05-29 06:32:57,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:57,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:57,352 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:57,560 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xgpx8tsf', purging
2023-05-29 06:32:57,561 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:57,561 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:57,717 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:57,741 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:58,073 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:58,373 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-20dv414a', purging
2023-05-29 06:32:58,374 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bgck7dd5', purging
2023-05-29 06:32:58,374 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r_ygm72_', purging
2023-05-29 06:32:58,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:58,375 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:58,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:58,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:58,556 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:58,904 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-486ku0pc', purging
2023-05-29 06:32:58,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:58,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:59,254 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:59,254 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:59,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:59,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:59,453 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:32:59,476 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:32:59,640 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xbd8_6bh', purging
2023-05-29 06:32:59,641 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-porc4a46', purging
2023-05-29 06:32:59,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:32:59,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:32:59,799 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:00,075 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q91hjuu3', purging
2023-05-29 06:33:00,076 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:00,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:00,319 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:00,344 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:00,653 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:00,751 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:01,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0mgxj29y', purging
2023-05-29 06:33:01,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p2foenpj', purging
2023-05-29 06:33:01,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7k82fbkl', purging
2023-05-29 06:33:01,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x88podxc', purging
2023-05-29 06:33:01,066 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:01,066 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:01,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:01,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:01,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:01,400 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:01,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:01,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:01,893 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:01,893 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:02,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:02,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:02,282 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:02,326 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:02,328 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ijtrbz2j', purging
2023-05-29 06:33:02,328 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0i9r5avj', purging
2023-05-29 06:33:02,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:02,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:03,078 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:03,139 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:03,396 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:03,524 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:03,735 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:03,833 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jeewyk9c', purging
2023-05-29 06:33:03,834 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-smloaq73', purging
2023-05-29 06:33:03,834 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6hsifrm8', purging
2023-05-29 06:33:03,835 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u9w0c5xq', purging
2023-05-29 06:33:03,835 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zy8m_y6v', purging
2023-05-29 06:33:03,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:03,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:03,847 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:03,847 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:04,638 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:04,674 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9g0xueix', purging
2023-05-29 06:33:04,675 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oj6c0_5r', purging
2023-05-29 06:33:04,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:04,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:04,678 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:04,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:04,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:04,997 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:04,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:05,164 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:05,164 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:05,331 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0xqf26fm', purging
2023-05-29 06:33:05,332 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:05,332 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:05,336 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:05,639 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:06,139 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:06,205 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ltt8b883', purging
2023-05-29 06:33:06,206 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h5c0dgc0', purging
2023-05-29 06:33:06,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:06,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:06,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:06,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:06,421 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:06,509 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:06,868 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-udyaev62', purging
2023-05-29 06:33:06,868 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yv5nglc4', purging
2023-05-29 06:33:06,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:06,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:07,124 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:07,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:07,531 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:07,558 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:07,715 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l4pnjsi0', purging
2023-05-29 06:33:07,716 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zxhfw33x', purging
2023-05-29 06:33:07,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:07,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:07,951 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:07,980 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-soawxqt0', purging
2023-05-29 06:33:07,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:07,981 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:08,092 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:08,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:08,323 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:08,844 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:08,889 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:09,139 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g17ojtat', purging
2023-05-29 06:33:09,139 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tg8k6mst', purging
2023-05-29 06:33:09,140 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fu89lix6', purging
2023-05-29 06:33:09,140 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:09,140 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:09,148 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:09,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:09,207 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:09,519 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-eeao6x60', purging
2023-05-29 06:33:09,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:09,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:09,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:09,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:10,064 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:10,091 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:10,428 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-aku7tevg', purging
2023-05-29 06:33:10,429 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kqqzax79', purging
2023-05-29 06:33:10,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:10,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:10,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:10,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:10,588 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:10,775 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fivmxafn', purging
2023-05-29 06:33:10,775 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:10,775 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:11,183 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:11,528 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:11,559 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:11,594 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f2rw3e9z', purging
2023-05-29 06:33:11,595 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6_gitxc_', purging
2023-05-29 06:33:11,595 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b96uyo84', purging
2023-05-29 06:33:11,596 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:11,596 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:11,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:11,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:11,884 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:12,165 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l6xxl_h_', purging
2023-05-29 06:33:12,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:12,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:12,702 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:12,729 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:12,772 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-49_8uxyj', purging
2023-05-29 06:33:12,773 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0kc50zvq', purging
2023-05-29 06:33:12,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:12,773 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:13,044 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:13,087 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0b91jzis', purging
2023-05-29 06:33:13,088 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:13,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:13,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:13,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:13,384 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:13,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:13,681 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:14,210 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bcfasm54', purging
2023-05-29 06:33:14,210 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:14,211 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:14,261 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:14,290 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:14,310 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-50lxlz4p', purging
2023-05-29 06:33:14,310 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-luz7qeac', purging
2023-05-29 06:33:14,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:14,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:14,426 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:14,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ogo2vg04', purging
2023-05-29 06:33:14,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:14,731 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:15,232 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:15,279 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6zd05vcx', purging
2023-05-29 06:33:15,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:15,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:15,486 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:15,748 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uq29mkmf', purging
2023-05-29 06:33:15,749 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tnjwcxhc', purging
2023-05-29 06:33:15,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:15,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:15,756 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:15,794 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:15,794 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:16,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:16,018 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:16,344 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:16,864 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4n3n2v08', purging
2023-05-29 06:33:16,865 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:16,865 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:16,909 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:16,949 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:17,016 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:17,083 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3iotal7k', purging
2023-05-29 06:33:17,083 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jd9cldxw', purging
2023-05-29 06:33:17,084 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wxpt9bd7', purging
2023-05-29 06:33:17,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:17,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:17,316 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:17,316 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:17,740 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:17,910 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7kat9hzn', purging
2023-05-29 06:33:17,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:17,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:18,127 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:18,487 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y7q6_6d4', purging
2023-05-29 06:33:18,487 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:18,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:18,507 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:18,507 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:18,572 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qg2w6uo8', purging
2023-05-29 06:33:18,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:18,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:18,575 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:19,171 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:19,232 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a2hiwtmg', purging
2023-05-29 06:33:19,233 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:19,233 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:19,688 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:19,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:19,762 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:19,799 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:19,841 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:20,125 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:20,156 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d3drayke', purging
2023-05-29 06:33:20,156 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0ln9lqyh', purging
2023-05-29 06:33:20,157 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kru3_5ub', purging
2023-05-29 06:33:20,157 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d4kgy01z', purging
2023-05-29 06:33:20,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:20,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:20,421 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:20,659 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w7vo98g3', purging
2023-05-29 06:33:20,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:20,660 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:21,015 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:21,233 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9y23lxwy', purging
2023-05-29 06:33:21,234 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:21,234 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:21,365 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:21,378 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p5cm9cwq', purging
2023-05-29 06:33:21,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:21,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:21,386 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:21,386 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:21,719 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:21,719 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:21,938 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:22,018 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t5lvnu1y', purging
2023-05-29 06:33:22,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:22,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:22,484 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:22,537 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:22,557 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gx48eoc1', purging
2023-05-29 06:33:22,558 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b1scoe9w', purging
2023-05-29 06:33:22,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:22,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:22,990 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1i3jzayx', purging
2023-05-29 06:33:22,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:22,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:23,065 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:23,114 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:23,476 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:23,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gh16s1ha', purging
2023-05-29 06:33:23,576 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sldc2kmr', purging
2023-05-29 06:33:23,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:23,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:24,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:24,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:24,182 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-04x6npy_', purging
2023-05-29 06:33:24,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:24,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:24,187 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:24,572 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:24,641 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b6x8i5oi', purging
2023-05-29 06:33:24,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:24,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:24,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:24,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:25,067 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_i9en_ul', purging
2023-05-29 06:33:25,068 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-as7a4fuk', purging
2023-05-29 06:33:25,068 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:25,068 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:25,139 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:25,335 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:25,710 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:25,731 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:25,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hmijacno', purging
2023-05-29 06:33:25,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9_ja5esc', purging
2023-05-29 06:33:25,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:25,737 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:26,025 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:26,119 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tw14se16', purging
2023-05-29 06:33:26,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:26,120 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:26,636 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:26,664 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-41zh22f2', purging
2023-05-29 06:33:26,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:26,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:26,893 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:26,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:27,158 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:27,297 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o6mz1dov', purging
2023-05-29 06:33:27,298 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:27,298 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:27,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:27,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:27,532 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:27,607 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pjsliwt3', purging
2023-05-29 06:33:27,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:27,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:28,012 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:28,264 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fb8wye2f', purging
2023-05-29 06:33:28,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:28,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:28,413 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:28,442 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:28,790 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-98roo6ry', purging
2023-05-29 06:33:28,791 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4e6se0ns', purging
2023-05-29 06:33:28,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:28,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:28,919 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:29,019 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-orkmjt53', purging
2023-05-29 06:33:29,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:29,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:29,099 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:29,605 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:29,650 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-co_wcbgp', purging
2023-05-29 06:33:29,651 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_ur9md99', purging
2023-05-29 06:33:29,651 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:29,651 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:29,997 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:29,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:30,045 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:30,045 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:30,174 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:30,479 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rathfwhk', purging
2023-05-29 06:33:30,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:30,480 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:30,694 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:30,703 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i_90wxnd', purging
2023-05-29 06:33:30,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:30,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:31,102 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:31,194 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-al54fm_x', purging
2023-05-29 06:33:31,195 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-eprrgbds', purging
2023-05-29 06:33:31,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:31,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:31,305 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:31,663 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:31,714 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:31,730 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nqk1n7f2', purging
2023-05-29 06:33:31,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sj11fs2n', purging
2023-05-29 06:33:31,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:31,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:32,274 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:32,313 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l5x2vzl2', purging
2023-05-29 06:33:32,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:32,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:32,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:32,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:32,635 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:32,919 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v87jl8rz', purging
2023-05-29 06:33:32,920 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:32,920 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:33,196 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:33,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r7zbtktq', purging
2023-05-29 06:33:33,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:33,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:33,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:33,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:33,455 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:33,795 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:33,900 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-frj8cs1i', purging
2023-05-29 06:33:33,900 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g0jlhcki', purging
2023-05-29 06:33:33,901 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:33,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:34,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:34,143 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:34,515 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:34,546 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:34,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gins0gjn', purging
2023-05-29 06:33:34,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lbqxg9mv', purging
2023-05-29 06:33:34,845 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:34,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:34,918 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:35,063 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pwyxkwqh', purging
2023-05-29 06:33:35,063 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:35,063 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:35,251 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:35,398 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m9nv5l1h', purging
2023-05-29 06:33:35,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:35,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:35,841 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:36,003 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9v3eq_0v', purging
2023-05-29 06:33:36,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:36,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:36,160 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:36,198 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dolv02a2', purging
2023-05-29 06:33:36,199 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-44j0hgek', purging
2023-05-29 06:33:36,199 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:36,199 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:36,210 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:36,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:36,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:36,705 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:36,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jxqc3jax', purging
2023-05-29 06:33:36,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:36,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:37,232 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:37,465 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_dvbru33', purging
2023-05-29 06:33:37,466 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:37,466 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:37,536 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:37,718 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jm2udl5i', purging
2023-05-29 06:33:37,719 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:37,719 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:37,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:37,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:37,883 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:38,321 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4wv9mvjp', purging
2023-05-29 06:33:38,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:38,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:38,419 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:38,758 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-amfmje1z', purging
2023-05-29 06:33:38,759 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:38,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:39,027 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:39,061 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:39,143 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x062ipva', purging
2023-05-29 06:33:39,143 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ew_wu1qe', purging
2023-05-29 06:33:39,144 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:39,144 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:39,369 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:39,536 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-06wkxpyf', purging
2023-05-29 06:33:39,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:39,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:39,666 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:40,003 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2_5txct4', purging
2023-05-29 06:33:40,003 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:40,003 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:40,188 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:40,523 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:40,639 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ih2uz5pz', purging
2023-05-29 06:33:40,639 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1is26twc', purging
2023-05-29 06:33:40,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:40,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:40,652 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:40,652 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:40,696 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:40,841 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-048ugu3m', purging
2023-05-29 06:33:40,842 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:40,842 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:41,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:41,258 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:41,706 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u0035gmx', purging
2023-05-29 06:33:41,707 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zkksv_pn', purging
2023-05-29 06:33:41,708 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:41,708 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:41,739 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:41,769 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:42,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:42,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:42,196 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:42,273 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9oks42zw', purging
2023-05-29 06:33:42,274 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:42,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:42,481 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:42,958 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:43,215 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-moyl4x6k', purging
2023-05-29 06:33:43,216 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-68agwlen', purging
2023-05-29 06:33:43,217 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:43,217 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:43,338 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:43,361 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2ox7tnfg', purging
2023-05-29 06:33:43,362 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qc2evvqf', purging
2023-05-29 06:33:43,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:43,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:43,376 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:43,659 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:43,659 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:43,917 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:44,096 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-eqk94s66', purging
2023-05-29 06:33:44,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:44,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:44,209 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:44,605 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-baapxc3c', purging
2023-05-29 06:33:44,606 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:44,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:44,717 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:44,893 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-04f3w0sq', purging
2023-05-29 06:33:44,894 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:44,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:44,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:44,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:45,064 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:45,395 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:45,405 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xy8z044e', purging
2023-05-29 06:33:45,405 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-921pil8l', purging
2023-05-29 06:33:45,406 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:45,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:45,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:45,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:45,939 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:45,969 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:46,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7t_ch468', purging
2023-05-29 06:33:46,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pqhht_bs', purging
2023-05-29 06:33:46,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:46,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:46,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:46,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:46,599 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:46,881 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:46,981 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cmoiqnu3', purging
2023-05-29 06:33:46,982 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8pgarat5', purging
2023-05-29 06:33:46,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:46,983 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:47,338 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:47,524 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-anp0z7fp', purging
2023-05-29 06:33:47,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:47,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:47,537 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:47,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:47,674 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:48,219 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:48,229 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kydg95sa', purging
2023-05-29 06:33:48,229 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-69levo6c', purging
2023-05-29 06:33:48,230 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:48,230 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:48,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:48,455 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:48,619 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:48,638 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:48,966 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w7pd1gc7', purging
2023-05-29 06:33:48,967 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jaz950a7', purging
2023-05-29 06:33:48,967 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:48,967 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:49,116 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:49,340 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vknxrna9', purging
2023-05-29 06:33:49,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:49,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:49,485 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:49,795 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:49,823 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oh_mqwk6', purging
2023-05-29 06:33:49,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7aymuw3s', purging
2023-05-29 06:33:49,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:49,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:50,139 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:50,205 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b58dxrco', purging
2023-05-29 06:33:50,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:50,205 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:50,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:50,245 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:50,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:50,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:50,941 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:50,992 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n2r0fdm7', purging
2023-05-29 06:33:50,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:50,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:51,305 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:51,323 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:51,432 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6ktsg9f2', purging
2023-05-29 06:33:51,432 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2tfsn6ev', purging
2023-05-29 06:33:51,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:51,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:51,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:51,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:51,819 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:52,108 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:52,505 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:52,548 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wh3tbpge', purging
2023-05-29 06:33:52,548 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5revvzyw', purging
2023-05-29 06:33:52,548 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yo3trc7t', purging
2023-05-29 06:33:52,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:52,549 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:52,744 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:52,837 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xg3iu22o', purging
2023-05-29 06:33:52,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:52,838 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:52,901 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:52,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:53,396 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:53,482 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_1gq46cm', purging
2023-05-29 06:33:53,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:53,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:53,742 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:53,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:53,944 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:53,962 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:54,197 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bk7g2xo3', purging
2023-05-29 06:33:54,197 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gzutkgbg', purging
2023-05-29 06:33:54,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:54,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:54,284 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:54,304 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2_60uekr', purging
2023-05-29 06:33:54,304 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:54,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:54,655 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:55,022 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6k_mjzjs', purging
2023-05-29 06:33:55,023 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:55,023 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:55,245 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:55,461 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-owldko1q', purging
2023-05-29 06:33:55,462 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xzoqb5br', purging
2023-05-29 06:33:55,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:55,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:55,490 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:55,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:55,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:55,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:55,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:55,872 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:56,172 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qn29ar0w', purging
2023-05-29 06:33:56,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:56,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:56,606 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:56,673 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:56,886 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nrt07j9p', purging
2023-05-29 06:33:56,887 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zzg1g_12', purging
2023-05-29 06:33:56,887 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:56,887 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:56,997 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:57,043 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:57,115 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o5gyn65e', purging
2023-05-29 06:33:57,115 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8dciky61', purging
2023-05-29 06:33:57,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:57,116 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:57,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:57,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:57,763 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:58,170 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:58,200 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8_dubk5a', purging
2023-05-29 06:33:58,200 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qcu4xp49', purging
2023-05-29 06:33:58,201 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:58,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:58,271 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:58,271 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:58,471 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:58,599 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fm1rrpbs', purging
2023-05-29 06:33:58,600 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:58,600 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:58,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:58,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:59,172 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:59,345 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i9rhjav8', purging
2023-05-29 06:33:59,346 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:59,346 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:59,629 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:33:59,773 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_61holkg', purging
2023-05-29 06:33:59,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:59,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:33:59,964 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:33:59,971 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0jqpw55g', purging
2023-05-29 06:33:59,972 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-47mf1rho', purging
2023-05-29 06:33:59,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:33:59,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:00,009 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:00,524 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:00,782 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r8amrsxw', purging
2023-05-29 06:34:00,783 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:00,783 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:00,992 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:01,032 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:01,193 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2xo8mmr_', purging
2023-05-29 06:34:01,194 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9xjewocx', purging
2023-05-29 06:34:01,194 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:01,194 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:01,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:01,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:01,570 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:01,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:01,785 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:02,101 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wcl717h6', purging
2023-05-29 06:34:02,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:02,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:02,355 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:02,591 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c5kgno6w', purging
2023-05-29 06:34:02,592 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bwoybuaa', purging
2023-05-29 06:34:02,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:02,593 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:02,607 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:02,607 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:02,679 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:02,778 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:03,245 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:03,357 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-26n207td', purging
2023-05-29 06:34:03,357 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-utffjhtm', purging
2023-05-29 06:34:03,358 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:03,358 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:03,634 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:03,675 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:03,906 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_bizf8om', purging
2023-05-29 06:34:03,906 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c0dzdmag', purging
2023-05-29 06:34:03,907 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:03,907 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:04,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9nvp75_x', purging
2023-05-29 06:34:04,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:04,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:04,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:04,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:04,445 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:04,660 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:04,822 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4wf74k6u', purging
2023-05-29 06:34:04,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:04,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:05,232 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:05,232 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:05,278 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:05,278 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:05,420 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:05,451 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:05,967 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:05,999 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0urshj9v', purging
2023-05-29 06:34:06,000 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d8ugda4x', purging
2023-05-29 06:34:06,000 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zzf0i1y4', purging
2023-05-29 06:34:06,001 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:06,001 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:06,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:06,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:06,388 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:06,421 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:07,054 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-by7hqdyx', purging
2023-05-29 06:34:07,054 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0jtvsz_p', purging
2023-05-29 06:34:07,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:07,055 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:07,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:07,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:07,145 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:07,280 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:07,594 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ga3mx03r', purging
2023-05-29 06:34:07,595 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-20v5ar_8', purging
2023-05-29 06:34:07,595 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:07,595 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:07,985 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:07,985 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:08,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:08,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:08,073 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:08,111 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:08,631 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:08,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i_thwjc0', purging
2023-05-29 06:34:08,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-81ci9cc8', purging
2023-05-29 06:34:08,736 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qbqxtla8', purging
2023-05-29 06:34:08,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:08,737 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:08,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:08,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:08,833 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:08,864 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:09,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xv8kb_kq', purging
2023-05-29 06:34:09,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zyur4und', purging
2023-05-29 06:34:09,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_8ukz6zr', purging
2023-05-29 06:34:09,649 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:09,649 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:09,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:09,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:09,663 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:09,962 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:10,129 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vnpili8d', purging
2023-05-29 06:34:10,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:10,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:10,401 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:10,401 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:10,459 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:10,459 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:11,085 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:11,142 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:11,232 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k_ua1kus', purging
2023-05-29 06:34:11,233 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c4eaab20', purging
2023-05-29 06:34:11,233 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:11,233 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:11,542 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b18kuak1', purging
2023-05-29 06:34:11,542 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c46awa8w', purging
2023-05-29 06:34:11,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:11,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:11,566 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:11,593 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:11,798 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:12,145 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:12,449 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:12,664 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4eerp_zs', purging
2023-05-29 06:34:12,665 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gjo0_wn_', purging
2023-05-29 06:34:12,665 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i1hrqkat', purging
2023-05-29 06:34:12,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:12,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:12,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:12,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:13,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:13,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:13,208 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:13,208 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:13,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:13,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:13,468 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:13,734 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xqmmo27e', purging
2023-05-29 06:34:13,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6tuo369t', purging
2023-05-29 06:34:13,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:13,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:13,844 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:14,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:14,038 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:14,493 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:14,535 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:14,592 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:14,940 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:15,067 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-avgfudxk', purging
2023-05-29 06:34:15,067 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tpsvfu7e', purging
2023-05-29 06:34:15,068 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-93sok1u4', purging
2023-05-29 06:34:15,068 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h03efosj', purging
2023-05-29 06:34:15,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:15,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:15,164 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:15,398 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qywosf9j', purging
2023-05-29 06:34:15,398 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:15,398 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:16,038 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:16,109 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wccprr6z', purging
2023-05-29 06:34:16,110 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:16,110 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:16,136 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9oenndew', purging
2023-05-29 06:34:16,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:16,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:16,138 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:16,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:16,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:16,565 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:16,565 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:16,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:16,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:17,010 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:17,053 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:17,516 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k95917tp', purging
2023-05-29 06:34:17,517 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ggieclc5', purging
2023-05-29 06:34:17,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:17,518 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:17,645 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:17,691 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:17,791 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6ppbnuff', purging
2023-05-29 06:34:17,791 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7uvfq6q1', purging
2023-05-29 06:34:17,792 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:17,792 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:18,122 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:18,531 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qnczek6t', purging
2023-05-29 06:34:18,532 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:18,532 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:18,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:18,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:18,729 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:19,245 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0ikjkgh1', purging
2023-05-29 06:34:19,245 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:19,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:19,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:19,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:19,308 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:19,621 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kcz5zakf', purging
2023-05-29 06:34:19,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:19,622 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:19,856 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:19,882 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:20,225 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:20,262 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:20,292 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dik6wj2m', purging
2023-05-29 06:34:20,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-abywgm62', purging
2023-05-29 06:34:20,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-85mtdrz4', purging
2023-05-29 06:34:20,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3u9rzqtq', purging
2023-05-29 06:34:20,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:20,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:20,397 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:20,907 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oybhz7xb', purging
2023-05-29 06:34:20,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:20,908 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:20,944 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:21,437 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5rwp_8mw', purging
2023-05-29 06:34:21,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:21,438 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:21,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:21,450 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:21,553 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:21,782 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j2s9kbkr', purging
2023-05-29 06:34:21,782 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:21,782 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:21,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:21,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:22,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:22,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:22,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:22,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:22,683 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:22,723 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:23,111 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jtwm5p2z', purging
2023-05-29 06:34:23,112 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-eb9qqshp', purging
2023-05-29 06:34:23,113 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:23,113 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:23,332 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:23,355 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:23,410 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:23,737 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:24,101 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:24,234 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sym5bcby', purging
2023-05-29 06:34:24,234 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9zn5gu56', purging
2023-05-29 06:34:24,235 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p1o09rzg', purging
2023-05-29 06:34:24,235 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dil11imj', purging
2023-05-29 06:34:24,235 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w7q1zqw2', purging
2023-05-29 06:34:24,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:24,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:24,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:24,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:24,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:24,924 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:24,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:24,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:24,968 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:24,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:25,024 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:25,069 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:25,335 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k0jao518', purging
2023-05-29 06:34:25,335 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-my19dnqm', purging
2023-05-29 06:34:25,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:25,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:25,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:25,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:25,939 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:25,959 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:26,267 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:26,566 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fc_pyzug', purging
2023-05-29 06:34:26,566 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b03f99yv', purging
2023-05-29 06:34:26,567 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-puwi125i', purging
2023-05-29 06:34:26,567 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:26,567 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:26,576 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:26,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:26,745 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:26,896 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:27,532 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lps9c163', purging
2023-05-29 06:34:27,532 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r81404rf', purging
2023-05-29 06:34:27,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:27,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:27,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:27,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:27,759 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:27,810 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:27,860 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i1i3g0uk', purging
2023-05-29 06:34:27,861 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2twf6c3a', purging
2023-05-29 06:34:27,862 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:27,862 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:28,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:28,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:28,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:28,400 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:28,553 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:28,590 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:29,115 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:29,322 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gxfrl7bn', purging
2023-05-29 06:34:29,323 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lw7_sqj8', purging
2023-05-29 06:34:29,323 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ishlto8l', purging
2023-05-29 06:34:29,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:29,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:29,423 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3mmvmy3a', purging
2023-05-29 06:34:29,423 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:29,423 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:29,438 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:29,487 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:30,219 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q_66i8zf', purging
2023-05-29 06:34:30,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:30,220 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:30,240 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:30,240 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:30,281 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:30,407 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:30,681 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uj5gr08w', purging
2023-05-29 06:34:30,681 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pamdnwbs', purging
2023-05-29 06:34:30,682 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:30,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:31,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:31,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:31,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:31,134 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:31,249 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:31,263 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:31,802 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-02vaopv9', purging
2023-05-29 06:34:31,802 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j5b5hpyk', purging
2023-05-29 06:34:31,803 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rjyuwgny', purging
2023-05-29 06:34:31,803 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:31,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:31,815 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:32,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:32,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:32,199 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:32,236 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:32,766 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r0qq8dmo', purging
2023-05-29 06:34:32,766 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oc9qlklh', purging
2023-05-29 06:34:32,767 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:32,767 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:32,834 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:32,895 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-333fcbmy', purging
2023-05-29 06:34:32,896 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:32,896 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:33,054 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:33,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9axkjyk0', purging
2023-05-29 06:34:33,287 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:33,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:33,819 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-63lmu4w_', purging
2023-05-29 06:34:33,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:33,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:33,855 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:33,855 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:33,859 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:33,979 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:34,447 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:34,504 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cytypxbw', purging
2023-05-29 06:34:34,504 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5sj0t6xu', purging
2023-05-29 06:34:34,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:34,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:34,634 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:34,673 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:34,679 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_de3_gv2', purging
2023-05-29 06:34:34,679 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5yupi8la', purging
2023-05-29 06:34:34,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:34,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:35,185 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:35,303 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4q896uvj', purging
2023-05-29 06:34:35,304 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:35,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:35,629 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:35,662 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nf83uj_x', purging
2023-05-29 06:34:35,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:35,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:36,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:36,073 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:36,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:36,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:36,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:36,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:36,367 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:36,734 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:36,833 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j20ksmxu', purging
2023-05-29 06:34:36,834 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hpnrsex7', purging
2023-05-29 06:34:36,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:36,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:37,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:37,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:37,306 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:37,572 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:37,915 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3h59ir_7', purging
2023-05-29 06:34:37,915 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hqq_sc9y', purging
2023-05-29 06:34:37,916 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7wzmfmsg', purging
2023-05-29 06:34:37,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:37,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:38,093 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:38,292 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n3ubjvwy', purging
2023-05-29 06:34:38,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:38,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:38,496 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:38,886 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wrjnxmwg', purging
2023-05-29 06:34:38,887 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:38,887 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:39,039 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:39,166 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2tye0co0', purging
2023-05-29 06:34:39,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:39,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:39,419 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:39,671 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-typxruzh', purging
2023-05-29 06:34:39,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:39,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:39,758 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:40,048 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:40,122 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dbixbwhy', purging
2023-05-29 06:34:40,123 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k2i_fknv', purging
2023-05-29 06:34:40,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:40,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:40,200 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:40,617 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rgq3a7nj', purging
2023-05-29 06:34:40,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:40,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:40,788 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:40,937 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hhdw13ew', purging
2023-05-29 06:34:40,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:40,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:41,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:41,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:41,417 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:41,632 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9y4x105i', purging
2023-05-29 06:34:41,633 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:41,633 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:41,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:41,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:41,937 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:42,424 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pe7iltpt', purging
2023-05-29 06:34:42,425 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:42,425 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:42,515 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:42,626 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:42,878 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:43,031 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-16ix4rkq', purging
2023-05-29 06:34:43,031 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ftlgsgmq', purging
2023-05-29 06:34:43,032 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j24ratpt', purging
2023-05-29 06:34:43,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:43,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:43,219 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:43,453 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zjy03qk6', purging
2023-05-29 06:34:43,454 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:43,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:43,823 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:43,977 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:44,026 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hu5iudd7', purging
2023-05-29 06:34:44,026 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xupq6d70', purging
2023-05-29 06:34:44,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:44,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:44,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:44,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:44,367 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:44,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:44,693 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:44,813 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s253r8t3', purging
2023-05-29 06:34:44,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:44,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:45,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:45,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:45,466 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:45,522 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:45,545 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4h8mnzv9', purging
2023-05-29 06:34:45,545 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hzufyk1i', purging
2023-05-29 06:34:45,546 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:45,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:45,855 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:46,163 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2gu9xw7a', purging
2023-05-29 06:34:46,164 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:46,164 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:46,708 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:46,734 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:46,882 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:47,095 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yijjib4h', purging
2023-05-29 06:34:47,095 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bcack_rw', purging
2023-05-29 06:34:47,095 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t3wob7u4', purging
2023-05-29 06:34:47,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:47,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:47,178 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:47,178 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:47,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:47,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:47,866 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:48,126 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:48,310 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sb4d_d8j', purging
2023-05-29 06:34:48,311 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0u7fy948', purging
2023-05-29 06:34:48,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:48,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:48,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:48,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:48,431 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:48,522 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z0215svs', purging
2023-05-29 06:34:48,523 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:48,523 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:49,239 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:49,328 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:49,515 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ysr0diu4', purging
2023-05-29 06:34:49,516 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n20fxgvx', purging
2023-05-29 06:34:49,517 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:49,517 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-05-29 06:34:49,772 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-x9khv5mz', purging
2023-05-29 06:34:49,772 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:49,773 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:50,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:50,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:50,239 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:50,545 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:50,843 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7oko0rhz', purging
2023-05-29 06:34:50,843 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7f14pup6', purging
2023-05-29 06:34:50,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qcxl7a6_', purging
2023-05-29 06:34:50,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:50,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:50,874 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:50,920 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:50,920 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:51,490 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:51,597 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:51,803 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-iambvxs9', purging
2023-05-29 06:34:51,804 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n36kfqlj', purging
2023-05-29 06:34:51,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:51,804 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:52,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:52,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:52,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:52,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:52,563 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:52,889 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:53,071 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-edjctvaw', purging
2023-05-29 06:34:53,071 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-csmbge4n', purging
2023-05-29 06:34:53,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:53,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:53,191 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:53,191 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:53,220 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:53,851 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:53,933 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:54,191 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qo16rxvw', purging
2023-05-29 06:34:54,191 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fzp85lex', purging
2023-05-29 06:34:54,192 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_3vn1zev', purging
2023-05-29 06:34:54,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:54,193 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:54,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:54,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:54,817 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:54,820 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yud5fz5e', purging
2023-05-29 06:34:54,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:54,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:54,971 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:55,311 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:55,445 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_z8o1wt5', purging
2023-05-29 06:34:55,446 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n0xng13h', purging
2023-05-29 06:34:55,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:55,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:55,563 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:55,563 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:56,336 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:56,405 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-61wnxuq3', purging
2023-05-29 06:34:56,406 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:56,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:56,435 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:56,577 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0kg6on09', purging
2023-05-29 06:34:56,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:56,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:56,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:56,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:57,261 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:57,657 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:57,697 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:57,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d44kxs2m', purging
2023-05-29 06:34:57,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6gk8_x6q', purging
2023-05-29 06:34:57,825 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ez39y9lh', purging
2023-05-29 06:34:57,826 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:57,826 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:58,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:58,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:58,467 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:58,786 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lfvtav0o', purging
2023-05-29 06:34:58,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:58,787 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:58,842 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:34:59,209 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:34:59,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ghmmrkao', purging
2023-05-29 06:34:59,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wdfznodm', purging
2023-05-29 06:34:59,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:59,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:34:59,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:34:59,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:00,106 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hn1n6cz5', purging
2023-05-29 06:35:00,106 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ir7hiuf9', purging
2023-05-29 06:35:00,107 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:00,107 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:00,149 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:00,150 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:00,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:00,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:00,738 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s9pr7l7x', purging
2023-05-29 06:35:00,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:00,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:00,748 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:01,039 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:01,426 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:01,668 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wt1s02wt', purging
2023-05-29 06:35:01,669 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ryen51ta', purging
2023-05-29 06:35:01,669 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:01,669 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:01,726 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:01,727 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:02,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:02,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:02,504 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:02,538 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:02,651 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-66mlk6w4', purging
2023-05-29 06:35:02,652 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y_mhc3e5', purging
2023-05-29 06:35:02,653 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:02,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:02,980 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:02,980 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:03,055 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:03,368 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:03,636 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:04,081 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kps93vmt', purging
2023-05-29 06:35:04,081 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7nr85_en', purging
2023-05-29 06:35:04,082 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-00e54v8l', purging
2023-05-29 06:35:04,082 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:04,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:04,094 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:04,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:04,548 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:04,548 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:04,897 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:04,897 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:04,994 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:05,157 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xuw1wq81', purging
2023-05-29 06:35:05,158 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-51ujkv4w', purging
2023-05-29 06:35:05,159 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:05,159 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:05,237 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:05,781 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:05,901 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:06,155 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:06,575 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bgl8ur7v', purging
2023-05-29 06:35:06,575 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3hmfea_8', purging
2023-05-29 06:35:06,575 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9179p_3a', purging
2023-05-29 06:35:06,576 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:06,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:06,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:06,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:07,203 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:07,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m7llqrx7', purging
2023-05-29 06:35:07,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:07,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:07,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:07,480 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:07,531 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:07,727 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ea9gf393', purging
2023-05-29 06:35:07,728 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:07,728 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:08,346 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:08,448 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:08,728 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:08,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tb1kl15r', purging
2023-05-29 06:35:08,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lrjana4l', purging
2023-05-29 06:35:08,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m9b5x4ib', purging
2023-05-29 06:35:08,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:08,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:09,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:09,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:09,177 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:09,590 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:09,883 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zs6y8tzp', purging
2023-05-29 06:35:09,883 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wmjv4tn7', purging
2023-05-29 06:35:09,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:09,884 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:10,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:10,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:10,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:10,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:10,574 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:10,693 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5iask3lb', purging
2023-05-29 06:35:10,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:10,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:10,913 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:11,183 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v5v9snt0', purging
2023-05-29 06:35:11,183 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jml49ryf', purging
2023-05-29 06:35:11,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:11,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:11,188 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:11,539 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:11,902 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:12,081 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mcrt77l3', purging
2023-05-29 06:35:12,081 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-swwvw3el', purging
2023-05-29 06:35:12,082 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:12,082 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:12,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:12,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:12,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:12,797 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:12,953 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:13,127 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hnhg5idb', purging
2023-05-29 06:35:13,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:13,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:13,262 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:13,489 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d1t33400', purging
2023-05-29 06:35:13,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:13,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:13,589 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:13,920 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:14,273 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:14,431 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mznf4az3', purging
2023-05-29 06:35:14,432 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p0v9ok85', purging
2023-05-29 06:35:14,432 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e5nco5zw', purging
2023-05-29 06:35:14,432 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:14,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:14,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:14,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:15,065 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:15,083 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d61luhnv', purging
2023-05-29 06:35:15,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:15,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:15,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:15,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:15,602 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:15,824 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-282yrd2k', purging
2023-05-29 06:35:15,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:15,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:15,913 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:16,191 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:16,480 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:16,669 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uzm2h2q2', purging
2023-05-29 06:35:16,669 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ral_0ws3', purging
2023-05-29 06:35:16,670 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6kil1ave', purging
2023-05-29 06:35:16,670 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:16,670 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:17,116 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:17,214 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uca80kgn', purging
2023-05-29 06:35:17,215 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:17,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:17,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:17,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:17,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:17,800 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:17,985 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:17,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:18,154 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:18,592 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:18,646 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5zd8b7rd', purging
2023-05-29 06:35:18,647 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rdiccp5d', purging
2023-05-29 06:35:18,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:18,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:18,988 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:19,043 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:19,422 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:19,730 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ndxn4mwo', purging
2023-05-29 06:35:19,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cx7uvl8j', purging
2023-05-29 06:35:19,731 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v53zn0ss', purging
2023-05-29 06:35:19,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:19,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:20,089 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:20,089 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:20,489 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:20,573 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e873oez0', purging
2023-05-29 06:35:20,574 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:20,574 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:20,614 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9giuw_nr', purging
2023-05-29 06:35:20,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:20,615 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:20,634 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:20,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:20,923 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:21,254 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:21,649 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:21,871 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:21,964 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-onl3oygv', purging
2023-05-29 06:35:21,964 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gz74y5ab', purging
2023-05-29 06:35:21,965 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pyw904ke', purging
2023-05-29 06:35:21,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:21,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:22,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:22,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:22,611 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:22,729 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0c56_j1f', purging
2023-05-29 06:35:22,729 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:22,730 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:23,134 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:23,135 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hfrk7ez9', purging
2023-05-29 06:35:23,136 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:23,136 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:23,432 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:23,493 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uz137kuk', purging
2023-05-29 06:35:23,494 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:23,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:23,583 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:24,138 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:24,211 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-id65y557', purging
2023-05-29 06:35:24,211 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tlyayrb8', purging
2023-05-29 06:35:24,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:24,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:24,656 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-34ozwj5d', purging
2023-05-29 06:35:24,657 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:24,657 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:24,683 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:25,014 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:25,015 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:25,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:25,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:25,223 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:25,753 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uzpdahci', purging
2023-05-29 06:35:25,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:25,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:25,940 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:25,984 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:26,307 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yzetm79j', purging
2023-05-29 06:35:26,308 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3fd12hzj', purging
2023-05-29 06:35:26,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:26,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:26,491 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:26,823 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yh8jlvep', purging
2023-05-29 06:35:26,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:26,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:26,925 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:27,293 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:27,556 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sxa5q4am', purging
2023-05-29 06:35:27,557 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mc2ludc2', purging
2023-05-29 06:35:27,557 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:27,557 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:27,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:27,581 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:28,122 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:28,122 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:28,513 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yo6qhuoc', purging
2023-05-29 06:35:28,513 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dbmr3gee', purging
2023-05-29 06:35:28,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:28,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:28,520 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:28,564 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:28,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:28,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:28,872 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:29,371 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:29,577 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:30,069 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xd2rzs3i', purging
2023-05-29 06:35:30,069 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i364n7am', purging
2023-05-29 06:35:30,070 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-brwob0nz', purging
2023-05-29 06:35:30,070 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:30,071 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:30,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:30,091 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:30,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:30,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:30,896 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:30,943 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-em7jue_o', purging
2023-05-29 06:35:30,943 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ej1tgygs', purging
2023-05-29 06:35:30,944 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:30,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:31,084 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:31,131 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ftpy_q15', purging
2023-05-29 06:35:31,132 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:31,132 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:31,601 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:31,933 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:32,377 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0rs76kk5', purging
2023-05-29 06:35:32,378 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lfio0mk4', purging
2023-05-29 06:35:32,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:32,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:32,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:32,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:32,994 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:33,141 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:33,154 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6xo6zzh5', purging
2023-05-29 06:35:33,154 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gnhplz7_', purging
2023-05-29 06:35:33,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:33,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:33,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:33,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:33,581 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:34,157 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:34,460 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-abxpv8sx', purging
2023-05-29 06:35:34,460 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l6r5zwma', purging
2023-05-29 06:35:34,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:34,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:34,635 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:34,635 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:35,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:35,087 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:35,268 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:35,503 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:35,655 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:35,660 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_wlgp152', purging
2023-05-29 06:35:35,661 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ks36y63i', purging
2023-05-29 06:35:35,661 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rpzdoh2z', purging
2023-05-29 06:35:35,662 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:35,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:36,075 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:36,732 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2wlpvwgm', purging
2023-05-29 06:35:36,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:36,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:37,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:37,048 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:37,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:37,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:37,344 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:37,615 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c7fg08cg', purging
2023-05-29 06:35:37,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:37,615 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:38,158 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:38,159 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:38,444 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:38,864 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wbr0y378', purging
2023-05-29 06:35:38,864 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rprdf6vz', purging
2023-05-29 06:35:38,864 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-prktr5t0', purging
2023-05-29 06:35:38,865 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:38,865 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:39,305 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:39,540 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3ybnx608', purging
2023-05-29 06:35:39,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:39,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:39,748 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:39,748 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:39,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:39,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:40,349 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:40,801 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wf9j6kb9', purging
2023-05-29 06:35:40,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:40,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:40,871 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:40,920 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:41,342 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:41,789 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g2qq7msx', purging
2023-05-29 06:35:41,790 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ybhh65yx', purging
2023-05-29 06:35:41,790 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4aximpba', purging
2023-05-29 06:35:41,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:41,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:42,223 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:42,372 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fw06tmyf', purging
2023-05-29 06:35:42,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:42,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:42,459 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:42,459 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:42,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:42,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:43,059 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:43,327 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:43,664 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:43,823 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8cwvc88j', purging
2023-05-29 06:35:43,823 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-awiscg6r', purging
2023-05-29 06:35:43,823 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ey6w6bvq', purging
2023-05-29 06:35:43,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:43,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:44,256 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:44,591 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c6ji1mkw', purging
2023-05-29 06:35:44,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:44,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:44,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:44,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:45,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:45,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:45,379 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:45,705 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:45,748 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v81cqiq3', purging
2023-05-29 06:35:45,748 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z69zj3t9', purging
2023-05-29 06:35:45,749 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:45,749 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:45,803 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:46,440 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:46,839 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ksvqi2pi', purging
2023-05-29 06:35:46,839 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a4rtkb9c', purging
2023-05-29 06:35:46,840 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:46,840 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:47,297 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:47,297 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:47,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:47,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:47,490 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:47,992 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:48,005 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u3qcpjwn', purging
2023-05-29 06:35:48,005 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i8c0lt3g', purging
2023-05-29 06:35:48,005 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g6_kcyxy', purging
2023-05-29 06:35:48,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:48,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:48,027 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:48,636 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:49,039 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_4jvb9ne', purging
2023-05-29 06:35:49,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:49,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:49,584 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:49,584 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:49,602 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:49,602 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:49,654 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:50,167 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zzatfppg', purging
2023-05-29 06:35:50,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:50,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:50,287 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:50,314 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:50,655 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:51,161 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p8nxy0cw', purging
2023-05-29 06:35:51,161 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w6nhdqon', purging
2023-05-29 06:35:51,161 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zewzolba', purging
2023-05-29 06:35:51,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:51,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:51,749 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:51,778 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tbye7421', purging
2023-05-29 06:35:51,779 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:51,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:51,786 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:51,786 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:52,159 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:52,159 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:52,643 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:52,670 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:52,823 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:53,272 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gt2gro9d', purging
2023-05-29 06:35:53,273 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4y47ukby', purging
2023-05-29 06:35:53,273 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sqxvcceg', purging
2023-05-29 06:35:53,274 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:53,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:53,699 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:54,192 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c77bqvcq', purging
2023-05-29 06:35:54,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:54,193 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:54,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:54,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:54,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:54,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:55,011 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:55,041 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:55,166 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oag8_6c1', purging
2023-05-29 06:35:55,166 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0oevoc33', purging
2023-05-29 06:35:55,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:55,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:55,339 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:55,797 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:56,478 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tt0kkbv2', purging
2023-05-29 06:35:56,479 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6x0spq84', purging
2023-05-29 06:35:56,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:56,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:56,537 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:56,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:56,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:56,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:57,093 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:57,307 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-om6_gdip', purging
2023-05-29 06:35:57,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:57,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:57,354 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:57,642 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:57,949 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:35:58,632 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mwop9gcr', purging
2023-05-29 06:35:58,633 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lnetvix2', purging
2023-05-29 06:35:58,633 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3vg82yi2', purging
2023-05-29 06:35:58,634 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:58,634 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:58,808 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:58,808 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:59,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:59,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:59,234 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:59,476 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-16rbi3op', purging
2023-05-29 06:35:59,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:35:59,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:35:59,536 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:35:59,827 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:00,114 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:00,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2pk0p_b4', purging
2023-05-29 06:36:00,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j1_x7ej7', purging
2023-05-29 06:36:00,755 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-od5beyt0', purging
2023-05-29 06:36:00,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:00,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:00,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:00,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:01,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:01,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:01,360 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:01,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n5glwjpv', purging
2023-05-29 06:36:01,648 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c_3mypie', purging
2023-05-29 06:36:01,649 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:01,649 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:01,672 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:01,957 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:02,100 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:02,834 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vityadvc', purging
2023-05-29 06:36:02,834 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2ml45d7a', purging
2023-05-29 06:36:02,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:02,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:03,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:03,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:03,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:03,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:03,619 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:03,628 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5xfjar4s', purging
2023-05-29 06:36:03,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:03,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:03,926 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:04,272 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:04,308 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:05,205 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-suu5_pew', purging
2023-05-29 06:36:05,206 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7nda5tnj', purging
2023-05-29 06:36:05,206 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3b0i3kda', purging
2023-05-29 06:36:05,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:05,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:05,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:05,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:05,847 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:05,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:05,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:05,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:05,931 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:06,099 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:06,619 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:06,652 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:07,398 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jkku32ls', purging
2023-05-29 06:36:07,398 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g1xojppd', purging
2023-05-29 06:36:07,398 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v7bl5_om', purging
2023-05-29 06:36:07,399 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v32ibdvx', purging
2023-05-29 06:36:07,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:07,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:07,583 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:07,583 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:08,001 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:08,119 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qrib3cf5', purging
2023-05-29 06:36:08,119 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:08,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:08,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:08,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:08,553 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:08,909 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:08,938 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:09,485 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ceugo4__', purging
2023-05-29 06:36:09,486 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7_dfrn20', purging
2023-05-29 06:36:09,486 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-40hxyc5k', purging
2023-05-29 06:36:09,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:09,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:09,898 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:10,008 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bwhegkjh', purging
2023-05-29 06:36:10,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:10,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:10,439 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:10,439 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:10,451 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:10,451 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:10,824 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:11,205 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:11,233 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:11,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wwto1uoa', purging
2023-05-29 06:36:11,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p42_4tvm', purging
2023-05-29 06:36:11,469 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5eyo76xl', purging
2023-05-29 06:36:11,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:11,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:12,019 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:12,218 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mpl99pkd', purging
2023-05-29 06:36:12,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:12,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:12,783 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:12,809 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5az91xxr', purging
2023-05-29 06:36:12,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:12,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:12,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:12,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:13,535 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:13,561 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:13,599 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9kbj25nk', purging
2023-05-29 06:36:13,599 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-13uhoo7g', purging
2023-05-29 06:36:13,600 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:13,600 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:14,269 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:14,353 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-htql4iv1', purging
2023-05-29 06:36:14,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:14,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:14,756 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:15,127 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m_a9lt8e', purging
2023-05-29 06:36:15,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:15,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:15,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:15,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:15,852 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:15,870 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:15,889 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4m5hzi7i', purging
2023-05-29 06:36:15,889 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vbfd4ppu', purging
2023-05-29 06:36:15,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:15,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:16,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:16,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:16,478 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:16,996 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:17,420 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4ehjcz0x', purging
2023-05-29 06:36:17,420 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4xuqenxw', purging
2023-05-29 06:36:17,421 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:17,421 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:17,421 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:17,421 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:18,074 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uij1_1v0', purging
2023-05-29 06:36:18,075 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jnbqybge', purging
2023-05-29 06:36:18,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:18,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:18,129 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:18,130 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:18,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:18,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:18,556 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:19,069 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:19,562 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2xa1w_o0', purging
2023-05-29 06:36:19,562 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r455lk8c', purging
2023-05-29 06:36:19,563 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:19,563 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:19,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:19,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:20,031 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:20,031 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:20,199 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:20,490 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:20,610 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a3dgqqhv', purging
2023-05-29 06:36:20,611 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5qprb31s', purging
2023-05-29 06:36:20,611 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xvbzey7s', purging
2023-05-29 06:36:20,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:20,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:20,644 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-05-29 06:36:21,679 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_4g2rij0', purging
2023-05-29 06:36:21,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:21,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:21,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:21,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:22,117 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:22,117 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:22,512 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:22,858 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 857, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 135, in get_nowait
    return self.get(False)
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/queues.py", line 116, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 859, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-05-29 06:36:23,909 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g4adt113', purging
2023-05-29 06:36:23,909 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7krjj5am', purging
2023-05-29 06:36:23,909 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ze3ej4mi', purging
2023-05-29 06:36:23,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:23,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:24,311 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h6gr6eqy', purging
2023-05-29 06:36:24,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:24,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:24,317 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:24,869 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:25,859 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m72f4ypa', purging
2023-05-29 06:36:25,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:25,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:26,304 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:26,390 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f9_slyun', purging
2023-05-29 06:36:26,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:26,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:26,943 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:27,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wp_95b51', purging
2023-05-29 06:36:27,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:27,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:28,306 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:28,421 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0xu7pmux', purging
2023-05-29 06:36:28,421 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:28,421 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:28,807 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:29,813 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sg3zum8y', purging
2023-05-29 06:36:29,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:29,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:30,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3fh9meeb', purging
2023-05-29 06:36:30,289 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:30,289 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:30,301 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:30,714 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:31,794 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l9adhne5', purging
2023-05-29 06:36:31,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:31,795 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:32,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:32,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:32,619 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:32,747 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:34,113 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yvswc4cm', purging
2023-05-29 06:36:34,114 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-v9lrfv_h', purging
2023-05-29 06:36:34,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:34,115 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:34,124 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:34,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:34,853 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:34,905 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:36,296 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6iyw6ayr', purging
2023-05-29 06:36:36,297 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xy75a6dz', purging
2023-05-29 06:36:36,297 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:36,297 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:36,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:36,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:36,837 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:37,143 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:38,323 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7b992q3l', purging
2023-05-29 06:36:38,324 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dbd2rb_f', purging
2023-05-29 06:36:38,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:38,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:38,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:38,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:38,712 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:39,118 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:40,218 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2yh34h5l', purging
2023-05-29 06:36:40,218 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4r3rm9bc', purging
2023-05-29 06:36:40,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:40,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:40,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:40,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:40,635 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:40,971 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:42,133 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-57fme3go', purging
2023-05-29 06:36:42,134 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cm50f0_i', purging
2023-05-29 06:36:42,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:42,134 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:42,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:42,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:42,605 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:42,943 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:44,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-whkeym6h', purging
2023-05-29 06:36:44,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-luujr7yi', purging
2023-05-29 06:36:44,065 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:44,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:44,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:44,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:44,843 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:44,947 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:46,262 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-t48d6nih', purging
2023-05-29 06:36:46,262 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gewxv8gd', purging
2023-05-29 06:36:46,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:46,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:46,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:46,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:46,854 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:47,237 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:48,292 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vve4rgbz', purging
2023-05-29 06:36:48,292 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mru0jdym', purging
2023-05-29 06:36:48,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:48,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:48,635 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:48,635 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:48,712 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:49,052 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:50,221 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fnm38o9h', purging
2023-05-29 06:36:50,221 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oqqm0zoy', purging
2023-05-29 06:36:50,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:50,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:50,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:50,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:50,653 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:50,994 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:52,155 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wfnctz1b', purging
2023-05-29 06:36:52,156 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hxe3q371', purging
2023-05-29 06:36:52,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:52,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:52,451 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:52,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:52,570 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:52,873 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:53,947 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-indgvvit', purging
2023-05-29 06:36:53,948 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-10xo6_ht', purging
2023-05-29 06:36:53,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:53,948 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:54,314 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:54,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:54,677 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:54,810 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:56,174 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7efcjxg5', purging
2023-05-29 06:36:56,174 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5by4ayyc', purging
2023-05-29 06:36:56,175 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:56,175 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:56,231 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:56,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:56,996 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:57,028 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:58,473 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-waia1m2e', purging
2023-05-29 06:36:58,474 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tzijaasr', purging
2023-05-29 06:36:58,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:58,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:36:58,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:36:58,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:36:59,283 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:36:59,340 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:00,693 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-la8b8yz_', purging
2023-05-29 06:37:00,694 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-73tu1mhy', purging
2023-05-29 06:37:00,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:00,695 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:00,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:00,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:01,262 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:01,556 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:02,773 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oul3pb2u', purging
2023-05-29 06:37:02,773 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-de8z1vbg', purging
2023-05-29 06:37:02,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:02,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:03,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:03,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:03,355 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:03,648 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:04,858 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hltvtaen', purging
2023-05-29 06:37:04,859 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kmgk3ips', purging
2023-05-29 06:37:04,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:04,859 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:05,159 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:05,159 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:05,304 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:05,758 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:06,676 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fi1p7wz3', purging
2023-05-29 06:37:06,677 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7hmfu818', purging
2023-05-29 06:37:06,678 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:06,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:07,082 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:07,244 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rsbxwphe', purging
2023-05-29 06:37:07,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:07,245 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:07,837 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:08,501 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rsxrj1r8', purging
2023-05-29 06:37:08,502 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:08,502 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:09,048 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:09,257 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s_vf2h4s', purging
2023-05-29 06:37:09,257 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:09,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:09,642 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:10,552 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m2hufz5d', purging
2023-05-29 06:37:10,553 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:10,553 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:11,108 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:11,138 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n7j6gru2', purging
2023-05-29 06:37:11,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:11,139 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:11,586 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:12,577 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2zkfx2b8', purging
2023-05-29 06:37:12,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:12,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:12,983 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:13,056 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vejjrgii', purging
2023-05-29 06:37:13,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:13,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:13,687 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:14,471 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sgt1h0jk', purging
2023-05-29 06:37:14,472 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:14,472 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:14,872 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:15,154 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nhgu0y5z', purging
2023-05-29 06:37:15,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:15,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:15,711 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:16,453 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-spyaqlny', purging
2023-05-29 06:37:16,454 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:16,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:16,924 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:17,243 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8djofk9q', purging
2023-05-29 06:37:17,244 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:17,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:17,646 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:18,401 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c5njmbzg', purging
2023-05-29 06:37:18,401 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:18,401 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:19,016 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:19,047 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-202eds5t', purging
2023-05-29 06:37:19,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:19,048 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:19,431 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:20,509 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0sis8nyo', purging
2023-05-29 06:37:20,510 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:20,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:20,867 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:20,867 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:21,392 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:21,536 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:22,920 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tls6il5v', purging
2023-05-29 06:37:22,920 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-viw8gxzt', purging
2023-05-29 06:37:22,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:22,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:23,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:23,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:23,701 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:23,864 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:25,089 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-97h694or', purging
2023-05-29 06:37:25,090 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1m5av94m', purging
2023-05-29 06:37:25,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:25,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:25,346 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:25,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:25,675 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:25,963 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:27,052 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-81_93n4b', purging
2023-05-29 06:37:27,052 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xgzven3l', purging
2023-05-29 06:37:27,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:27,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:27,424 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-46t9bepl', purging
2023-05-29 06:37:27,425 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:27,425 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:27,438 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:27,851 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:28,859 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e5ekczl5', purging
2023-05-29 06:37:28,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:28,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:29,263 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:29,429 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ix6nsvn4', purging
2023-05-29 06:37:29,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:29,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:29,884 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:30,775 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tgoqg7oh', purging
2023-05-29 06:37:30,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:30,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:31,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:31,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:31,341 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:31,714 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:32,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q1io485g', purging
2023-05-29 06:37:32,833 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-68w_l_oe', purging
2023-05-29 06:37:32,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:32,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:33,215 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:33,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:33,445 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:33,813 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:34,909 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7tlo7ek9', purging
2023-05-29 06:37:34,910 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jx5l7t9b', purging
2023-05-29 06:37:34,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:34,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:35,325 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0z9v39sj', purging
2023-05-29 06:37:35,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:35,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:35,339 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:35,906 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:36,832 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nykdja2r', purging
2023-05-29 06:37:36,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:36,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:37,237 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:37,305 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pdpnqukw', purging
2023-05-29 06:37:37,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:37,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:37,705 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:38,764 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6e67r_77', purging
2023-05-29 06:37:38,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:38,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:39,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:39,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:39,186 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:39,511 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:40,645 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mxrrpsje', purging
2023-05-29 06:37:40,645 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-aqztgbu5', purging
2023-05-29 06:37:40,646 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:40,646 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:40,927 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:40,927 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:41,381 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:41,530 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:42,849 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nmkprdgd', purging
2023-05-29 06:37:42,849 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_pd4xfnz', purging
2023-05-29 06:37:42,850 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:42,850 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:42,883 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:42,883 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:43,687 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:43,718 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:45,146 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gcjr_um6', purging
2023-05-29 06:37:45,147 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kv223c65', purging
2023-05-29 06:37:45,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:45,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:45,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:45,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:46,019 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:46,056 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:47,424 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h_yz64__', purging
2023-05-29 06:37:47,425 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-atofqap6', purging
2023-05-29 06:37:47,425 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:47,425 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:37:47,521 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:47,521 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:47,981 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:48,513 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:49,374 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5y25fbt7', purging
2023-05-29 06:37:49,375 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0c5cma61', purging
2023-05-29 06:37:49,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:49,375 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:49,804 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:49,950 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-guacew49', purging
2023-05-29 06:37:49,951 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:49,951 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:50,629 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:51,317 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-15_ywy3f', purging
2023-05-29 06:37:51,318 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:51,318 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:51,722 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:52,162 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-btp5q6nu', purging
2023-05-29 06:37:52,163 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:52,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:52,766 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:53,125 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wc21brgj', purging
2023-05-29 06:37:53,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:53,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:53,629 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:54,272 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-07s09jgx', purging
2023-05-29 06:37:54,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:54,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:54,872 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:55,071 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_9rwgo6b', purging
2023-05-29 06:37:55,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:55,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:55,608 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:56,271 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ez7l5h91', purging
2023-05-29 06:37:56,272 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:56,272 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:56,682 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:57,089 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-esyibxhl', purging
2023-05-29 06:37:57,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:57,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:57,697 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:58,083 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9jvkj_ii', purging
2023-05-29 06:37:58,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:58,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:58,492 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:59,165 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-iw72cygp', purging
2023-05-29 06:37:59,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:59,165 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:37:59,813 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:37:59,954 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cjisbgkd', purging
2023-05-29 06:37:59,954 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:37:59,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:00,381 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:01,241 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pdslz9z1', purging
2023-05-29 06:38:01,241 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:01,241 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:01,831 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zbge2gd2', purging
2023-05-29 06:38:01,831 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:01,831 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:38:01,832 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:02,426 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:03,318 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o53u6kn4', purging
2023-05-29 06:38:03,319 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:03,319 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:03,869 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:03,914 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jym3posu', purging
2023-05-29 06:38:03,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:03,915 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:04,521 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:05,399 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kr7wqtb5', purging
2023-05-29 06:38:05,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:05,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:05,970 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:05,970 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:38:06,017 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:06,420 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:07,501 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p9xnda5v', purging
2023-05-29 06:38:07,502 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w3bt0rlq', purging
2023-05-29 06:38:07,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:07,503 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:38:07,876 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:07,876 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:08,059 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:08,463 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:09,583 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q40mw4z2', purging
2023-05-29 06:38:09,583 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r1k88kua', purging
2023-05-29 06:38:09,584 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:09,584 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:09,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:09,945 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:38:09,994 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:10,495 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:11,484 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ukdtaqy2', purging
2023-05-29 06:38:11,484 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-r54c2ntn', purging
2023-05-29 06:38:11,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:11,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:11,878 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:11,983 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rwnfuyci', purging
2023-05-29 06:38:11,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:11,984 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:12,419 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:13,344 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-n_wia1ns', purging
2023-05-29 06:38:13,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:13,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:13,772 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:13,903 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m3_wfqha', purging
2023-05-29 06:38:13,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:13,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:14,538 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:15,246 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1q_qksbt', purging
2023-05-29 06:38:15,247 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:15,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:38:16,064 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:16,064 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:17,391 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:17,456 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:18,846 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-iunf1hze', purging
2023-05-29 06:38:18,847 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-48fnaujc', purging
2023-05-29 06:38:18,847 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:18,847 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-29 06:38:18,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-29 06:38:18,915 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-29 06:38:21,217 - distributed.nanny - WARNING - Restarting worker
2023-05-29 06:38:21,245 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 924 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
