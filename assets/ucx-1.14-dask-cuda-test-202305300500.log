============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-30 06:09:18,141 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:09:18,146 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44369 instead
  warnings.warn(
2023-05-30 06:09:18,149 - distributed.scheduler - INFO - State start
2023-05-30 06:09:18,168 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:09:18,169 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-30 06:09:18,169 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44369/status
2023-05-30 06:09:18,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38017'
2023-05-30 06:09:18,228 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36347'
2023-05-30 06:09:18,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36331'
2023-05-30 06:09:18,237 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39791'
2023-05-30 06:09:18,290 - distributed.scheduler - INFO - Receive client connection: Client-81f9a707-feb0-11ed-a5db-d8c49764f6bb
2023-05-30 06:09:18,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58288
2023-05-30 06:09:19,669 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nhel1aoa', purging
2023-05-30 06:09:19,669 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i22xav8z', purging
2023-05-30 06:09:19,670 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xoqajmwi', purging
2023-05-30 06:09:19,670 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mkojoa66', purging
2023-05-30 06:09:19,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:19,671 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:19,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:19,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:19,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:19,688 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:19,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-30 06:09:19,689 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36947
2023-05-30 06:09:19,689 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36947
2023-05-30 06:09:19,689 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43757
2023-05-30 06:09:19,689 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-30 06:09:19,689 - distributed.worker - INFO - -------------------------------------------------
2023-05-30 06:09:19,689 - distributed.worker - INFO -               Threads:                          4
2023-05-30 06:09:19,689 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-30 06:09:19,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tqk0fmzu
2023-05-30 06:09:19,690 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:19,690 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:19,690 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ac63d6a-3568-40cd-bc04-8b947965e9b3
2023-05-30 06:09:19,690 - distributed.worker - INFO - Starting Worker plugin PreImport-885993f1-baf5-4ef8-8f1f-f6861ac44765
2023-05-30 06:09:19,690 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efff04f6-74e6-4030-96d6-13749ce577db
2023-05-30 06:09:19,690 - distributed.worker - INFO - -------------------------------------------------
2023-05-30 06:09:19,694 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:19,695 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:19,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:19,703 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36947', status: init, memory: 0, processing: 0>
2023-05-30 06:09:19,705 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36947
2023-05-30 06:09:19,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58298
2023-05-30 06:09:19,705 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-30 06:09:19,705 - distributed.worker - INFO - -------------------------------------------------
2023-05-30 06:09:19,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:20,459 - distributed.nanny - INFO - Worker process 26448 exited with status 127
2023-05-30 06:09:20,460 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:20,480 - distributed.nanny - INFO - Worker process 26451 exited with status 127
2023-05-30 06:09:20,481 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:20,505 - distributed.nanny - INFO - Worker process 26455 exited with status 127
2023-05-30 06:09:20,506 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:21,162 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34077', status: init, memory: 0, processing: 0>
2023-05-30 06:09:21,162 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34077
2023-05-30 06:09:21,162 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35240
2023-05-30 06:09:21,216 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35240; closing.
2023-05-30 06:09:21,216 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34077', status: closing, memory: 0, processing: 0>
2023-05-30 06:09:21,216 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34077
2023-05-30 06:09:21,217 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34077
2023-05-30 06:09:21,446 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:35248 closed before handshake completed
2023-05-30 06:09:21,852 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hbffrp2u', purging
2023-05-30 06:09:21,853 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ch1qvuuq', purging
2023-05-30 06:09:21,853 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qfezgjei', purging
2023-05-30 06:09:21,854 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:21,854 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:21,860 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:21,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:21,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:21,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:21,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:21,910 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:21,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:22,527 - distributed.nanny - INFO - Worker process 26496 exited with status 127
2023-05-30 06:09:22,528 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:22,649 - distributed.nanny - INFO - Worker process 26499 exited with status 127
2023-05-30 06:09:22,650 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:22,672 - distributed.nanny - INFO - Worker process 26493 exited with status 127
2023-05-30 06:09:22,673 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:22,953 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45387', status: init, memory: 0, processing: 0>
2023-05-30 06:09:22,953 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45387
2023-05-30 06:09:22,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35256
2023-05-30 06:09:23,909 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jvuhv3im', purging
2023-05-30 06:09:23,910 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lqg1ab07', purging
2023-05-30 06:09:23,910 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h_vzg8he', purging
2023-05-30 06:09:23,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:23,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:23,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:24,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:24,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:24,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:24,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:24,053 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:24,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:24,600 - distributed.nanny - INFO - Worker process 26521 exited with status 127
2023-05-30 06:09:24,601 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:24,734 - distributed.nanny - INFO - Worker process 26529 exited with status 127
2023-05-30 06:09:24,734 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:24,755 - distributed.nanny - INFO - Worker process 26526 exited with status 127
2023-05-30 06:09:24,756 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:26,124 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kedk7y0z', purging
2023-05-30 06:09:26,124 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mo0idh36', purging
2023-05-30 06:09:26,125 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f89hdx3j', purging
2023-05-30 06:09:26,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:26,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:26,132 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:26,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:26,220 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:26,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:26,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:26,226 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:26,230 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:26,816 - distributed.nanny - INFO - Worker process 26551 exited with status 127
2023-05-30 06:09:26,817 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:26,957 - distributed.nanny - INFO - Worker process 26559 exited with status 127
2023-05-30 06:09:26,957 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:26,978 - distributed.nanny - INFO - Worker process 26556 exited with status 127
2023-05-30 06:09:26,979 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:09:28,257 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yvoft9rx', purging
2023-05-30 06:09:28,258 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2nosown3', purging
2023-05-30 06:09:28,258 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yw681z_d', purging
2023-05-30 06:09:28,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:28,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:28,265 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:28,362 - distributed.scheduler - INFO - Remove client Client-81f9a707-feb0-11ed-a5db-d8c49764f6bb
2023-05-30 06:09:28,363 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58288; closing.
2023-05-30 06:09:28,363 - distributed.scheduler - INFO - Remove client Client-81f9a707-feb0-11ed-a5db-d8c49764f6bb
2023-05-30 06:09:28,363 - distributed.scheduler - INFO - Close client connection: Client-81f9a707-feb0-11ed-a5db-d8c49764f6bb
2023-05-30 06:09:28,364 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36347'. Reason: nanny-close
2023-05-30 06:09:28,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38017'. Reason: nanny-close
2023-05-30 06:09:28,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36331'. Reason: nanny-close
2023-05-30 06:09:28,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39791'. Reason: nanny-close
2023-05-30 06:09:28,365 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-30 06:09:28,367 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36947. Reason: nanny-close
2023-05-30 06:09:28,368 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58298; closing.
2023-05-30 06:09:28,369 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-30 06:09:28,369 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36947', status: closing, memory: 0, processing: 0>
2023-05-30 06:09:28,369 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36947
2023-05-30 06:09:28,370 - distributed.nanny - INFO - Worker closed
2023-05-30 06:09:28,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:28,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:28,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:09:28,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:09:28,426 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:09:28,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:28,995 - distributed.nanny - INFO - Worker process 26581 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:09:29,262 - distributed.nanny - INFO - Worker process 26586 exited with status 127
2023-05-30 06:09:29,300 - distributed.nanny - INFO - Worker process 26589 exited with status 127
2023-05-30 06:09:35,323 - distributed.core - INFO - Connection to tcp://127.0.0.1:35256 has been closed.
2023-05-30 06:09:35,324 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45387', status: running, memory: 0, processing: 0>
2023-05-30 06:09:35,324 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45387
2023-05-30 06:09:35,324 - distributed.scheduler - INFO - Lost all workers
2023-05-30 06:09:58,397 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-30 06:09:58,397 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:09:58,398 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:09:58,399 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-30 06:09:58,400 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-30 06:10:00,460 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:10:00,464 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37521 instead
  warnings.warn(
2023-05-30 06:10:00,467 - distributed.scheduler - INFO - State start
2023-05-30 06:10:00,485 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:10:00,486 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:10:00,486 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:10:00,487 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:10:00,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42561'
2023-05-30 06:10:00,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43651'
2023-05-30 06:10:00,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46069'
2023-05-30 06:10:00,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45823'
2023-05-30 06:10:00,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33377'
2023-05-30 06:10:00,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32997'
2023-05-30 06:10:00,710 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45331'
2023-05-30 06:10:00,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38259'
2023-05-30 06:10:02,127 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gbuejnk2', purging
2023-05-30 06:10:02,127 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-46r3saru', purging
2023-05-30 06:10:02,128 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hvueo8o3', purging
2023-05-30 06:10:02,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:02,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:02,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:02,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:02,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:02,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:02,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:02,220 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:02,244 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:02,247 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:02,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:02,264 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:02,264 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:02,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:02,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:02,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:02,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:02,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:02,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:02,275 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:02,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:02,313 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:02,316 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:02,318 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:03,839 - distributed.nanny - INFO - Worker process 26790 exited with status 127
2023-05-30 06:10:03,840 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:03,960 - distributed.nanny - INFO - Worker process 26786 exited with status 127
2023-05-30 06:10:03,961 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:04,233 - distributed.nanny - INFO - Worker process 26801 exited with status 127
2023-05-30 06:10:04,234 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:04,260 - distributed.nanny - INFO - Worker process 26783 exited with status 127
2023-05-30 06:10:04,260 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:04,314 - distributed.nanny - INFO - Worker process 26799 exited with status 127
2023-05-30 06:10:04,315 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:04,339 - distributed.nanny - INFO - Worker process 26807 exited with status 127
2023-05-30 06:10:04,339 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:04,362 - distributed.nanny - INFO - Worker process 26794 exited with status 127
2023-05-30 06:10:04,363 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:04,387 - distributed.nanny - INFO - Worker process 26804 exited with status 127
2023-05-30 06:10:04,388 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:05,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qlk0asdd', purging
2023-05-30 06:10:05,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qfgm_crf', purging
2023-05-30 06:10:05,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5xnvl7tl', purging
2023-05-30 06:10:05,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g2b0veeg', purging
2023-05-30 06:10:05,287 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6qvxp14r', purging
2023-05-30 06:10:05,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lx2_rwk2', purging
2023-05-30 06:10:05,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jw_86zlf', purging
2023-05-30 06:10:05,288 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2bivhlmb', purging
2023-05-30 06:10:05,289 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:05,289 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:05,311 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:05,533 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:05,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:05,565 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:05,716 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:05,716 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:05,777 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:05,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:05,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:05,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:05,846 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qpp2i417', purging
2023-05-30 06:10:05,847 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:05,847 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:05,851 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:05,853 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:05,874 - distributed.nanny - INFO - Worker process 26860 exited with status 127
2023-05-30 06:10:05,875 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:05,880 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:05,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:05,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:05,906 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:05,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:05,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:05,932 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:05,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:06,086 - distributed.nanny - INFO - Worker process 26863 exited with status 127
2023-05-30 06:10:06,086 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:07,418 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zk3s1qj3', purging
2023-05-30 06:10:07,419 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:07,419 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:07,455 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:07,532 - distributed.nanny - INFO - Worker process 26875 exited with status 127
2023-05-30 06:10:07,533 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:07,554 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tiuepz88', purging
2023-05-30 06:10:07,555 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ihbxu4s2', purging
2023-05-30 06:10:07,555 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-big96dkz', purging
2023-05-30 06:10:07,555 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:07,555 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:07,556 - distributed.nanny - INFO - Worker process 26878 exited with status 127
2023-05-30 06:10:07,557 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:07,582 - distributed.nanny - INFO - Worker process 26872 exited with status 127
2023-05-30 06:10:07,583 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:07,617 - distributed.nanny - INFO - Worker process 26884 exited with status 127
2023-05-30 06:10:07,618 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:07,643 - distributed.nanny - INFO - Worker process 26881 exited with status 127
2023-05-30 06:10:07,644 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:07,673 - distributed.nanny - INFO - Worker process 26887 exited with status 127
2023-05-30 06:10:07,674 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:07,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:08,245 - distributed.nanny - INFO - Worker process 26919 exited with status 127
2023-05-30 06:10:08,246 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:08,312 - distributed.nanny - INFO - Worker process 26931 exited with status 127
2023-05-30 06:10:08,313 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:09,070 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2ajm17no', purging
2023-05-30 06:10:09,070 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gyzn45pv', purging
2023-05-30 06:10:09,071 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4czaogaf', purging
2023-05-30 06:10:09,071 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zuv1qsqn', purging
2023-05-30 06:10:09,071 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-toq1__hp', purging
2023-05-30 06:10:09,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:09,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:09,097 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:09,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:09,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:09,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:09,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:09,124 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:09,126 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:09,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:09,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:09,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:09,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:09,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:09,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:09,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:09,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:09,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:09,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:09,731 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:09,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:09,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:09,850 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:09,883 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:10,200 - distributed.nanny - INFO - Worker process 26964 exited with status 127
2023-05-30 06:10:10,201 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:10,681 - distributed.nanny - INFO - Worker process 26957 exited with status 127
2023-05-30 06:10:10,682 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:10,709 - distributed.nanny - INFO - Worker process 26961 exited with status 127
2023-05-30 06:10:10,710 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:11,187 - distributed.nanny - INFO - Worker process 26967 exited with status 127
2023-05-30 06:10:11,187 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:11,313 - distributed.nanny - INFO - Worker process 26975 exited with status 127
2023-05-30 06:10:11,314 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:11,336 - distributed.nanny - INFO - Worker process 26970 exited with status 127
2023-05-30 06:10:11,337 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:11,425 - distributed.nanny - INFO - Worker process 26987 exited with status 127
2023-05-30 06:10:11,426 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:11,456 - distributed.nanny - INFO - Worker process 26992 exited with status 127
2023-05-30 06:10:11,457 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:11,700 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rij623z2', purging
2023-05-30 06:10:11,701 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3wi1a6yx', purging
2023-05-30 06:10:11,701 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yi3uxw6f', purging
2023-05-30 06:10:11,701 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zzcwcr7v', purging
2023-05-30 06:10:11,702 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_39ssd2e', purging
2023-05-30 06:10:11,702 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-so9raup1', purging
2023-05-30 06:10:11,702 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rcpffmga', purging
2023-05-30 06:10:11,702 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pwd22z9d', purging
2023-05-30 06:10:11,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:11,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:11,725 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:12,112 - distributed.nanny - INFO - Worker process 27031 exited with status 127
2023-05-30 06:10:12,113 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:12,264 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ndgg_pp1', purging
2023-05-30 06:10:12,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:12,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:12,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:12,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:12,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:12,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:12,749 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:12,749 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:12,753 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:12,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:12,804 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:12,804 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:12,919 - distributed.nanny - INFO - Worker process 27040 exited with status 127
2023-05-30 06:10:12,920 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:12,931 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lsacq9mx', purging
2023-05-30 06:10:12,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:12,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:12,935 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:12,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:12,968 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:12,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:12,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:12,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:13,329 - distributed.nanny - INFO - Worker process 27043 exited with status 127
2023-05-30 06:10:13,330 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:13,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:13,652 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-425p0rng', purging
2023-05-30 06:10:13,653 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:13,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:13,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:13,913 - distributed.nanny - INFO - Worker process 27052 exited with status 127
2023-05-30 06:10:13,914 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:13,942 - distributed.nanny - INFO - Worker process 27059 exited with status 127
2023-05-30 06:10:13,943 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:14,333 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-m8y4zen8', purging
2023-05-30 06:10:14,333 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yre0xlhi', purging
2023-05-30 06:10:14,334 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:14,334 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:14,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:14,529 - distributed.nanny - INFO - Worker process 27062 exited with status 127
2023-05-30 06:10:14,530 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:14,552 - distributed.nanny - INFO - Worker process 27070 exited with status 127
2023-05-30 06:10:14,553 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:14,608 - distributed.nanny - INFO - Worker process 27067 exited with status 127
2023-05-30 06:10:14,609 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:14,882 - distributed.nanny - INFO - Worker process 27084 exited with status 127
2023-05-30 06:10:14,883 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:14,898 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7svbpdhe', purging
2023-05-30 06:10:14,898 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cg7b0emv', purging
2023-05-30 06:10:14,899 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p4d7rn39', purging
2023-05-30 06:10:14,899 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-oe3_1f2j', purging
2023-05-30 06:10:14,900 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:14,900 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:14,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:15,040 - distributed.nanny - INFO - Worker process 27106 exited with status 127
2023-05-30 06:10:15,041 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:15,381 - distributed.nanny - INFO - Worker process 27120 exited with status 127
2023-05-30 06:10:15,383 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:15,417 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wvv1mgx8', purging
2023-05-30 06:10:15,417 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ziiytgsd', purging
2023-05-30 06:10:15,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:15,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:15,441 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:15,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:15,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:15,496 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:16,059 - distributed.nanny - INFO - Worker process 27139 exited with status 127
2023-05-30 06:10:16,060 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:16,121 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a8vdg6al', purging
2023-05-30 06:10:16,122 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:16,122 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:16,153 - distributed.nanny - INFO - Worker process 27135 exited with status 127
2023-05-30 06:10:16,154 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:16,163 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wujca8ru', purging
2023-05-30 06:10:16,164 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:16,164 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:16,168 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:16,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:16,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:16,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:16,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:16,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:16,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:16,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:16,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:16,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:16,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:16,976 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:16,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:17,011 - distributed.nanny - INFO - Worker process 27157 exited with status 127
2023-05-30 06:10:17,012 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:17,048 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:17,284 - distributed.nanny - INFO - Worker process 27154 exited with status 127
2023-05-30 06:10:17,285 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:17,595 - distributed.nanny - INFO - Worker process 27160 exited with status 127
2023-05-30 06:10:17,596 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:17,617 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bbkdcfq3', purging
2023-05-30 06:10:17,617 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rwbigs3c', purging
2023-05-30 06:10:17,618 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-al4a2ce4', purging
2023-05-30 06:10:17,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:17,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:17,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:17,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:17,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:17,852 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:17,876 - distributed.nanny - INFO - Worker process 27166 exited with status 127
2023-05-30 06:10:17,877 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:17,975 - distributed.nanny - INFO - Worker process 27173 exited with status 127
2023-05-30 06:10:17,976 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:18,244 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46069'. Reason: nanny-close
2023-05-30 06:10:18,244 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32997'. Reason: nanny-close
2023-05-30 06:10:18,244 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42561'. Reason: nanny-close
2023-05-30 06:10:18,245 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43651'. Reason: nanny-close
2023-05-30 06:10:18,245 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45823'. Reason: nanny-close
2023-05-30 06:10:18,245 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33377'. Reason: nanny-close
2023-05-30 06:10:18,245 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45331'. Reason: nanny-close
2023-05-30 06:10:18,245 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38259'. Reason: nanny-close
2023-05-30 06:10:18,255 - distributed.nanny - INFO - Worker process 27182 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:18,626 - distributed.nanny - INFO - Worker process 27199 exited with status 127
2023-05-30 06:10:18,661 - distributed.nanny - INFO - Worker process 27207 exited with status 127
2023-05-30 06:10:18,661 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jl0gartt', purging
2023-05-30 06:10:18,661 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ev7l_r43', purging
2023-05-30 06:10:18,661 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6_fjpb0e', purging
2023-05-30 06:10:18,662 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_60ck0le', purging
2023-05-30 06:10:18,662 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9mj91g6u', purging
2023-05-30 06:10:18,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:18,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:18,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:18,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:18,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:18,966 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:19,107 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:19,107 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:19,277 - distributed.nanny - INFO - Worker process 27231 exited with status 127
2023-05-30 06:10:19,282 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:19,428 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-krskd0ds', purging
2023-05-30 06:10:19,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:19,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:19,480 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:19,491 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:19,491 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:19,536 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:19,563 - distributed.nanny - INFO - Worker process 27239 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:22,105 - distributed.nanny - INFO - Worker process 27245 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:22,406 - distributed.nanny - INFO - Worker process 27263 exited with status 127
2023-05-30 06:10:22,430 - distributed.nanny - INFO - Worker process 27258 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-30 06:10:49,954 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:10:49,958 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38773 instead
  warnings.warn(
2023-05-30 06:10:49,961 - distributed.scheduler - INFO - State start
2023-05-30 06:10:49,979 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:10:49,980 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:10:49,980 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:10:49,981 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:10:50,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35815'
2023-05-30 06:10:50,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41239'
2023-05-30 06:10:50,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34669'
2023-05-30 06:10:50,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46645'
2023-05-30 06:10:50,223 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43587'
2023-05-30 06:10:50,231 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41281'
2023-05-30 06:10:50,239 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42561'
2023-05-30 06:10:50,247 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43567'
2023-05-30 06:10:51,608 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9qeqy_kg', purging
2023-05-30 06:10:51,608 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_yq2kys9', purging
2023-05-30 06:10:51,609 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7nh7dqjy', purging
2023-05-30 06:10:51,609 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-al2no9sb', purging
2023-05-30 06:10:51,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:51,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:51,634 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:51,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:51,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:51,726 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:51,738 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:51,738 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:51,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:51,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:51,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:51,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:51,783 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:51,783 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:51,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:51,795 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:51,799 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:51,799 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:51,919 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:51,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:51,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:51,950 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:51,952 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:51,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:53,312 - distributed.nanny - INFO - Worker process 27473 exited with status 127
2023-05-30 06:10:53,313 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:53,743 - distributed.nanny - INFO - Worker process 27480 exited with status 127
2023-05-30 06:10:53,744 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:53,767 - distributed.nanny - INFO - Worker process 27476 exited with status 127
2023-05-30 06:10:53,768 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:53,822 - distributed.nanny - INFO - Worker process 27497 exited with status 127
2023-05-30 06:10:53,822 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:53,848 - distributed.nanny - INFO - Worker process 27491 exited with status 127
2023-05-30 06:10:53,849 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:53,873 - distributed.nanny - INFO - Worker process 27494 exited with status 127
2023-05-30 06:10:53,874 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:53,895 - distributed.nanny - INFO - Worker process 27484 exited with status 127
2023-05-30 06:10:53,895 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:53,931 - distributed.nanny - INFO - Worker process 27488 exited with status 127
2023-05-30 06:10:53,931 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:54,913 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j3td8l7w', purging
2023-05-30 06:10:54,913 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sjvveqdd', purging
2023-05-30 06:10:54,913 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gmwoq005', purging
2023-05-30 06:10:54,914 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b6z0vm7n', purging
2023-05-30 06:10:54,914 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-um4oy21t', purging
2023-05-30 06:10:54,914 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4tb2_lqv', purging
2023-05-30 06:10:54,915 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y2w37rz1', purging
2023-05-30 06:10:54,915 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-axadwc5k', purging
2023-05-30 06:10:54,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:54,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:54,940 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:55,298 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:55,298 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:55,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:55,331 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wwb3b07y', purging
2023-05-30 06:10:55,332 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:55,332 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:55,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:55,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:55,355 - distributed.nanny - INFO - Worker process 27549 exited with status 127
2023-05-30 06:10:55,356 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:55,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:55,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:55,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:55,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:55,423 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:55,424 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:55,424 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:55,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:55,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:55,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:55,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:55,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:55,625 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:55,642 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:56,861 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-g_ubdcr9', purging
2023-05-30 06:10:56,862 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:56,862 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:56,920 - distributed.nanny - INFO - Worker process 27571 exited with status 127
2023-05-30 06:10:56,921 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:56,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:57,161 - distributed.nanny - INFO - Worker process 27559 exited with status 127
2023-05-30 06:10:57,162 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:57,200 - distributed.nanny - INFO - Worker process 27562 exited with status 127
2023-05-30 06:10:57,201 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:57,252 - distributed.nanny - INFO - Worker process 27574 exited with status 127
2023-05-30 06:10:57,253 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:57,305 - distributed.nanny - INFO - Worker process 27565 exited with status 127
2023-05-30 06:10:57,306 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:57,546 - distributed.nanny - INFO - Worker process 27578 exited with status 127
2023-05-30 06:10:57,547 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:57,571 - distributed.nanny - INFO - Worker process 27568 exited with status 127
2023-05-30 06:10:57,572 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:57,723 - distributed.nanny - INFO - Worker process 27599 exited with status 127
2023-05-30 06:10:57,724 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:58,374 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-s2317cef', purging
2023-05-30 06:10:58,374 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-woaii7xe', purging
2023-05-30 06:10:58,375 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k43i5nqi', purging
2023-05-30 06:10:58,375 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pdan3iqn', purging
2023-05-30 06:10:58,375 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-c6a8gl6k', purging
2023-05-30 06:10:58,375 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ugicwysf', purging
2023-05-30 06:10:58,376 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qeyia3p7', purging
2023-05-30 06:10:58,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:58,376 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:58,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:10:58,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:58,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:58,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:58,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:58,795 - distributed.nanny - INFO - Worker process 27635 exited with status 127
2023-05-30 06:10:58,796 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:10:58,802 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:58,802 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:58,809 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-33piufin', purging
2023-05-30 06:10:58,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:58,810 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:58,833 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:58,868 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:58,868 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:58,900 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:59,002 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:59,002 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:59,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:59,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:59,257 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:10:59,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:10:59,304 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:59,310 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:10:59,313 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:00,306 - distributed.nanny - INFO - Worker process 27646 exited with status 127
2023-05-30 06:11:00,307 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:00,330 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y8hj_jtn', purging
2023-05-30 06:11:00,330 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pjyfp9lf', purging
2023-05-30 06:11:00,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:00,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:00,507 - distributed.nanny - INFO - Worker process 27643 exited with status 127
2023-05-30 06:11:00,508 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:00,544 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:00,575 - distributed.nanny - INFO - Worker process 27650 exited with status 127
2023-05-30 06:11:00,576 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:00,794 - distributed.nanny - INFO - Worker process 27653 exited with status 127
2023-05-30 06:11:00,795 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:00,969 - distributed.nanny - INFO - Worker process 27661 exited with status 127
2023-05-30 06:11:00,970 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:00,992 - distributed.nanny - INFO - Worker process 27668 exited with status 127
2023-05-30 06:11:00,993 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:01,013 - distributed.nanny - INFO - Worker process 27664 exited with status 127
2023-05-30 06:11:01,014 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:01,161 - distributed.nanny - INFO - Worker process 27691 exited with status 127
2023-05-30 06:11:01,162 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:01,843 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-356cntdc', purging
2023-05-30 06:11:01,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4eodu9gk', purging
2023-05-30 06:11:01,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8t4ae8xy', purging
2023-05-30 06:11:01,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h9fosa4f', purging
2023-05-30 06:11:01,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ghzvf00j', purging
2023-05-30 06:11:01,845 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ke4c8qp_', purging
2023-05-30 06:11:01,845 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:01,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:01,868 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:01,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:01,977 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:02,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:02,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:02,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:02,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:02,363 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:02,363 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:02,410 - distributed.nanny - INFO - Worker process 27726 exited with status 127
2023-05-30 06:11:02,411 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:02,426 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:02,521 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bqt9p06z', purging
2023-05-30 06:11:02,522 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:02,522 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:02,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:02,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:02,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:02,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:02,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:02,687 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:02,687 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:02,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:02,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:02,782 - distributed.nanny - INFO - Worker process 27730 exited with status 127
2023-05-30 06:11:02,783 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:02,794 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:03,121 - distributed.nanny - INFO - Worker process 27735 exited with status 127
2023-05-30 06:11:03,122 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:03,879 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3w8lg5y5', purging
2023-05-30 06:11:03,880 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-51eo35gq', purging
2023-05-30 06:11:03,880 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-trvmc6_6', purging
2023-05-30 06:11:03,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:03,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:03,938 - distributed.nanny - INFO - Worker process 27742 exited with status 127
2023-05-30 06:11:03,939 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:03,994 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:04,188 - distributed.nanny - INFO - Worker process 27754 exited with status 127
2023-05-30 06:11:04,189 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:04,240 - distributed.nanny - INFO - Worker process 27751 exited with status 127
2023-05-30 06:11:04,240 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:04,265 - distributed.nanny - INFO - Worker process 27748 exited with status 127
2023-05-30 06:11:04,266 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:04,335 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p4r1d04_', purging
2023-05-30 06:11:04,336 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-lbaafq82', purging
2023-05-30 06:11:04,336 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-5qzmolfu', purging
2023-05-30 06:11:04,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:04,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:04,480 - distributed.nanny - INFO - Worker process 27759 exited with status 127
2023-05-30 06:11:04,481 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:04,487 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:04,601 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z__xquxy', purging
2023-05-30 06:11:04,602 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:04,602 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:04,788 - distributed.nanny - INFO - Worker process 27785 exited with status 127
2023-05-30 06:11:04,789 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:04,793 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:05,088 - distributed.nanny - INFO - Worker process 27803 exited with status 127
2023-05-30 06:11:05,089 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:05,233 - distributed.nanny - INFO - Worker process 27810 exited with status 127
2023-05-30 06:11:05,233 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:05,538 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_aqiyyil', purging
2023-05-30 06:11:05,538 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wx6rdyy8', purging
2023-05-30 06:11:05,539 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6psd5c1b', purging
2023-05-30 06:11:05,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:05,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:05,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:05,845 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:05,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:05,878 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:05,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:05,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:05,915 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:05,935 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:05,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:05,982 - distributed.nanny - INFO - Worker process 27825 exited with status 127
2023-05-30 06:11:05,983 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:05,994 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:06,083 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f6ntyomr', purging
2023-05-30 06:11:06,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:06,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:06,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:06,276 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:06,276 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:06,451 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:06,614 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7f8bd06ed1c0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1318, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 1348, in _ensure_connected
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-05-30 06:11:06,616 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34669'. Reason: nanny-close
2023-05-30 06:11:06,617 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46645'. Reason: nanny-close
2023-05-30 06:11:06,617 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42561'. Reason: nanny-close
2023-05-30 06:11:06,618 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35815'. Reason: nanny-close
2023-05-30 06:11:06,618 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41239'. Reason: nanny-close
2023-05-30 06:11:06,618 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43587'. Reason: nanny-close
2023-05-30 06:11:06,618 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41281'. Reason: nanny-close
2023-05-30 06:11:06,619 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43567'. Reason: nanny-close
2023-05-30 06:11:06,677 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-61f9o0d8', purging
2023-05-30 06:11:06,678 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:06,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:06,711 - distributed.nanny - INFO - Worker process 27834 exited with status 127
2023-05-30 06:11:06,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:06,861 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:06,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:06,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:07,048 - distributed.nanny - INFO - Worker process 27839 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:07,352 - distributed.nanny - INFO - Worker process 27842 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:07,606 - distributed.nanny - INFO - Worker process 27848 exited with status 127
2023-05-30 06:11:07,614 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xi10jq1v', purging
2023-05-30 06:11:07,615 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-amu0ytl2', purging
2023-05-30 06:11:07,615 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-be8s4mdb', purging
2023-05-30 06:11:07,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:07,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:07,794 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:07,862 - distributed.nanny - INFO - Worker process 27858 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:08,145 - distributed.nanny - INFO - Worker process 27867 exited with status 127
2023-05-30 06:11:08,174 - distributed.nanny - INFO - Worker process 27872 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:08,467 - distributed.nanny - INFO - Worker process 27893 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-30 06:11:38,422 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:11:38,426 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37331 instead
  warnings.warn(
2023-05-30 06:11:38,429 - distributed.scheduler - INFO - State start
2023-05-30 06:11:38,447 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:11:38,448 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:11:38,448 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:11:38,448 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:11:38,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45057'
2023-05-30 06:11:38,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44077'
2023-05-30 06:11:38,604 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36089'
2023-05-30 06:11:38,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36657'
2023-05-30 06:11:38,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46237'
2023-05-30 06:11:38,621 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40157'
2023-05-30 06:11:38,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46457'
2023-05-30 06:11:38,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33641'
2023-05-30 06:11:40,048 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-d8q5rxhv', purging
2023-05-30 06:11:40,048 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ionbfyrf', purging
2023-05-30 06:11:40,049 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y6k5q774', purging
2023-05-30 06:11:40,049 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-o_wplopv', purging
2023-05-30 06:11:40,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:40,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:40,072 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:40,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:40,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:40,149 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:40,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:40,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:40,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:40,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:40,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:40,205 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:40,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:40,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:40,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:40,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:40,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:40,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:40,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:40,354 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:40,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:40,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:40,356 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:40,357 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:40,743 - distributed.nanny - INFO - Worker process 28103 exited with status 127
2023-05-30 06:11:40,744 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:42,032 - distributed.nanny - INFO - Worker process 28114 exited with status 127
2023-05-30 06:11:42,033 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:42,146 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-usb55_p8', purging
2023-05-30 06:11:42,146 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3rn8yxmw', purging
2023-05-30 06:11:42,147 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9gbm7xje', purging
2023-05-30 06:11:42,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:42,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:42,163 - distributed.nanny - INFO - Worker process 28124 exited with status 127
2023-05-30 06:11:42,164 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:42,201 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:42,229 - distributed.nanny - INFO - Worker process 28118 exited with status 127
2023-05-30 06:11:42,230 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:42,252 - distributed.nanny - INFO - Worker process 28127 exited with status 127
2023-05-30 06:11:42,253 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:42,275 - distributed.nanny - INFO - Worker process 28106 exited with status 127
2023-05-30 06:11:42,275 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:42,299 - distributed.nanny - INFO - Worker process 28110 exited with status 127
2023-05-30 06:11:42,300 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:42,323 - distributed.nanny - INFO - Worker process 28121 exited with status 127
2023-05-30 06:11:42,324 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:42,675 - distributed.nanny - INFO - Worker process 28167 exited with status 127
2023-05-30 06:11:42,676 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:43,559 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uih84opj', purging
2023-05-30 06:11:43,560 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yku9ggor', purging
2023-05-30 06:11:43,560 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6ck8wjjo', purging
2023-05-30 06:11:43,561 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xpswhriq', purging
2023-05-30 06:11:43,561 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h1hydl7n', purging
2023-05-30 06:11:43,561 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nyq5z5a1', purging
2023-05-30 06:11:43,562 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:43,562 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:43,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:43,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:43,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:43,731 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:43,731 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:43,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:43,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:43,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:43,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:43,851 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:43,853 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:43,853 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:43,853 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:43,863 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:43,872 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:43,890 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:43,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:43,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:43,941 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:44,090 - distributed.nanny - INFO - Worker process 28184 exited with status 127
2023-05-30 06:11:44,091 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:44,222 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hgpntety', purging
2023-05-30 06:11:44,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:44,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:44,755 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:45,467 - distributed.nanny - INFO - Worker process 28194 exited with status 127
2023-05-30 06:11:45,468 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:45,494 - distributed.nanny - INFO - Worker process 28205 exited with status 127
2023-05-30 06:11:45,495 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:45,551 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-83x4n2g3', purging
2023-05-30 06:11:45,551 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mq4deyz5', purging
2023-05-30 06:11:45,552 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:45,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:45,696 - distributed.nanny - INFO - Worker process 28199 exited with status 127
2023-05-30 06:11:45,697 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:45,711 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:45,735 - distributed.nanny - INFO - Worker process 28208 exited with status 127
2023-05-30 06:11:45,736 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:45,759 - distributed.nanny - INFO - Worker process 28202 exited with status 127
2023-05-30 06:11:45,759 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:45,782 - distributed.nanny - INFO - Worker process 28211 exited with status 127
2023-05-30 06:11:45,783 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:46,060 - distributed.nanny - INFO - Worker process 28218 exited with status 127
2023-05-30 06:11:46,061 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:46,217 - distributed.nanny - INFO - Worker process 28252 exited with status 127
2023-05-30 06:11:46,218 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:47,040 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ro5n1gcw', purging
2023-05-30 06:11:47,040 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w3k3snhl', purging
2023-05-30 06:11:47,041 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7nczuw5w', purging
2023-05-30 06:11:47,041 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-epra_1ds', purging
2023-05-30 06:11:47,041 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-73vyhqyz', purging
2023-05-30 06:11:47,042 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nb4vye96', purging
2023-05-30 06:11:47,042 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:47,042 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:47,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:47,088 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:47,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:47,113 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:47,210 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:47,210 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:47,211 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:47,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:47,275 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:47,275 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:47,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:47,352 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:47,353 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:47,353 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:47,354 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:47,394 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:47,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:47,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:47,664 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:47,750 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-68pr4kb4', purging
2023-05-30 06:11:47,751 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:47,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:47,767 - distributed.nanny - INFO - Worker process 28279 exited with status 127
2023-05-30 06:11:47,768 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:47,814 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:48,569 - distributed.nanny - INFO - Worker process 28282 exited with status 127
2023-05-30 06:11:48,570 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:49,186 - distributed.nanny - INFO - Worker process 28294 exited with status 127
2023-05-30 06:11:49,187 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:49,257 - distributed.nanny - INFO - Worker process 28287 exited with status 127
2023-05-30 06:11:49,257 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:49,265 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vvmx59fj', purging
2023-05-30 06:11:49,266 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u4srk7ej', purging
2023-05-30 06:11:49,266 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e8gj4r8d', purging
2023-05-30 06:11:49,267 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3pvk1ghs', purging
2023-05-30 06:11:49,267 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uqgtvlha', purging
2023-05-30 06:11:49,267 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:49,267 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:49,279 - distributed.nanny - INFO - Worker process 28291 exited with status 127
2023-05-30 06:11:49,280 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:49,305 - distributed.nanny - INFO - Worker process 28297 exited with status 127
2023-05-30 06:11:49,306 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:49,336 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:49,388 - distributed.nanny - INFO - Worker process 28303 exited with status 127
2023-05-30 06:11:49,389 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:49,479 - distributed.nanny - INFO - Worker process 28309 exited with status 127
2023-05-30 06:11:49,480 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:49,817 - distributed.nanny - INFO - Worker process 28345 exited with status 127
2023-05-30 06:11:49,818 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:50,039 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b6nui4ki', purging
2023-05-30 06:11:50,040 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0zfqpvm0', purging
2023-05-30 06:11:50,040 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j3f937ah', purging
2023-05-30 06:11:50,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:50,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:50,063 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:50,462 - distributed.nanny - INFO - Worker process 28360 exited with status 127
2023-05-30 06:11:50,463 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:50,697 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i6271t41', purging
2023-05-30 06:11:50,697 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:50,697 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:50,721 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:50,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:50,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:50,801 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:50,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:50,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:50,850 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:50,850 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:50,925 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:50,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:50,985 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:50,985 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:50,986 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:51,012 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:51,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:51,050 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:51,370 - distributed.nanny - INFO - Worker process 28372 exited with status 127
2023-05-30 06:11:51,371 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:51,379 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-nepj9_pc', purging
2023-05-30 06:11:51,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:51,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:51,869 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:51,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:51,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:51,949 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:52,415 - distributed.nanny - INFO - Worker process 28382 exited with status 127
2023-05-30 06:11:52,416 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:52,479 - distributed.nanny - INFO - Worker process 28388 exited with status 127
2023-05-30 06:11:52,479 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:52,674 - distributed.nanny - INFO - Worker process 28376 exited with status 127
2023-05-30 06:11:52,675 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:52,702 - distributed.nanny - INFO - Worker process 28379 exited with status 127
2023-05-30 06:11:52,703 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:52,842 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7cxdg0kk', purging
2023-05-30 06:11:52,843 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k0ao4_sg', purging
2023-05-30 06:11:52,843 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-61nnyk6v', purging
2023-05-30 06:11:52,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-z8qzb_xv', purging
2023-05-30 06:11:52,845 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:52,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:52,924 - distributed.nanny - INFO - Worker process 28394 exited with status 127
2023-05-30 06:11:52,925 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:11:52,943 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:52,995 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36089'. Reason: nanny-close
2023-05-30 06:11:52,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36657'. Reason: nanny-close
2023-05-30 06:11:52,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46457'. Reason: nanny-close
2023-05-30 06:11:52,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45057'. Reason: nanny-close
2023-05-30 06:11:52,996 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44077'. Reason: nanny-close
2023-05-30 06:11:52,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46237'. Reason: nanny-close
2023-05-30 06:11:52,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40157'. Reason: nanny-close
2023-05-30 06:11:52,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33641'. Reason: nanny-close
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:53,273 - distributed.nanny - INFO - Worker process 28414 exited with status 127
2023-05-30 06:11:53,301 - distributed.nanny - INFO - Worker process 28399 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:53,439 - distributed.nanny - INFO - Worker process 28444 exited with status 127
2023-05-30 06:11:53,889 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k1vjuat7', purging
2023-05-30 06:11:53,889 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-glgt3rsg', purging
2023-05-30 06:11:53,890 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6muojg_8', purging
2023-05-30 06:11:53,890 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jfzgdycc', purging
2023-05-30 06:11:53,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:53,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:53,913 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:53,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:53,984 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:54,159 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:54,159 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:54,168 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:54,188 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:11:54,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:54,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:54,231 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:54,330 - distributed.nanny - INFO - Worker process 28463 exited with status 127
2023-05-30 06:11:54,410 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fwc5fdli', purging
2023-05-30 06:11:54,410 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:11:54,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:11:54,532 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:55,079 - distributed.nanny - INFO - Worker process 28468 exited with status 127
2023-05-30 06:11:55,263 - distributed.nanny - INFO - Worker process 28471 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:55,345 - distributed.nanny - INFO - Worker process 28475 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:11:55,444 - distributed.nanny - INFO - Worker process 28481 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-30 06:12:24,696 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:12:24,700 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40231 instead
  warnings.warn(
2023-05-30 06:12:24,703 - distributed.scheduler - INFO - State start
2023-05-30 06:12:24,721 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:12:24,722 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:12:24,722 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:12:24,723 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:12:24,884 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46031'
2023-05-30 06:12:24,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42811'
2023-05-30 06:12:24,912 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34153'
2023-05-30 06:12:24,914 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35823'
2023-05-30 06:12:24,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34483'
2023-05-30 06:12:24,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33941'
2023-05-30 06:12:24,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40479'
2023-05-30 06:12:24,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42865'
2023-05-30 06:12:26,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j5l8wmva', purging
2023-05-30 06:12:26,295 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-csgiaasm', purging
2023-05-30 06:12:26,295 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kn8ffab6', purging
2023-05-30 06:12:26,295 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j8wpwtjb', purging
2023-05-30 06:12:26,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:26,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:26,320 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:26,384 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:26,384 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:26,408 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:26,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:26,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:26,432 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:26,432 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:26,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:26,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:26,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:26,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:26,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:26,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:26,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:26,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:26,589 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:26,593 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:26,600 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:26,601 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:26,602 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:26,603 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:27,002 - distributed.nanny - INFO - Worker process 28693 exited with status 127
2023-05-30 06:12:27,003 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:28,430 - distributed.nanny - INFO - Worker process 28704 exited with status 127
2023-05-30 06:12:28,431 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:28,479 - distributed.nanny - INFO - Worker process 28700 exited with status 127
2023-05-30 06:12:28,480 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:28,523 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-yh8ttj8v', purging
2023-05-30 06:12:28,524 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-j6fnjfjd', purging
2023-05-30 06:12:28,524 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hu81jaft', purging
2023-05-30 06:12:28,524 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ex6erbge', purging
2023-05-30 06:12:28,525 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-l8z0hqge', purging
2023-05-30 06:12:28,525 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-efk8vv4q', purging
2023-05-30 06:12:28,525 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2knuhz3c', purging
2023-05-30 06:12:28,526 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2w1lc7df', purging
2023-05-30 06:12:28,526 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:28,526 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:28,539 - distributed.nanny - INFO - Worker process 28708 exited with status 127
2023-05-30 06:12:28,540 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:28,563 - distributed.nanny - INFO - Worker process 28714 exited with status 127
2023-05-30 06:12:28,564 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:28,586 - distributed.nanny - INFO - Worker process 28696 exited with status 127
2023-05-30 06:12:28,587 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:28,612 - distributed.nanny - INFO - Worker process 28711 exited with status 127
2023-05-30 06:12:28,612 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:28,636 - distributed.nanny - INFO - Worker process 28717 exited with status 127
2023-05-30 06:12:28,637 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:28,650 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:29,009 - distributed.nanny - INFO - Worker process 28757 exited with status 127
2023-05-30 06:12:29,010 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:30,005 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3ku1suva', purging
2023-05-30 06:12:30,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:30,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:30,030 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:30,049 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:30,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:30,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:30,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:30,074 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:30,081 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:30,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:30,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:30,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:30,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:30,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:30,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:30,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:30,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:30,324 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:30,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:30,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:30,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:30,430 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:30,430 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:30,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:30,892 - distributed.nanny - INFO - Worker process 28775 exited with status 127
2023-05-30 06:12:30,893 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:32,085 - distributed.nanny - INFO - Worker process 28783 exited with status 127
2023-05-30 06:12:32,086 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:32,115 - distributed.nanny - INFO - Worker process 28787 exited with status 127
2023-05-30 06:12:32,115 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:32,175 - distributed.nanny - INFO - Worker process 28790 exited with status 127
2023-05-30 06:12:32,176 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:32,237 - distributed.nanny - INFO - Worker process 28796 exited with status 127
2023-05-30 06:12:32,237 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:32,262 - distributed.nanny - INFO - Worker process 28793 exited with status 127
2023-05-30 06:12:32,263 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:32,286 - distributed.nanny - INFO - Worker process 28799 exited with status 127
2023-05-30 06:12:32,287 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:32,378 - distributed.nanny - INFO - Worker process 28807 exited with status 127
2023-05-30 06:12:32,379 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:32,421 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gad0e9fj', purging
2023-05-30 06:12:32,422 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_0_sn0qn', purging
2023-05-30 06:12:32,422 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3eehfbf_', purging
2023-05-30 06:12:32,423 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a86utdfr', purging
2023-05-30 06:12:32,423 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-y1xosv_a', purging
2023-05-30 06:12:32,423 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-plw7yi6y', purging
2023-05-30 06:12:32,423 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-91ve3384', purging
2023-05-30 06:12:32,424 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dj6dd74i', purging
2023-05-30 06:12:32,424 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:32,424 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:32,449 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:32,863 - distributed.nanny - INFO - Worker process 28849 exited with status 127
2023-05-30 06:12:32,864 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:33,611 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-driitr_l', purging
2023-05-30 06:12:33,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:33,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:33,635 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:33,687 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:33,687 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:33,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:33,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:33,713 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:33,720 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:33,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:33,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:33,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:33,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:33,842 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:33,842 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:33,921 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:33,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:33,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:33,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:33,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:33,962 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:34,365 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-h_q27wsa', purging
2023-05-30 06:12:34,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:34,366 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:34,489 - distributed.nanny - INFO - Worker process 28865 exited with status 127
2023-05-30 06:12:34,490 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:34,508 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:35,545 - distributed.nanny - INFO - Worker process 28881 exited with status 127
2023-05-30 06:12:35,546 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:35,569 - distributed.nanny - INFO - Worker process 28869 exited with status 127
2023-05-30 06:12:35,570 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:35,759 - distributed.nanny - INFO - Worker process 28875 exited with status 127
2023-05-30 06:12:35,761 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:35,785 - distributed.nanny - INFO - Worker process 28884 exited with status 127
2023-05-30 06:12:35,786 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:35,834 - distributed.nanny - INFO - Worker process 28888 exited with status 127
2023-05-30 06:12:35,835 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:35,855 - distributed.nanny - INFO - Worker process 28878 exited with status 127
2023-05-30 06:12:35,856 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:35,974 - distributed.nanny - INFO - Worker process 28902 exited with status 127
2023-05-30 06:12:35,975 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:36,018 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u0fjgvc2', purging
2023-05-30 06:12:36,019 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-01gmof0o', purging
2023-05-30 06:12:36,019 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tot_rzyx', purging
2023-05-30 06:12:36,019 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kne8m6n3', purging
2023-05-30 06:12:36,020 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-6tl1_ofs', purging
2023-05-30 06:12:36,020 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cs7wi8ru', purging
2023-05-30 06:12:36,020 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hvn8r6ih', purging
2023-05-30 06:12:36,021 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:36,021 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:36,046 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:36,474 - distributed.nanny - INFO - Worker process 28937 exited with status 127
2023-05-30 06:12:36,475 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:37,044 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9eqdtce_', purging
2023-05-30 06:12:37,044 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:37,044 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:37,067 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:37,110 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:37,110 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:37,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:37,231 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:37,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:37,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:37,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:37,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:37,340 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:37,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:37,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:37,410 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:37,424 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:37,424 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:37,465 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:37,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:37,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:37,663 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:37,686 - distributed.nanny - INFO - Worker process 28957 exited with status 127
2023-05-30 06:12:37,687 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:38,006 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hdyuicup', purging
2023-05-30 06:12:38,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:38,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:38,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:38,801 - distributed.nanny - INFO - Worker process 28960 exited with status 127
2023-05-30 06:12:38,802 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:38,934 - distributed.nanny - INFO - Worker process 28965 exited with status 127
2023-05-30 06:12:38,935 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:38,959 - distributed.nanny - INFO - Worker process 28974 exited with status 127
2023-05-30 06:12:38,960 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:39,133 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gch05ij8', purging
2023-05-30 06:12:39,133 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k6b46atl', purging
2023-05-30 06:12:39,134 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bi0tetss', purging
2023-05-30 06:12:39,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:39,134 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:39,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:39,227 - distributed.nanny - INFO - Worker process 28968 exited with status 127
2023-05-30 06:12:39,228 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:39,287 - distributed.nanny - INFO - Worker process 28971 exited with status 127
2023-05-30 06:12:39,288 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:39,363 - distributed.nanny - INFO - Worker process 28978 exited with status 127
2023-05-30 06:12:39,364 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:12:39,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42811'. Reason: nanny-close
2023-05-30 06:12:39,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34153'. Reason: nanny-close
2023-05-30 06:12:39,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33941'. Reason: nanny-close
2023-05-30 06:12:39,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46031'. Reason: nanny-close
2023-05-30 06:12:39,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35823'. Reason: nanny-close
2023-05-30 06:12:39,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34483'. Reason: nanny-close
2023-05-30 06:12:39,379 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40479'. Reason: nanny-close
2023-05-30 06:12:39,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42865'. Reason: nanny-close
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:39,572 - distributed.nanny - INFO - Worker process 28993 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:39,710 - distributed.nanny - INFO - Worker process 29024 exited with status 127
2023-05-30 06:12:40,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1zoiuj78', purging
2023-05-30 06:12:40,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1h4asx_o', purging
2023-05-30 06:12:40,281 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-tspy0gql', purging
2023-05-30 06:12:40,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jq6u8kvg', purging
2023-05-30 06:12:40,282 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mq1byz4m', purging
2023-05-30 06:12:40,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:40,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:40,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:40,461 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:40,461 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:40,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:40,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:40,542 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:40,544 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:40,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:40,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:40,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:40,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:40,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:12:40,884 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:12:41,008 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:41,009 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:41,016 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-30 06:12:41,039 - distributed.nanny - INFO - Worker process 29042 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:41,549 - distributed.nanny - INFO - Worker process 29050 exited with status 127
2023-05-30 06:12:41,578 - distributed.nanny - INFO - Worker process 29047 exited with status 127
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:12:41,968 - distributed.nanny - INFO - Worker process 29063 exited with status 127
2023-05-30 06:12:42,012 - distributed.nanny - INFO - Worker process 29060 exited with status 127
2023-05-30 06:12:42,037 - distributed.nanny - INFO - Worker process 29067 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-30 06:13:11,189 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:13:11,193 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43899 instead
  warnings.warn(
2023-05-30 06:13:11,197 - distributed.scheduler - INFO - State start
2023-05-30 06:13:11,215 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:13:11,216 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:13:11,216 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:13:11,217 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:13:11,253 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44669'
2023-05-30 06:13:12,553 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kade4b6k', purging
2023-05-30 06:13:12,553 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-pveogipv', purging
2023-05-30 06:13:12,554 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bcxptkfv', purging
2023-05-30 06:13:12,554 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b5wpnglt', purging
2023-05-30 06:13:12,555 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3lwihjhr', purging
2023-05-30 06:13:12,555 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-sy1dofvi', purging
2023-05-30 06:13:12,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:12,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:12,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:13,169 - distributed.nanny - INFO - Worker process 29283 exited with status 127
2023-05-30 06:13:13,170 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:14,481 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-phlhcs_f', purging
2023-05-30 06:13:14,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:14,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:14,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:15,099 - distributed.nanny - INFO - Worker process 29293 exited with status 127
2023-05-30 06:13:15,100 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:16,412 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-xbza3bk3', purging
2023-05-30 06:13:16,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:16,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:16,666 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:17,032 - distributed.nanny - INFO - Worker process 29303 exited with status 127
2023-05-30 06:13:17,033 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:18,348 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p1yrz0zz', purging
2023-05-30 06:13:18,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:18,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:18,600 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:18,968 - distributed.nanny - INFO - Worker process 29313 exited with status 127
2023-05-30 06:13:18,969 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:19,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44669'. Reason: nanny-close
2023-05-30 06:13:20,283 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-se4o9c7c', purging
2023-05-30 06:13:20,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:20,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:20,539 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:20,930 - distributed.nanny - INFO - Worker process 29323 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-30 06:13:53,130 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:13:53,133 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40695 instead
  warnings.warn(
2023-05-30 06:13:53,137 - distributed.scheduler - INFO - State start
2023-05-30 06:13:53,155 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:13:53,156 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:13:53,156 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:13:53,157 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-30 06:13:53,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37965'
2023-05-30 06:13:54,522 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wqhf5_pn', purging
2023-05-30 06:13:54,523 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:54,523 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:54,772 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:55,133 - distributed.nanny - INFO - Worker process 29581 exited with status 127
2023-05-30 06:13:55,133 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:56,447 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-om4zld8r', purging
2023-05-30 06:13:56,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:56,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:56,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:57,073 - distributed.nanny - INFO - Worker process 29591 exited with status 127
2023-05-30 06:13:57,074 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:13:58,374 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-k4gb2v9u', purging
2023-05-30 06:13:58,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:13:58,375 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:13:58,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:13:58,992 - distributed.nanny - INFO - Worker process 29601 exited with status 127
2023-05-30 06:13:58,992 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:14:00,305 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kjl33zrj', purging
2023-05-30 06:14:00,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:14:00,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:14:00,558 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:14:00,941 - distributed.nanny - INFO - Worker process 29611 exited with status 127
2023-05-30 06:14:00,942 - distributed.nanny - WARNING - Restarting worker
2023-05-30 06:14:01,720 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37965'. Reason: nanny-close
2023-05-30 06:14:02,251 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-uk0v1tpw', purging
2023-05-30 06:14:02,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-30 06:14:02,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-30 06:14:02,506 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
/opt/conda/envs/gdf/bin/python3.9: symbol lookup error: /opt/conda/envs/gdf/lib/ucx/libuct_ib.so.0: undefined symbol: ibv_reg_dmabuf_mr
2023-05-30 06:14:02,873 - distributed.nanny - INFO - Worker process 29621 exited with status 127
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-30 06:14:33,448 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:14:33,452 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35347 instead
  warnings.warn(
2023-05-30 06:14:33,456 - distributed.scheduler - INFO - State start
2023-05-30 06:14:33,481 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-30 06:14:33,482 - distributed.scheduler - INFO - Scheduler closing...
2023-05-30 06:14:33,482 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-30 06:14:33,483 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
