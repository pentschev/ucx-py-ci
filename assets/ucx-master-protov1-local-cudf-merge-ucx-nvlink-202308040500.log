2023-08-04 06:24:25,333 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:25,333 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:25,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:25,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:25,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:25,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:25,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:25,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:25,401 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:25,401 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:25,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:25,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:25,442 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:25,442 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:25,460 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:25,460 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:70423:0:70423] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  70423) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fa539044ced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2aee4) [0x7fa539044ee4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2b0aa) [0x7fa5390450aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fa5dc898420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fa5390c46f4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fa5390eca49]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2045f) [0x7fa538ffe45f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23548) [0x7fa539001548]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fa53904e399]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fa53900065d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fa5390c152a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fa53917617a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55c57a9ecb08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55c57a9dd112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c57a9d627a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c57a9e7c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c57a9d781b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c57a9e7ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x55c57a9f5a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x55c57ab059b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55c57a993817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55c57a9def83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55c57a9dcd36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c57a9e7ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c57a9d781b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c57a9e7ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c57a9d781b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c57a9e7ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c57a9d781b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c57a9e7ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c57a9d781b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c57a9d627a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c57a9e7c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55c57a9dbfa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c57a9d627a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55c57a9f5935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55c57a9f6104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55c57aabcfc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55c57a9e02bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55c57a9db1bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c57a9e7ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55c57a9f5c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55c57a9db1bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c57a9e7ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c57a9d781b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c57a9d627a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c57a9e7c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55c57a9d781b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55c57a9e7ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55c57a9d7568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c57a9d627a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55c57a9e7c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55c57a9d83cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55c57a9d627a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55c57a9d5f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55c57a9d5eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55c57aa868bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55c57aab4adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55c57aab0c24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55c57aaa87ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55c57aaa86bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55c57aaa78a2]
=================================
[dgx13:70432:0:70432] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  70432) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fb657590ced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2aee4) [0x7fb657590ee4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2b0aa) [0x7fb6575910aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fb6fade8420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fb6576106f4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fb657638a49]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2045f) [0x7fb65754a45f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23548) [0x7fb65754d548]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fb65759a399]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fb65754c65d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fb65760d52a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fb6576c217a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55baf74e8b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55baf74d9112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55baf74d227a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55baf74e3c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55baf74d381b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55baf74e3ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x55baf74f1a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x55baf76019b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55baf748f817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55baf74daf83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55baf74d8d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55baf74e3ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55baf74d381b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55baf74e3ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55baf74d381b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55baf74e3ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55baf74d381b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55baf74e3ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55baf74d381b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55baf74d227a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55baf74e3c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55baf74d7fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55baf74d227a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55baf74f1935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55baf74f2104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55baf75b8fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55baf74dc2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55baf74d71bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55baf74e3ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55baf74f1c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55baf74d71bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55baf74e3ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55baf74d381b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55baf74d227a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55baf74e3c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55baf74d381b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55baf74e3ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55baf74d3568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55baf74d227a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55baf74e3c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55baf74d43cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55baf74d227a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55baf74d1f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55baf74d1eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55baf75828bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55baf75b0adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55baf75acc24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55baf75a47ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55baf75a46bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55baf75a38a2]
=================================
[dgx13:70415:0:70415] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  70415) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f257f5beced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2aee4) [0x7f257f5beee4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2b0aa) [0x7f257f5bf0aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f262ee16420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f257f63e6f4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f257f666a49]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2045f) [0x7f257f57845f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23548) [0x7f257f57b548]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f257f5c8399]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f257f57a65d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f257f63b52a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7f257f6f017a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5640eb42eb08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5640eb41f112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640eb41827a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5640eb429c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640eb41981b]
17  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5640eb43e70e]
18  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f25af9c62fe]
19  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5640eb4222bc]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5640eb3d5817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5640eb420f83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5640eb41ed36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640eb429ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640eb41981b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640eb429ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640eb41981b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640eb429ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640eb41981b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640eb429ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640eb41981b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640eb41827a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5640eb429c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5640eb41dfa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640eb41827a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5640eb437935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5640eb438104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5640eb4fefc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5640eb4222bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5640eb41d1bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640eb429ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5640eb437c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5640eb41d1bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640eb429ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640eb41981b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640eb41827a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5640eb429c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640eb41981b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640eb429ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5640eb419568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640eb41827a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5640eb429c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5640eb41a3cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640eb41827a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5640eb417f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5640eb417eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5640eb4c88bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5640eb4f6adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5640eb4f2c24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5640eb4ea7ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5640eb4ea6bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x5640eb4e98a2]
=================================
2023-08-04 06:24:34,357 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40933
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fac0ce301c0, tag: 0x9034d7eb1809e49a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fac0ce301c0, tag: 0x9034d7eb1809e49a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,357 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40933
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f0e91375100, tag: 0x533cb3d806cdf9cc, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f0e91375100, tag: 0x533cb3d806cdf9cc, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,357 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40933
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f8ef0079280, tag: 0x9250b54217e949ba, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f8ef0079280, tag: 0x9250b54217e949ba, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,357 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40933
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f1303771280, tag: 0x946b39e299296aba, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f1303771280, tag: 0x946b39e299296aba, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,360 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51475 -> ucx://127.0.0.1:40933
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f8ef0079340, tag: 0xfd3d16a35aa5de1f, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,363 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35167
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f1303771100, tag: 0x1f957407270c64fe, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f1303771100, tag: 0x1f957407270c64fe, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-08-04 06:24:34,363 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35167
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7fac0ce30240, tag: 0xe15baf5fc85b6c3b, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7fac0ce30240, tag: 0xe15baf5fc85b6c3b, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-04 06:24:34,363 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35167
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f0e91375240, tag: 0xe16cc42f9e98312a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f0e91375240, tag: 0xe16cc42f9e98312a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,363 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35167
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f8ef00791c0, tag: 0x4a13cf70df3146b3, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f8ef00791c0, tag: 0x4a13cf70df3146b3, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,363 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:43837 -> ucx://127.0.0.1:35167
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f13037713c0, tag: 0x3d1bd615e54929f1, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,378 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58403 -> ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fac0ce303c0, tag: 0x41ff8d01322586fb, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,379 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fac0ce30280, tag: 0x833a79aea04a0529, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fac0ce30280, tag: 0x833a79aea04a0529, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-08-04 06:24:34,389 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:43837 -> ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f1303771380, tag: 0xd75894cd53737bf, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,406 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f1303771180, tag: 0x54dab887c4528010, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f1303771180, tag: 0x54dab887c4528010, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,414 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f8ef0079180, tag: 0x27f024176ac023f0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f8ef0079180, tag: 0x27f024176ac023f0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-08-04 06:24:34,443 - distributed.nanny - WARNING - Restarting worker
2023-08-04 06:24:34,466 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 708, in recv
    self._ep.raise_on_error()
  File "ucp/_libs/ucx_endpoint.pyx", line 353, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXConnectionReset: Endpoint 0x7efc21cab100 error: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXConnectionReset('Endpoint 0x7efc21cab100 error: Connection reset by remote peer')
2023-08-04 06:24:34,467 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:54787 -> ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 289, in write
    raise CommClosedError("Endpoint is closed -- unable to send message")
distributed.comm.core.CommClosedError: Endpoint is closed -- unable to send message
2023-08-04 06:24:34,480 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f0e91375140, tag: 0x312f845292d1155a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f0e91375140, tag: 0x312f845292d1155a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-08-04 06:24:34,489 - distributed.nanny - WARNING - Restarting worker
2023-08-04 06:24:34,489 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:40933
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7efc21cab240, tag: 0x8e119dec81169034, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7efc21cab240, tag: 0x8e119dec81169034, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,490 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:35167
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7efc21cab140, tag: 0x21358b519e0edfa2, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7efc21cab140, tag: 0x21358b519e0edfa2, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
[dgx13:70426:0:70426] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  70426) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7efc21ef9ced]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2aee4) [0x7efc21ef9ee4]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2b0aa) [0x7efc21efa0aa]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7efcc778e420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7efc21f796f4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7efc21fa1a49]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2045f) [0x7efc2802d45f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23548) [0x7efc28030548]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7efc21f03399]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7efc2802f65d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7efc21f7652a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7efc2807417a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55a9ebc20b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55a9ebc11112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a9ebc0a27a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55a9ebc1bc05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a9ebc0b81b]
17  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55a9ebc3070e]
18  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7efc483392fe]
19  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55a9ebc142bc]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55a9ebbc7817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55a9ebc12f83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55a9ebc10d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a9ebc1bef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a9ebc0b81b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a9ebc1bef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a9ebc0b81b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a9ebc1bef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a9ebc0b81b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a9ebc1bef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a9ebc0b81b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a9ebc0a27a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55a9ebc1bc05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55a9ebc0ffa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a9ebc0a27a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55a9ebc29935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55a9ebc2a104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55a9ebcf0fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55a9ebc142bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55a9ebc0f1bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a9ebc1bef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55a9ebc29c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55a9ebc0f1bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a9ebc1bef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a9ebc0b81b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a9ebc0a27a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55a9ebc1bc05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a9ebc0b81b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a9ebc1bef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55a9ebc0b568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a9ebc0a27a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55a9ebc1bc05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55a9ebc0c3cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a9ebc0a27a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55a9ebc09f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55a9ebc09eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55a9ebcba8bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55a9ebce8adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55a9ebce4c24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55a9ebcdc7ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55a9ebcdc6bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55a9ebcdb8a2]
=================================
2023-08-04 06:24:34,533 - distributed.nanny - WARNING - Restarting worker
2023-08-04 06:24:34,745 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54787
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fac0ce30140, tag: 0x79e8d9d611d312fe, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fac0ce30140, tag: 0x79e8d9d611d312fe, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,746 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54787
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f8ef0079240, tag: 0x4c50773a6b93afb2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f8ef0079240, tag: 0x4c50773a6b93afb2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-08-04 06:24:34,746 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:43837 -> ucx://127.0.0.1:40933
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f1303771400, tag: 0x4158b386ffc58ff7, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,747 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58403 -> ucx://127.0.0.1:54787
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fac0ce30440, tag: 0x20181f39b0e622c3, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,747 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51475 -> ucx://127.0.0.1:54787
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f8ef0079380, tag: 0xf8f290df73b4974, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,747 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:43837 -> ucx://127.0.0.1:54787
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f1303771340, tag: 0xaa6b173e33914056, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,747 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58403 -> ucx://127.0.0.1:40933
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fac0ce30300, tag: 0xa81b1aa43c2d5eb5, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,747 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51475 -> ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #009] ep: 0x7f8ef00793c0, tag: 0xa22844ee79602ab1, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,747 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54787
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f1303771140, tag: 0x40e79d7926ab89d9, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f1303771140, tag: 0x40e79d7926ab89d9, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-08-04 06:24:34,747 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58403 -> ucx://127.0.0.1:35167
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fac0ce30380, tag: 0xef6dad2ac17c3daa, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,750 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:38967 -> ucx://127.0.0.1:54787
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f0e91375400, tag: 0xfad9e3e0502839f6, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,751 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54787
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f0e91375180, tag: 0xea700f0d2f2daaee, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f0e91375180, tag: 0xea700f0d2f2daaee, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-08-04 06:24:34,751 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:38967 -> ucx://127.0.0.1:57681
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f0e91375340, tag: 0xc3451be2cc8aeb94, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:34,827 - distributed.nanny - WARNING - Restarting worker
2023-08-04 06:24:36,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:36,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:36,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:36,075 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:36,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:36,075 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:36,101 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-08-04 06:24:36,102 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-08-04 06:24:36,105 - distributed.worker - ERROR - ('Unexpected response', {'op': 'get_data', 'keys': {"('split-simple-shuffle-054f65580a5efedc40c26858beb83d02', 5, 6)"}, 'who': 'ucx://127.0.0.1:43837', 'max_connections': None, 'reply': True})
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2902, in get_data_from_worker
    status = response["status"]
KeyError: 'status'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2904, in get_data_from_worker
    raise ValueError("Unexpected response", response)
ValueError: ('Unexpected response', {'op': 'get_data', 'keys': {"('split-simple-shuffle-054f65580a5efedc40c26858beb83d02', 5, 6)"}, 'who': 'ucx://127.0.0.1:43837', 'max_connections': None, 'reply': True})
2023-08-04 06:24:36,109 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 830, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 987, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1794, in get_data
    assert response == "OK", response
AssertionError: {'op': 'get_data', 'keys': {"('split-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 5, 3)"}, 'who': 'ucx://127.0.0.1:51475', 'max_connections': None, 'reply': True}
2023-08-04 06:24:36,193 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-054f65580a5efedc40c26858beb83d02', 1)
Function:  <dask.layers.CallableLazyImport object at 0x7f0934
args:      ([               key   payload
shuffle                     
0           632373  68429687
0           603291   7388916
0           610470  88556538
0           543264  54331897
0           656821  35600601
...            ...       ...
0        799992614  70107038
0        799941398  38005589
0        799960476  38548347
0        799948064  33581048
0        799856767  95763058

[12497508 rows x 2 columns],                key   payload
shuffle                     
1           360030  84518954
1           579721  67253464
1           183334   4338136
1           321427  20668557
1           223254   3461268
...            ...       ...
1        799936438  36701630
1        799921560  90418206
1        799898983  75593138
1        799873567  71707276
1        799927565  77439998

[12503907 rows x 2 columns],                key   payload
shuffle                     
2           485032  63437162
2           364577  92173763
2           529614  15053532
2           260318  28466280
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-04 06:24:36,216 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7fa6bc
args:      ([                key   payload
19041     848807118  39307915
19044     825085784    946287
19062     843827877  51532075
39585     840536170  12497167
19063     819321144  61171054
...             ...       ...
99957013  701170172  38730573
99997662  108789256  96722582
99957021  307826541  45164447
99957114  841366329  76009611
99957115    4191317  95400519

[12498632 rows x 2 columns],                 key   payload
61553      22854345  68674259
61558     317162876  63559145
122946    935495471  10304674
122950    965103569   2320415
20522     968385493  61786113
...             ...       ...
99937560  902986642  95185475
99999708  965781322  42567687
99999552  951445226   2460748
99999576  900686200  67663918
99999578  935501666  18754206

[12502237 rows x 2 columns],                  key   payload
2756      1015091735   4458392
2775       328357585  60864956
2781      1061319512   7101080
11400     1063809893  55002864
34861     1009945266  56670474
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-04 06:24:36,306 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:43837
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 359, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #084] ep: 0x7f8ef0079100, tag: 0xd1ce1bfe4fad0ef2, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #084] ep: 0x7f8ef0079100, tag: 0xd1ce1bfe4fad0ef2, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated")
2023-08-04 06:24:36,307 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:43837 -> ucx://127.0.0.1:51475
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 315, in write
    await self.ep.send(struct.pack("?Q", False, nframes))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 629, in send
    self._ep.raise_on_error()
  File "ucp/_libs/ucx_endpoint.pyx", line 353, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXConnectionReset: Endpoint 0x7f13037712c0 error: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-08-04 06:24:36,365 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-f1e9396f54451b9cde4495a9a287c08c', 0)
Function:  subgraph_callable-4b662b11-375d-470b-ac27-356d7fa6
args:      (               key   payload
shuffle                     
0           579608  15810101
0           350125  64561504
0           615536  86243568
0           529004   9208092
0           419133  31243327
...            ...       ...
7        799978192  79312350
7        799905886  81824050
7        799990127  40604807
7        799925872  48090401
7        799905526  70939076

[99999977 rows x 2 columns],                  key   payload
19055      104994826  49810746
19059      300687017  43868929
19071      869856326  73633342
39600      811154700  81461087
39605      864484935  77988816
...              ...       ...
99982596  1561698440  80340862
99982597    97371525  49062550
99982613  1537047758  56114754
99982615  1531305547  52144834
99982620  1505486820  32758658

[100005446 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-04 06:24:36,382 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-04 06:24:36,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-04 06:24:36,440 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 4)
Function:  <dask.layers.CallableLazyImport object at 0x7fa6bc
args:      ([                key   payload
19040     855657050  58258327
19046     830642909  86998203
19050     845429647  38113848
39586     803854154  67415714
19064       4554853  67386439
...             ...       ...
99957009  406164646  53650912
99957011  828098082  19076909
99957016  852556053  99026547
99957091  804452181  33206874
99957097  203093876  99108839

[12500589 rows x 2 columns],                 key   payload
61537     911771374  59056876
61538      18092565  18001366
61566     724924093  52540254
122945    613787940  34504259
122947    314273761  12196873
...             ...       ...
99999553  714304900  10496465
99999554  923025556  10656670
99999564  959545822  69217132
99999565  951687774  48103226
99999582  937284490  95321281

[12501715 rows x 2 columns],                  key   payload
2770      1042028145  23267582
2773      1012908881  25442108
2779       334562010  95516318
11393      228056959  83208818
34854      433195841  15146581
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-04 06:24:36,464 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-054f65580a5efedc40c26858beb83d02', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7f0db4
args:      ([               key   payload
shuffle                     
0           384913   7889426
0           397587  34313633
0           338786  86520648
0           742576  52229898
0           471612  19984130
...            ...       ...
0        799861972  83414045
0        799929156  21339810
0        799788751  65547869
0        799945572  56976462
0        799757858  80975452

[12502296 rows x 2 columns],                key   payload
shuffle                     
1           349701  26510019
1           644618  95368818
1           213548  22638478
1           408705  74228017
1           175111  55457659
...            ...       ...
1        799855618  79919165
1        799870587  22503062
1        799805957  73477666
1        799930296  90572510
1        799971110  41280619

[12499115 rows x 2 columns],                key   payload
shuffle                     
2           611386  59478278
2           349324  23525516
2           512042  82619194
2           245478  70630129
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-04 06:24:36,498 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-054f65580a5efedc40c26858beb83d02', 4)
Function:  <dask.layers.CallableLazyImport object at 0x7f0934
args:      ([               key   payload
shuffle                     
0           372578  48164344
0           505950  56029732
0           602719   2532811
0           330190  44679592
0           749156  52872767
...            ...       ...
0        799954335  95325808
0        799852642  91118295
0        799771715  77654795
0        799938864  41166128
0        799793835   4772200

[12503392 rows x 2 columns],                key   payload
shuffle                     
1           253431  42644085
1           622295  44546108
1           126509    513338
1            15647  32043263
1            47640  73050800
...            ...       ...
1        799855622  51689635
1        799977730  11869057
1        799949569  38984929
1        799771830  44365582
1        799924101  65123634

[12500389 rows x 2 columns],                key   payload
shuffle                     
2           536655  68231951
2           337413  99633435
2           592584  54424926
2           216310  37732160
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-04 06:24:36,570 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7fa6bc
args:      ([                key   payload
19042     828909264  92227906
19058     859714444  90106675
19061     838180985  49941454
39587     848182252   7096110
39590     835843362  89345503
...             ...       ...
99957098  709365136  51545501
99957101  826330237  13224060
99957103  839878634  48117770
99957107  405040930  62444894
99957119  826522731   4379513

[12497796 rows x 2 columns],                 key   payload
61539     906542223  42451310
61545     915898390  95640750
61557     900209360  89240504
122955    913757874  59467571
122956    622492447  68786285
...             ...       ...
99999710  919055398    199535
99937554   23437480  69675429
99999562  118320143  94492579
99999568  920716940  17436967
99999581  963339630  39564469

[12497151 rows x 2 columns],                  key   payload
2760      1027708581  78601151
2763        33000940  36515536
2771      1045863155  76699424
2774      1049876576  93037511
11394      336455332  66327578
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-04 06:24:36,922 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-054f65580a5efedc40c26858beb83d02', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7f0934
args:      ([               key   payload
shuffle                     
0           377321  21517031
0           512139  22818693
0           559104  44683001
0           626443  93223031
0           590061   3584461
...            ...       ...
0        799967145  52060777
0        799946205  12604334
0        799876757  37714171
0        799929182  39227337
0        799884341  63719683

[12498811 rows x 2 columns],                key   payload
shuffle                     
1           346725  32746484
1           376831  97191974
1            89697  97503356
1           387668  60176081
1           178911  55235796
...            ...       ...
1        799778970  59726804
1        799933815  33790303
1        799837188  40886978
1        799961992  18160188
1        799758057  56791048

[12498510 rows x 2 columns],                key   payload
shuffle                     
2           535796  74763300
2           296335  57700756
2           387951  36867042
2           226471  51808942
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-08-04 06:24:37,294 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-f1e9396f54451b9cde4495a9a287c08c', 7)
Function:  subgraph_callable-4b662b11-375d-470b-ac27-356d7fa6
args:      (               key   payload
shuffle                     
0           495186  66945416
0           498287  86537421
0           386408  38018410
0           465081  22677977
0           638420  94216137
...            ...       ...
7        799938087  81366591
7        799905873  80231813
7        799877887   5965093
7        799955170  24480387
7        799975993  18407073

[100001353 rows x 2 columns],                  key   payload
19045      817109905  47054274
19048      862764563  12723158
19049      821370830   7551365
39598      854336539  90625077
19051      860083132  14285297
...              ...       ...
99977086  1527670558  64395175
99982599  1545125373  49700676
99982609  1501302724  43829817
99982618  1513834320  28019213
99982619  1518449716  78696842

[99993358 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
