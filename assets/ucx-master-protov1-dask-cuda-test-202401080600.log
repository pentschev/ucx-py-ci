============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-08 06:26:19,592 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:19,597 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39703 instead
  warnings.warn(
2024-01-08 06:26:19,601 - distributed.scheduler - INFO - State start
2024-01-08 06:26:19,624 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:19,625 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-08 06:26:19,625 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39703/status
2024-01-08 06:26:19,626 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:26:19,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46859'
2024-01-08 06:26:19,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39897'
2024-01-08 06:26:19,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37931'
2024-01-08 06:26:19,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38045'
2024-01-08 06:26:19,776 - distributed.scheduler - INFO - Receive client connection: Client-d4b3f377-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:19,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32828
2024-01-08 06:26:21,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:21,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:21,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:21,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:21,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:21,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:21,489 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44035
2024-01-08 06:26:21,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44035
2024-01-08 06:26:21,489 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34827
2024-01-08 06:26:21,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34827
2024-01-08 06:26:21,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46481
2024-01-08 06:26:21,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:26:21,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39487
2024-01-08 06:26:21,489 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:26:21,489 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:26:21,489 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,489 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:26:21,489 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:26:21,489 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-do825qvk
2024-01-08 06:26:21,489 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:26:21,489 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-2r9zxvqq
2024-01-08 06:26:21,489 - distributed.worker - INFO - Starting Worker plugin PreImport-a1213d82-f4d1-453d-9c1a-37ccf5c7f683
2024-01-08 06:26:21,489 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-122e8429-dd41-40e2-ad2b-2aff5d3cd053
2024-01-08 06:26:21,489 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32431705-4c4c-43ab-84eb-c52808d9624e
2024-01-08 06:26:21,489 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b376ee0d-e125-43fc-9d12-f09e9b4e87ca
2024-01-08 06:26:21,490 - distributed.worker - INFO - Starting Worker plugin PreImport-100716a3-2408-4b9d-9c9f-ff304881deb1
2024-01-08 06:26:21,490 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe4debf0-5f2c-48c4-a1a4-5951824db7f7
2024-01-08 06:26:21,490 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,491 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,494 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:21,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:21,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:21,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35145
2024-01-08 06:26:21,499 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35145
2024-01-08 06:26:21,499 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37519
2024-01-08 06:26:21,499 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:26:21,499 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,499 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:26:21,499 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:26:21,499 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-fl0lomxs
2024-01-08 06:26:21,499 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ace17146-21c3-4b2d-b177-86c9a3538b17
2024-01-08 06:26:21,500 - distributed.worker - INFO - Starting Worker plugin PreImport-4325e4cf-0e1b-432e-bf9e-916d28c1de49
2024-01-08 06:26:21,500 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3429e7d4-4604-40c8-8356-f54b2a87bbb0
2024-01-08 06:26:21,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:21,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:21,500 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,504 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:21,505 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33323
2024-01-08 06:26:21,505 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33323
2024-01-08 06:26:21,505 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46193
2024-01-08 06:26:21,505 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:26:21,505 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,505 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:26:21,506 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:26:21,506 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-75u1i5u9
2024-01-08 06:26:21,506 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a11cbc05-c4e5-4492-acf4-9113e6f78ee8
2024-01-08 06:26:21,507 - distributed.worker - INFO - Starting Worker plugin PreImport-bcd27f01-875b-41ef-8afc-c4f1bf5d8de6
2024-01-08 06:26:21,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-588de5ca-565b-4ae5-9ddd-65168d02b3c0
2024-01-08 06:26:21,508 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,610 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44035', status: init, memory: 0, processing: 0>
2024-01-08 06:26:21,611 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44035
2024-01-08 06:26:21,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38422
2024-01-08 06:26:21,613 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:21,613 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:26:21,613 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:26:21,617 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34827', status: init, memory: 0, processing: 0>
2024-01-08 06:26:21,618 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34827
2024-01-08 06:26:21,618 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38418
2024-01-08 06:26:21,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:21,619 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:26:21,619 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,620 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:26:21,629 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35145', status: init, memory: 0, processing: 0>
2024-01-08 06:26:21,630 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35145
2024-01-08 06:26:21,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38428
2024-01-08 06:26:21,631 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:21,632 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:26:21,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:26:21,633 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33323', status: init, memory: 0, processing: 0>
2024-01-08 06:26:21,633 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33323
2024-01-08 06:26:21,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38442
2024-01-08 06:26:21,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:21,635 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:26:21,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:21,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:26:21,733 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:26:21,733 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:26:21,734 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:26:21,734 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:26:21,739 - distributed.scheduler - INFO - Remove client Client-d4b3f377-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:21,739 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32828; closing.
2024-01-08 06:26:21,739 - distributed.scheduler - INFO - Remove client Client-d4b3f377-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:21,739 - distributed.scheduler - INFO - Close client connection: Client-d4b3f377-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:21,740 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46859'. Reason: nanny-close
2024-01-08 06:26:21,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:21,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39897'. Reason: nanny-close
2024-01-08 06:26:21,742 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:21,742 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37931'. Reason: nanny-close
2024-01-08 06:26:21,742 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34827. Reason: nanny-close
2024-01-08 06:26:21,742 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:21,743 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38045'. Reason: nanny-close
2024-01-08 06:26:21,743 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33323. Reason: nanny-close
2024-01-08 06:26:21,743 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:21,743 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44035. Reason: nanny-close
2024-01-08 06:26:21,744 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35145. Reason: nanny-close
2024-01-08 06:26:21,744 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:26:21,745 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:26:21,745 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:26:21,746 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:26:21,746 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:21,746 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:21,746 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38418; closing.
2024-01-08 06:26:21,747 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:21,747 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:21,747 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695181.7477999')
2024-01-08 06:26:21,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38442; closing.
2024-01-08 06:26:21,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38422; closing.
2024-01-08 06:26:21,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38428; closing.
2024-01-08 06:26:21,750 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33323', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695181.7502134')
2024-01-08 06:26:21,750 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44035', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695181.750549')
2024-01-08 06:26:21,751 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35145', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695181.750971')
2024-01-08 06:26:21,751 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:22,456 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:22,456 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:22,457 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:22,458 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-08 06:26:22,459 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-08 06:26:24,635 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:24,640 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45741 instead
  warnings.warn(
2024-01-08 06:26:24,644 - distributed.scheduler - INFO - State start
2024-01-08 06:26:24,665 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:24,666 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:26:24,667 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45741/status
2024-01-08 06:26:24,667 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:26:24,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43925'
2024-01-08 06:26:24,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39535'
2024-01-08 06:26:24,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34163'
2024-01-08 06:26:24,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43825'
2024-01-08 06:26:24,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33177'
2024-01-08 06:26:24,817 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43705'
2024-01-08 06:26:24,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46577'
2024-01-08 06:26:24,834 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38487'
2024-01-08 06:26:26,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,587 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,588 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39293
2024-01-08 06:26:26,588 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39293
2024-01-08 06:26:26,588 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35357
2024-01-08 06:26:26,588 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:26,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,588 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,588 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:26,588 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dbei6hx8
2024-01-08 06:26:26,589 - distributed.worker - INFO - Starting Worker plugin PreImport-ec7001db-46fc-4b15-9bd1-ac85612c21d6
2024-01-08 06:26:26,589 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c9f2104-9ca3-45f7-84bc-8ef93b260253
2024-01-08 06:26:26,589 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8da150ff-7f72-4837-b406-08a83bd7ef1a
2024-01-08 06:26:26,621 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44833
2024-01-08 06:26:26,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44833
2024-01-08 06:26:26,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46097
2024-01-08 06:26:26,627 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:26,627 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,627 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,627 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:26,627 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q5vk60v6
2024-01-08 06:26:26,627 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62e9f261-712e-4d5f-ae21-6bd80bc8da51
2024-01-08 06:26:26,827 - distributed.scheduler - INFO - Receive client connection: Client-d7bb2af6-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:26,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52124
2024-01-08 06:26:26,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,859 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,861 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,864 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,865 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,865 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,865 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,865 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,865 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,865 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,866 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36735
2024-01-08 06:26:26,866 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36735
2024-01-08 06:26:26,866 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45129
2024-01-08 06:26:26,866 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:26,866 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,866 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,866 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:26,866 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k702d3j8
2024-01-08 06:26:26,866 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35823
2024-01-08 06:26:26,867 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35823
2024-01-08 06:26:26,867 - distributed.worker - INFO - Starting Worker plugin PreImport-1fd65d25-297b-4e97-a541-e1503fcd9cc5
2024-01-08 06:26:26,867 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35225
2024-01-08 06:26:26,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-854a2a91-3af8-4074-81d4-04d3a825b1e4
2024-01-08 06:26:26,867 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:26,867 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,867 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,867 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:26,867 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3ip4x9dm
2024-01-08 06:26:26,867 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13179670-4681-421b-bec4-79050f19e874
2024-01-08 06:26:26,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-189ff6ea-07c1-4c22-b8d9-f01cbc45256c
2024-01-08 06:26:26,867 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,868 - distributed.worker - INFO - Starting Worker plugin PreImport-e9f06654-2067-4075-a738-141c9b2dba68
2024-01-08 06:26:26,868 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f7323d0-8954-4491-affa-535df95ecfc0
2024-01-08 06:26:26,869 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44405
2024-01-08 06:26:26,869 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44405
2024-01-08 06:26:26,869 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43811
2024-01-08 06:26:26,869 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:26,869 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,869 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,869 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:26,869 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-30kx7spw
2024-01-08 06:26:26,870 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86352c09-454b-412d-9b4e-ade92ed591e9
2024-01-08 06:26:26,870 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,870 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,870 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,871 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41089
2024-01-08 06:26:26,871 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41089
2024-01-08 06:26:26,871 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35267
2024-01-08 06:26:26,871 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39065
2024-01-08 06:26:26,871 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:26,871 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44095
2024-01-08 06:26:26,871 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,871 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39065
2024-01-08 06:26:26,871 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,871 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44095
2024-01-08 06:26:26,871 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42837
2024-01-08 06:26:26,871 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:26,871 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37777
2024-01-08 06:26:26,871 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:26,871 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n850hs51
2024-01-08 06:26:26,871 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:26,871 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,872 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,872 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,872 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,872 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:26,872 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:26,872 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nwh8par4
2024-01-08 06:26:26,872 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9iot65r0
2024-01-08 06:26:26,872 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ad6b545-5b6d-45ff-8021-0d71a160b591
2024-01-08 06:26:26,872 - distributed.worker - INFO - Starting Worker plugin RMMSetup-22b0014c-8068-4db6-94e2-8ea4083c9340
2024-01-08 06:26:26,872 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56da60f5-ae44-41f4-8280-65e5f9ff391f
2024-01-08 06:26:27,220 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:27,245 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39293', status: init, memory: 0, processing: 0>
2024-01-08 06:26:27,246 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39293
2024-01-08 06:26:27,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52148
2024-01-08 06:26:27,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:27,248 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:27,248 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:27,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:27,705 - distributed.scheduler - INFO - Receive client connection: Client-da9d83c8-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:27,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52154
2024-01-08 06:26:28,559 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b94dade2-5314-49ff-947c-f48586e13468
2024-01-08 06:26:28,560 - distributed.worker - INFO - Starting Worker plugin PreImport-c9d18184-758f-40f0-8c8f-bad6046b3c4e
2024-01-08 06:26:28,561 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,592 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44833', status: init, memory: 0, processing: 0>
2024-01-08 06:26:28,593 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44833
2024-01-08 06:26:28,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52182
2024-01-08 06:26:28,594 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:28,595 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:28,596 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:28,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3bbd28f-6478-474f-b7f8-8be2b15f4058
2024-01-08 06:26:28,746 - distributed.worker - INFO - Starting Worker plugin PreImport-bd75bcad-4e83-4115-9995-5b6352b7ac1b
2024-01-08 06:26:28,747 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,765 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5642bf76-7938-4d03-a7b1-2cf3fd1bdada
2024-01-08 06:26:28,765 - distributed.worker - INFO - Starting Worker plugin PreImport-ccbfbe50-80ba-4904-9da3-fd51d25e874f
2024-01-08 06:26:28,768 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,772 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca56b38e-88c6-4e28-9ca5-f4a818040c29
2024-01-08 06:26:28,773 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39065', status: init, memory: 0, processing: 0>
2024-01-08 06:26:28,773 - distributed.worker - INFO - Starting Worker plugin PreImport-56de2969-bbc9-4896-8861-c98755f44542
2024-01-08 06:26:28,774 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,774 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39065
2024-01-08 06:26:28,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52194
2024-01-08 06:26:28,775 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:28,776 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:28,776 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,777 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-069de2c1-f885-4ebf-87ba-61fcc5916fcb
2024-01-08 06:26:28,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:28,778 - distributed.worker - INFO - Starting Worker plugin PreImport-3abe8e58-8d05-4ffd-95ac-6c43a0485e77
2024-01-08 06:26:28,779 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,789 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,796 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,801 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44405', status: init, memory: 0, processing: 0>
2024-01-08 06:26:28,802 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44405
2024-01-08 06:26:28,802 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52206
2024-01-08 06:26:28,803 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:28,804 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:28,804 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:28,810 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44095', status: init, memory: 0, processing: 0>
2024-01-08 06:26:28,811 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44095
2024-01-08 06:26:28,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52204
2024-01-08 06:26:28,812 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:28,814 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:28,814 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:28,825 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41089', status: init, memory: 0, processing: 0>
2024-01-08 06:26:28,826 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41089
2024-01-08 06:26:28,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52220
2024-01-08 06:26:28,828 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:28,830 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:28,830 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,831 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36735', status: init, memory: 0, processing: 0>
2024-01-08 06:26:28,831 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36735
2024-01-08 06:26:28,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52230
2024-01-08 06:26:28,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:28,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:28,834 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:28,834 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:28,837 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35823', status: init, memory: 0, processing: 0>
2024-01-08 06:26:28,838 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35823
2024-01-08 06:26:28,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52240
2024-01-08 06:26:28,839 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:28,840 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:28,840 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:28,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:28,907 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,907 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,907 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,907 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,908 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,908 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,908 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,908 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,910 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,911 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,911 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,911 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,911 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,911 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,911 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,911 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:28,916 - distributed.scheduler - INFO - Remove client Client-d7bb2af6-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:28,917 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52124; closing.
2024-01-08 06:26:28,917 - distributed.scheduler - INFO - Remove client Client-d7bb2af6-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:28,917 - distributed.scheduler - INFO - Close client connection: Client-d7bb2af6-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:28,918 - distributed.scheduler - INFO - Remove client Client-da9d83c8-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:28,918 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52154; closing.
2024-01-08 06:26:28,918 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43925'. Reason: nanny-close
2024-01-08 06:26:28,919 - distributed.scheduler - INFO - Remove client Client-da9d83c8-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:28,919 - distributed.scheduler - INFO - Close client connection: Client-da9d83c8-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:28,919 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:28,919 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39535'. Reason: nanny-close
2024-01-08 06:26:28,920 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:28,920 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34163'. Reason: nanny-close
2024-01-08 06:26:28,921 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:28,921 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35823. Reason: nanny-close
2024-01-08 06:26:28,921 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43825'. Reason: nanny-close
2024-01-08 06:26:28,921 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:28,921 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36735. Reason: nanny-close
2024-01-08 06:26:28,921 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33177'. Reason: nanny-close
2024-01-08 06:26:28,921 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39293. Reason: nanny-close
2024-01-08 06:26:28,922 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:28,922 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43705'. Reason: nanny-close
2024-01-08 06:26:28,922 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44833. Reason: nanny-close
2024-01-08 06:26:28,922 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:28,922 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46577'. Reason: nanny-close
2024-01-08 06:26:28,922 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44095. Reason: nanny-close
2024-01-08 06:26:28,922 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:28,923 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38487'. Reason: nanny-close
2024-01-08 06:26:28,923 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:28,923 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41089. Reason: nanny-close
2024-01-08 06:26:28,923 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39065. Reason: nanny-close
2024-01-08 06:26:28,923 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:28,924 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52240; closing.
2024-01-08 06:26:28,924 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:28,924 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:28,924 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:28,924 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35823', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695188.9246483')
2024-01-08 06:26:28,924 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44405. Reason: nanny-close
2024-01-08 06:26:28,925 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52148; closing.
2024-01-08 06:26:28,925 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:28,925 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52230; closing.
2024-01-08 06:26:28,925 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:28,925 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:28,925 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:28,926 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:28,926 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:28,926 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39293', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695188.926244')
2024-01-08 06:26:28,926 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36735', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695188.926713')
2024-01-08 06:26:28,927 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:28,927 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:28,927 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52182; closing.
2024-01-08 06:26:28,927 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:28,927 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:28,928 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52204; closing.
2024-01-08 06:26:28,928 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:28,928 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695188.9285438')
2024-01-08 06:26:28,929 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:28,929 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52230>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:26:28,931 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52148>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:26:28,932 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695188.9321485')
2024-01-08 06:26:28,932 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52194; closing.
2024-01-08 06:26:28,932 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52220; closing.
2024-01-08 06:26:28,932 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52206; closing.
2024-01-08 06:26:28,933 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39065', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695188.9332662')
2024-01-08 06:26:28,933 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41089', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695188.9336479')
2024-01-08 06:26:28,934 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44405', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695188.9340706')
2024-01-08 06:26:28,934 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:29,807 - distributed.scheduler - INFO - Receive client connection: Client-dbe3a75b-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:29,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52256
2024-01-08 06:26:29,985 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:29,985 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:29,986 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:29,988 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:26:29,988 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-08 06:26:32,285 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:32,289 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40687 instead
  warnings.warn(
2024-01-08 06:26:32,294 - distributed.scheduler - INFO - State start
2024-01-08 06:26:32,317 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:32,318 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-08 06:26:32,318 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:32,319 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-08 06:26:32,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38839'
2024-01-08 06:26:32,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42255'
2024-01-08 06:26:32,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44917'
2024-01-08 06:26:32,631 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43155'
2024-01-08 06:26:32,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42751'
2024-01-08 06:26:32,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33561'
2024-01-08 06:26:32,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39501'
2024-01-08 06:26:32,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46375'
2024-01-08 06:26:34,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:34,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:34,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:34,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:34,485 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:34,485 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:34,486 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44969
2024-01-08 06:26:34,486 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40973
2024-01-08 06:26:34,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44969
2024-01-08 06:26:34,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40973
2024-01-08 06:26:34,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42137
2024-01-08 06:26:34,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34951
2024-01-08 06:26:34,486 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,486 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,486 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,486 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,486 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:34,486 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:34,486 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:34,486 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:34,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_7hryk9x
2024-01-08 06:26:34,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4sj39fgw
2024-01-08 06:26:34,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6999d71a-7a38-4ae4-8bcd-b89ffd2d4fc0
2024-01-08 06:26:34,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-985f9ad7-5727-445a-a9f8-ae2a4d96d761
2024-01-08 06:26:34,487 - distributed.worker - INFO - Starting Worker plugin PreImport-7d26d40a-ec0d-4250-94fa-978998e4b630
2024-01-08 06:26:34,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a173fce5-4514-4281-baff-cba871cbb148
2024-01-08 06:26:34,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:34,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:34,523 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:34,524 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45333
2024-01-08 06:26:34,524 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45333
2024-01-08 06:26:34,524 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44023
2024-01-08 06:26:34,524 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,524 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,524 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:34,524 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:34,524 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8y2j6fg3
2024-01-08 06:26:34,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3118518a-4f3f-46a8-91d1-b96d2a617882
2024-01-08 06:26:34,528 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:34,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:34,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:34,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35605
2024-01-08 06:26:34,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35605
2024-01-08 06:26:34,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46455
2024-01-08 06:26:34,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,534 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,534 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:34,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:34,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a1q040rb
2024-01-08 06:26:34,535 - distributed.worker - INFO - Starting Worker plugin PreImport-b06358bf-1d21-467b-834d-fa96e96debd1
2024-01-08 06:26:34,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-beb748dc-5d79-4ded-a7ea-30e4146745f2
2024-01-08 06:26:34,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ffa4fce-3d26-45cb-ad04-83d75260089d
2024-01-08 06:26:34,555 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:34,555 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:34,559 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:34,560 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38913
2024-01-08 06:26:34,560 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38913
2024-01-08 06:26:34,560 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40101
2024-01-08 06:26:34,560 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,560 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,560 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:34,560 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:34,560 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rv50kqzw
2024-01-08 06:26:34,560 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2521e2b3-79b6-4f39-b848-f7e7234e934a
2024-01-08 06:26:34,596 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:34,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:34,601 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:34,602 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37275
2024-01-08 06:26:34,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37275
2024-01-08 06:26:34,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46377
2024-01-08 06:26:34,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,602 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,602 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:34,603 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:34,603 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n8831awv
2024-01-08 06:26:34,603 - distributed.worker - INFO - Starting Worker plugin PreImport-58d06e50-a584-4fd7-b848-7cb002dc3434
2024-01-08 06:26:34,603 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82c94c1b-8cd9-4375-9ae8-aafaf22fe5f3
2024-01-08 06:26:34,603 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49c1f9b7-c6a7-412e-bde0-38692be83279
2024-01-08 06:26:34,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:34,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:34,613 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:34,614 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41981
2024-01-08 06:26:34,614 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41981
2024-01-08 06:26:34,614 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39389
2024-01-08 06:26:34,614 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,614 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,614 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:34,614 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:34,614 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v_lj6urb
2024-01-08 06:26:34,615 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d7c09a9c-fbb7-46e2-a242-c05e32d1b811
2024-01-08 06:26:34,676 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:34,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:34,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:34,684 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35349
2024-01-08 06:26:34,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35349
2024-01-08 06:26:34,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39739
2024-01-08 06:26:34,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,684 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,684 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:34,684 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:34,684 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iklme3gg
2024-01-08 06:26:34,684 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b30ee951-d38b-45f9-b110-b1e28ba5d267
2024-01-08 06:26:36,512 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55416293-420d-4280-b261-ff68bfa8044d
2024-01-08 06:26:36,516 - distributed.worker - INFO - Starting Worker plugin PreImport-deb11100-33a4-41cf-80fb-36dcc9d8bf96
2024-01-08 06:26:36,517 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:36,523 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:36,646 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5dd016f-3f61-4b9c-a28f-c16339957b62
2024-01-08 06:26:36,646 - distributed.worker - INFO - Starting Worker plugin PreImport-91c93d3f-8690-4a0a-a207-ac844f00a572
2024-01-08 06:26:36,647 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:36,648 - distributed.worker - INFO - Starting Worker plugin PreImport-9a4c2702-fb96-4773-9d5f-413f4cd912d4
2024-01-08 06:26:36,650 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-851aa18a-6421-4236-b9d7-b4f8c78408b9
2024-01-08 06:26:36,650 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:36,656 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:36,728 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:36,733 - distributed.worker - INFO - Starting Worker plugin PreImport-710324a4-b063-4098-8ec1-d1b03650822f
2024-01-08 06:26:36,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-51daef64-f3cf-4ceb-8170-cb34d57369fb
2024-01-08 06:26:36,736 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:36,748 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0f51090-3868-465e-a9e1-592f7d123891
2024-01-08 06:26:36,749 - distributed.worker - INFO - Starting Worker plugin PreImport-a4af6130-f92e-4011-b470-7d4e7098ca45
2024-01-08 06:26:36,750 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:37,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:37,551 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:37,551 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:37,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:37,587 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44917'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,587 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,589 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45333. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,590 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:37,592 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:37,616 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:37,617 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:37,617 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:37,619 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:37,630 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33561'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,631 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,631 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35349. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,634 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:37,636 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:37,758 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:37,759 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:37,759 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:37,760 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:37,807 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46375'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,808 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,809 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41981. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-08 06:26:37,811 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:37,812 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:58222 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-08 06:26:37,889 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64994 parent=64802 started daemon>
2024-01-08 06:26:37,890 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64990 parent=64802 started daemon>
2024-01-08 06:26:37,890 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64986 parent=64802 started daemon>
2024-01-08 06:26:37,890 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64983 parent=64802 started daemon>
2024-01-08 06:26:37,890 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64980 parent=64802 started daemon>
2024-01-08 06:26:37,891 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64971 parent=64802 started daemon>
2024-01-08 06:26:37,891 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64968 parent=64802 started daemon>
2024-01-08 06:26:38,025 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 64971 exit status was already read will report exitcode 255
2024-01-08 06:26:38,137 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 64968 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-08 06:27:12,515 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:12,520 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36543 instead
  warnings.warn(
2024-01-08 06:27:12,524 - distributed.scheduler - INFO - State start
2024-01-08 06:27:12,526 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-n8831awv', purging
2024-01-08 06:27:12,527 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4sj39fgw', purging
2024-01-08 06:27:12,527 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_7hryk9x', purging
2024-01-08 06:27:12,527 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-a1q040rb', purging
2024-01-08 06:27:12,528 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rv50kqzw', purging
2024-01-08 06:27:12,550 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:12,551 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:27:12,551 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36543/status
2024-01-08 06:27:12,552 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:27:12,558 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41445'
2024-01-08 06:27:12,574 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46629'
2024-01-08 06:27:12,590 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36673'
2024-01-08 06:27:12,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37407'
2024-01-08 06:27:12,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40293'
2024-01-08 06:27:12,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36345'
2024-01-08 06:27:12,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37871'
2024-01-08 06:27:12,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46167'
2024-01-08 06:27:12,694 - distributed.scheduler - INFO - Receive client connection: Client-f426094c-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:12,711 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52396
2024-01-08 06:27:14,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:14,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:14,485 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:14,486 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35335
2024-01-08 06:27:14,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35335
2024-01-08 06:27:14,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46789
2024-01-08 06:27:14,486 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:14,486 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:14,486 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:14,487 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:14,487 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vz1gkigy
2024-01-08 06:27:14,487 - distributed.worker - INFO - Starting Worker plugin PreImport-66892bb4-47ef-44b4-a958-f51a201cf155
2024-01-08 06:27:14,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de6bd915-cf84-4dd9-b180-95de4fe1b8c8
2024-01-08 06:27:14,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-47591760-18d7-4b1d-8a46-7bc15ff9b759
2024-01-08 06:27:14,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:14,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:14,497 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:14,498 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46635
2024-01-08 06:27:14,498 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46635
2024-01-08 06:27:14,498 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38897
2024-01-08 06:27:14,498 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:14,498 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:14,498 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:14,498 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:14,499 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1wwv900l
2024-01-08 06:27:14,499 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02d5f384-d3b2-40fd-b172-e3018436c2ad
2024-01-08 06:27:14,504 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:14,505 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:14,509 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:14,510 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43499
2024-01-08 06:27:14,510 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43499
2024-01-08 06:27:14,510 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42521
2024-01-08 06:27:14,510 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:14,510 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:14,510 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:14,510 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:14,510 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dztcb_fi
2024-01-08 06:27:14,511 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68dd884d-2fdd-48a5-b5e2-085debc54d31
2024-01-08 06:27:14,529 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:14,529 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:14,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:14,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:14,532 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:14,532 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:14,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:14,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33761
2024-01-08 06:27:14,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33761
2024-01-08 06:27:14,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38939
2024-01-08 06:27:14,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:14,534 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:14,534 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:14,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:14,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-izponz0w
2024-01-08 06:27:14,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:14,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26e93fc0-ab65-4e09-bc6a-03359453af1f
2024-01-08 06:27:14,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34137
2024-01-08 06:27:14,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34137
2024-01-08 06:27:14,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40365
2024-01-08 06:27:14,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:14,536 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:14,536 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:14,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:14,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wokcky0x
2024-01-08 06:27:14,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d63bc438-326e-4782-855d-a5754979de66
2024-01-08 06:27:14,537 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:14,538 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40517
2024-01-08 06:27:14,538 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40517
2024-01-08 06:27:14,538 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38709
2024-01-08 06:27:14,538 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:14,538 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:14,538 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:14,538 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:14,538 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7s2wlaex
2024-01-08 06:27:14,538 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e5b7c2f-6835-4d54-957d-1243052f0c6a
2024-01-08 06:27:14,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:14,549 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:14,553 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:14,554 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43345
2024-01-08 06:27:14,554 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43345
2024-01-08 06:27:14,554 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41845
2024-01-08 06:27:14,554 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:14,554 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:14,554 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:14,554 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:14,554 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_gqpk_u2
2024-01-08 06:27:14,555 - distributed.worker - INFO - Starting Worker plugin RMMSetup-533c4df7-7b42-4914-9df0-167549e2bb70
2024-01-08 06:27:14,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:14,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:14,617 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:14,618 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36285
2024-01-08 06:27:14,618 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36285
2024-01-08 06:27:14,619 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35305
2024-01-08 06:27:14,619 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:14,619 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:14,619 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:14,619 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:14,619 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ojc0lj91
2024-01-08 06:27:14,620 - distributed.worker - INFO - Starting Worker plugin PreImport-36106792-6773-4262-95ab-e614c2a97627
2024-01-08 06:27:14,620 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62b11db8-cd75-4447-82d4-fa3158e82d60
2024-01-08 06:27:14,620 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91db6c37-bab0-436c-81e6-04691ed7d4f3
2024-01-08 06:27:16,658 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,692 - distributed.worker - INFO - Starting Worker plugin PreImport-c8ca6b90-eaaa-40cd-934c-8737399b42ab
2024-01-08 06:27:16,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d063b10-0c19-4d11-9caf-82e984daeb54
2024-01-08 06:27:16,695 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35335', status: init, memory: 0, processing: 0>
2024-01-08 06:27:16,697 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35335
2024-01-08 06:27:16,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52412
2024-01-08 06:27:16,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:16,700 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:16,700 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,702 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:16,719 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75f6cbb5-5cdb-4cb2-8efc-7662e1119daf
2024-01-08 06:27:16,721 - distributed.worker - INFO - Starting Worker plugin PreImport-a1875eb1-bae9-40ca-b440-7941a3f47dbb
2024-01-08 06:27:16,721 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,731 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46635', status: init, memory: 0, processing: 0>
2024-01-08 06:27:16,732 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46635
2024-01-08 06:27:16,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52428
2024-01-08 06:27:16,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:16,734 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86022e56-79f4-4bbf-b708-6de9f923faea
2024-01-08 06:27:16,734 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:16,734 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,735 - distributed.worker - INFO - Starting Worker plugin PreImport-bf30a331-b09c-4000-85cc-405a9cff6eb7
2024-01-08 06:27:16,736 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,737 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:16,749 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d3e6355a-6790-4fdb-8b43-c446bb26a896
2024-01-08 06:27:16,750 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34137', status: init, memory: 0, processing: 0>
2024-01-08 06:27:16,750 - distributed.worker - INFO - Starting Worker plugin PreImport-20b50100-228a-411f-85b3-435476d49fd6
2024-01-08 06:27:16,750 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34137
2024-01-08 06:27:16,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52440
2024-01-08 06:27:16,751 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:16,752 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:16,752 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:16,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b11e07f-4e43-45c9-8929-5f68c5d81f67
2024-01-08 06:27:16,759 - distributed.worker - INFO - Starting Worker plugin PreImport-3abe9817-2fe6-400c-9450-28c8324d2644
2024-01-08 06:27:16,760 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,773 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43499', status: init, memory: 0, processing: 0>
2024-01-08 06:27:16,773 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43499
2024-01-08 06:27:16,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52444
2024-01-08 06:27:16,774 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,775 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:16,776 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:16,776 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:16,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a48ca17e-d287-4c50-aa7a-cd06e65fe377
2024-01-08 06:27:16,781 - distributed.worker - INFO - Starting Worker plugin PreImport-b86f4d25-d39e-4dae-8375-63a50a0ad030
2024-01-08 06:27:16,782 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,787 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33761', status: init, memory: 0, processing: 0>
2024-01-08 06:27:16,787 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33761
2024-01-08 06:27:16,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52460
2024-01-08 06:27:16,788 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43345', status: init, memory: 0, processing: 0>
2024-01-08 06:27:16,789 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43345
2024-01-08 06:27:16,789 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52464
2024-01-08 06:27:16,789 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:16,790 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:16,790 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:16,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,790 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:16,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,792 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:16,792 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:16,799 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36285', status: init, memory: 0, processing: 0>
2024-01-08 06:27:16,800 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36285
2024-01-08 06:27:16,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52470
2024-01-08 06:27:16,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:16,802 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:16,802 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:16,812 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40517', status: init, memory: 0, processing: 0>
2024-01-08 06:27:16,813 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40517
2024-01-08 06:27:16,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52472
2024-01-08 06:27:16,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:16,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:16,815 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:16,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:16,913 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:16,913 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:16,913 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:16,913 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:16,913 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:16,913 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:16,914 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:16,914 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:16,925 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:16,925 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:16,925 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:16,925 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:16,925 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:16,925 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:16,925 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:16,926 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:16,934 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:16,935 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:16,938 - distributed.scheduler - INFO - Remove client Client-f426094c-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:16,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52396; closing.
2024-01-08 06:27:16,938 - distributed.scheduler - INFO - Remove client Client-f426094c-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:16,938 - distributed.scheduler - INFO - Close client connection: Client-f426094c-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:16,939 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46629'. Reason: nanny-close
2024-01-08 06:27:16,940 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:16,940 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36673'. Reason: nanny-close
2024-01-08 06:27:16,941 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:16,941 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46167'. Reason: nanny-close
2024-01-08 06:27:16,942 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46635. Reason: nanny-close
2024-01-08 06:27:16,942 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:16,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37407'. Reason: nanny-close
2024-01-08 06:27:16,942 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35335. Reason: nanny-close
2024-01-08 06:27:16,942 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:16,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40293'. Reason: nanny-close
2024-01-08 06:27:16,943 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36285. Reason: nanny-close
2024-01-08 06:27:16,943 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:16,943 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40517. Reason: nanny-close
2024-01-08 06:27:16,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41445'. Reason: nanny-close
2024-01-08 06:27:16,944 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:16,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37871'. Reason: nanny-close
2024-01-08 06:27:16,944 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33761. Reason: nanny-close
2024-01-08 06:27:16,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52428; closing.
2024-01-08 06:27:16,944 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:16,944 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:16,944 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695236.9448266')
2024-01-08 06:27:16,945 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36345'. Reason: nanny-close
2024-01-08 06:27:16,945 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:16,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43499. Reason: nanny-close
2024-01-08 06:27:16,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:16,945 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:16,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43345. Reason: nanny-close
2024-01-08 06:27:16,945 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:16,946 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34137. Reason: nanny-close
2024-01-08 06:27:16,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52472; closing.
2024-01-08 06:27:16,946 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:16,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52412; closing.
2024-01-08 06:27:16,947 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52470; closing.
2024-01-08 06:27:16,947 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:16,947 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:16,947 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:16,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:16,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695236.9476109')
2024-01-08 06:27:16,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:16,948 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:16,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695236.9479973')
2024-01-08 06:27:16,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36285', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695236.9483337')
2024-01-08 06:27:16,948 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:16,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52460; closing.
2024-01-08 06:27:16,949 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:16,949 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:16,949 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:16,949 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33761', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695236.9497278')
2024-01-08 06:27:16,950 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52444; closing.
2024-01-08 06:27:16,950 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:16,950 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52464; closing.
2024-01-08 06:27:16,950 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43499', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695236.9508502')
2024-01-08 06:27:16,951 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43345', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695236.951293')
2024-01-08 06:27:16,951 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52440; closing.
2024-01-08 06:27:16,952 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695236.9520752')
2024-01-08 06:27:16,952 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:27:17,905 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:27:17,906 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:27:17,906 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:27:17,907 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:27:17,908 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-08 06:27:20,322 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:20,327 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42497 instead
  warnings.warn(
2024-01-08 06:27:20,331 - distributed.scheduler - INFO - State start
2024-01-08 06:27:20,445 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:20,446 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:27:20,448 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42497/status
2024-01-08 06:27:20,448 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:27:20,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45985'
2024-01-08 06:27:20,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45197'
2024-01-08 06:27:20,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44725'
2024-01-08 06:27:20,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43463'
2024-01-08 06:27:20,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43613'
2024-01-08 06:27:20,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37341'
2024-01-08 06:27:20,554 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44909'
2024-01-08 06:27:20,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34249'
2024-01-08 06:27:21,638 - distributed.scheduler - INFO - Receive client connection: Client-f8d04d2c-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:21,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47466
2024-01-08 06:27:22,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:22,375 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:22,379 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:22,380 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34435
2024-01-08 06:27:22,380 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34435
2024-01-08 06:27:22,381 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40829
2024-01-08 06:27:22,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:22,381 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:22,381 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:22,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:22,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qd4x1mr9
2024-01-08 06:27:22,381 - distributed.worker - INFO - Starting Worker plugin PreImport-1779a9b4-9550-42ae-b8c1-0a1ab4e09f24
2024-01-08 06:27:22,381 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3793ac5-65a5-479e-bff1-ffca7cdffd20
2024-01-08 06:27:22,381 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c9b0055b-ed64-4a91-8879-5b355e05c252
2024-01-08 06:27:22,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:22,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:22,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:22,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:22,451 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:22,451 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:22,451 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42859
2024-01-08 06:27:22,451 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42859
2024-01-08 06:27:22,452 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34891
2024-01-08 06:27:22,452 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:22,452 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:22,452 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:22,452 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:22,452 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nj02x16_
2024-01-08 06:27:22,452 - distributed.worker - INFO - Starting Worker plugin PreImport-4bf52763-4ea2-4e1d-aa82-49c42bff77a7
2024-01-08 06:27:22,452 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea5addc3-0b7e-4f4e-af47-0dec185f9411
2024-01-08 06:27:22,452 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42179
2024-01-08 06:27:22,452 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d95cf6bc-ee0e-40fc-8f1d-e619d0d05a9c
2024-01-08 06:27:22,452 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42179
2024-01-08 06:27:22,452 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34405
2024-01-08 06:27:22,452 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:22,452 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:22,452 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:22,453 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:22,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v9d7dotl
2024-01-08 06:27:22,453 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a0039c2-2820-4eb4-a1e9-ce395ef32338
2024-01-08 06:27:22,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:22,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:22,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:22,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:22,543 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:22,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:22,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:22,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:22,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:22,544 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43019
2024-01-08 06:27:22,544 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43019
2024-01-08 06:27:22,544 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35749
2024-01-08 06:27:22,544 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:22,544 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:22,544 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:22,544 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:22,544 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t2pgt1rl
2024-01-08 06:27:22,544 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cf6df1b-3b7c-4e75-9b8c-b1d423ba03d4
2024-01-08 06:27:22,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:22,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:22,547 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42155
2024-01-08 06:27:22,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42155
2024-01-08 06:27:22,548 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:22,548 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34431
2024-01-08 06:27:22,548 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:22,548 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:22,548 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:22,548 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:22,548 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sv9b3nez
2024-01-08 06:27:22,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42307
2024-01-08 06:27:22,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c01232bd-c56f-4fd3-ba4c-34f424827f90
2024-01-08 06:27:22,548 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42307
2024-01-08 06:27:22,548 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35785
2024-01-08 06:27:22,548 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:22,548 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:22,548 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:22,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37957
2024-01-08 06:27:22,548 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:22,548 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37957
2024-01-08 06:27:22,548 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d5wospn7
2024-01-08 06:27:22,548 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43293
2024-01-08 06:27:22,548 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:22,549 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:22,549 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:22,549 - distributed.worker - INFO - Starting Worker plugin RMMSetup-95101c36-153a-4742-90cf-3fba105b01f5
2024-01-08 06:27:22,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:22,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hfe8_e1f
2024-01-08 06:27:22,549 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d95b779f-5f28-41e4-92e7-13a8f719dd5d
2024-01-08 06:27:22,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:22,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:22,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:22,565 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32827
2024-01-08 06:27:22,565 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32827
2024-01-08 06:27:22,566 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44277
2024-01-08 06:27:22,566 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:22,566 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:22,566 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:22,566 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:22,566 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-300b1mnk
2024-01-08 06:27:22,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dee3a192-e23b-40ad-a458-1a6f731b6d86
2024-01-08 06:27:24,787 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,812 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42859', status: init, memory: 0, processing: 0>
2024-01-08 06:27:24,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42859
2024-01-08 06:27:24,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47490
2024-01-08 06:27:24,815 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:24,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:24,815 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:24,839 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e79c02c1-ef9a-4c98-866b-097676b9b75f
2024-01-08 06:27:24,840 - distributed.worker - INFO - Starting Worker plugin PreImport-a1cda782-78df-4291-8fe7-2be60506cb95
2024-01-08 06:27:24,840 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,845 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,856 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b948cbce-adde-46cc-ba28-5b537852ac2f
2024-01-08 06:27:24,857 - distributed.worker - INFO - Starting Worker plugin PreImport-cf9fe770-efce-45fe-9c0c-d9e720e19705
2024-01-08 06:27:24,858 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,863 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42155', status: init, memory: 0, processing: 0>
2024-01-08 06:27:24,864 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42155
2024-01-08 06:27:24,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47506
2024-01-08 06:27:24,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:24,865 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:24,865 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b7396ac-8ca0-4de0-b554-668fb1503aff
2024-01-08 06:27:24,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:24,867 - distributed.worker - INFO - Starting Worker plugin PreImport-f54e57ee-f5f0-4f4c-93bd-86db2fdfcac8
2024-01-08 06:27:24,868 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,868 - distributed.worker - INFO - Starting Worker plugin PreImport-a56e5b3f-4e45-4ea4-9e87-9945edfe4c90
2024-01-08 06:27:24,869 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45460979-f1cd-403c-99a3-63e3128b82d4
2024-01-08 06:27:24,871 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b443f81-e44a-4764-93e4-55be4427c4b8
2024-01-08 06:27:24,873 - distributed.worker - INFO - Starting Worker plugin PreImport-87611336-4b9a-4095-8fcc-0872430aebe1
2024-01-08 06:27:24,874 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,876 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94151955-8540-48c6-91b0-832dc96bbed3
2024-01-08 06:27:24,877 - distributed.worker - INFO - Starting Worker plugin PreImport-964eb564-84ed-4487-9108-c52197bde8f0
2024-01-08 06:27:24,878 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34435', status: init, memory: 0, processing: 0>
2024-01-08 06:27:24,878 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,878 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34435
2024-01-08 06:27:24,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47518
2024-01-08 06:27:24,880 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:24,881 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:24,881 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:24,890 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43019', status: init, memory: 0, processing: 0>
2024-01-08 06:27:24,891 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43019
2024-01-08 06:27:24,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47528
2024-01-08 06:27:24,892 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42307', status: init, memory: 0, processing: 0>
2024-01-08 06:27:24,892 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:24,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42307
2024-01-08 06:27:24,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47524
2024-01-08 06:27:24,892 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:24,893 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:24,894 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:24,895 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:24,895 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32827', status: init, memory: 0, processing: 0>
2024-01-08 06:27:24,897 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32827
2024-01-08 06:27:24,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47554
2024-01-08 06:27:24,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:24,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:24,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:24,898 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:24,907 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42179', status: init, memory: 0, processing: 0>
2024-01-08 06:27:24,907 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42179
2024-01-08 06:27:24,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47542
2024-01-08 06:27:24,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:24,910 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:24,910 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,911 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37957', status: init, memory: 0, processing: 0>
2024-01-08 06:27:24,911 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37957
2024-01-08 06:27:24,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47562
2024-01-08 06:27:24,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:24,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:24,913 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:24,914 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:24,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:24,994 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:24,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:24,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:24,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:24,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:24,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:24,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:24,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:25,007 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:25,007 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:25,007 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:25,007 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:25,007 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:25,007 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:25,007 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:25,008 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:27:25,016 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:25,017 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:25,020 - distributed.scheduler - INFO - Remove client Client-f8d04d2c-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:25,020 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47466; closing.
2024-01-08 06:27:25,020 - distributed.scheduler - INFO - Remove client Client-f8d04d2c-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:25,021 - distributed.scheduler - INFO - Close client connection: Client-f8d04d2c-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:25,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45985'. Reason: nanny-close
2024-01-08 06:27:25,022 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:25,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45197'. Reason: nanny-close
2024-01-08 06:27:25,023 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:25,023 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44725'. Reason: nanny-close
2024-01-08 06:27:25,023 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42179. Reason: nanny-close
2024-01-08 06:27:25,024 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:25,024 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43463'. Reason: nanny-close
2024-01-08 06:27:25,024 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:25,024 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34435. Reason: nanny-close
2024-01-08 06:27:25,024 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43613'. Reason: nanny-close
2024-01-08 06:27:25,024 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42859. Reason: nanny-close
2024-01-08 06:27:25,025 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:25,025 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37341'. Reason: nanny-close
2024-01-08 06:27:25,025 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43019. Reason: nanny-close
2024-01-08 06:27:25,025 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:25,025 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44909'. Reason: nanny-close
2024-01-08 06:27:25,026 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37957. Reason: nanny-close
2024-01-08 06:27:25,026 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:25,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:25,026 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47542; closing.
2024-01-08 06:27:25,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34249'. Reason: nanny-close
2024-01-08 06:27:25,026 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:25,026 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42307. Reason: nanny-close
2024-01-08 06:27:25,026 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42179', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695245.0267115')
2024-01-08 06:27:25,026 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:25,026 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32827. Reason: nanny-close
2024-01-08 06:27:25,027 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:25,027 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:25,027 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42155. Reason: nanny-close
2024-01-08 06:27:25,027 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47528; closing.
2024-01-08 06:27:25,028 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:25,028 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:25,028 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:25,028 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:25,028 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695245.0288718')
2024-01-08 06:27:25,028 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:25,029 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:25,029 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47518; closing.
2024-01-08 06:27:25,029 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47490; closing.
2024-01-08 06:27:25,029 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:25,029 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:25,030 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:25,030 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:25,031 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:25,031 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:25,030 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47528>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47528>: Stream is closed
2024-01-08 06:27:25,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34435', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695245.0319488')
2024-01-08 06:27:25,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42859', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695245.032391')
2024-01-08 06:27:25,032 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47562; closing.
2024-01-08 06:27:25,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37957', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695245.0333765')
2024-01-08 06:27:25,033 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47554; closing.
2024-01-08 06:27:25,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47524; closing.
2024-01-08 06:27:25,034 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695245.034518')
2024-01-08 06:27:25,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42307', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695245.0349383')
2024-01-08 06:27:25,035 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47506; closing.
2024-01-08 06:27:25,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42155', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695245.0357642')
2024-01-08 06:27:25,035 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:27:25,036 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47506>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:27:26,138 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:27:26,139 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:27:26,139 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:27:26,140 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:27:26,141 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-08 06:27:28,366 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:28,371 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:27:28,374 - distributed.scheduler - INFO - State start
2024-01-08 06:27:28,399 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:28,400 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:27:28,401 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:27:28,401 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:27:28,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35077'
2024-01-08 06:27:28,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42049'
2024-01-08 06:27:28,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38673'
2024-01-08 06:27:28,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42695'
2024-01-08 06:27:28,714 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34803'
2024-01-08 06:27:28,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44535'
2024-01-08 06:27:28,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41577'
2024-01-08 06:27:28,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33251'
2024-01-08 06:27:29,361 - distributed.scheduler - INFO - Receive client connection: Client-fdb74489-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:29,375 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47698
2024-01-08 06:27:30,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:30,515 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:30,519 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:30,520 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40275
2024-01-08 06:27:30,520 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40275
2024-01-08 06:27:30,520 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46209
2024-01-08 06:27:30,520 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,520 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,520 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:30,520 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:30,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ife6vd12
2024-01-08 06:27:30,520 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d48d250e-f7c6-481d-bf63-73dcf8c6480c
2024-01-08 06:27:30,557 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:30,557 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:30,562 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:30,563 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36641
2024-01-08 06:27:30,563 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36641
2024-01-08 06:27:30,563 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43365
2024-01-08 06:27:30,563 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,563 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,563 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:30,563 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:30,563 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7d42j8xb
2024-01-08 06:27:30,563 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d537bde6-f2bb-4e01-adfb-6bd776465c24
2024-01-08 06:27:30,754 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:30,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:30,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:30,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:30,758 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:30,759 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44527
2024-01-08 06:27:30,759 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44527
2024-01-08 06:27:30,759 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33809
2024-01-08 06:27:30,759 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,759 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,759 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:30,760 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:30,760 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k53my8cz
2024-01-08 06:27:30,760 - distributed.worker - INFO - Starting Worker plugin PreImport-a5e34eae-0c17-40ff-a130-2e8366a1a38c
2024-01-08 06:27:30,760 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e8c31f9-c1d1-46a1-b4f3-44044cb057af
2024-01-08 06:27:30,760 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:30,760 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58c1fd3e-f8aa-4614-a626-fab9f2182040
2024-01-08 06:27:30,761 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37185
2024-01-08 06:27:30,761 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37185
2024-01-08 06:27:30,761 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37797
2024-01-08 06:27:30,761 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,761 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,761 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:30,761 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:30,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-61jiyjas
2024-01-08 06:27:30,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9069f1b-d523-4fa1-a6e3-1a3646445507
2024-01-08 06:27:30,767 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:30,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:30,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:30,770 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:30,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:30,770 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:30,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:30,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:30,773 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:30,774 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44977
2024-01-08 06:27:30,774 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44977
2024-01-08 06:27:30,774 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40455
2024-01-08 06:27:30,774 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,774 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,774 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:30,774 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:30,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iv576ltd
2024-01-08 06:27:30,774 - distributed.worker - INFO - Starting Worker plugin PreImport-0378d992-8ea0-42b8-a44f-aa22e028aee2
2024-01-08 06:27:30,774 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d646e4ef-8892-42b8-85e0-3ed3b4538efa
2024-01-08 06:27:30,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:30,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-863f14ce-a60d-4a6e-b3b8-f21c146f78d5
2024-01-08 06:27:30,776 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:30,776 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33001
2024-01-08 06:27:30,776 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33001
2024-01-08 06:27:30,776 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37493
2024-01-08 06:27:30,776 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,776 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,776 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:30,776 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:30,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rhr2i74n
2024-01-08 06:27:30,777 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e8b2d0a-c9ac-412c-93c0-901f8db94415
2024-01-08 06:27:30,777 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44467
2024-01-08 06:27:30,777 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44467
2024-01-08 06:27:30,777 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37005
2024-01-08 06:27:30,777 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,777 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,777 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:30,777 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:30,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gl2cccwq
2024-01-08 06:27:30,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e2e0753-4d5d-401d-bdea-fa139d1f118f
2024-01-08 06:27:30,778 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:30,780 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46689
2024-01-08 06:27:30,780 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46689
2024-01-08 06:27:30,780 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36489
2024-01-08 06:27:30,780 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,780 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:30,780 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:27:30,780 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4hh7epa_
2024-01-08 06:27:30,781 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c8516f0-ba50-4bb9-9978-746426fcfc4b
2024-01-08 06:27:30,956 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bda17db5-c581-4a26-9332-776bc609fd51
2024-01-08 06:27:30,956 - distributed.worker - INFO - Starting Worker plugin PreImport-c86f9373-b4b2-4a87-b4a9-731c5f1d49eb
2024-01-08 06:27:30,957 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,980 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40275', status: init, memory: 0, processing: 0>
2024-01-08 06:27:30,981 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40275
2024-01-08 06:27:30,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46150
2024-01-08 06:27:30,982 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:30,983 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:30,983 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:30,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:32,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-492f7982-a1c3-4b56-8293-565e5873d0a5
2024-01-08 06:27:32,746 - distributed.worker - INFO - Starting Worker plugin PreImport-7253359a-edc8-431d-8dcd-1a9b1a4d4c49
2024-01-08 06:27:32,746 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,764 - distributed.worker - INFO - Starting Worker plugin PreImport-cf837a3e-0d5c-4a21-8d99-ce0303b15654
2024-01-08 06:27:32,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc26e8ff-fec5-4907-a3aa-ce962e57c4ab
2024-01-08 06:27:32,765 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,770 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37185', status: init, memory: 0, processing: 0>
2024-01-08 06:27:32,771 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37185
2024-01-08 06:27:32,771 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46166
2024-01-08 06:27:32,772 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:32,773 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:32,773 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,774 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:32,799 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36641', status: init, memory: 0, processing: 0>
2024-01-08 06:27:32,799 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36641
2024-01-08 06:27:32,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46180
2024-01-08 06:27:32,800 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:32,802 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:32,802 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:32,846 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,870 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44527', status: init, memory: 0, processing: 0>
2024-01-08 06:27:32,870 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44527
2024-01-08 06:27:32,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46186
2024-01-08 06:27:32,871 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:32,872 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:32,872 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:32,879 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-febc1b40-23ad-4501-8d0d-d34f76e19ba4
2024-01-08 06:27:32,879 - distributed.worker - INFO - Starting Worker plugin PreImport-66f0f96f-3b27-48b2-adbc-ddbd4f9e708c
2024-01-08 06:27:32,880 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,911 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33001', status: init, memory: 0, processing: 0>
2024-01-08 06:27:32,911 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e0e5537-c778-49f3-8e68-6b5505064df5
2024-01-08 06:27:32,911 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33001
2024-01-08 06:27:32,911 - distributed.worker - INFO - Starting Worker plugin PreImport-c35beb62-65c1-4555-bad0-051b33138f30
2024-01-08 06:27:32,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46198
2024-01-08 06:27:32,912 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:32,914 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:32,914 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:32,934 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44467', status: init, memory: 0, processing: 0>
2024-01-08 06:27:32,935 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44467
2024-01-08 06:27:32,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46210
2024-01-08 06:27:32,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:32,937 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:32,937 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:32,940 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,947 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3994893b-8cf1-4371-b3c0-ccefb6cb8d41
2024-01-08 06:27:32,950 - distributed.worker - INFO - Starting Worker plugin PreImport-fb12837d-bcc6-423c-aa13-0b95edee0201
2024-01-08 06:27:32,951 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,971 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44977', status: init, memory: 0, processing: 0>
2024-01-08 06:27:32,972 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44977
2024-01-08 06:27:32,972 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46216
2024-01-08 06:27:32,973 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:32,974 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46689', status: init, memory: 0, processing: 0>
2024-01-08 06:27:32,975 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:32,975 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46689
2024-01-08 06:27:32,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46222
2024-01-08 06:27:32,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:32,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:32,977 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:32,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:32,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:33,060 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:33,060 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:33,060 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:33,060 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:33,060 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:33,060 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:33,060 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:33,061 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:27:33,065 - distributed.scheduler - INFO - Remove client Client-fdb74489-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:33,065 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47698; closing.
2024-01-08 06:27:33,066 - distributed.scheduler - INFO - Remove client Client-fdb74489-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:33,066 - distributed.scheduler - INFO - Close client connection: Client-fdb74489-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:33,067 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35077'. Reason: nanny-close
2024-01-08 06:27:33,067 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:33,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42049'. Reason: nanny-close
2024-01-08 06:27:33,068 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:33,068 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38673'. Reason: nanny-close
2024-01-08 06:27:33,068 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36641. Reason: nanny-close
2024-01-08 06:27:33,069 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:33,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42695'. Reason: nanny-close
2024-01-08 06:27:33,069 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44977. Reason: nanny-close
2024-01-08 06:27:33,069 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:33,069 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44527. Reason: nanny-close
2024-01-08 06:27:33,069 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34803'. Reason: nanny-close
2024-01-08 06:27:33,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:33,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44535'. Reason: nanny-close
2024-01-08 06:27:33,070 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37185. Reason: nanny-close
2024-01-08 06:27:33,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:33,070 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41577'. Reason: nanny-close
2024-01-08 06:27:33,070 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33001. Reason: nanny-close
2024-01-08 06:27:33,070 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:33,071 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46180; closing.
2024-01-08 06:27:33,071 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33251'. Reason: nanny-close
2024-01-08 06:27:33,071 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:33,071 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40275. Reason: nanny-close
2024-01-08 06:27:33,071 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:33,071 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36641', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695253.0714207')
2024-01-08 06:27:33,071 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:33,071 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46689. Reason: nanny-close
2024-01-08 06:27:33,071 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:33,072 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:33,072 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44467. Reason: nanny-close
2024-01-08 06:27:33,072 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:33,073 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:33,073 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:33,073 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:33,073 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:33,073 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:33,073 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:33,073 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:33,074 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46166; closing.
2024-01-08 06:27:33,074 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:33,074 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46216; closing.
2024-01-08 06:27:33,074 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:33,074 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:33,075 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:33,075 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37185', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695253.0757174')
2024-01-08 06:27:33,076 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44977', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695253.0761242')
2024-01-08 06:27:33,076 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46186; closing.
2024-01-08 06:27:33,077 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46166>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46166>: Stream is closed
2024-01-08 06:27:33,079 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695253.079199')
2024-01-08 06:27:33,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46198; closing.
2024-01-08 06:27:33,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46150; closing.
2024-01-08 06:27:33,080 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46222; closing.
2024-01-08 06:27:33,080 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46210; closing.
2024-01-08 06:27:33,080 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33001', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695253.0804918')
2024-01-08 06:27:33,080 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40275', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695253.0808432')
2024-01-08 06:27:33,081 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46689', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695253.0811832')
2024-01-08 06:27:33,081 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44467', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695253.0815134')
2024-01-08 06:27:33,081 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:27:35,085 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:27:35,086 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:27:35,086 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:27:35,087 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:27:35,088 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-08 06:27:37,355 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:37,360 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:27:37,364 - distributed.scheduler - INFO - State start
2024-01-08 06:27:37,388 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:37,389 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:27:37,390 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:27:37,390 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:27:37,515 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36751'
2024-01-08 06:27:38,253 - distributed.scheduler - INFO - Receive client connection: Client-0307ed71-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:38,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46320
2024-01-08 06:27:39,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:39,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:39,869 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:39,870 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46105
2024-01-08 06:27:39,871 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46105
2024-01-08 06:27:39,871 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-08 06:27:39,871 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:39,871 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:39,871 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:39,871 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:27:39,871 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i4yfdlj7
2024-01-08 06:27:39,871 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-623779fe-a854-40c0-ac04-145dd3e033d2
2024-01-08 06:27:39,871 - distributed.worker - INFO - Starting Worker plugin PreImport-56f49364-0787-4fe4-bab0-c51ab5a417e8
2024-01-08 06:27:39,872 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb07ffbb-74c5-411e-a702-c06d19f1476c
2024-01-08 06:27:39,872 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:39,931 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46105', status: init, memory: 0, processing: 0>
2024-01-08 06:27:39,934 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46105
2024-01-08 06:27:39,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46344
2024-01-08 06:27:39,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:39,935 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:39,935 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:39,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:40,003 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:40,006 - distributed.scheduler - INFO - Remove client Client-0307ed71-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:40,006 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46320; closing.
2024-01-08 06:27:40,006 - distributed.scheduler - INFO - Remove client Client-0307ed71-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:40,007 - distributed.scheduler - INFO - Close client connection: Client-0307ed71-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:40,007 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36751'. Reason: nanny-close
2024-01-08 06:27:40,008 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:40,009 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46105. Reason: nanny-close
2024-01-08 06:27:40,011 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:40,011 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46344; closing.
2024-01-08 06:27:40,011 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46105', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695260.0116885')
2024-01-08 06:27:40,012 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:27:40,012 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:40,673 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:27:40,673 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:27:40,674 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:27:40,675 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:27:40,676 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-08 06:27:45,129 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:45,133 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43973 instead
  warnings.warn(
2024-01-08 06:27:45,137 - distributed.scheduler - INFO - State start
2024-01-08 06:27:45,377 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:45,378 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:27:45,379 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43973/status
2024-01-08 06:27:45,379 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:27:45,713 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38003'
2024-01-08 06:27:46,251 - distributed.scheduler - INFO - Receive client connection: Client-07ac2be3-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:46,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46850
2024-01-08 06:27:47,513 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:47,513 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:48,177 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:48,177 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36501
2024-01-08 06:27:48,178 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36501
2024-01-08 06:27:48,178 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35831
2024-01-08 06:27:48,178 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:27:48,178 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:48,178 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:48,178 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:27:48,178 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zk4eedrt
2024-01-08 06:27:48,178 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1896f924-eb51-43a6-bcfc-b6db817610f9
2024-01-08 06:27:48,178 - distributed.worker - INFO - Starting Worker plugin PreImport-731894e1-6011-48c2-ae71-6b9a6990afa0
2024-01-08 06:27:48,179 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dad6a10b-9755-4c07-be92-8035cad1dcf0
2024-01-08 06:27:48,180 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:48,246 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36501', status: init, memory: 0, processing: 0>
2024-01-08 06:27:48,247 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36501
2024-01-08 06:27:48,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46874
2024-01-08 06:27:48,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:48,248 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:27:48,248 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:48,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:27:48,307 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:48,310 - distributed.scheduler - INFO - Remove client Client-07ac2be3-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:48,311 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46850; closing.
2024-01-08 06:27:48,311 - distributed.scheduler - INFO - Remove client Client-07ac2be3-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:48,311 - distributed.scheduler - INFO - Close client connection: Client-07ac2be3-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:48,312 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38003'. Reason: nanny-close
2024-01-08 06:27:48,312 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:48,313 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36501. Reason: nanny-close
2024-01-08 06:27:48,315 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:27:48,315 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46874; closing.
2024-01-08 06:27:48,315 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36501', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695268.315774')
2024-01-08 06:27:48,316 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:27:48,316 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:49,027 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:27:49,028 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:27:49,028 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:27:49,029 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:27:49,030 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-08 06:27:51,270 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:51,275 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:27:51,278 - distributed.scheduler - INFO - State start
2024-01-08 06:27:51,301 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:51,302 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:27:51,302 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:27:51,303 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:27:53,830 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:36988'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36988>: Stream is closed
2024-01-08 06:27:54,098 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:27:54,098 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:27:54,099 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:27:54,100 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:27:54,100 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-08 06:27:56,314 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:56,318 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:27:56,322 - distributed.scheduler - INFO - State start
2024-01-08 06:27:56,380 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:27:56,381 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-08 06:27:56,381 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:27:56,382 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:27:56,546 - distributed.scheduler - INFO - Receive client connection: Client-0e5d92e9-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:56,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45414
2024-01-08 06:27:56,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46125'
2024-01-08 06:27:58,323 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:27:58,323 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:27:58,327 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:27:58,328 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42183
2024-01-08 06:27:58,328 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42183
2024-01-08 06:27:58,328 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42527
2024-01-08 06:27:58,328 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:27:58,328 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:58,328 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:27:58,328 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:27:58,329 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-iztdr6q_
2024-01-08 06:27:58,329 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ae22a37-f821-46f7-adb1-f426c53246bd
2024-01-08 06:27:58,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbe0befe-7f5c-4b73-beb2-8572d2031dbf
2024-01-08 06:27:58,329 - distributed.worker - INFO - Starting Worker plugin PreImport-20557293-63e0-433f-81ee-5b0ca5ea02a7
2024-01-08 06:27:58,329 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:58,382 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42183', status: init, memory: 0, processing: 0>
2024-01-08 06:27:58,383 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42183
2024-01-08 06:27:58,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45442
2024-01-08 06:27:58,384 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:27:58,385 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:27:58,385 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:27:58,386 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:27:58,435 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:27:58,438 - distributed.scheduler - INFO - Remove client Client-0e5d92e9-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:58,438 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45414; closing.
2024-01-08 06:27:58,438 - distributed.scheduler - INFO - Remove client Client-0e5d92e9-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:58,439 - distributed.scheduler - INFO - Close client connection: Client-0e5d92e9-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:27:58,439 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46125'. Reason: nanny-close
2024-01-08 06:27:58,440 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:27:58,441 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42183. Reason: nanny-close
2024-01-08 06:27:58,442 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:27:58,442 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45442; closing.
2024-01-08 06:27:58,443 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42183', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695278.4430306')
2024-01-08 06:27:58,443 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:27:58,443 - distributed.nanny - INFO - Worker closed
2024-01-08 06:27:59,004 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:27:59,005 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:27:59,005 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:27:59,006 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-08 06:27:59,007 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-08 06:28:01,479 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:28:01,484 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:28:01,489 - distributed.scheduler - INFO - State start
2024-01-08 06:28:01,514 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:28:01,516 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:28:01,517 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:28:01,517 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:28:01,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41393'
2024-01-08 06:28:01,843 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43805'
2024-01-08 06:28:01,852 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36375'
2024-01-08 06:28:01,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37241'
2024-01-08 06:28:01,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36709'
2024-01-08 06:28:01,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42513'
2024-01-08 06:28:01,884 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38001'
2024-01-08 06:28:01,894 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46575'
2024-01-08 06:28:02,447 - distributed.scheduler - INFO - Receive client connection: Client-11554f0f-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:02,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56768
2024-01-08 06:28:03,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:03,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:03,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:03,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:03,910 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:03,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:03,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:03,911 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39313
2024-01-08 06:28:03,911 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39313
2024-01-08 06:28:03,911 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35083
2024-01-08 06:28:03,911 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:03,911 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:03,911 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:03,911 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:28:03,912 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d9f5eke7
2024-01-08 06:28:03,912 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bddd39ce-4f79-4103-819d-c54065b29218
2024-01-08 06:28:03,914 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:03,915 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37091
2024-01-08 06:28:03,915 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37091
2024-01-08 06:28:03,915 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35871
2024-01-08 06:28:03,915 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:03,915 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:03,915 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:03,915 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:03,915 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:28:03,915 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dr746kbn
2024-01-08 06:28:03,915 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2a85d23-f36f-4496-8c55-6e64e212d285
2024-01-08 06:28:03,916 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46311
2024-01-08 06:28:03,916 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46311
2024-01-08 06:28:03,916 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33979
2024-01-08 06:28:03,916 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:03,916 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:03,916 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:03,916 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:28:03,916 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4pbuwp17
2024-01-08 06:28:03,916 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea1c735c-0bd3-4e79-b797-6980d4b811cd
2024-01-08 06:28:03,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:03,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:03,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:03,938 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44593
2024-01-08 06:28:03,938 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44593
2024-01-08 06:28:03,938 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32857
2024-01-08 06:28:03,938 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:03,938 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:03,938 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:03,938 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:28:03,938 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-opu7m838
2024-01-08 06:28:03,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cca91e5a-dfb4-49b3-a3e1-e2e6ffa89d21
2024-01-08 06:28:04,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:04,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:04,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:04,105 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33111
2024-01-08 06:28:04,105 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33111
2024-01-08 06:28:04,105 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44025
2024-01-08 06:28:04,105 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:04,105 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:04,105 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:04,105 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:28:04,105 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3vy2n1cj
2024-01-08 06:28:04,106 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e5e4318-3b48-4bb4-adf1-f7e80bac783e
2024-01-08 06:28:04,106 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de5d7a69-e388-42e5-8bfe-2d3bf747a6b2
2024-01-08 06:28:04,106 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:04,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:04,111 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:04,112 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32805
2024-01-08 06:28:04,112 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32805
2024-01-08 06:28:04,112 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44173
2024-01-08 06:28:04,113 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:04,113 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:04,113 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:04,113 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:28:04,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t7vwztjp
2024-01-08 06:28:04,113 - distributed.worker - INFO - Starting Worker plugin PreImport-01783c74-7e30-4c2d-8a1d-18c4e0278808
2024-01-08 06:28:04,113 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d452fad-084f-421b-bdbe-4761d3d6730f
2024-01-08 06:28:04,115 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9d4d98f-27e4-4b01-8851-a1f1b54e0d0f
2024-01-08 06:28:04,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:04,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:04,132 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:04,133 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39983
2024-01-08 06:28:04,133 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39983
2024-01-08 06:28:04,133 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46453
2024-01-08 06:28:04,133 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:04,133 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:04,134 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:04,134 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:28:04,134 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n4pjz4ub
2024-01-08 06:28:04,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:04,134 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:04,134 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49eb7919-374f-468e-980c-f86d33caf0ec
2024-01-08 06:28:04,134 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dff2e8f1-b215-4c3c-a15b-29e378dbeb73
2024-01-08 06:28:04,139 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:04,140 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45335
2024-01-08 06:28:04,140 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45335
2024-01-08 06:28:04,140 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44731
2024-01-08 06:28:04,140 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:04,140 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:04,140 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:04,140 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:28:04,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oo25d3t5
2024-01-08 06:28:04,141 - distributed.worker - INFO - Starting Worker plugin PreImport-0258e8af-a1e1-42c8-9ca6-2d42d69ae5b9
2024-01-08 06:28:04,141 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-092c3c7f-ccd7-4ece-b6f5-86d93da55f36
2024-01-08 06:28:04,141 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f012898-3031-4436-99cd-af17ea8bf1e0
2024-01-08 06:28:06,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-351a067d-0a03-42a0-b6a2-bbba8841aa03
2024-01-08 06:28:06,969 - distributed.worker - INFO - Starting Worker plugin PreImport-4c787761-e292-469e-83c0-4fb8a28102cb
2024-01-08 06:28:06,971 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb9e2052-667c-470d-b555-e95acaf559f8
2024-01-08 06:28:07,010 - distributed.worker - INFO - Starting Worker plugin PreImport-aec262a4-2ca6-495a-8a03-19987a98f34c
2024-01-08 06:28:07,010 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,041 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39313', status: init, memory: 0, processing: 0>
2024-01-08 06:28:07,044 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39313
2024-01-08 06:28:07,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56786
2024-01-08 06:28:07,046 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:07,049 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:07,049 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:07,054 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44593', status: init, memory: 0, processing: 0>
2024-01-08 06:28:07,055 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44593
2024-01-08 06:28:07,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56794
2024-01-08 06:28:07,057 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:07,058 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:07,059 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:07,082 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,089 - distributed.worker - INFO - Starting Worker plugin PreImport-40a43dbd-987a-4323-aebb-24d1d94baa27
2024-01-08 06:28:07,091 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,099 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,108 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45335', status: init, memory: 0, processing: 0>
2024-01-08 06:28:07,109 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45335
2024-01-08 06:28:07,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56808
2024-01-08 06:28:07,109 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c62f959b-9b9f-44b5-b7b2-277e6b05808b
2024-01-08 06:28:07,110 - distributed.worker - INFO - Starting Worker plugin PreImport-0aa326be-2cb5-4f06-9fe8-d3e73c32648c
2024-01-08 06:28:07,110 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,110 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:07,111 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:07,111 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:07,114 - distributed.worker - INFO - Starting Worker plugin PreImport-3643e991-e0c1-4935-b7e3-e30995dc16e5
2024-01-08 06:28:07,116 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,129 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33111', status: init, memory: 0, processing: 0>
2024-01-08 06:28:07,130 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33111
2024-01-08 06:28:07,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56812
2024-01-08 06:28:07,133 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:07,135 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46311', status: init, memory: 0, processing: 0>
2024-01-08 06:28:07,135 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:07,135 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,135 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ebc5d24-dbef-4f80-977c-e7e6081f40af
2024-01-08 06:28:07,136 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46311
2024-01-08 06:28:07,136 - distributed.worker - INFO - Starting Worker plugin PreImport-18f5e919-2049-4e15-851b-9171f3ed4fb3
2024-01-08 06:28:07,136 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56840
2024-01-08 06:28:07,136 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:07,137 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32805', status: init, memory: 0, processing: 0>
2024-01-08 06:28:07,138 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32805
2024-01-08 06:28:07,138 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:07,138 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,138 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56826
2024-01-08 06:28:07,138 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:07,140 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:07,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:07,141 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:07,141 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,144 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:07,167 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39983', status: init, memory: 0, processing: 0>
2024-01-08 06:28:07,168 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39983
2024-01-08 06:28:07,168 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56842
2024-01-08 06:28:07,169 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37091', status: init, memory: 0, processing: 0>
2024-01-08 06:28:07,171 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:07,172 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37091
2024-01-08 06:28:07,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56848
2024-01-08 06:28:07,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:07,173 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:07,173 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,174 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:07,175 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:07,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:07,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:07,268 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:28:07,268 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:28:07,268 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:28:07,268 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:28:07,268 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:28:07,269 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:28:07,269 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:28:07,271 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:28:07,291 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:07,292 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:07,292 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:07,292 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:07,291 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:07,292 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:07,292 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:07,296 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:07,300 - distributed.scheduler - INFO - Remove client Client-11554f0f-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:07,300 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56768; closing.
2024-01-08 06:28:07,301 - distributed.scheduler - INFO - Remove client Client-11554f0f-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:07,301 - distributed.scheduler - INFO - Close client connection: Client-11554f0f-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:07,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41393'. Reason: nanny-close
2024-01-08 06:28:07,303 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:07,304 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43805'. Reason: nanny-close
2024-01-08 06:28:07,304 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:07,304 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37091. Reason: nanny-close
2024-01-08 06:28:07,305 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36375'. Reason: nanny-close
2024-01-08 06:28:07,305 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:07,305 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37241'. Reason: nanny-close
2024-01-08 06:28:07,306 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46311. Reason: nanny-close
2024-01-08 06:28:07,306 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:07,306 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36709'. Reason: nanny-close
2024-01-08 06:28:07,307 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:07,307 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:07,307 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32805. Reason: nanny-close
2024-01-08 06:28:07,307 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42513'. Reason: nanny-close
2024-01-08 06:28:07,308 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:07,308 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39313. Reason: nanny-close
2024-01-08 06:28:07,308 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38001'. Reason: nanny-close
2024-01-08 06:28:07,308 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45335. Reason: nanny-close
2024-01-08 06:28:07,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56848; closing.
2024-01-08 06:28:07,308 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:07,309 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46575'. Reason: nanny-close
2024-01-08 06:28:07,309 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44593. Reason: nanny-close
2024-01-08 06:28:07,309 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37091', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695287.3094635')
2024-01-08 06:28:07,309 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33111. Reason: nanny-close
2024-01-08 06:28:07,309 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:07,309 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:07,310 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:07,311 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:07,311 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56826; closing.
2024-01-08 06:28:07,311 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39983. Reason: nanny-close
2024-01-08 06:28:07,311 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:07,311 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:07,311 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:07,312 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:07,312 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:07,312 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32805', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695287.312714')
2024-01-08 06:28:07,313 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:07,313 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56786; closing.
2024-01-08 06:28:07,313 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:07,313 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:07,314 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:07,315 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:07,315 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:07,313 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56826>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56826>: Stream is closed
2024-01-08 06:28:07,317 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695287.3175404')
2024-01-08 06:28:07,317 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:07,318 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56808; closing.
2024-01-08 06:28:07,318 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56794; closing.
2024-01-08 06:28:07,319 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695287.3192194')
2024-01-08 06:28:07,319 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44593', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695287.3198943')
2024-01-08 06:28:07,320 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56812; closing.
2024-01-08 06:28:07,321 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33111', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695287.3214567')
2024-01-08 06:28:07,322 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56842; closing.
2024-01-08 06:28:07,322 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56840; closing.
2024-01-08 06:28:07,323 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695287.323056')
2024-01-08 06:28:07,323 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46311', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695287.323598')
2024-01-08 06:28:07,325 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:28:07,325 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56842>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:28:07,327 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56840>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:28:08,318 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:28:08,319 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:28:08,320 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:28:08,321 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:28:08,321 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-08 06:28:10,628 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:28:10,632 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:28:10,636 - distributed.scheduler - INFO - State start
2024-01-08 06:28:10,763 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:28:10,764 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:28:10,765 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:28:10,765 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:28:10,802 - distributed.scheduler - INFO - Receive client connection: Client-16ee73a7-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:10,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56648
2024-01-08 06:28:10,878 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41885'
2024-01-08 06:28:12,633 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:12,633 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:12,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:12,639 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44659
2024-01-08 06:28:12,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44659
2024-01-08 06:28:12,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46435
2024-01-08 06:28:12,639 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:12,639 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:12,639 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:12,639 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:28:12,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2hh8ytkh
2024-01-08 06:28:12,639 - distributed.worker - INFO - Starting Worker plugin RMMSetup-783f2f0b-4f63-43f1-9810-d5cdf9478edf
2024-01-08 06:28:13,293 - distributed.worker - INFO - Starting Worker plugin PreImport-a4c1ed78-398c-4bd4-91da-42203fff50e1
2024-01-08 06:28:13,293 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1679616c-5945-432c-8258-2e17cf7f81d9
2024-01-08 06:28:13,294 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:13,345 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44659', status: init, memory: 0, processing: 0>
2024-01-08 06:28:13,347 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44659
2024-01-08 06:28:13,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56678
2024-01-08 06:28:13,348 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:13,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:13,348 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:13,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:13,361 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:28:13,365 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:13,367 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:13,369 - distributed.scheduler - INFO - Remove client Client-16ee73a7-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:13,369 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56648; closing.
2024-01-08 06:28:13,369 - distributed.scheduler - INFO - Remove client Client-16ee73a7-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:13,370 - distributed.scheduler - INFO - Close client connection: Client-16ee73a7-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:13,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41885'. Reason: nanny-close
2024-01-08 06:28:13,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:13,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44659. Reason: nanny-close
2024-01-08 06:28:13,374 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56678; closing.
2024-01-08 06:28:13,374 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:13,374 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695293.3742883')
2024-01-08 06:28:13,374 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:28:13,375 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:14,036 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:28:14,037 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:28:14,037 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:28:14,038 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:28:14,039 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-08 06:28:16,481 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:28:16,486 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:28:16,489 - distributed.scheduler - INFO - State start
2024-01-08 06:28:16,514 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:28:16,516 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:28:16,518 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:28:16,518 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:28:16,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38685'
2024-01-08 06:28:17,471 - distributed.scheduler - INFO - Receive client connection: Client-1a536188-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:17,488 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56780
2024-01-08 06:28:18,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:28:18,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:28:18,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:28:18,574 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45471
2024-01-08 06:28:18,574 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45471
2024-01-08 06:28:18,574 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39353
2024-01-08 06:28:18,574 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:28:18,574 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:18,574 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:28:18,575 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:28:18,575 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wb7r00tc
2024-01-08 06:28:18,575 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50cb756c-4a99-4150-a318-da56e275bc14
2024-01-08 06:28:18,873 - distributed.worker - INFO - Starting Worker plugin PreImport-4be8274d-adf1-43b2-aa7f-df116c14a92a
2024-01-08 06:28:18,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-385c2172-c441-4fd9-9b45-c81675e5ab3b
2024-01-08 06:28:18,873 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:18,956 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45471', status: init, memory: 0, processing: 0>
2024-01-08 06:28:18,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45471
2024-01-08 06:28:18,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56792
2024-01-08 06:28:18,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:28:18,959 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:28:18,959 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:28:18,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:28:19,022 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-08 06:28:19,026 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:28:19,030 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:19,032 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:28:19,034 - distributed.scheduler - INFO - Remove client Client-1a536188-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:19,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56780; closing.
2024-01-08 06:28:19,035 - distributed.scheduler - INFO - Remove client Client-1a536188-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:19,035 - distributed.scheduler - INFO - Close client connection: Client-1a536188-adef-11ee-ba76-d8c49764f6bb
2024-01-08 06:28:19,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38685'. Reason: nanny-close
2024-01-08 06:28:19,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:28:19,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45471. Reason: nanny-close
2024-01-08 06:28:19,039 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:28:19,039 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56792; closing.
2024-01-08 06:28:19,039 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45471', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695299.039802')
2024-01-08 06:28:19,040 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:28:19,040 - distributed.nanny - INFO - Worker closed
2024-01-08 06:28:19,601 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:28:19,602 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:28:19,602 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:28:19,603 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:28:19,604 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43783 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41983 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38441 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42393 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46037 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33379 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33421 instead
  warnings.warn(
[1704695466.757800] [dgx13:70542:0]            sock.c:481  UCX  ERROR bind(fd=176 addr=0.0.0.0:39328) failed: Address already in use
[1704695466.757880] [dgx13:70542:0]            sock.c:481  UCX  ERROR bind(fd=176 addr=0.0.0.0:37752) failed: Address already in use
[1704695471.855498] [dgx13:70656:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:59718) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44119 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45057 instead
  warnings.warn(
[1704695487.582801] [dgx13:70938:0]            sock.c:481  UCX  ERROR bind(fd=125 addr=0.0.0.0:58032) failed: Address already in use
[1704695487.582890] [dgx13:70938:0]            sock.c:481  UCX  ERROR bind(fd=125 addr=0.0.0.0:36798) failed: Address already in use
[1704695487.763956] [dgx13:70938:0]            sock.c:481  UCX  ERROR bind(fd=136 addr=0.0.0.0:57416) failed: Address already in use
[1704695487.764074] [dgx13:70938:0]            sock.c:481  UCX  ERROR bind(fd=136 addr=0.0.0.0:57764) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39187 instead
  warnings.warn(
[1704695501.250535] [dgx13:71197:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:34594) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42925 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35739 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33425 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35239 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38957 instead
  warnings.warn(
[1704695671.864603] [dgx13:74327:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:37168) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39101 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35323 instead
  warnings.warn(
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f34c460e140, tag: 0x84905ce44c14c151>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f34c460e140, tag: 0x84905ce44c14c151>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-5440' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33135 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42775 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45021 instead
  warnings.warn(
[1704695792.909671] [dgx13:76209:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:53782) failed: Address already in use
[1704695794.894575] [dgx13:76205:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:47790) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45281 instead
  warnings.warn(
[1704695815.494563] [dgx13:76445:0]            sock.c:481  UCX  ERROR bind(fd=124 addr=0.0.0.0:41994) failed: Address already in use
[1704695817.368174] [dgx13:76538:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:39296) failed: Address already in use
[1704695817.422582] [dgx13:76543:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:54368) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39279 instead
  warnings.warn(
[1704695844.006430] [dgx13:76894:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:43248) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36393 instead
  warnings.warn(
[1704695875.260041] [dgx13:77185:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:53722) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45565 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41629 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43681 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45487 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39337 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37833 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42029 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34793 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37043 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36279 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41561 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39429 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41823 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41891 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] [1704696346.980510] [dgx13:84205:0]            sock.c:481  UCX  ERROR bind(fd=158 addr=0.0.0.0:33810) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36517 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46015 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35685 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40815 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37787 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37417 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40561 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44245 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38555 instead
  warnings.warn(
[1704696550.004763] [dgx13:87269:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:39324) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44763 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37473 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42369 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46335 instead
  warnings.warn(
[1704696605.769634] [dgx13:88012:0]            sock.c:481  UCX  ERROR bind(fd=160 addr=0.0.0.0:42056) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34301 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44877 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44753 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1704696685.379253] [dgx13:64118:0]            sock.c:481  UCX  ERROR bind(fd=176 addr=0.0.0.0:37276) failed: Address already in use
[1704696689.233080] [dgx13:89248:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:48854) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43137 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42923 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35499 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46271 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44305 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36621 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1704696774.278546] [dgx13:64118:1]            sock.c:481  UCX  ERROR bind(fd=244 addr=0.0.0.0:33704) failed: Address already in use
[1704696778.573629] [dgx13:90053:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:48630) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1704696781.122526] [dgx13:64118:1]            sock.c:481  UCX  ERROR bind(fd=247 addr=0.0.0.0:49036) failed: Address already in use
2024-01-08 06:53:06,743 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
[1704696788.129893] [dgx13:64118] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
[1704696788.129988] [dgx13:64118] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-08 06:53:34,112 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:53:34,112 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:53:34,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:53:34,115 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:53:34,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:53:34,116 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:53:34,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:53:34,117 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:53:34,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:53:34,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:53:34,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:53:34,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:53:34,159 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:53:34,159 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:53:34,159 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:53:34,160 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:53:34,767 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:53:34,767 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:53:34,768 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40563
2024-01-08 06:53:34,768 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40563
2024-01-08 06:53:34,768 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42061
2024-01-08 06:53:34,768 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42061
2024-01-08 06:53:34,768 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37169
2024-01-08 06:53:34,768 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,768 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39183
2024-01-08 06:53:34,768 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,768 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,768 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,768 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:53:34,768 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:53:34,768 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-btc7695l
2024-01-08 06:53:34,768 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ly4ah_c3
2024-01-08 06:53:34,769 - distributed.worker - INFO - Starting Worker plugin PreImport-3abb9c7e-f65c-49d7-8aed-82813e6c43dc
2024-01-08 06:53:34,769 - distributed.worker - INFO - Starting Worker plugin PreImport-a18df635-a0c9-43f9-95ab-3ca25369b6db
2024-01-08 06:53:34,769 - distributed.worker - INFO - Starting Worker plugin RMMSetup-075eadfb-255b-41e2-bd86-ea194cfd839b
2024-01-08 06:53:34,769 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3afbae46-e643-4b0f-96f5-c6379de70256
2024-01-08 06:53:34,769 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cfcfcd6a-6365-4cb6-b1b6-eb25bb6d58c3
2024-01-08 06:53:34,769 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,769 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7cdfa663-eab2-4054-a2da-ff934e302856
2024-01-08 06:53:34,769 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,788 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:53:34,789 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33347
2024-01-08 06:53:34,789 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33347
2024-01-08 06:53:34,789 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36685
2024-01-08 06:53:34,789 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,790 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:53:34,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dulpbbpc
2024-01-08 06:53:34,790 - distributed.worker - INFO - Starting Worker plugin PreImport-0dc751e5-2786-4305-bd3f-9d52f38becbb
2024-01-08 06:53:34,790 - distributed.worker - INFO - Starting Worker plugin RMMSetup-345ef94f-8746-4e05-8fb1-c975f15b57bb
2024-01-08 06:53:34,790 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7cc8de3f-42d0-41fd-b126-1118233ac3b2
2024-01-08 06:53:34,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:53:34,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:53:34,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35365
2024-01-08 06:53:34,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35365
2024-01-08 06:53:34,810 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39531
2024-01-08 06:53:34,810 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,811 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,811 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:53:34,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lw_bziu0
2024-01-08 06:53:34,811 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38341
2024-01-08 06:53:34,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38341
2024-01-08 06:53:34,811 - distributed.worker - INFO - Starting Worker plugin PreImport-cc86ac0a-63f0-41df-b85d-9af9672553ff
2024-01-08 06:53:34,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39769
2024-01-08 06:53:34,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-510b43cc-06f8-4b41-9926-c3ba772c71b0
2024-01-08 06:53:34,811 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,811 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ba879d4-f99e-480a-be56-4685d8522c13
2024-01-08 06:53:34,811 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:53:34,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h6a4j163
2024-01-08 06:53:34,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fd59783-1868-4ded-b132-329c835a73d1
2024-01-08 06:53:34,811 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,812 - distributed.worker - INFO - Starting Worker plugin PreImport-29c6fe71-0575-4f90-95e3-a9f4c456b33e
2024-01-08 06:53:34,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec6fb639-9a8a-49b1-b703-c8043e078294
2024-01-08 06:53:34,812 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,819 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:53:34,820 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37741
2024-01-08 06:53:34,820 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37741
2024-01-08 06:53:34,821 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34223
2024-01-08 06:53:34,821 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,821 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,821 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:53:34,821 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nrvjl9i2
2024-01-08 06:53:34,821 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:53:34,821 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bdc1c68f-4ee2-4073-924e-eea80d1730d8
2024-01-08 06:53:34,821 - distributed.worker - INFO - Starting Worker plugin PreImport-a2c0053d-ede8-47f7-8f6f-97cb238e3cd1
2024-01-08 06:53:34,821 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-578b2760-1315-4154-a603-fe5d03959899
2024-01-08 06:53:34,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45669
2024-01-08 06:53:34,822 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45669
2024-01-08 06:53:34,822 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45395
2024-01-08 06:53:34,822 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,822 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,822 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:53:34,822 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eh3xwzbv
2024-01-08 06:53:34,822 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,822 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4c830e9-144f-4283-8ee3-0daaa79a5047
2024-01-08 06:53:34,822 - distributed.worker - INFO - Starting Worker plugin PreImport-0273d58b-7e05-4c45-8de2-e4275dee1628
2024-01-08 06:53:34,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8068aea-3ff0-476b-a243-6fd1e0a68a84
2024-01-08 06:53:34,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,840 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:53:34,841 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38617
2024-01-08 06:53:34,841 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38617
2024-01-08 06:53:34,841 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41591
2024-01-08 06:53:34,841 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,841 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,841 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:53:34,841 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dsw0b3ey
2024-01-08 06:53:34,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8832cfb6-c399-4840-a9b3-3a85f3f48a5d
2024-01-08 06:53:34,842 - distributed.worker - INFO - Starting Worker plugin PreImport-d3f8ec65-354f-44cb-adb7-b58409994bdd
2024-01-08 06:53:34,842 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3077251e-c38e-40f1-84e7-752f24e955bb
2024-01-08 06:53:34,843 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,920 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:53:34,921 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,921 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38797
2024-01-08 06:53:34,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:53:34,986 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38797
2024-01-08 06:53:34,986 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:34,987 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38797
2024-01-08 06:53:35,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:53:35,050 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38797
2024-01-08 06:53:35,050 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:35,052 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38797
2024-01-08 06:53:35,140 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:53:35,141 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38797
2024-01-08 06:53:35,141 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:35,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38797
2024-01-08 06:53:35,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:53:35,153 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38797
2024-01-08 06:53:35,153 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:35,154 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38797
2024-01-08 06:53:35,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:53:35,155 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38797
2024-01-08 06:53:35,156 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:35,157 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38797
2024-01-08 06:53:35,162 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:53:35,163 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38797
2024-01-08 06:53:35,163 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:35,164 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38797
2024-01-08 06:53:35,164 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:53:35,166 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38797
2024-01-08 06:53:35,166 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:53:35,167 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38797
2024-01-08 06:53:35,207 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:53:35,207 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:53:35,207 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:53:35,208 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:53:35,208 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:53:35,208 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:53:35,208 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:53:35,208 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:53:35,213 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33347. Reason: nanny-close
2024-01-08 06:53:35,214 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42061. Reason: nanny-close
2024-01-08 06:53:35,215 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40563. Reason: nanny-close
2024-01-08 06:53:35,215 - distributed.core - INFO - Connection to tcp://127.0.0.1:38797 has been closed.
2024-01-08 06:53:35,215 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37741. Reason: nanny-close
2024-01-08 06:53:35,216 - distributed.core - INFO - Connection to tcp://127.0.0.1:38797 has been closed.
2024-01-08 06:53:35,216 - distributed.nanny - INFO - Worker closed
2024-01-08 06:53:35,217 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38341. Reason: nanny-close
2024-01-08 06:53:35,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:38797 has been closed.
2024-01-08 06:53:35,217 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45669. Reason: nanny-close
2024-01-08 06:53:35,217 - distributed.nanny - INFO - Worker closed
2024-01-08 06:53:35,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:38797 has been closed.
2024-01-08 06:53:35,218 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38617. Reason: nanny-close
2024-01-08 06:53:35,218 - distributed.nanny - INFO - Worker closed
2024-01-08 06:53:35,218 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35365. Reason: nanny-close
2024-01-08 06:53:35,218 - distributed.core - INFO - Connection to tcp://127.0.0.1:38797 has been closed.
2024-01-08 06:53:35,218 - distributed.core - INFO - Connection to tcp://127.0.0.1:38797 has been closed.
2024-01-08 06:53:35,219 - distributed.nanny - INFO - Worker closed
2024-01-08 06:53:35,220 - distributed.nanny - INFO - Worker closed
2024-01-08 06:53:35,220 - distributed.nanny - INFO - Worker closed
2024-01-08 06:53:35,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:38797 has been closed.
2024-01-08 06:53:35,221 - distributed.core - INFO - Connection to tcp://127.0.0.1:38797 has been closed.
2024-01-08 06:53:35,222 - distributed.nanny - INFO - Worker closed
2024-01-08 06:53:35,223 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-08 06:54:10,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:10,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:10,313 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:10,314 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40249
2024-01-08 06:54:10,314 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40249
2024-01-08 06:54:10,314 - distributed.worker - INFO -           Worker name:                          0
2024-01-08 06:54:10,315 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35289
2024-01-08 06:54:10,315 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40747
2024-01-08 06:54:10,315 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:10,315 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:10,315 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:54:10,315 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-67gw_yin
2024-01-08 06:54:10,315 - distributed.worker - INFO - Starting Worker plugin PreImport-9b494a9c-b69e-4315-b142-4923e572bf52
2024-01-08 06:54:10,319 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-08 06:54:10,319 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-19c2a6d5-bd4e-494b-a094-764e48afaf9f
2024-01-08 06:54:10,319 - distributed.worker - INFO - Starting Worker plugin RMMSetup-730e5b3e-1746-4304-9056-91bfba2e401d
2024-01-08 06:54:10,319 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40249. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-08 06:54:10,319 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-08 06:54:10,321 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-08 06:54:14,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:14,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:14,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:14,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:14,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:14,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:14,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:14,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:14,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:14,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:14,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:14,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:14,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:14,948 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:14,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:54:14,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:54:15,363 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:15,363 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45279
2024-01-08 06:54:15,363 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45279
2024-01-08 06:54:15,364 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42387
2024-01-08 06:54:15,364 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,364 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,364 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:15,364 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:54:15,364 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-agme60bn
2024-01-08 06:54:15,364 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e26081a-4ac3-4dbe-8e94-935444a763a9
2024-01-08 06:54:15,364 - distributed.worker - INFO - Starting Worker plugin PreImport-6d9854be-03cf-479a-b863-ea65356aced2
2024-01-08 06:54:15,364 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6395d280-913a-4fb4-95a9-3ae02e52fdbb
2024-01-08 06:54:15,364 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:15,368 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33837
2024-01-08 06:54:15,368 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33837
2024-01-08 06:54:15,368 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43483
2024-01-08 06:54:15,368 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,368 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,368 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:15,368 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:54:15,368 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uos31c1t
2024-01-08 06:54:15,369 - distributed.worker - INFO - Starting Worker plugin PreImport-fef39195-c6b3-4241-9ddd-3893cd6d4d7e
2024-01-08 06:54:15,369 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be8de02a-8eb3-4533-a0f2-0379e878df2d
2024-01-08 06:54:15,369 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3fc2038c-7446-44c1-999d-4e43ad8acf06
2024-01-08 06:54:15,369 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,451 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:15,452 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44849
2024-01-08 06:54:15,452 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44849
2024-01-08 06:54:15,452 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34579
2024-01-08 06:54:15,452 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,452 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,452 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:15,452 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:54:15,452 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qxx6koi3
2024-01-08 06:54:15,452 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef819b03-de64-468e-95b2-3f82a2779ae2
2024-01-08 06:54:15,453 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04e6e383-2499-465b-8e4b-8625882076d4
2024-01-08 06:54:15,453 - distributed.worker - INFO - Starting Worker plugin PreImport-ce8854e2-2230-43d3-8bdb-53e15976f8e7
2024-01-08 06:54:15,453 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:54:15,463 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,463 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,464 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40309
2024-01-08 06:54:15,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:54:15,466 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,466 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40309
2024-01-08 06:54:15,469 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:15,470 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34597
2024-01-08 06:54:15,470 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34597
2024-01-08 06:54:15,470 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33821
2024-01-08 06:54:15,470 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,470 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,470 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:15,470 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:54:15,470 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fzyp18ag
2024-01-08 06:54:15,470 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08215eb4-88f4-4852-8add-648f35e81c84
2024-01-08 06:54:15,474 - distributed.worker - INFO - Starting Worker plugin RMMSetup-724e6737-7ede-4840-9733-bd1b9a70f747
2024-01-08 06:54:15,474 - distributed.worker - INFO - Starting Worker plugin PreImport-aa7b003b-b97a-4f19-971b-1ccff81e39c8
2024-01-08 06:54:15,474 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,528 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:54:15,528 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,528 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,530 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40309
2024-01-08 06:54:15,542 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:15,543 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38483
2024-01-08 06:54:15,543 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38483
2024-01-08 06:54:15,543 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37341
2024-01-08 06:54:15,543 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,543 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,543 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:15,544 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:15,544 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:54:15,544 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vt57cggy
2024-01-08 06:54:15,544 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-711fc218-8be9-495b-9fce-683ef6f41b57
2024-01-08 06:54:15,544 - distributed.worker - INFO - Starting Worker plugin PreImport-a6814b97-6eb9-47c8-8450-ce8bb63d20f5
2024-01-08 06:54:15,544 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ac3f64a-d715-4fcf-9a7e-bae3208c5d68
2024-01-08 06:54:15,544 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,544 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40211
2024-01-08 06:54:15,544 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40211
2024-01-08 06:54:15,544 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39439
2024-01-08 06:54:15,544 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,544 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,545 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:15,545 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:54:15,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-88y0ycdl
2024-01-08 06:54:15,545 - distributed.worker - INFO - Starting Worker plugin PreImport-b6c1f581-3362-403a-a540-25b9651a31f6
2024-01-08 06:54:15,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b92404b-6518-4237-8ce1-41564d804cc9
2024-01-08 06:54:15,545 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6808464-9ff5-4dd6-befe-643fef7de52f
2024-01-08 06:54:15,545 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:54:15,551 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40309
2024-01-08 06:54:15,629 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:15,630 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41359
2024-01-08 06:54:15,630 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41359
2024-01-08 06:54:15,630 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33233
2024-01-08 06:54:15,630 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,630 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,630 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:15,630 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:54:15,630 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-udnn8lnp
2024-01-08 06:54:15,631 - distributed.worker - INFO - Starting Worker plugin PreImport-a7cc3e6c-ca3f-47ba-a780-068660c2c209
2024-01-08 06:54:15,631 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f18cdc13-1917-4c6c-ba8a-5476010ef59a
2024-01-08 06:54:15,631 - distributed.worker - INFO - Starting Worker plugin RMMSetup-93b7575c-a6e0-4eba-bf6c-d1f7cbb69717
2024-01-08 06:54:15,631 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,632 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:54:15,633 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,633 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,634 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:54:15,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40309
2024-01-08 06:54:15,635 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,636 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:54:15,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40309
2024-01-08 06:54:15,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33685
2024-01-08 06:54:15,637 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33685
2024-01-08 06:54:15,637 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35783
2024-01-08 06:54:15,637 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,637 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,637 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:54:15,637 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:54:15,637 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7qbg6___
2024-01-08 06:54:15,637 - distributed.worker - INFO - Starting Worker plugin PreImport-9fad2187-e57f-4ea2-8bfc-386b8777b575
2024-01-08 06:54:15,637 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9dcaa612-010b-4eb5-86d8-4fe94926410f
2024-01-08 06:54:15,637 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47ad81da-9891-44b1-917c-2bb7dba88ef9
2024-01-08 06:54:15,639 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:54:15,724 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40309
2024-01-08 06:54:15,729 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:54:15,729 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40309
2024-01-08 06:54:15,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:54:15,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40309
2024-01-08 06:54:15,770 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33837. Reason: nanny-close
2024-01-08 06:54:15,770 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45279. Reason: nanny-close
2024-01-08 06:54:15,771 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34597. Reason: nanny-close
2024-01-08 06:54:15,772 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44849. Reason: nanny-close
2024-01-08 06:54:15,772 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40211. Reason: nanny-close
2024-01-08 06:54:15,772 - distributed.core - INFO - Connection to tcp://127.0.0.1:40309 has been closed.
2024-01-08 06:54:15,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:40309 has been closed.
2024-01-08 06:54:15,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38483. Reason: nanny-close
2024-01-08 06:54:15,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41359. Reason: nanny-close
2024-01-08 06:54:15,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:40309 has been closed.
2024-01-08 06:54:15,774 - distributed.nanny - INFO - Worker closed
2024-01-08 06:54:15,774 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33685. Reason: nanny-close
2024-01-08 06:54:15,774 - distributed.nanny - INFO - Worker closed
2024-01-08 06:54:15,774 - distributed.core - INFO - Connection to tcp://127.0.0.1:40309 has been closed.
2024-01-08 06:54:15,774 - distributed.core - INFO - Connection to tcp://127.0.0.1:40309 has been closed.
2024-01-08 06:54:15,775 - distributed.core - INFO - Connection to tcp://127.0.0.1:40309 has been closed.
2024-01-08 06:54:15,775 - distributed.nanny - INFO - Worker closed
2024-01-08 06:54:15,775 - distributed.core - INFO - Connection to tcp://127.0.0.1:40309 has been closed.
2024-01-08 06:54:15,775 - distributed.nanny - INFO - Worker closed
2024-01-08 06:54:15,775 - distributed.nanny - INFO - Worker closed
2024-01-08 06:54:15,776 - distributed.nanny - INFO - Worker closed
2024-01-08 06:54:15,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:40309 has been closed.
2024-01-08 06:54:15,777 - distributed.nanny - INFO - Worker closed
2024-01-08 06:54:15,778 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk PASSED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-08 06:55:41,622 - distributed.worker - WARNING - RMM allocation of 1.00 MiB failed, spill-on-demand couldn't find any device memory to spill.
RMM allocs: 1.00 MiB, <ProxyManager dev_limit=25.60 GiB host_limit=0.98 TiB disk=0 B(0) host=0 B(0) dev=0 B(0)>, traceback:
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/threadpoolexecutor.py", line 57, in _worker
    task.run()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py", line 65, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1541, in <lambda>
    executor, lambda: context.run(func, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2954, in apply_function
    msg = apply_function_simple(function, args, kwargs, time_delay)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2990, in apply_function_simple
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_proxify_host_file.py", line 467, in task
    rmm.DeviceBuffer(size=rmm_pool_size),  # Trigger OOM
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/proxify_host_file.py", line 617, in oom
    traceback.print_stack(file=f)


2024-01-08 06:55:41,853 - distributed.worker - WARNING - Compute Failed
Key:       task-5c302f54392a5ca98adb3c55c8357b0e
Function:  task
args:      ()
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_serializer PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[numpy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[cupy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_name PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj0] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj1] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[dask] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[pickle] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[disk] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-send_serializers1] /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
