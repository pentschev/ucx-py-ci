============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-03 06:36:17,398 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:17,404 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45775 instead
  warnings.warn(
2023-12-03 06:36:17,408 - distributed.scheduler - INFO - State start
2023-12-03 06:36:17,435 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:17,436 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-03 06:36:17,437 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45775/status
2023-12-03 06:36:17,437 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:36:17,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46827'
2023-12-03 06:36:17,533 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41811'
2023-12-03 06:36:17,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45753'
2023-12-03 06:36:17,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40607'
2023-12-03 06:36:19,035 - distributed.scheduler - INFO - Receive client connection: Client-421858bb-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:19,054 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52806
2023-12-03 06:36:19,404 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:19,404 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:19,404 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:19,404 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:19,408 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:19,408 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:19,424 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:19,424 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:19,428 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:19,441 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:19,441 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:19,445 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-03 06:36:19,465 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44937
2023-12-03 06:36:19,465 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44937
2023-12-03 06:36:19,465 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35539
2023-12-03 06:36:19,465 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-03 06:36:19,465 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:19,466 - distributed.worker - INFO -               Threads:                          4
2023-12-03 06:36:19,466 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-03 06:36:19,466 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-yk8_c9c0
2023-12-03 06:36:19,466 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4e93323-636c-4e03-8f4e-a6ab8236f16a
2023-12-03 06:36:19,466 - distributed.worker - INFO - Starting Worker plugin PreImport-35954939-250b-4efe-be9f-5c0faca782e8
2023-12-03 06:36:19,466 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c82f3c17-3c7d-4a61-a93b-1a415900fa91
2023-12-03 06:36:19,466 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,145 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44937', status: init, memory: 0, processing: 0>
2023-12-03 06:36:20,146 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44937
2023-12-03 06:36:20,146 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52824
2023-12-03 06:36:20,147 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:20,148 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-03 06:36:20,148 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,149 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-03 06:36:20,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33345
2023-12-03 06:36:20,872 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33345
2023-12-03 06:36:20,872 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44837
2023-12-03 06:36:20,873 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-03 06:36:20,873 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,873 - distributed.worker - INFO -               Threads:                          4
2023-12-03 06:36:20,873 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-03 06:36:20,873 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44423
2023-12-03 06:36:20,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44423
2023-12-03 06:36:20,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ieh8h3mv
2023-12-03 06:36:20,873 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38915
2023-12-03 06:36:20,873 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-03 06:36:20,873 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39727
2023-12-03 06:36:20,873 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39727
2023-12-03 06:36:20,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8739a6bc-7a82-491c-ba63-5327165f7fc6
2023-12-03 06:36:20,873 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38379
2023-12-03 06:36:20,873 - distributed.worker - INFO -               Threads:                          4
2023-12-03 06:36:20,874 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-03 06:36:20,874 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-03 06:36:20,874 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-4802nxsn
2023-12-03 06:36:20,874 - distributed.worker - INFO - Starting Worker plugin PreImport-fc56c5de-6f73-4893-9ce4-4bd86b002cc1
2023-12-03 06:36:20,874 - distributed.worker - INFO -               Threads:                          4
2023-12-03 06:36:20,874 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b00ac08-0559-4246-97c0-7807675583f6
2023-12-03 06:36:20,874 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-03 06:36:20,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-tgtjhb98
2023-12-03 06:36:20,874 - distributed.worker - INFO - Starting Worker plugin PreImport-421e6678-67fe-414e-8f1d-df3701150e86
2023-12-03 06:36:20,874 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1962290-589e-4eec-9d52-7271fc5354ce
2023-12-03 06:36:20,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6925cfe8-a077-46f1-bbaf-03108747118f
2023-12-03 06:36:20,875 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3221876c-77a3-4b2f-a585-9ffdcfa00f43
2023-12-03 06:36:20,875 - distributed.worker - INFO - Starting Worker plugin PreImport-68748b1b-bfc8-4faf-aa2e-0fb3aeaceb72
2023-12-03 06:36:20,878 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e7297a9-0b7e-4b3e-a43d-c2812a303418
2023-12-03 06:36:20,880 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33345', status: init, memory: 0, processing: 0>
2023-12-03 06:36:20,895 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33345
2023-12-03 06:36:20,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44058
2023-12-03 06:36:20,896 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:20,897 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-03 06:36:20,897 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-03 06:36:20,913 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39727', status: init, memory: 0, processing: 0>
2023-12-03 06:36:20,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39727
2023-12-03 06:36:20,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44078
2023-12-03 06:36:20,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:20,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-03 06:36:20,916 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44423', status: init, memory: 0, processing: 0>
2023-12-03 06:36:20,916 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44423
2023-12-03 06:36:20,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44062
2023-12-03 06:36:20,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:20,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-03 06:36:20,919 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-03 06:36:20,919 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:20,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-03 06:36:21,018 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-03 06:36:21,019 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-03 06:36:21,019 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-03 06:36:21,020 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-03 06:36:21,025 - distributed.scheduler - INFO - Remove client Client-421858bb-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:21,025 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52806; closing.
2023-12-03 06:36:21,025 - distributed.scheduler - INFO - Remove client Client-421858bb-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:21,026 - distributed.scheduler - INFO - Close client connection: Client-421858bb-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:21,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46827'. Reason: nanny-close
2023-12-03 06:36:21,027 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:21,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41811'. Reason: nanny-close
2023-12-03 06:36:21,029 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:21,029 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44423. Reason: nanny-close
2023-12-03 06:36:21,029 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33345. Reason: nanny-close
2023-12-03 06:36:21,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44062; closing.
2023-12-03 06:36:21,031 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-03 06:36:21,031 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-03 06:36:21,031 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44423', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585381.031785')
2023-12-03 06:36:21,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45753'. Reason: nanny-close
2023-12-03 06:36:21,032 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:21,032 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44058; closing.
2023-12-03 06:36:21,032 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40607'. Reason: nanny-close
2023-12-03 06:36:21,033 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:21,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33345', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585381.0331779')
2023-12-03 06:36:21,033 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:21,033 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39727. Reason: nanny-close
2023-12-03 06:36:21,033 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:21,034 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44937. Reason: nanny-close
2023-12-03 06:36:21,035 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52824; closing.
2023-12-03 06:36:21,035 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-03 06:36:21,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44937', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585381.0361812')
2023-12-03 06:36:21,036 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-03 06:36:21,036 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44078; closing.
2023-12-03 06:36:21,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585381.0369997')
2023-12-03 06:36:21,037 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:36:21,037 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:21,038 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:22,194 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:36:22,194 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:36:22,195 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:36:22,372 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-03 06:36:22,373 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-03 06:36:24,491 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:24,496 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45101 instead
  warnings.warn(
2023-12-03 06:36:24,500 - distributed.scheduler - INFO - State start
2023-12-03 06:36:24,520 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:24,521 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:36:24,522 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45101/status
2023-12-03 06:36:24,522 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:36:24,554 - distributed.scheduler - INFO - Receive client connection: Client-467f6703-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:24,565 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37970
2023-12-03 06:36:24,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41355'
2023-12-03 06:36:24,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46801'
2023-12-03 06:36:24,793 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37131'
2023-12-03 06:36:24,795 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42311'
2023-12-03 06:36:24,803 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43515'
2023-12-03 06:36:24,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34427'
2023-12-03 06:36:24,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37495'
2023-12-03 06:36:24,828 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40059'
2023-12-03 06:36:27,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:27,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:27,095 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:27,095 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:27,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:27,100 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:27,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:27,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:27,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:27,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:27,128 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:27,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:27,144 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:27,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:27,148 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:27,148 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:27,150 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:27,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:27,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:27,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:27,155 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:27,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:27,157 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:27,163 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:31,527 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40727
2023-12-03 06:36:31,528 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40727
2023-12-03 06:36:31,528 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40951
2023-12-03 06:36:31,528 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,528 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,528 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:31,528 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:31,528 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-irllvc8b
2023-12-03 06:36:31,529 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73d4ef54-f340-49f0-aac1-05660a6bda6b
2023-12-03 06:36:31,537 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41867
2023-12-03 06:36:31,538 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41867
2023-12-03 06:36:31,538 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46653
2023-12-03 06:36:31,538 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,537 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37111
2023-12-03 06:36:31,538 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,538 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37111
2023-12-03 06:36:31,538 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:31,538 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42815
2023-12-03 06:36:31,538 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44441
2023-12-03 06:36:31,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:31,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-urw88m5n
2023-12-03 06:36:31,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44441
2023-12-03 06:36:31,539 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40267
2023-12-03 06:36:31,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,539 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,539 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:31,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:31,539 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:31,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lx_16x_9
2023-12-03 06:36:31,539 - distributed.worker - INFO - Starting Worker plugin PreImport-3b6c6b3b-4c9b-46a0-97d0-ea514e3a6674
2023-12-03 06:36:31,539 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:31,539 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dife7w17
2023-12-03 06:36:31,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5e31376-66ca-4ebb-ae9e-2430b00eccd7
2023-12-03 06:36:31,539 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96520d93-6b72-4ae2-8584-7b4b28e6087c
2023-12-03 06:36:31,539 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba849684-b0a6-4b40-8a68-e04125d601c9
2023-12-03 06:36:31,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9769b85-d173-43a4-bd73-61c01a4d8796
2023-12-03 06:36:31,546 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39025
2023-12-03 06:36:31,547 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39025
2023-12-03 06:36:31,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42191
2023-12-03 06:36:31,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,547 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,547 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:31,548 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:31,548 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q8u73tg4
2023-12-03 06:36:31,548 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52bb1dc9-c38a-4ad8-948d-0d28f22ef0bc
2023-12-03 06:36:31,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36005
2023-12-03 06:36:31,549 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36005
2023-12-03 06:36:31,549 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41563
2023-12-03 06:36:31,549 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,549 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,549 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:31,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:31,550 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-flhvj1ii
2023-12-03 06:36:31,550 - distributed.worker - INFO - Starting Worker plugin RMMSetup-114bed57-057e-4223-b244-4b0d43aede94
2023-12-03 06:36:31,555 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33713
2023-12-03 06:36:31,556 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33713
2023-12-03 06:36:31,556 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40085
2023-12-03 06:36:31,556 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,556 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,555 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35469
2023-12-03 06:36:31,556 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:31,557 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35469
2023-12-03 06:36:31,557 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:31,557 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wafdy5lq
2023-12-03 06:36:31,557 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45233
2023-12-03 06:36:31,557 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,557 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,557 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:31,557 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:31,557 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e0f5f6b-ec5a-487b-afec-434f421f2fd5
2023-12-03 06:36:31,557 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5fj0b67s
2023-12-03 06:36:31,558 - distributed.worker - INFO - Starting Worker plugin PreImport-2d848573-439b-4512-9d50-19f9d924ac34
2023-12-03 06:36:31,558 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01f1d000-3f9f-4f1e-97cc-75bcb71f4a46
2023-12-03 06:36:31,558 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf4d97dc-eeda-4b9e-a045-7503c1137b21
2023-12-03 06:36:31,726 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c1d46a8-98da-4011-8eec-2779ed1797d6
2023-12-03 06:36:31,726 - distributed.worker - INFO - Starting Worker plugin PreImport-2d5ca732-ee70-4404-a051-54c031b192ad
2023-12-03 06:36:31,727 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8c7217f-3d11-4657-989e-302a14a022ea
2023-12-03 06:36:31,745 - distributed.worker - INFO - Starting Worker plugin PreImport-1798efd6-5594-4a3e-985e-d0828d405d34
2023-12-03 06:36:31,745 - distributed.worker - INFO - Starting Worker plugin PreImport-8fdf6613-b584-4c18-be04-109edb665fad
2023-12-03 06:36:31,745 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,745 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57924771-b82f-479a-b4da-040a5890d7a2
2023-12-03 06:36:31,745 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2511ca1-8e2f-4b41-9397-d47c00278582
2023-12-03 06:36:31,746 - distributed.worker - INFO - Starting Worker plugin PreImport-c8db14e2-51b7-4805-b145-95ec1e393f49
2023-12-03 06:36:31,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06d74754-8084-420b-a3a9-bb356cacc4b6
2023-12-03 06:36:31,746 - distributed.worker - INFO - Starting Worker plugin PreImport-5dfc401a-6940-47d1-9b99-0fa74fcf3780
2023-12-03 06:36:31,746 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a20d5e53-249b-441f-8419-3b382850b097
2023-12-03 06:36:31,746 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,747 - distributed.worker - INFO - Starting Worker plugin PreImport-b753129f-9e56-4d6d-ab1e-2b63828b9b5a
2023-12-03 06:36:31,747 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,751 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40727', status: init, memory: 0, processing: 0>
2023-12-03 06:36:31,753 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40727
2023-12-03 06:36:31,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43318
2023-12-03 06:36:31,754 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:31,755 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,755 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:31,772 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36005', status: init, memory: 0, processing: 0>
2023-12-03 06:36:31,773 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36005
2023-12-03 06:36:31,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43324
2023-12-03 06:36:31,774 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:31,774 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44441', status: init, memory: 0, processing: 0>
2023-12-03 06:36:31,775 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,775 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,775 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44441
2023-12-03 06:36:31,775 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43354
2023-12-03 06:36:31,776 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37111', status: init, memory: 0, processing: 0>
2023-12-03 06:36:31,776 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:31,777 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37111
2023-12-03 06:36:31,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43340
2023-12-03 06:36:31,777 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,777 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:31,777 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39025', status: init, memory: 0, processing: 0>
2023-12-03 06:36:31,778 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39025
2023-12-03 06:36:31,778 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:31,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43358
2023-12-03 06:36:31,778 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,779 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:31,779 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:31,779 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,780 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:31,781 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41867', status: init, memory: 0, processing: 0>
2023-12-03 06:36:31,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:31,782 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41867
2023-12-03 06:36:31,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43364
2023-12-03 06:36:31,783 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:31,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,784 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,787 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33713', status: init, memory: 0, processing: 0>
2023-12-03 06:36:31,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:31,788 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33713
2023-12-03 06:36:31,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43378
2023-12-03 06:36:31,790 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:31,791 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,791 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,793 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:31,803 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,827 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35469', status: init, memory: 0, processing: 0>
2023-12-03 06:36:31,827 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35469
2023-12-03 06:36:31,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43390
2023-12-03 06:36:31,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:31,829 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:31,829 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:31,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:31,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:31,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:31,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:31,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:31,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:31,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:31,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:31,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:31,930 - distributed.scheduler - INFO - Remove client Client-467f6703-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:31,931 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37970; closing.
2023-12-03 06:36:31,931 - distributed.scheduler - INFO - Remove client Client-467f6703-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:31,931 - distributed.scheduler - INFO - Close client connection: Client-467f6703-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:31,933 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41355'. Reason: nanny-close
2023-12-03 06:36:31,933 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:31,934 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46801'. Reason: nanny-close
2023-12-03 06:36:31,934 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:31,934 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37111. Reason: nanny-close
2023-12-03 06:36:31,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37131'. Reason: nanny-close
2023-12-03 06:36:31,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:31,935 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44441. Reason: nanny-close
2023-12-03 06:36:31,935 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42311'. Reason: nanny-close
2023-12-03 06:36:31,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:31,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35469. Reason: nanny-close
2023-12-03 06:36:31,936 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43515'. Reason: nanny-close
2023-12-03 06:36:31,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:31,936 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43340; closing.
2023-12-03 06:36:31,936 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:31,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36005. Reason: nanny-close
2023-12-03 06:36:31,937 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37111', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585391.9372044')
2023-12-03 06:36:31,937 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34427'. Reason: nanny-close
2023-12-03 06:36:31,937 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:31,937 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:31,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40727. Reason: nanny-close
2023-12-03 06:36:31,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37495'. Reason: nanny-close
2023-12-03 06:36:31,938 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:31,938 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39025. Reason: nanny-close
2023-12-03 06:36:31,938 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:31,938 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40059'. Reason: nanny-close
2023-12-03 06:36:31,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:31,938 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:31,939 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43354; closing.
2023-12-03 06:36:31,939 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:31,939 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:31,939 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33713. Reason: nanny-close
2023-12-03 06:36:31,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:31,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44441', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585391.9402885')
2023-12-03 06:36:31,940 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41867. Reason: nanny-close
2023-12-03 06:36:31,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:31,940 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:31,940 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43390; closing.
2023-12-03 06:36:31,941 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:31,941 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:31,941 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35469', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585391.9418626')
2023-12-03 06:36:31,942 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:31,942 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43324; closing.
2023-12-03 06:36:31,942 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:31,943 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:31,943 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36005', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585391.943579')
2023-12-03 06:36:31,944 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:31,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43318; closing.
2023-12-03 06:36:31,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43358; closing.
2023-12-03 06:36:31,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585391.945577')
2023-12-03 06:36:31,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39025', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585391.9462907')
2023-12-03 06:36:31,946 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:31,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43378; closing.
2023-12-03 06:36:31,947 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43364; closing.
2023-12-03 06:36:31,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33713', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585391.9480164')
2023-12-03 06:36:31,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585391.948501')
2023-12-03 06:36:31,948 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:36:31,949 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43378>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-03 06:36:31,951 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:43364>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-03 06:36:33,501 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:36:33,501 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:36:33,502 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:36:33,503 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:36:33,503 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-03 06:36:35,691 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:35,695 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43831 instead
  warnings.warn(
2023-12-03 06:36:35,699 - distributed.scheduler - INFO - State start
2023-12-03 06:36:35,722 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:35,722 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:36:35,723 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43831/status
2023-12-03 06:36:35,723 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:36:35,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45425'
2023-12-03 06:36:36,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36069'
2023-12-03 06:36:36,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37397'
2023-12-03 06:36:36,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39047'
2023-12-03 06:36:36,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43619'
2023-12-03 06:36:36,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37399'
2023-12-03 06:36:36,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42725'
2023-12-03 06:36:36,082 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33327'
2023-12-03 06:36:37,906 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:37,906 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:37,910 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:37,936 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:37,936 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:37,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:37,939 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:37,940 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:37,943 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:37,968 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:37,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:37,972 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:38,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:38,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:38,013 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:38,017 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:38,017 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:38,022 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:38,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:38,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:38,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:38,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:38,058 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:38,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:40,427 - distributed.scheduler - INFO - Receive client connection: Client-4d204b82-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:40,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51858
2023-12-03 06:36:41,000 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40161
2023-12-03 06:36:41,001 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40161
2023-12-03 06:36:41,002 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37549
2023-12-03 06:36:41,002 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,002 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,002 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:41,002 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:41,002 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-whblxb6c
2023-12-03 06:36:41,003 - distributed.worker - INFO - Starting Worker plugin PreImport-d1d1546d-40e3-4fbf-8d4a-7304dc75aece
2023-12-03 06:36:41,003 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-267249d1-83da-4251-8113-c73bf7211935
2023-12-03 06:36:41,004 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40ddb928-2b43-433f-8da1-639138a826e7
2023-12-03 06:36:41,098 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35587
2023-12-03 06:36:41,100 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35587
2023-12-03 06:36:41,100 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32783
2023-12-03 06:36:41,100 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,100 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,100 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:41,101 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:41,101 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-468sgfoe
2023-12-03 06:36:41,102 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ac30ef2-1022-4964-a31f-85eae3136e50
2023-12-03 06:36:41,111 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35807
2023-12-03 06:36:41,112 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35807
2023-12-03 06:36:41,112 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33745
2023-12-03 06:36:41,112 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,112 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,112 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:41,112 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:41,112 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7l17_so4
2023-12-03 06:36:41,110 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45807
2023-12-03 06:36:41,112 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45807
2023-12-03 06:36:41,112 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42991
2023-12-03 06:36:41,112 - distributed.worker - INFO - Starting Worker plugin PreImport-585a9fad-eed7-4461-ae58-6229c4cc50dd
2023-12-03 06:36:41,112 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,113 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,113 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ddedb0df-1c37-439c-a5c6-7ca7f866a006
2023-12-03 06:36:41,113 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:41,113 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:41,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j76h9wx7
2023-12-03 06:36:41,113 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c8bbd1a-fb7e-4585-83e8-800ae89c4731
2023-12-03 06:36:41,114 - distributed.worker - INFO - Starting Worker plugin PreImport-e6718ca3-de1e-48a8-b247-1187440861e1
2023-12-03 06:36:41,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57e75f65-4072-4e1e-946b-5aba51288b45
2023-12-03 06:36:41,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10dcd949-0397-4791-9e9c-20215f549d0a
2023-12-03 06:36:41,126 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,165 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40161', status: init, memory: 0, processing: 0>
2023-12-03 06:36:41,169 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40161
2023-12-03 06:36:41,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51882
2023-12-03 06:36:41,170 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:41,170 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,170 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:41,195 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35195
2023-12-03 06:36:41,196 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35195
2023-12-03 06:36:41,196 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35167
2023-12-03 06:36:41,196 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,196 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,197 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:41,197 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:41,197 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-phynfuay
2023-12-03 06:36:41,197 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74d24445-4b75-40e8-9039-1b127d854384
2023-12-03 06:36:41,195 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33847
2023-12-03 06:36:41,198 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33847
2023-12-03 06:36:41,198 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37469
2023-12-03 06:36:41,198 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,198 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,198 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:41,198 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:41,198 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oemy10vu
2023-12-03 06:36:41,199 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c73107c4-6235-4f56-8ba1-0b53651867be
2023-12-03 06:36:41,214 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82da72da-7cdb-4871-98b0-c74fb334e812
2023-12-03 06:36:41,214 - distributed.worker - INFO - Starting Worker plugin PreImport-e2b14b96-c3f6-447b-869d-9595839c043f
2023-12-03 06:36:41,214 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,216 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,220 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,224 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34921
2023-12-03 06:36:41,225 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34921
2023-12-03 06:36:41,225 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39171
2023-12-03 06:36:41,225 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,225 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,225 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:41,225 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:41,225 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5k3_pw_v
2023-12-03 06:36:41,226 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5195232f-043f-4936-9b35-6ae238d720da
2023-12-03 06:36:41,240 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c998be08-f7e4-4e48-982f-dca2f4b86388
2023-12-03 06:36:41,241 - distributed.worker - INFO - Starting Worker plugin PreImport-c1d6e9c3-d122-4da1-a49c-253b516213bf
2023-12-03 06:36:41,241 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,243 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f355cdfd-c5fb-4f66-bd8f-bba20ca361ec
2023-12-03 06:36:41,242 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43497
2023-12-03 06:36:41,243 - distributed.worker - INFO - Starting Worker plugin PreImport-06f36b3d-721a-4250-8270-19751aed9788
2023-12-03 06:36:41,243 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43497
2023-12-03 06:36:41,243 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42219
2023-12-03 06:36:41,243 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,243 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,243 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,243 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:41,243 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:41,243 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h02o19p1
2023-12-03 06:36:41,244 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07f65c20-f8b5-416d-8207-a6c61abd6fee
2023-12-03 06:36:41,245 - distributed.worker - INFO - Starting Worker plugin PreImport-05535454-8cc2-4ad5-b3ee-93455ad31a52
2023-12-03 06:36:41,245 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4b8411ba-686e-46ce-ab8e-98a3ae382961
2023-12-03 06:36:41,246 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,249 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5563cb1c-fa45-4c18-a47b-0bb95d09cede
2023-12-03 06:36:41,250 - distributed.worker - INFO - Starting Worker plugin PreImport-a69e67f9-64df-4552-8f76-ff09291c5e7c
2023-12-03 06:36:41,251 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,251 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35587', status: init, memory: 0, processing: 0>
2023-12-03 06:36:41,252 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35587
2023-12-03 06:36:41,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51886
2023-12-03 06:36:41,253 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45807', status: init, memory: 0, processing: 0>
2023-12-03 06:36:41,254 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:41,254 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45807
2023-12-03 06:36:41,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51906
2023-12-03 06:36:41,255 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,255 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,256 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35807', status: init, memory: 0, processing: 0>
2023-12-03 06:36:41,256 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:41,256 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35807
2023-12-03 06:36:41,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51892
2023-12-03 06:36:41,257 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,257 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:41,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:41,259 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,259 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,259 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:41,261 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:41,270 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35195', status: init, memory: 0, processing: 0>
2023-12-03 06:36:41,271 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35195
2023-12-03 06:36:41,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51926
2023-12-03 06:36:41,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:41,273 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,273 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,273 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34921', status: init, memory: 0, processing: 0>
2023-12-03 06:36:41,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34921
2023-12-03 06:36:41,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51942
2023-12-03 06:36:41,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:41,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:41,275 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33847', status: init, memory: 0, processing: 0>
2023-12-03 06:36:41,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,275 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,275 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33847
2023-12-03 06:36:41,276 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51912
2023-12-03 06:36:41,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:41,276 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:41,277 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,277 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,279 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:41,287 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43497', status: init, memory: 0, processing: 0>
2023-12-03 06:36:41,288 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43497
2023-12-03 06:36:41,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51946
2023-12-03 06:36:41,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:41,290 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:41,290 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:41,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:41,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:41,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:41,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:41,387 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:41,387 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:41,387 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:41,387 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:41,388 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:41,393 - distributed.scheduler - INFO - Remove client Client-4d204b82-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:41,393 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51858; closing.
2023-12-03 06:36:41,393 - distributed.scheduler - INFO - Remove client Client-4d204b82-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:41,394 - distributed.scheduler - INFO - Close client connection: Client-4d204b82-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:41,395 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45425'. Reason: nanny-close
2023-12-03 06:36:41,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:41,397 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36069'. Reason: nanny-close
2023-12-03 06:36:41,397 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:41,397 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35807. Reason: nanny-close
2023-12-03 06:36:41,397 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37397'. Reason: nanny-close
2023-12-03 06:36:41,398 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:41,398 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35587. Reason: nanny-close
2023-12-03 06:36:41,398 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39047'. Reason: nanny-close
2023-12-03 06:36:41,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:41,399 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34921. Reason: nanny-close
2023-12-03 06:36:41,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43619'. Reason: nanny-close
2023-12-03 06:36:41,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:41,399 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:41,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35195. Reason: nanny-close
2023-12-03 06:36:41,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51892; closing.
2023-12-03 06:36:41,400 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37399'. Reason: nanny-close
2023-12-03 06:36:41,400 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585401.4004369')
2023-12-03 06:36:41,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45807. Reason: nanny-close
2023-12-03 06:36:41,400 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:41,400 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:41,400 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:41,401 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42725'. Reason: nanny-close
2023-12-03 06:36:41,401 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:41,401 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:41,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43497. Reason: nanny-close
2023-12-03 06:36:41,402 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:41,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33327'. Reason: nanny-close
2023-12-03 06:36:41,402 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:41,402 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:41,402 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40161. Reason: nanny-close
2023-12-03 06:36:41,402 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:41,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:41,403 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:41,403 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51942; closing.
2023-12-03 06:36:41,403 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33847. Reason: nanny-close
2023-12-03 06:36:41,403 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51886; closing.
2023-12-03 06:36:41,404 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34921', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585401.4047287')
2023-12-03 06:36:41,404 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:41,405 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:41,405 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:41,405 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:41,405 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585401.4053056')
2023-12-03 06:36:41,405 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51926; closing.
2023-12-03 06:36:41,406 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35195', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585401.4066348')
2023-12-03 06:36:41,406 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:41,406 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:41,406 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:41,407 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51906; closing.
2023-12-03 06:36:41,408 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585401.4081438')
2023-12-03 06:36:41,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51946; closing.
2023-12-03 06:36:41,409 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51882; closing.
2023-12-03 06:36:41,409 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43497', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585401.4097595')
2023-12-03 06:36:41,410 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40161', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585401.4103236')
2023-12-03 06:36:41,410 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51912; closing.
2023-12-03 06:36:41,411 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33847', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585401.4117208')
2023-12-03 06:36:41,411 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:36:41,412 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51912>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-03 06:36:43,113 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:36:43,113 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:36:43,114 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:36:43,116 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:36:43,117 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-03 06:36:45,524 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:45,528 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39761 instead
  warnings.warn(
2023-12-03 06:36:45,532 - distributed.scheduler - INFO - State start
2023-12-03 06:36:46,286 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:46,287 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:36:46,288 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39761/status
2023-12-03 06:36:46,288 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:36:46,520 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41617'
2023-12-03 06:36:46,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41265'
2023-12-03 06:36:46,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40505'
2023-12-03 06:36:46,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39317'
2023-12-03 06:36:46,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35683'
2023-12-03 06:36:46,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40239'
2023-12-03 06:36:46,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41723'
2023-12-03 06:36:46,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35687'
2023-12-03 06:36:47,692 - distributed.scheduler - INFO - Receive client connection: Client-52f15d90-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:47,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52100
2023-12-03 06:36:48,328 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:48,328 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:48,332 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:48,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:48,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:48,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:48,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:48,378 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:48,381 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:48,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:48,397 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:48,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:48,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:48,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:48,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:48,473 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:48,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:48,477 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:48,478 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:48,478 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:48,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:48,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:48,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:48,717 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:50,336 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42723
2023-12-03 06:36:50,337 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42723
2023-12-03 06:36:50,337 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37791
2023-12-03 06:36:50,337 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:50,337 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:50,337 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:50,338 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:50,338 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pupvh_hm
2023-12-03 06:36:50,338 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7baa32fd-0ae9-41fd-b5cc-29f1f11c683a
2023-12-03 06:36:50,338 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0971592d-2112-47ed-b10d-9b19da40251c
2023-12-03 06:36:50,638 - distributed.worker - INFO - Starting Worker plugin PreImport-3b15d47f-287c-4629-b152-6103dcee2ddd
2023-12-03 06:36:50,639 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:50,666 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42723', status: init, memory: 0, processing: 0>
2023-12-03 06:36:50,667 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42723
2023-12-03 06:36:50,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49728
2023-12-03 06:36:50,668 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:50,670 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:50,670 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:50,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:51,154 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44459
2023-12-03 06:36:51,155 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44459
2023-12-03 06:36:51,155 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35343
2023-12-03 06:36:51,155 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,155 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,155 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:51,156 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:51,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qig_2tf9
2023-12-03 06:36:51,156 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6bebc80-d369-4b88-8e8f-f5a541130431
2023-12-03 06:36:51,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43581
2023-12-03 06:36:51,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43581
2023-12-03 06:36:51,164 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41553
2023-12-03 06:36:51,164 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,164 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,164 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:51,164 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:51,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-llp_pujv
2023-12-03 06:36:51,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-89a6f4a9-d0f3-40f1-b93c-07affbee19de
2023-12-03 06:36:51,240 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42565
2023-12-03 06:36:51,241 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42565
2023-12-03 06:36:51,241 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35527
2023-12-03 06:36:51,241 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,241 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,241 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:51,241 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:51,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y5fvb14b
2023-12-03 06:36:51,242 - distributed.worker - INFO - Starting Worker plugin PreImport-e817bacf-c9bd-4125-a395-d2b98625c215
2023-12-03 06:36:51,242 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e22c0bd8-b120-484b-9981-355aaa0c9c84
2023-12-03 06:36:51,242 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c43d73af-2e6e-42ee-b3a1-8e620cc5f831
2023-12-03 06:36:51,245 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38339
2023-12-03 06:36:51,246 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38339
2023-12-03 06:36:51,246 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37603
2023-12-03 06:36:51,246 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,246 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,246 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:51,246 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:51,246 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1phvbk40
2023-12-03 06:36:51,247 - distributed.worker - INFO - Starting Worker plugin RMMSetup-094a3a12-f5e8-489e-a14c-e788c3c356a1
2023-12-03 06:36:51,252 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34609
2023-12-03 06:36:51,253 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34609
2023-12-03 06:36:51,253 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36721
2023-12-03 06:36:51,253 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,253 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,253 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:51,253 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:51,254 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qvxk327s
2023-12-03 06:36:51,254 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf98206c-035c-46f7-b1a4-8b3c37076ed4
2023-12-03 06:36:51,252 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37779
2023-12-03 06:36:51,254 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37779
2023-12-03 06:36:51,254 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42949
2023-12-03 06:36:51,254 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,254 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,255 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:51,255 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:51,255 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h8283ga4
2023-12-03 06:36:51,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c81dd237-1cdc-4b61-8d13-ae2200eacbc1
2023-12-03 06:36:51,255 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40113
2023-12-03 06:36:51,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40113
2023-12-03 06:36:51,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41661
2023-12-03 06:36:51,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,257 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,257 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:36:51,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:36:51,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bggurvzx
2023-12-03 06:36:51,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ad2c4ec-d6ce-415a-a457-9693d7f61f85
2023-12-03 06:36:51,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-380cd253-2c6a-410f-9017-fb1f85a2d264
2023-12-03 06:36:51,329 - distributed.worker - INFO - Starting Worker plugin PreImport-97991213-05fd-4caa-b810-7bc5f83ea766
2023-12-03 06:36:51,330 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,334 - distributed.worker - INFO - Starting Worker plugin PreImport-6b1cf4db-a520-449d-9bba-01c93addf76f
2023-12-03 06:36:51,334 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd89e1bd-6157-4530-80d8-6c52cc95113b
2023-12-03 06:36:51,334 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,363 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44459', status: init, memory: 0, processing: 0>
2023-12-03 06:36:51,364 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44459
2023-12-03 06:36:51,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49730
2023-12-03 06:36:51,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:51,366 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,366 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:51,370 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43581', status: init, memory: 0, processing: 0>
2023-12-03 06:36:51,370 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43581
2023-12-03 06:36:51,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49744
2023-12-03 06:36:51,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:51,373 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,373 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,375 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:51,428 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d55a045-4e2c-4fee-b71d-1f370a52dc12
2023-12-03 06:36:51,433 - distributed.worker - INFO - Starting Worker plugin PreImport-386486cd-0f7d-49d7-91e3-5ecb9e9d0728
2023-12-03 06:36:51,434 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,437 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-970bd7aa-ca62-4740-9c3e-f8396a73f395
2023-12-03 06:36:51,438 - distributed.worker - INFO - Starting Worker plugin PreImport-f21b33fd-1a25-4df8-923d-ba94363d01fc
2023-12-03 06:36:51,438 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,439 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f7e2336-dc03-4a23-aff6-681543c552a0
2023-12-03 06:36:51,439 - distributed.worker - INFO - Starting Worker plugin PreImport-b542a795-b3c1-43ee-895d-2da046f6c1d9
2023-12-03 06:36:51,439 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6b95e15-e92d-4ef8-9d50-b0a146b6f9b7
2023-12-03 06:36:51,441 - distributed.worker - INFO - Starting Worker plugin PreImport-80d86596-6cf9-47a7-9c6b-1527f1f6078f
2023-12-03 06:36:51,441 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,455 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42565', status: init, memory: 0, processing: 0>
2023-12-03 06:36:51,456 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42565
2023-12-03 06:36:51,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49752
2023-12-03 06:36:51,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:51,458 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,458 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:51,466 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37779', status: init, memory: 0, processing: 0>
2023-12-03 06:36:51,467 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37779
2023-12-03 06:36:51,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49756
2023-12-03 06:36:51,468 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38339', status: init, memory: 0, processing: 0>
2023-12-03 06:36:51,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38339
2023-12-03 06:36:51,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49758
2023-12-03 06:36:51,468 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:51,469 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34609', status: init, memory: 0, processing: 0>
2023-12-03 06:36:51,469 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34609
2023-12-03 06:36:51,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49774
2023-12-03 06:36:51,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:51,469 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,469 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,470 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,470 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:51,471 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,471 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:51,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:51,473 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:51,479 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40113', status: init, memory: 0, processing: 0>
2023-12-03 06:36:51,480 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40113
2023-12-03 06:36:51,480 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49786
2023-12-03 06:36:51,481 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:36:51,482 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:36:51,482 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:36:51,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:36:51,490 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:36:51,503 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:36:51,503 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:36:51,503 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:36:51,503 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:36:51,503 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:36:51,503 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:36:51,503 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:36:51,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:36:51,510 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:36:51,512 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:36:51,514 - distributed.scheduler - INFO - Remove client Client-52f15d90-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:51,514 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52100; closing.
2023-12-03 06:36:51,514 - distributed.scheduler - INFO - Remove client Client-52f15d90-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:51,515 - distributed.scheduler - INFO - Close client connection: Client-52f15d90-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:51,516 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41617'. Reason: nanny-close
2023-12-03 06:36:51,516 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:51,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41265'. Reason: nanny-close
2023-12-03 06:36:51,517 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:51,517 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43581. Reason: nanny-close
2023-12-03 06:36:51,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40505'. Reason: nanny-close
2023-12-03 06:36:51,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:51,518 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42723. Reason: nanny-close
2023-12-03 06:36:51,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39317'. Reason: nanny-close
2023-12-03 06:36:51,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:51,518 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44459. Reason: nanny-close
2023-12-03 06:36:51,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35683'. Reason: nanny-close
2023-12-03 06:36:51,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:51,519 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42565. Reason: nanny-close
2023-12-03 06:36:51,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40239'. Reason: nanny-close
2023-12-03 06:36:51,519 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49744; closing.
2023-12-03 06:36:51,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:51,519 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:51,520 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585411.5200574')
2023-12-03 06:36:51,520 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:51,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37779. Reason: nanny-close
2023-12-03 06:36:51,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41723'. Reason: nanny-close
2023-12-03 06:36:51,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:51,520 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:51,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40113. Reason: nanny-close
2023-12-03 06:36:51,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35687'. Reason: nanny-close
2023-12-03 06:36:51,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49728; closing.
2023-12-03 06:36:51,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:36:51,521 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34609. Reason: nanny-close
2023-12-03 06:36:51,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:51,521 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:51,521 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42723', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585411.5217767')
2023-12-03 06:36:51,521 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:51,521 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38339. Reason: nanny-close
2023-12-03 06:36:51,522 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:51,522 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49730; closing.
2023-12-03 06:36:51,522 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:51,522 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:51,522 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49752; closing.
2023-12-03 06:36:51,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44459', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585411.5230541')
2023-12-03 06:36:51,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:51,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:51,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42565', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585411.5237389')
2023-12-03 06:36:51,524 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49756; closing.
2023-12-03 06:36:51,524 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:51,524 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:51,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37779', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585411.5246966')
2023-12-03 06:36:51,524 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:36:51,525 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:51,525 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49786; closing.
2023-12-03 06:36:51,525 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49774; closing.
2023-12-03 06:36:51,525 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40113', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585411.5257201')
2023-12-03 06:36:51,526 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585411.5260773')
2023-12-03 06:36:51,526 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49758; closing.
2023-12-03 06:36:51,526 - distributed.nanny - INFO - Worker closed
2023-12-03 06:36:51,526 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38339', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585411.5267782')
2023-12-03 06:36:51,526 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:36:53,133 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:36:53,134 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:36:53,134 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:36:53,135 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:36:53,136 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-12-03 06:36:55,112 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:55,116 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33429 instead
  warnings.warn(
2023-12-03 06:36:55,119 - distributed.scheduler - INFO - State start
2023-12-03 06:36:55,140 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:36:55,141 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:36:55,142 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33429/status
2023-12-03 06:36:55,142 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:36:55,347 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38883'
2023-12-03 06:36:55,362 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35121'
2023-12-03 06:36:55,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46007'
2023-12-03 06:36:55,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34061'
2023-12-03 06:36:55,398 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34311'
2023-12-03 06:36:55,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35903'
2023-12-03 06:36:55,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43797'
2023-12-03 06:36:55,435 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43061'
2023-12-03 06:36:56,974 - distributed.scheduler - INFO - Receive client connection: Client-58cb891c-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:36:56,991 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49976
2023-12-03 06:36:57,180 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:57,180 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:57,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:57,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:57,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:57,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:57,381 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:57,381 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:57,386 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:57,403 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:57,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:57,407 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:57,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:57,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:57,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:57,410 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:57,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:57,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:57,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:57,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:57,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:36:57,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:36:57,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:36:57,420 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:00,222 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32769
2023-12-03 06:37:00,223 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32769
2023-12-03 06:37:00,223 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45395
2023-12-03 06:37:00,223 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,223 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,223 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:00,223 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:00,224 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-53yrl8va
2023-12-03 06:37:00,224 - distributed.worker - INFO - Starting Worker plugin RMMSetup-16e321bf-e50b-4e6a-a3ed-64e00af32fae
2023-12-03 06:37:00,233 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33095
2023-12-03 06:37:00,234 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33095
2023-12-03 06:37:00,233 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44903
2023-12-03 06:37:00,234 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32805
2023-12-03 06:37:00,234 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44903
2023-12-03 06:37:00,234 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,234 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35503
2023-12-03 06:37:00,234 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,234 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,234 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,234 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:00,235 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:00,234 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:00,235 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2qca_f86
2023-12-03 06:37:00,235 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:00,235 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-57xsqhde
2023-12-03 06:37:00,235 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37a2943e-99c4-4602-b8e8-32181efa04f6
2023-12-03 06:37:00,235 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb1f17d7-8b2a-477f-9d05-3955c4d3de01
2023-12-03 06:37:00,238 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39393
2023-12-03 06:37:00,239 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39393
2023-12-03 06:37:00,239 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42215
2023-12-03 06:37:00,239 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,239 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,239 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:00,239 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:00,239 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6aoyp_yj
2023-12-03 06:37:00,240 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e2687b9-b11b-41da-971c-96dba67e1527
2023-12-03 06:37:00,243 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38563
2023-12-03 06:37:00,243 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42047
2023-12-03 06:37:00,244 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38563
2023-12-03 06:37:00,244 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42047
2023-12-03 06:37:00,244 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42379
2023-12-03 06:37:00,244 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,244 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42541
2023-12-03 06:37:00,244 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,244 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,244 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,244 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:00,244 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:00,244 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:00,244 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pf3dxokm
2023-12-03 06:37:00,244 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:00,244 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h1y84n4z
2023-12-03 06:37:00,245 - distributed.worker - INFO - Starting Worker plugin PreImport-ab062f4d-0b2a-456c-b714-924c755d8f4a
2023-12-03 06:37:00,245 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29d93a71-7929-49d4-89ce-ad25c34a3f40
2023-12-03 06:37:00,245 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06dd6e58-eae6-43dd-8586-01b39aadb55f
2023-12-03 06:37:00,245 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2af3fe9e-0bb6-42ca-8f71-97626085abe6
2023-12-03 06:37:00,249 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44395
2023-12-03 06:37:00,251 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44395
2023-12-03 06:37:00,251 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41333
2023-12-03 06:37:00,249 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34655
2023-12-03 06:37:00,251 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,251 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34655
2023-12-03 06:37:00,251 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,251 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35181
2023-12-03 06:37:00,251 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,251 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:00,251 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,252 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:00,252 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ig53fr4o
2023-12-03 06:37:00,252 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:00,252 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:00,252 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iyemh4sc
2023-12-03 06:37:00,253 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76252b6b-b620-4816-b37e-02b9f242c657
2023-12-03 06:37:00,253 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e25f675-0c11-48ad-ab11-e48c1cb0dae9
2023-12-03 06:37:00,489 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47696175-fbfe-40d7-a587-363703ddb5e8
2023-12-03 06:37:00,489 - distributed.worker - INFO - Starting Worker plugin PreImport-04810376-dd9b-43d7-9d28-4054029a809b
2023-12-03 06:37:00,490 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,500 - distributed.worker - INFO - Starting Worker plugin PreImport-690d6631-60ba-43af-bba3-348e8082b9dc
2023-12-03 06:37:00,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86b1088d-990c-4429-9680-b69b0532a252
2023-12-03 06:37:00,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f2e90e0-6739-4c74-90a0-8bb138cd419a
2023-12-03 06:37:00,500 - distributed.worker - INFO - Starting Worker plugin PreImport-d2f29aa0-0186-45c7-b111-7e16eee3f932
2023-12-03 06:37:00,501 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,501 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,505 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e84165ca-0711-44aa-b7ea-20f3a4dcb522
2023-12-03 06:37:00,506 - distributed.worker - INFO - Starting Worker plugin PreImport-322a7233-4700-49c6-afc6-233908951e30
2023-12-03 06:37:00,506 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,516 - distributed.worker - INFO - Starting Worker plugin PreImport-72ac130c-599f-48a1-b71c-7be2dd8554fc
2023-12-03 06:37:00,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe5ca07c-d5f2-4ace-90a1-85c9546ed218
2023-12-03 06:37:00,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18af054e-bef0-41cf-8b20-9f144826cddc
2023-12-03 06:37:00,516 - distributed.worker - INFO - Starting Worker plugin PreImport-6f55ed3a-4caa-4979-937e-cc0896209d89
2023-12-03 06:37:00,517 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,517 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,521 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,522 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8aab4beb-f3cd-4262-af73-8fb63dcfa715
2023-12-03 06:37:00,523 - distributed.worker - INFO - Starting Worker plugin PreImport-22eef56a-a43c-475e-b538-fb1af151d19c
2023-12-03 06:37:00,524 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,525 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44395', status: init, memory: 0, processing: 0>
2023-12-03 06:37:00,526 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44395
2023-12-03 06:37:00,526 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37198
2023-12-03 06:37:00,527 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:00,528 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,528 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,530 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:00,536 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39393', status: init, memory: 0, processing: 0>
2023-12-03 06:37:00,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39393
2023-12-03 06:37:00,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37232
2023-12-03 06:37:00,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:00,539 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,539 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:00,542 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32769', status: init, memory: 0, processing: 0>
2023-12-03 06:37:00,543 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32769
2023-12-03 06:37:00,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37222
2023-12-03 06:37:00,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:00,545 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33095', status: init, memory: 0, processing: 0>
2023-12-03 06:37:00,546 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,546 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33095
2023-12-03 06:37:00,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37214
2023-12-03 06:37:00,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:00,548 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:00,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44903', status: init, memory: 0, processing: 0>
2023-12-03 06:37:00,549 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44903
2023-12-03 06:37:00,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37236
2023-12-03 06:37:00,549 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,549 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,550 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:00,550 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,551 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,551 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:00,551 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38563', status: init, memory: 0, processing: 0>
2023-12-03 06:37:00,552 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:00,552 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38563
2023-12-03 06:37:00,552 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37250
2023-12-03 06:37:00,553 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:00,554 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,554 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,555 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34655', status: init, memory: 0, processing: 0>
2023-12-03 06:37:00,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:00,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34655
2023-12-03 06:37:00,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37240
2023-12-03 06:37:00,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:00,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,558 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:00,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42047', status: init, memory: 0, processing: 0>
2023-12-03 06:37:00,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42047
2023-12-03 06:37:00,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37260
2023-12-03 06:37:00,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:00,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:00,569 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:00,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:00,583 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,584 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,584 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,584 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,584 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,584 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,584 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,584 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,594 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:00,594 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:00,595 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:00,595 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:00,595 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:00,595 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:00,595 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:00,595 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:00,601 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,602 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:00,605 - distributed.scheduler - INFO - Remove client Client-58cb891c-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:00,605 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49976; closing.
2023-12-03 06:37:00,605 - distributed.scheduler - INFO - Remove client Client-58cb891c-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:00,605 - distributed.scheduler - INFO - Close client connection: Client-58cb891c-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:00,606 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38883'. Reason: nanny-close
2023-12-03 06:37:00,607 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:00,607 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35121'. Reason: nanny-close
2023-12-03 06:37:00,608 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:00,608 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34655. Reason: nanny-close
2023-12-03 06:37:00,608 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46007'. Reason: nanny-close
2023-12-03 06:37:00,608 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:00,608 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32769. Reason: nanny-close
2023-12-03 06:37:00,609 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34061'. Reason: nanny-close
2023-12-03 06:37:00,609 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:00,609 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44903. Reason: nanny-close
2023-12-03 06:37:00,609 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34311'. Reason: nanny-close
2023-12-03 06:37:00,609 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:00,610 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38563. Reason: nanny-close
2023-12-03 06:37:00,610 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35903'. Reason: nanny-close
2023-12-03 06:37:00,610 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:00,610 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37240; closing.
2023-12-03 06:37:00,610 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:00,610 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585420.6109018')
2023-12-03 06:37:00,611 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42047. Reason: nanny-close
2023-12-03 06:37:00,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43797'. Reason: nanny-close
2023-12-03 06:37:00,611 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:00,611 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:00,611 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:00,611 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33095. Reason: nanny-close
2023-12-03 06:37:00,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43061'. Reason: nanny-close
2023-12-03 06:37:00,611 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:00,611 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:00,612 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44395. Reason: nanny-close
2023-12-03 06:37:00,612 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:00,612 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37222; closing.
2023-12-03 06:37:00,612 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:00,612 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37236; closing.
2023-12-03 06:37:00,612 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:00,613 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39393. Reason: nanny-close
2023-12-03 06:37:00,613 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:00,613 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37250; closing.
2023-12-03 06:37:00,613 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32769', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585420.6136832')
2023-12-03 06:37:00,613 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:00,614 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44903', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585420.6140306')
2023-12-03 06:37:00,614 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:00,614 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:00,614 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38563', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585420.6145725')
2023-12-03 06:37:00,614 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:00,615 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:00,615 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37260; closing.
2023-12-03 06:37:00,615 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37214; closing.
2023-12-03 06:37:00,616 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37198; closing.
2023-12-03 06:37:00,616 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:00,616 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:00,616 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:00,616 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42047', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585420.6164155')
2023-12-03 06:37:00,616 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585420.6168404')
2023-12-03 06:37:00,617 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44395', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585420.6171923')
2023-12-03 06:37:00,617 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37232; closing.
2023-12-03 06:37:00,617 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39393', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585420.617911')
2023-12-03 06:37:00,618 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:37:02,174 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:37:02,174 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:37:02,175 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:37:02,176 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:37:02,176 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-03 06:37:04,321 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:04,325 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43421 instead
  warnings.warn(
2023-12-03 06:37:04,328 - distributed.scheduler - INFO - State start
2023-12-03 06:37:04,710 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:04,711 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:37:04,712 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43421/status
2023-12-03 06:37:04,712 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:37:04,746 - distributed.scheduler - INFO - Receive client connection: Client-5e428879-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:04,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37330
2023-12-03 06:37:05,244 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42001'
2023-12-03 06:37:05,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45587'
2023-12-03 06:37:05,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39093'
2023-12-03 06:37:05,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33061'
2023-12-03 06:37:05,291 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38847'
2023-12-03 06:37:05,300 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35317'
2023-12-03 06:37:05,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43897'
2023-12-03 06:37:05,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32831'
2023-12-03 06:37:07,165 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:07,165 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:07,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:07,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:07,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:07,172 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:07,228 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:07,228 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:07,230 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:07,230 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:07,230 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:07,230 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:07,232 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:07,232 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:07,233 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:07,234 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:07,234 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:07,237 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:07,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:07,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:07,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:07,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:07,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:07,388 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:10,521 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33291
2023-12-03 06:37:10,522 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33291
2023-12-03 06:37:10,522 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45185
2023-12-03 06:37:10,522 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,522 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,522 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:10,523 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:10,523 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xu4g_t40
2023-12-03 06:37:10,523 - distributed.worker - INFO - Starting Worker plugin RMMSetup-155ad03a-e437-4441-9152-384292e43608
2023-12-03 06:37:10,523 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32855
2023-12-03 06:37:10,524 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32855
2023-12-03 06:37:10,524 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40737
2023-12-03 06:37:10,524 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,524 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,524 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:10,524 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:10,524 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lr71j0v4
2023-12-03 06:37:10,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fad4ad1-3638-4a19-bd0b-e6626efe0162
2023-12-03 06:37:10,571 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40927
2023-12-03 06:37:10,573 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40927
2023-12-03 06:37:10,573 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35743
2023-12-03 06:37:10,573 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,573 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,573 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:10,573 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:10,573 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r8qxrr5b
2023-12-03 06:37:10,574 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f82b0ac-1a30-4054-a9a4-a0cc2cbb734b
2023-12-03 06:37:10,576 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40839
2023-12-03 06:37:10,577 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40839
2023-12-03 06:37:10,577 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32921
2023-12-03 06:37:10,577 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,577 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,577 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:10,577 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:10,577 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-grgcuied
2023-12-03 06:37:10,578 - distributed.worker - INFO - Starting Worker plugin PreImport-e8ae1371-7e83-4f32-b603-b0a4d3bff1e3
2023-12-03 06:37:10,578 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef25f5ed-9f26-4aa9-b017-b1d0d8a0fdaa
2023-12-03 06:37:10,578 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c4d8c8e-4265-4a1a-a21b-736fc83665f9
2023-12-03 06:37:10,621 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43841
2023-12-03 06:37:10,622 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43841
2023-12-03 06:37:10,622 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35663
2023-12-03 06:37:10,622 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,622 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,622 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:10,623 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:10,623 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-af8noem1
2023-12-03 06:37:10,623 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4ef1c54-c561-47c0-a7f6-5f1a4b133d82
2023-12-03 06:37:10,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44757
2023-12-03 06:37:10,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42045
2023-12-03 06:37:10,678 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44757
2023-12-03 06:37:10,678 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42045
2023-12-03 06:37:10,678 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39455
2023-12-03 06:37:10,678 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43183
2023-12-03 06:37:10,678 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,678 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,678 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,678 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,678 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:10,678 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:10,678 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:10,678 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:10,678 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bezk3nh9
2023-12-03 06:37:10,678 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_5y69rqr
2023-12-03 06:37:10,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56779058-f89c-4d8e-a2c7-72567e0f7f64
2023-12-03 06:37:10,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-883b6d95-3b34-459b-97c6-4d2a1646bdea
2023-12-03 06:37:10,678 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46757
2023-12-03 06:37:10,679 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46757
2023-12-03 06:37:10,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44735
2023-12-03 06:37:10,680 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,680 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,680 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:10,680 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:10,680 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4hsvwlae
2023-12-03 06:37:10,680 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f1bd381-cd71-46ee-86d7-448a67d30e6a
2023-12-03 06:37:10,708 - distributed.worker - INFO - Starting Worker plugin PreImport-258d8a55-9ebc-4328-ba38-9626a04c88d3
2023-12-03 06:37:10,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5113a3f8-7c8f-4bbb-8f44-c22189375a14
2023-12-03 06:37:10,709 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,711 - distributed.worker - INFO - Starting Worker plugin PreImport-7ab30cf7-678f-4816-aa08-1fe12d161af7
2023-12-03 06:37:10,711 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93373dda-72d3-4f42-9b7c-e92e577be609
2023-12-03 06:37:10,714 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,720 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,742 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94333b77-5c30-45ee-8623-e79ae0e8e5c6
2023-12-03 06:37:10,742 - distributed.worker - INFO - Starting Worker plugin PreImport-fa50a684-171f-4ba5-bd6e-6e00948851cc
2023-12-03 06:37:10,743 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,745 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33291', status: init, memory: 0, processing: 0>
2023-12-03 06:37:10,746 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33291
2023-12-03 06:37:10,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57252
2023-12-03 06:37:10,748 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40839', status: init, memory: 0, processing: 0>
2023-12-03 06:37:10,748 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40839
2023-12-03 06:37:10,748 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57274
2023-12-03 06:37:10,748 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:10,749 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,749 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,749 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:10,750 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,750 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32855', status: init, memory: 0, processing: 0>
2023-12-03 06:37:10,750 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,751 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32855
2023-12-03 06:37:10,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57258
2023-12-03 06:37:10,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:10,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:10,752 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:10,753 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,753 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,756 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:10,775 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40927', status: init, memory: 0, processing: 0>
2023-12-03 06:37:10,776 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40927
2023-12-03 06:37:10,776 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57276
2023-12-03 06:37:10,777 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:10,778 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,778 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:10,789 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e528dfb-e07a-4ab5-92eb-a5dc804fb1cc
2023-12-03 06:37:10,789 - distributed.worker - INFO - Starting Worker plugin PreImport-cf1a8560-33b0-4d41-891b-2cf975d86beb
2023-12-03 06:37:10,790 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5bbc7d97-7fc9-4c5b-a4ac-9b92fc72c526
2023-12-03 06:37:10,798 - distributed.worker - INFO - Starting Worker plugin PreImport-4956fc36-cb97-49a8-acbe-3c7d0451e8d5
2023-12-03 06:37:10,799 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,800 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b35b9047-3c43-4185-b8b9-82e050219024
2023-12-03 06:37:10,800 - distributed.worker - INFO - Starting Worker plugin PreImport-ab48a862-d37c-4746-8158-653a70db328c
2023-12-03 06:37:10,801 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,803 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3efef59c-cd4b-4022-92c8-08c56be4c81f
2023-12-03 06:37:10,804 - distributed.worker - INFO - Starting Worker plugin PreImport-ac77fdc5-1051-4135-b5be-2133913b036f
2023-12-03 06:37:10,804 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,825 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43841', status: init, memory: 0, processing: 0>
2023-12-03 06:37:10,825 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43841
2023-12-03 06:37:10,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57278
2023-12-03 06:37:10,826 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46757', status: init, memory: 0, processing: 0>
2023-12-03 06:37:10,827 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46757
2023-12-03 06:37:10,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57296
2023-12-03 06:37:10,827 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:10,827 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44757', status: init, memory: 0, processing: 0>
2023-12-03 06:37:10,828 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,828 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,828 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44757
2023-12-03 06:37:10,828 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:10,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57300
2023-12-03 06:37:10,829 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,829 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,829 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:10,830 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,830 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:10,830 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42045', status: init, memory: 0, processing: 0>
2023-12-03 06:37:10,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:10,831 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42045
2023-12-03 06:37:10,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57284
2023-12-03 06:37:10,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:10,832 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:10,833 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:10,833 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:10,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:10,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:10,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:10,889 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:10,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:10,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:10,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:10,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:10,890 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:10,894 - distributed.scheduler - INFO - Remove client Client-5e428879-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:10,894 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37330; closing.
2023-12-03 06:37:10,895 - distributed.scheduler - INFO - Remove client Client-5e428879-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:10,895 - distributed.scheduler - INFO - Close client connection: Client-5e428879-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:10,896 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42001'. Reason: nanny-close
2023-12-03 06:37:10,896 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:10,897 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45587'. Reason: nanny-close
2023-12-03 06:37:10,898 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:10,898 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32855. Reason: nanny-close
2023-12-03 06:37:10,898 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39093'. Reason: nanny-close
2023-12-03 06:37:10,898 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:10,899 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33291. Reason: nanny-close
2023-12-03 06:37:10,899 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33061'. Reason: nanny-close
2023-12-03 06:37:10,899 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:10,899 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44757. Reason: nanny-close
2023-12-03 06:37:10,899 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38847'. Reason: nanny-close
2023-12-03 06:37:10,899 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:10,900 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40839. Reason: nanny-close
2023-12-03 06:37:10,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35317'. Reason: nanny-close
2023-12-03 06:37:10,900 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:10,900 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42045. Reason: nanny-close
2023-12-03 06:37:10,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43897'. Reason: nanny-close
2023-12-03 06:37:10,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:10,901 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57252; closing.
2023-12-03 06:37:10,901 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:10,901 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:10,901 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:10,901 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43841. Reason: nanny-close
2023-12-03 06:37:10,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32831'. Reason: nanny-close
2023-12-03 06:37:10,901 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33291', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585430.9015374')
2023-12-03 06:37:10,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:10,901 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:10,902 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46757. Reason: nanny-close
2023-12-03 06:37:10,902 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57258; closing.
2023-12-03 06:37:10,902 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40927. Reason: nanny-close
2023-12-03 06:37:10,902 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32855', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585430.9025898')
2023-12-03 06:37:10,902 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57300; closing.
2023-12-03 06:37:10,902 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:10,902 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:10,903 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:10,903 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:10,903 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:10,903 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585430.9038029')
2023-12-03 06:37:10,903 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:10,904 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57274; closing.
2023-12-03 06:37:10,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:10,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:10,904 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:10,905 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:10,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:57258>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-03 06:37:10,906 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:10,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57284; closing.
2023-12-03 06:37:10,906 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:10,906 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40839', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585430.906362')
2023-12-03 06:37:10,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42045', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585430.9070523')
2023-12-03 06:37:10,907 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57278; closing.
2023-12-03 06:37:10,907 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57296; closing.
2023-12-03 06:37:10,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43841', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585430.9080656')
2023-12-03 06:37:10,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585430.908439')
2023-12-03 06:37:10,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57276; closing.
2023-12-03 06:37:10,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585430.9092116')
2023-12-03 06:37:10,909 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:37:12,615 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:37:12,615 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:37:12,615 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:37:12,616 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:37:12,617 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-12-03 06:37:14,687 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:14,692 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37205 instead
  warnings.warn(
2023-12-03 06:37:14,697 - distributed.scheduler - INFO - State start
2023-12-03 06:37:14,720 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:14,720 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:37:14,721 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37205/status
2023-12-03 06:37:14,721 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:37:14,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39829'
2023-12-03 06:37:15,029 - distributed.scheduler - INFO - Receive client connection: Client-647509d7-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:15,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57408
2023-12-03 06:37:16,633 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:16,633 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:17,185 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:18,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40527
2023-12-03 06:37:18,267 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40527
2023-12-03 06:37:18,267 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-12-03 06:37:18,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:18,267 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:18,267 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:18,267 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-03 06:37:18,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k61phlqp
2023-12-03 06:37:18,268 - distributed.worker - INFO - Starting Worker plugin PreImport-0da324e4-09da-4ea1-aa60-466d42d75970
2023-12-03 06:37:18,268 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06efab16-03db-4b23-a7df-839fe2e00ab1
2023-12-03 06:37:18,268 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8fff6eac-e8ee-4db3-aa27-916ff2dbde7c
2023-12-03 06:37:18,268 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:18,304 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40527', status: init, memory: 0, processing: 0>
2023-12-03 06:37:18,305 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40527
2023-12-03 06:37:18,305 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57440
2023-12-03 06:37:18,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:18,307 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:18,307 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:18,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:18,352 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:18,355 - distributed.scheduler - INFO - Remove client Client-647509d7-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:18,355 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57408; closing.
2023-12-03 06:37:18,355 - distributed.scheduler - INFO - Remove client Client-647509d7-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:18,356 - distributed.scheduler - INFO - Close client connection: Client-647509d7-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:18,357 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39829'. Reason: nanny-close
2023-12-03 06:37:18,357 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:18,358 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40527. Reason: nanny-close
2023-12-03 06:37:18,360 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:18,360 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57440; closing.
2023-12-03 06:37:18,361 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585438.3612168')
2023-12-03 06:37:18,361 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:37:18,362 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:20,576 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:37:20,576 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:37:20,577 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:37:20,578 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:37:20,578 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-03 06:37:24,836 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:24,840 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42373 instead
  warnings.warn(
2023-12-03 06:37:24,844 - distributed.scheduler - INFO - State start
2023-12-03 06:37:25,176 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:25,177 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:37:25,179 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42373/status
2023-12-03 06:37:25,179 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:37:25,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36651'
2023-12-03 06:37:25,443 - distributed.scheduler - INFO - Receive client connection: Client-6a4e3aa1-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:25,455 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55098
2023-12-03 06:37:27,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:27,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:27,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:29,020 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45297
2023-12-03 06:37:29,021 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45297
2023-12-03 06:37:29,021 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35151
2023-12-03 06:37:29,022 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:29,022 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:29,022 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:29,022 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-03 06:37:29,022 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rnajzbnz
2023-12-03 06:37:29,023 - distributed.worker - INFO - Starting Worker plugin PreImport-06dc8056-6f0f-45bf-8cdb-a1bdc094e2d2
2023-12-03 06:37:29,025 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73e47c6a-7a26-4111-bd76-87c38bf7e56e
2023-12-03 06:37:29,026 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f01285b1-2a82-4019-a036-1e04010ddbb5
2023-12-03 06:37:29,026 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:29,053 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45297', status: init, memory: 0, processing: 0>
2023-12-03 06:37:29,054 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45297
2023-12-03 06:37:29,054 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55114
2023-12-03 06:37:29,055 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:29,056 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:29,056 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:29,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:29,128 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:29,130 - distributed.scheduler - INFO - Remove client Client-6a4e3aa1-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:29,130 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55098; closing.
2023-12-03 06:37:29,131 - distributed.scheduler - INFO - Remove client Client-6a4e3aa1-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:29,131 - distributed.scheduler - INFO - Close client connection: Client-6a4e3aa1-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:29,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36651'. Reason: nanny-close
2023-12-03 06:37:29,132 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:29,133 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45297. Reason: nanny-close
2023-12-03 06:37:29,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55114; closing.
2023-12-03 06:37:29,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:29,135 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45297', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585449.1356232')
2023-12-03 06:37:29,136 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:37:29,137 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:30,299 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:37:30,299 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:37:30,299 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:37:30,300 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:37:30,301 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-12-03 06:37:32,511 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:32,516 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42803 instead
  warnings.warn(
2023-12-03 06:37:32,520 - distributed.scheduler - INFO - State start
2023-12-03 06:37:32,896 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:32,897 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:37:32,898 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42803/status
2023-12-03 06:37:32,898 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:37:37,132 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:35686'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35686>: Stream is closed
2023-12-03 06:37:37,420 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:37:37,421 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:37:37,421 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:37:37,422 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:37:37,422 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-03 06:37:39,413 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:39,417 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33483 instead
  warnings.warn(
2023-12-03 06:37:39,420 - distributed.scheduler - INFO - State start
2023-12-03 06:37:39,578 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:39,579 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-03 06:37:39,580 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33483/status
2023-12-03 06:37:39,580 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:37:39,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39065'
2023-12-03 06:37:40,058 - distributed.scheduler - INFO - Receive client connection: Client-7479a2b4-91a6-11ee-b37c-d8c49764f6bb
2023-12-03 06:37:40,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47180
2023-12-03 06:37:40,307 - distributed.scheduler - INFO - Receive client connection: Client-732d0c85-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:40,308 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47186
2023-12-03 06:37:41,286 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:41,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:41,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:42,178 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45627
2023-12-03 06:37:42,178 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45627
2023-12-03 06:37:42,179 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45913
2023-12-03 06:37:42,179 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-03 06:37:42,179 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:42,179 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:42,179 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-03 06:37:42,179 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-xz1fpfi2
2023-12-03 06:37:42,179 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6e58325-a3ea-42c0-ba84-6b35bd7853ba
2023-12-03 06:37:42,180 - distributed.worker - INFO - Starting Worker plugin PreImport-1bc1d953-8f98-4b91-863d-cfbbc3eb0f52
2023-12-03 06:37:42,180 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-110b1a1a-4102-4cdb-b5fb-090782b196fa
2023-12-03 06:37:42,180 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:42,202 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45627', status: init, memory: 0, processing: 0>
2023-12-03 06:37:42,203 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45627
2023-12-03 06:37:42,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47222
2023-12-03 06:37:42,204 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:42,205 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-03 06:37:42,205 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:42,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-03 06:37:42,242 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:42,244 - distributed.scheduler - INFO - Remove client Client-732d0c85-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:42,244 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47186; closing.
2023-12-03 06:37:42,245 - distributed.scheduler - INFO - Remove client Client-732d0c85-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:42,245 - distributed.scheduler - INFO - Close client connection: Client-732d0c85-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:42,246 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39065'. Reason: nanny-close
2023-12-03 06:37:42,246 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:42,247 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45627. Reason: nanny-close
2023-12-03 06:37:42,249 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47222; closing.
2023-12-03 06:37:42,249 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-03 06:37:42,249 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45627', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585462.2498293')
2023-12-03 06:37:42,250 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:37:42,250 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:43,112 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:37:43,112 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:37:43,113 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:37:43,115 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-03 06:37:43,116 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-03 06:37:45,254 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:45,258 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41165 instead
  warnings.warn(
2023-12-03 06:37:45,261 - distributed.scheduler - INFO - State start
2023-12-03 06:37:45,294 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:45,295 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:37:45,295 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41165/status
2023-12-03 06:37:45,296 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:37:45,502 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44759'
2023-12-03 06:37:45,517 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38105'
2023-12-03 06:37:45,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38067'
2023-12-03 06:37:45,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36967'
2023-12-03 06:37:45,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36065'
2023-12-03 06:37:45,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35333'
2023-12-03 06:37:45,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40941'
2023-12-03 06:37:45,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39169'
2023-12-03 06:37:45,976 - distributed.scheduler - INFO - Receive client connection: Client-76ab2b1b-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:45,995 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42524
2023-12-03 06:37:47,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:47,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:47,369 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:47,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:47,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:47,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:47,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:47,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:47,451 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:47,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:47,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:47,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:47,455 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:47,457 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:47,460 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:47,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:47,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:47,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:47,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:47,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:47,489 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:47,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:47,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:47,514 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:50,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40925
2023-12-03 06:37:50,638 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40925
2023-12-03 06:37:50,638 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34925
2023-12-03 06:37:50,638 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,638 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,639 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:50,639 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:50,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ughnqjg0
2023-12-03 06:37:50,639 - distributed.worker - INFO - Starting Worker plugin PreImport-171289a4-5b5f-44b9-820f-8e02dbf3b073
2023-12-03 06:37:50,639 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12365717-08ea-48a9-9781-5b8d8f0ebdaa
2023-12-03 06:37:50,640 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd66d745-beda-4987-bc7d-fe1de28e3f6f
2023-12-03 06:37:50,651 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41121
2023-12-03 06:37:50,652 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41121
2023-12-03 06:37:50,652 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39427
2023-12-03 06:37:50,652 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,652 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,652 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:50,652 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:50,652 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_bihzrt5
2023-12-03 06:37:50,653 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4bcba998-4006-45f4-bbd7-f8b29a780683
2023-12-03 06:37:50,660 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34661
2023-12-03 06:37:50,662 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34661
2023-12-03 06:37:50,662 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42217
2023-12-03 06:37:50,662 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,662 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,662 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:50,662 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:50,662 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-byee34dl
2023-12-03 06:37:50,663 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3cb0f302-f02e-4cd8-9316-ca89ccf11c98
2023-12-03 06:37:50,671 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37403
2023-12-03 06:37:50,670 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46359
2023-12-03 06:37:50,672 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37403
2023-12-03 06:37:50,672 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46359
2023-12-03 06:37:50,672 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44043
2023-12-03 06:37:50,672 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35619
2023-12-03 06:37:50,672 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,672 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,672 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,672 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,672 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:50,672 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:50,672 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:50,672 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yuom5tma
2023-12-03 06:37:50,672 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:50,673 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c7cu0onp
2023-12-03 06:37:50,673 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19077976-c824-4469-bf6f-5f4d65e24960
2023-12-03 06:37:50,674 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a84334c4-9ee1-4e90-98a0-70157ce71149
2023-12-03 06:37:50,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46867
2023-12-03 06:37:50,678 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46867
2023-12-03 06:37:50,678 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35685
2023-12-03 06:37:50,678 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,678 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,678 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:50,678 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:50,678 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dthp_yda
2023-12-03 06:37:50,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad63afce-bfea-421e-86c2-140439c5869b
2023-12-03 06:37:50,679 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33423
2023-12-03 06:37:50,680 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33423
2023-12-03 06:37:50,680 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42805
2023-12-03 06:37:50,680 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,680 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,680 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:50,681 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:50,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3qdlq81r
2023-12-03 06:37:50,681 - distributed.worker - INFO - Starting Worker plugin PreImport-74682e68-2ab4-44c8-b1f4-40e0ca94984a
2023-12-03 06:37:50,681 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-66eecfe6-82db-4200-8445-8c07612ef5d7
2023-12-03 06:37:50,683 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c4d4353-b00e-4ce2-aaaa-15f207b66493
2023-12-03 06:37:50,705 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41337
2023-12-03 06:37:50,708 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41337
2023-12-03 06:37:50,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42155
2023-12-03 06:37:50,708 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,708 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,708 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:50,708 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-03 06:37:50,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s84cu_hr
2023-12-03 06:37:50,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c11c0ef5-b5cb-4c74-96c2-bacaf8248fa3
2023-12-03 06:37:50,853 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5340dd12-44f5-4583-b1a3-68deb4ee07f2
2023-12-03 06:37:50,854 - distributed.worker - INFO - Starting Worker plugin PreImport-e06d9eaf-13fc-4ed2-b260-46c74123a4a5
2023-12-03 06:37:50,854 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,854 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,854 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-effc251e-19f5-451e-9a3a-7b5bf1c8e5c8
2023-12-03 06:37:50,855 - distributed.worker - INFO - Starting Worker plugin PreImport-fe4c1afd-455f-4537-aa3b-38d3f0fdd30b
2023-12-03 06:37:50,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5a3ba13-a6f5-4e16-90bb-4e39c415fcad
2023-12-03 06:37:50,855 - distributed.worker - INFO - Starting Worker plugin PreImport-00dc1cc0-512a-4e38-bc28-3fa5964efe94
2023-12-03 06:37:50,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9dfd5a8d-0413-4406-afa4-d39947ae5f73
2023-12-03 06:37:50,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd51cbd3-9c99-4d9e-9f57-c4c4ddbf7273
2023-12-03 06:37:50,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e615ee2d-f9d0-4c50-a77f-98f265b0f9ee
2023-12-03 06:37:50,855 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,855 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,855 - distributed.worker - INFO - Starting Worker plugin PreImport-6fec6e4a-3418-4dcd-bf05-8c71c6ba4b65
2023-12-03 06:37:50,856 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,856 - distributed.worker - INFO - Starting Worker plugin PreImport-bfaceba1-077b-4885-b422-dc50712ca701
2023-12-03 06:37:50,856 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,857 - distributed.worker - INFO - Starting Worker plugin PreImport-dd8eacda-49cc-44f8-9075-24343c2a8c00
2023-12-03 06:37:50,858 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,860 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,884 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37403', status: init, memory: 0, processing: 0>
2023-12-03 06:37:50,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37403
2023-12-03 06:37:50,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55962
2023-12-03 06:37:50,886 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34661', status: init, memory: 0, processing: 0>
2023-12-03 06:37:50,887 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:50,887 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34661
2023-12-03 06:37:50,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55948
2023-12-03 06:37:50,887 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,887 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,888 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46359', status: init, memory: 0, processing: 0>
2023-12-03 06:37:50,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:50,888 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46359
2023-12-03 06:37:50,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56024
2023-12-03 06:37:50,889 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41337', status: init, memory: 0, processing: 0>
2023-12-03 06:37:50,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,889 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:50,890 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41337
2023-12-03 06:37:50,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56016
2023-12-03 06:37:50,890 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:50,890 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,890 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:50,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:50,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:50,895 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,895 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,895 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40925', status: init, memory: 0, processing: 0>
2023-12-03 06:37:50,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40925
2023-12-03 06:37:50,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55968
2023-12-03 06:37:50,897 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33423', status: init, memory: 0, processing: 0>
2023-12-03 06:37:50,898 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33423
2023-12-03 06:37:50,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55982
2023-12-03 06:37:50,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:50,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:50,899 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,899 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,899 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:50,900 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,900 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,900 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41121', status: init, memory: 0, processing: 0>
2023-12-03 06:37:50,901 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41121
2023-12-03 06:37:50,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55996
2023-12-03 06:37:50,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:50,903 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:50,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:50,904 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,904 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,904 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46867', status: init, memory: 0, processing: 0>
2023-12-03 06:37:50,905 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46867
2023-12-03 06:37:50,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56008
2023-12-03 06:37:50,906 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:50,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:50,907 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:50,907 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:50,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:51,013 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:51,013 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:51,013 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:51,014 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:51,014 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:51,014 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:51,014 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:51,014 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-03 06:37:51,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:51,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:51,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:51,028 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:51,028 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:51,028 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:51,028 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:51,028 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:51,032 - distributed.scheduler - INFO - Remove client Client-76ab2b1b-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:51,032 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42524; closing.
2023-12-03 06:37:51,033 - distributed.scheduler - INFO - Remove client Client-76ab2b1b-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:51,033 - distributed.scheduler - INFO - Close client connection: Client-76ab2b1b-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:51,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44759'. Reason: nanny-close
2023-12-03 06:37:51,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:51,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38105'. Reason: nanny-close
2023-12-03 06:37:51,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:51,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46359. Reason: nanny-close
2023-12-03 06:37:51,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38067'. Reason: nanny-close
2023-12-03 06:37:51,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:51,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41337. Reason: nanny-close
2023-12-03 06:37:51,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36967'. Reason: nanny-close
2023-12-03 06:37:51,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:51,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40925. Reason: nanny-close
2023-12-03 06:37:51,038 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36065'. Reason: nanny-close
2023-12-03 06:37:51,038 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56024; closing.
2023-12-03 06:37:51,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:51,038 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:51,038 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41121. Reason: nanny-close
2023-12-03 06:37:51,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46359', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585471.0387654')
2023-12-03 06:37:51,038 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35333'. Reason: nanny-close
2023-12-03 06:37:51,039 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:51,039 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34661. Reason: nanny-close
2023-12-03 06:37:51,039 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:51,039 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40941'. Reason: nanny-close
2023-12-03 06:37:51,039 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56016; closing.
2023-12-03 06:37:51,039 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:51,039 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37403. Reason: nanny-close
2023-12-03 06:37:51,040 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:51,040 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39169'. Reason: nanny-close
2023-12-03 06:37:51,040 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:51,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:51,040 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46867. Reason: nanny-close
2023-12-03 06:37:51,040 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585471.0405881')
2023-12-03 06:37:51,041 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:51,041 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33423. Reason: nanny-close
2023-12-03 06:37:51,041 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:51,041 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:51,042 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:51,042 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:51,041 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56016>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56016>: Stream is closed
2023-12-03 06:37:51,043 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55968; closing.
2023-12-03 06:37:51,043 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:51,043 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40925', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585471.043663')
2023-12-03 06:37:51,043 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:51,043 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:51,043 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:51,043 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:51,044 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55996; closing.
2023-12-03 06:37:51,044 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585471.0446677')
2023-12-03 06:37:51,045 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55948; closing.
2023-12-03 06:37:51,045 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55962; closing.
2023-12-03 06:37:51,045 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:51,045 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34661', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585471.0457902')
2023-12-03 06:37:51,046 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:51,046 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37403', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585471.0461833')
2023-12-03 06:37:51,046 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56008; closing.
2023-12-03 06:37:51,046 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55982; closing.
2023-12-03 06:37:51,047 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585471.0471656')
2023-12-03 06:37:51,047 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33423', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585471.0476205')
2023-12-03 06:37:51,047 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:37:51,048 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55982>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-03 06:37:52,752 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:37:52,752 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:37:52,753 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:37:52,754 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:37:52,754 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-03 06:37:54,796 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:54,800 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35689 instead
  warnings.warn(
2023-12-03 06:37:54,804 - distributed.scheduler - INFO - State start
2023-12-03 06:37:54,852 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:37:54,853 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:37:54,854 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35689/status
2023-12-03 06:37:54,854 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:37:54,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46101'
2023-12-03 06:37:56,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:37:56,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:37:56,483 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:37:56,597 - distributed.scheduler - INFO - Receive client connection: Client-7c53958d-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:56,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56136
2023-12-03 06:37:57,561 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44635
2023-12-03 06:37:57,562 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44635
2023-12-03 06:37:57,562 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38213
2023-12-03 06:37:57,562 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:37:57,562 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:57,562 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:37:57,562 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-03 06:37:57,562 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pnbj_tsp
2023-12-03 06:37:57,563 - distributed.worker - INFO - Starting Worker plugin PreImport-391c4eab-da85-407e-a633-b44b5940b110
2023-12-03 06:37:57,563 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc71c60e-1963-4aad-8e0e-3eee753909ac
2023-12-03 06:37:57,563 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b13b39d1-46c2-4217-9220-a20e152dab17
2023-12-03 06:37:57,655 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:57,683 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44635', status: init, memory: 0, processing: 0>
2023-12-03 06:37:57,684 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44635
2023-12-03 06:37:57,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56166
2023-12-03 06:37:57,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:37:57,685 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:37:57,686 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:37:57,687 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:37:57,751 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:37:57,754 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:57,756 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:37:57,758 - distributed.scheduler - INFO - Remove client Client-7c53958d-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:57,758 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56136; closing.
2023-12-03 06:37:57,758 - distributed.scheduler - INFO - Remove client Client-7c53958d-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:57,759 - distributed.scheduler - INFO - Close client connection: Client-7c53958d-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:37:57,759 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46101'. Reason: nanny-close
2023-12-03 06:37:57,760 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:37:57,761 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44635. Reason: nanny-close
2023-12-03 06:37:57,763 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56166; closing.
2023-12-03 06:37:57,763 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:37:57,763 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585477.7634857')
2023-12-03 06:37:57,763 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:37:57,765 - distributed.nanny - INFO - Worker closed
2023-12-03 06:37:58,776 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:37:58,776 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:37:58,777 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:37:58,778 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:37:58,778 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-03 06:38:00,804 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:38:00,808 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41979 instead
  warnings.warn(
2023-12-03 06:38:00,812 - distributed.scheduler - INFO - State start
2023-12-03 06:38:00,833 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-03 06:38:00,834 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-03 06:38:00,835 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41979/status
2023-12-03 06:38:00,835 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-03 06:38:00,915 - distributed.scheduler - INFO - Receive client connection: Client-7fe806c5-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:38:00,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40618
2023-12-03 06:38:00,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40023'
2023-12-03 06:38:02,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-03 06:38:02,618 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-03 06:38:02,622 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-03 06:38:03,839 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37385
2023-12-03 06:38:03,840 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37385
2023-12-03 06:38:03,840 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38487
2023-12-03 06:38:03,840 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-03 06:38:03,840 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:38:03,840 - distributed.worker - INFO -               Threads:                          1
2023-12-03 06:38:03,840 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-03 06:38:03,840 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fcc1967j
2023-12-03 06:38:03,840 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51d33300-99e7-4003-8e92-020c28f6a787
2023-12-03 06:38:03,970 - distributed.worker - INFO - Starting Worker plugin PreImport-8ea54053-d7f0-4266-9ae7-8a44e6ffc986
2023-12-03 06:38:03,970 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ed9b488-4230-4a81-8f3f-6491128540bc
2023-12-03 06:38:03,971 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:38:03,999 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37385', status: init, memory: 0, processing: 0>
2023-12-03 06:38:04,000 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37385
2023-12-03 06:38:04,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40644
2023-12-03 06:38:04,001 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-03 06:38:04,002 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-03 06:38:04,002 - distributed.worker - INFO - -------------------------------------------------
2023-12-03 06:38:04,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-03 06:38:04,047 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-03 06:38:04,052 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-03 06:38:04,056 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:38:04,057 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-03 06:38:04,060 - distributed.scheduler - INFO - Remove client Client-7fe806c5-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:38:04,060 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40618; closing.
2023-12-03 06:38:04,060 - distributed.scheduler - INFO - Remove client Client-7fe806c5-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:38:04,060 - distributed.scheduler - INFO - Close client connection: Client-7fe806c5-91a6-11ee-84ad-d8c49764f6bb
2023-12-03 06:38:04,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40023'. Reason: nanny-close
2023-12-03 06:38:04,062 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-03 06:38:04,063 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37385. Reason: nanny-close
2023-12-03 06:38:04,064 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40644; closing.
2023-12-03 06:38:04,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-03 06:38:04,065 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701585484.0652695')
2023-12-03 06:38:04,065 - distributed.scheduler - INFO - Lost all workers
2023-12-03 06:38:04,066 - distributed.nanny - INFO - Worker closed
2023-12-03 06:38:05,629 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-03 06:38:05,630 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-03 06:38:05,630 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-03 06:38:05,632 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-03 06:38:05,632 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43309 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39199 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40809 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45737 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34651 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36879 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35901 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46505 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39855 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33905 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33899 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40089 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44291 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35783 instead
  warnings.warn(
2023-12-03 06:41:13,986 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-03 06:41:13,991 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:43373', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44303 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44529 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40835 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39201 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36089 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46095 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42427 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38191 instead
  warnings.warn(
[1701585828.840989] [dgx13:59841:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:51856) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33603 instead
  warnings.warn(
[1701585853.171959] [dgx13:60253:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:60522) failed: Address already in use
2023-12-03 06:44:32,540 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-03 06:44:32,551 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:46618', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37937 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46299 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34385 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41573 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39747 instead
  warnings.warn(
[1701586046.410845] [dgx13:63761:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:58768) failed: Address already in use
2023-12-03 06:47:32,271 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-03 06:47:32,483 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-03 06:47:32,506 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:45449'.
2023-12-03 06:47:32,507 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:45449'. Shutting down.
2023-12-03 06:47:32,534 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe454f3b970>>, <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-15' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-03 06:47:34,537 - distributed.nanny - ERROR - Worker process died unexpectedly
