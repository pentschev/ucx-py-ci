============================= test session starts ==============================
platform linux -- Python 3.9.19, pytest-8.1.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.6
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-03-21 05:50:17,802 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:17,806 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34327 instead
  warnings.warn(
2024-03-21 05:50:17,810 - distributed.scheduler - INFO - State start
2024-03-21 05:50:17,831 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:17,832 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-21 05:50:17,832 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34327/status
2024-03-21 05:50:17,833 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:50:18,050 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35257'
2024-03-21 05:50:18,069 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38723'
2024-03-21 05:50:18,072 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39613'
2024-03-21 05:50:18,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41879'
2024-03-21 05:50:18,478 - distributed.scheduler - INFO - Receive client connection: Client-e46e85b3-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:18,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43852
2024-03-21 05:50:19,879 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:19,879 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:19,883 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:19,884 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37419
2024-03-21 05:50:19,884 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37419
2024-03-21 05:50:19,884 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34121
2024-03-21 05:50:19,884 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:50:19,884 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,884 - distributed.worker - INFO -               Threads:                          4
2024-03-21 05:50:19,884 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-21 05:50:19,884 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-nqytexua
2024-03-21 05:50:19,885 - distributed.worker - INFO - Starting Worker plugin PreImport-121f3c85-06e6-4a76-99df-660f4c2decb0
2024-03-21 05:50:19,885 - distributed.worker - INFO - Starting Worker plugin RMMSetup-16cc2ec2-5676-4785-a522-a2f4cafb623f
2024-03-21 05:50:19,885 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0ca9133-2365-44f1-8187-e83a7d6b1946
2024-03-21 05:50:19,885 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,890 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:19,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:19,894 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:19,895 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43491
2024-03-21 05:50:19,895 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43491
2024-03-21 05:50:19,895 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35151
2024-03-21 05:50:19,895 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:50:19,895 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,895 - distributed.worker - INFO -               Threads:                          4
2024-03-21 05:50:19,895 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-21 05:50:19,895 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-q3s1zk0t
2024-03-21 05:50:19,896 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f77ff555-1bd6-4785-b27d-70eb1f24607e
2024-03-21 05:50:19,897 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96d40f0e-3add-4af7-a7af-9f2d6b58585c
2024-03-21 05:50:19,897 - distributed.worker - INFO - Starting Worker plugin PreImport-bcb74b3f-8350-49c1-814c-2adcde4f164a
2024-03-21 05:50:19,897 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:19,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:19,906 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:19,907 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46305
2024-03-21 05:50:19,907 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46305
2024-03-21 05:50:19,907 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46783
2024-03-21 05:50:19,907 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:50:19,907 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,907 - distributed.worker - INFO -               Threads:                          4
2024-03-21 05:50:19,907 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-21 05:50:19,907 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6aogv9j6
2024-03-21 05:50:19,908 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77859b0a-be4a-4cfd-b6c9-53c2cb827ce7
2024-03-21 05:50:19,908 - distributed.worker - INFO - Starting Worker plugin PreImport-17cb170e-9756-4e9b-bef0-3c4e16411fcb
2024-03-21 05:50:19,908 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef1d19b4-0089-45f4-abaa-c942937431a6
2024-03-21 05:50:19,908 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:19,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:19,930 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:19,931 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35199
2024-03-21 05:50:19,931 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35199
2024-03-21 05:50:19,931 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34681
2024-03-21 05:50:19,931 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:50:19,931 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,931 - distributed.worker - INFO -               Threads:                          4
2024-03-21 05:50:19,931 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-21 05:50:19,931 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gx9zn11n
2024-03-21 05:50:19,932 - distributed.worker - INFO - Starting Worker plugin PreImport-e53189c0-5832-43c5-9dd7-08609cc5e9a2
2024-03-21 05:50:19,932 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b67ee17-c81f-4de2-a682-a7537418f14b
2024-03-21 05:50:19,932 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebf4dfcf-28b6-464a-95d6-0848d6199f3a
2024-03-21 05:50:19,932 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37419', status: init, memory: 0, processing: 0>
2024-03-21 05:50:19,955 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37419
2024-03-21 05:50:19,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43880
2024-03-21 05:50:19,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:19,957 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:50:19,957 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:50:19,993 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43491', status: init, memory: 0, processing: 0>
2024-03-21 05:50:19,994 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43491
2024-03-21 05:50:19,994 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43894
2024-03-21 05:50:19,995 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:19,995 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:50:19,995 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:19,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:50:20,001 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46305', status: init, memory: 0, processing: 0>
2024-03-21 05:50:20,002 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46305
2024-03-21 05:50:20,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52834
2024-03-21 05:50:20,003 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:20,003 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:50:20,003 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:20,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:50:20,013 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35199', status: init, memory: 0, processing: 0>
2024-03-21 05:50:20,014 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35199
2024-03-21 05:50:20,014 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52842
2024-03-21 05:50:20,015 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:20,015 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:50:20,015 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:20,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:50:20,022 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-21 05:50:20,022 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-21 05:50:20,022 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-21 05:50:20,022 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-21 05:50:20,027 - distributed.scheduler - INFO - Remove client Client-e46e85b3-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:20,027 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43852; closing.
2024-03-21 05:50:20,027 - distributed.scheduler - INFO - Remove client Client-e46e85b3-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:20,028 - distributed.scheduler - INFO - Close client connection: Client-e46e85b3-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:20,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35257'. Reason: nanny-close
2024-03-21 05:50:20,030 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:20,030 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38723'. Reason: nanny-close
2024-03-21 05:50:20,030 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39613'. Reason: nanny-close
2024-03-21 05:50:20,031 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:20,031 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37419. Reason: nanny-close
2024-03-21 05:50:20,031 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41879'. Reason: nanny-close
2024-03-21 05:50:20,031 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:20,031 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43491. Reason: nanny-close
2024-03-21 05:50:20,032 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46305. Reason: nanny-close
2024-03-21 05:50:20,033 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:50:20,033 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43880; closing.
2024-03-21 05:50:20,033 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:50:20,033 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:50:20,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37419', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000220.0336719')
2024-03-21 05:50:20,034 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:20,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52834; closing.
2024-03-21 05:50:20,034 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:20,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43894; closing.
2024-03-21 05:50:20,035 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:20,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46305', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000220.0354142')
2024-03-21 05:50:20,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43491', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000220.0358336')
2024-03-21 05:50:20,067 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:20,068 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35199. Reason: nanny-close
2024-03-21 05:50:20,070 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:50:20,070 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52842; closing.
2024-03-21 05:50:20,070 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000220.0703547')
2024-03-21 05:50:20,070 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:50:20,071 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:20,694 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:50:20,695 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:50:20,695 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:50:20,696 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-21 05:50:20,697 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-21 05:50:22,895 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:22,900 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40553 instead
  warnings.warn(
2024-03-21 05:50:22,904 - distributed.scheduler - INFO - State start
2024-03-21 05:50:22,927 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:22,928 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:50:22,929 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40553/status
2024-03-21 05:50:22,929 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:50:23,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45821'
2024-03-21 05:50:23,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44765'
2024-03-21 05:50:23,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38973'
2024-03-21 05:50:23,170 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35371'
2024-03-21 05:50:23,175 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40697'
2024-03-21 05:50:23,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34533'
2024-03-21 05:50:23,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35625'
2024-03-21 05:50:23,205 - distributed.scheduler - INFO - Receive client connection: Client-e761df99-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:23,210 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41483'
2024-03-21 05:50:23,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37042
2024-03-21 05:50:25,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:25,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:25,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:25,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:25,923 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:25,923 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:25,924 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39837
2024-03-21 05:50:25,924 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35313
2024-03-21 05:50:25,925 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39837
2024-03-21 05:50:25,925 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35313
2024-03-21 05:50:25,925 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42605
2024-03-21 05:50:25,925 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39305
2024-03-21 05:50:25,925 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:25,925 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:25,925 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:25,925 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:25,925 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:25,925 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:25,925 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:25,925 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:25,925 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0wk_jvgl
2024-03-21 05:50:25,925 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1ttrz6wn
2024-03-21 05:50:25,925 - distributed.worker - INFO - Starting Worker plugin PreImport-5bcc13ab-1b42-430f-b2f3-37e16f47180a
2024-03-21 05:50:25,925 - distributed.worker - INFO - Starting Worker plugin PreImport-22fad609-dc29-471f-964c-d00d997134df
2024-03-21 05:50:25,925 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b686c1ae-4470-4ac9-8a06-40bc734d124a
2024-03-21 05:50:25,925 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1eda01c-a356-443d-bc43-a19ba240a3e9
2024-03-21 05:50:25,926 - distributed.worker - INFO - Starting Worker plugin RMMSetup-316fdd1b-3303-4ad9-a690-26a2d72928ed
2024-03-21 05:50:26,059 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:26,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:26,061 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:26,061 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:26,066 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:26,066 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:26,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:26,068 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36205
2024-03-21 05:50:26,068 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36205
2024-03-21 05:50:26,068 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38515
2024-03-21 05:50:26,068 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:26,068 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:26,068 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:26,068 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:26,068 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:26,068 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cme9byqt
2024-03-21 05:50:26,068 - distributed.worker - INFO - Starting Worker plugin PreImport-0ddc95f5-24f4-408c-ba84-59eb11e91a68
2024-03-21 05:50:26,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1d72d46e-7e5b-4614-9afb-ffc690100c11
2024-03-21 05:50:26,069 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b61e056b-88da-4ed9-92f6-32176d7f0a87
2024-03-21 05:50:26,069 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46301
2024-03-21 05:50:26,069 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46301
2024-03-21 05:50:26,069 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43993
2024-03-21 05:50:26,069 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:26,069 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:26,070 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:26,070 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:26,070 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0blwpbru
2024-03-21 05:50:26,070 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dbd8a036-69f1-4674-966f-3421c1230d03
2024-03-21 05:50:26,070 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19a4bd25-9281-42ed-be72-bf6b5a2cfa42
2024-03-21 05:50:26,071 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:26,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:26,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:26,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45527
2024-03-21 05:50:26,074 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45527
2024-03-21 05:50:26,074 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40545
2024-03-21 05:50:26,074 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:26,074 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:26,074 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:26,074 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:26,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_8l3x2ki
2024-03-21 05:50:26,075 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59297584-441a-44d8-a0ca-d0800b7e4f58
2024-03-21 05:50:26,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:26,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:26,078 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:26,079 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39111
2024-03-21 05:50:26,080 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39111
2024-03-21 05:50:26,080 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36553
2024-03-21 05:50:26,080 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:26,080 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:26,080 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:26,080 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:26,080 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kompxbga
2024-03-21 05:50:26,080 - distributed.worker - INFO - Starting Worker plugin PreImport-9ebde282-1cc5-4987-8693-9447283f8ef7
2024-03-21 05:50:26,080 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3106e162-068a-4970-a9ee-aa572ffb3f16
2024-03-21 05:50:26,082 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:26,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44763
2024-03-21 05:50:26,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44763
2024-03-21 05:50:26,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36803
2024-03-21 05:50:26,084 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:26,084 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:26,084 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:26,084 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:26,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gdvx746v
2024-03-21 05:50:26,084 - distributed.worker - INFO - Starting Worker plugin PreImport-ac583be8-1273-4a19-accb-020a1c81d641
2024-03-21 05:50:26,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19dcb481-ba53-4eb8-8d18-e89800146b29
2024-03-21 05:50:26,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:26,091 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:26,100 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:26,101 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38519
2024-03-21 05:50:26,101 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38519
2024-03-21 05:50:26,101 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45853
2024-03-21 05:50:26,101 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:26,101 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:26,102 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:26,102 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:26,102 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qg0vz6b1
2024-03-21 05:50:26,102 - distributed.worker - INFO - Starting Worker plugin PreImport-ed62d79d-f7e6-4f8c-8f3e-7b79ff0c989a
2024-03-21 05:50:26,102 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f5b7f92-9223-4dc9-b187-cbcd33df15fa
2024-03-21 05:50:29,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-029ab1bb-8109-4e08-b432-1b3c1416cffd
2024-03-21 05:50:29,441 - distributed.worker - INFO - Starting Worker plugin PreImport-60832837-5385-47b7-992d-a8d508808667
2024-03-21 05:50:29,441 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,446 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,453 - distributed.worker - INFO - Starting Worker plugin PreImport-7092918a-f118-4335-acc6-c31cb6fb238b
2024-03-21 05:50:29,454 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,464 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45527', status: init, memory: 0, processing: 0>
2024-03-21 05:50:29,466 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45527
2024-03-21 05:50:29,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37062
2024-03-21 05:50:29,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:29,468 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:29,468 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,469 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:29,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36205', status: init, memory: 0, processing: 0>
2024-03-21 05:50:29,477 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36205
2024-03-21 05:50:29,477 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37072
2024-03-21 05:50:29,478 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46301', status: init, memory: 0, processing: 0>
2024-03-21 05:50:29,479 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46301
2024-03-21 05:50:29,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37074
2024-03-21 05:50:29,479 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:29,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:29,480 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:29,480 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,481 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:29,481 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:29,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:29,486 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a7b487f-cec8-4436-a59c-ccf3643831ba
2024-03-21 05:50:29,487 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,500 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39837', status: init, memory: 0, processing: 0>
2024-03-21 05:50:29,500 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39837
2024-03-21 05:50:29,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37082
2024-03-21 05:50:29,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:29,503 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:29,503 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,505 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:29,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd553b4c-69ee-4d42-90f6-c5681fb5d3e8
2024-03-21 05:50:29,511 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,511 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38519', status: init, memory: 0, processing: 0>
2024-03-21 05:50:29,511 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38519
2024-03-21 05:50:29,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37090
2024-03-21 05:50:29,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:29,513 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:29,514 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,515 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:29,532 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39111', status: init, memory: 0, processing: 0>
2024-03-21 05:50:29,532 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39111
2024-03-21 05:50:29,532 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37096
2024-03-21 05:50:29,533 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:29,534 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:29,534 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,535 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:29,549 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81f3b250-fa96-4732-8480-1186a2adfaed
2024-03-21 05:50:29,551 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,583 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44763', status: init, memory: 0, processing: 0>
2024-03-21 05:50:29,583 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44763
2024-03-21 05:50:29,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37108
2024-03-21 05:50:29,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:29,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:29,586 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:29,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:29,961 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:50:29,963 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-317aa2e1-dc66-407d-b355-2036c2dc0d09
2024-03-21 05:50:29,964 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35313. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:50:29,964 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:50:29,971 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:29,977 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:29,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45821'. Reason: nanny-instantiate-failed
2024-03-21 05:50:29,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:50:29,997 - distributed.nanny - INFO - Worker process 43422 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:50:30,001 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43454 parent=43254 started daemon>
2024-03-21 05:50:29,999 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:36976'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36976>: Stream is closed
2024-03-21 05:50:30,002 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43450 parent=43254 started daemon>
2024-03-21 05:50:30,002 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43445 parent=43254 started daemon>
2024-03-21 05:50:30,002 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43442 parent=43254 started daemon>
2024-03-21 05:50:30,002 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43436 parent=43254 started daemon>
2024-03-21 05:50:30,002 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43431 parent=43254 started daemon>
2024-03-21 05:50:30,002 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43426 parent=43254 started daemon>
2024-03-21 05:50:30,029 - distributed.core - INFO - Connection to tcp://127.0.0.1:37096 has been closed.
2024-03-21 05:50:30,029 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39111', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000230.0297537')
2024-03-21 05:50:30,030 - distributed.core - INFO - Connection to tcp://127.0.0.1:37074 has been closed.
2024-03-21 05:50:30,031 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46301', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000230.0309784')
2024-03-21 05:50:30,032 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37074>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-21 05:50:30,033 - distributed.core - INFO - Connection to tcp://127.0.0.1:37090 has been closed.
2024-03-21 05:50:30,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38519', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000230.0336196')
2024-03-21 05:50:30,034 - distributed.core - INFO - Connection to tcp://127.0.0.1:37082 has been closed.
2024-03-21 05:50:30,034 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39837', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000230.034504')
2024-03-21 05:50:30,035 - distributed.core - INFO - Connection to tcp://127.0.0.1:37062 has been closed.
2024-03-21 05:50:30,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45527', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000230.0351186')
2024-03-21 05:50:30,035 - distributed.core - INFO - Connection to tcp://127.0.0.1:37072 has been closed.
2024-03-21 05:50:30,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36205', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000230.0357473')
2024-03-21 05:50:30,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:37108 has been closed.
2024-03-21 05:50:30,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44763', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000230.037293')
2024-03-21 05:50:30,037 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:50:30,287 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43442 exit status was already read will report exitcode 255
2024-03-21 05:50:30,345 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43436 exit status was already read will report exitcode 255
2024-03-21 05:50:39,320 - distributed.scheduler - INFO - Remove client Client-e761df99-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:39,321 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37042; closing.
2024-03-21 05:50:39,321 - distributed.scheduler - INFO - Remove client Client-e761df99-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:39,323 - distributed.scheduler - INFO - Close client connection: Client-e761df99-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:39,325 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:50:39,325 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:50:39,326 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:50:39,328 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:50:39,329 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-21 05:50:41,734 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:41,738 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37579 instead
  warnings.warn(
2024-03-21 05:50:41,742 - distributed.scheduler - INFO - State start
2024-03-21 05:50:41,744 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0wk_jvgl', purging
2024-03-21 05:50:41,745 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_8l3x2ki', purging
2024-03-21 05:50:41,745 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-cme9byqt', purging
2024-03-21 05:50:41,745 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0blwpbru', purging
2024-03-21 05:50:41,746 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gdvx746v', purging
2024-03-21 05:50:41,746 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-kompxbga', purging
2024-03-21 05:50:41,746 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qg0vz6b1', purging
2024-03-21 05:50:41,768 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:41,769 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:50:41,769 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37579/status
2024-03-21 05:50:41,770 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:50:41,879 - distributed.scheduler - INFO - Receive client connection: Client-f29fce6c-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:41,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37982
2024-03-21 05:50:41,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44679'
2024-03-21 05:50:41,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45917'
2024-03-21 05:50:41,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42079'
2024-03-21 05:50:41,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38007'
2024-03-21 05:50:41,936 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42597'
2024-03-21 05:50:41,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42731'
2024-03-21 05:50:41,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45779'
2024-03-21 05:50:41,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33293'
2024-03-21 05:50:44,013 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:44,013 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:44,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:44,020 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42187
2024-03-21 05:50:44,020 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42187
2024-03-21 05:50:44,020 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39981
2024-03-21 05:50:44,020 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:44,020 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:44,020 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:44,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:44,020 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:44,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:44,021 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p05r_a3k
2024-03-21 05:50:44,021 - distributed.worker - INFO - Starting Worker plugin PreImport-e7d568a1-de2b-4229-8e57-73b58a306e2a
2024-03-21 05:50:44,021 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f43ed36b-4555-41fd-a7ff-b3a987d4d2d6
2024-03-21 05:50:44,021 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5805e2f5-7faf-4169-9641-18291060a466
2024-03-21 05:50:44,025 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:44,026 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44607
2024-03-21 05:50:44,026 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44607
2024-03-21 05:50:44,026 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42963
2024-03-21 05:50:44,026 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:44,026 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:44,026 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:44,026 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:44,026 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-02wedzjo
2024-03-21 05:50:44,027 - distributed.worker - INFO - Starting Worker plugin PreImport-e9cb1117-92e6-4916-9cb4-861a4c682439
2024-03-21 05:50:44,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7383a593-8f55-4738-ac64-56e12d10f946
2024-03-21 05:50:44,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:44,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:44,036 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:44,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:44,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:44,037 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45311
2024-03-21 05:50:44,037 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45311
2024-03-21 05:50:44,037 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37377
2024-03-21 05:50:44,037 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:44,037 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:44,037 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:44,037 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:44,037 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2y4cvtiz
2024-03-21 05:50:44,037 - distributed.worker - INFO - Starting Worker plugin PreImport-ebbae0e4-0e60-4caf-9f1d-b879f92f4178
2024-03-21 05:50:44,037 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a6842d7-876e-4487-844d-3d6d9dc06221
2024-03-21 05:50:44,038 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4191c135-542f-450b-b31d-71c086740a55
2024-03-21 05:50:44,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:44,041 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41721
2024-03-21 05:50:44,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41721
2024-03-21 05:50:44,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40171
2024-03-21 05:50:44,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:44,042 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:44,042 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:44,042 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:44,042 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0pqnau0t
2024-03-21 05:50:44,042 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f86bc6f8-b7df-4d72-91b1-656d4c1b72c7
2024-03-21 05:50:44,043 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:44,043 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:44,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14ea4b8e-cff5-445d-b271-7e9a205e3821
2024-03-21 05:50:44,045 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:44,045 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:44,047 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:44,048 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39503
2024-03-21 05:50:44,048 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39503
2024-03-21 05:50:44,048 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32987
2024-03-21 05:50:44,048 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:44,048 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:44,048 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:44,048 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:44,048 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-40yjq9q3
2024-03-21 05:50:44,048 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a225b65-da5e-49c1-98ef-0b710226266a
2024-03-21 05:50:44,049 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:44,050 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37983
2024-03-21 05:50:44,050 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37983
2024-03-21 05:50:44,050 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37927
2024-03-21 05:50:44,050 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:44,050 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:44,050 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:44,050 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:44,050 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x7u0w7b1
2024-03-21 05:50:44,050 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1188b62-25bd-4271-a1ae-4ff432d65ab1
2024-03-21 05:50:44,052 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:44,052 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:44,052 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:44,052 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:44,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:44,056 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:44,057 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35987
2024-03-21 05:50:44,057 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41297
2024-03-21 05:50:44,057 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35987
2024-03-21 05:50:44,057 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41297
2024-03-21 05:50:44,057 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40349
2024-03-21 05:50:44,057 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42099
2024-03-21 05:50:44,057 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:44,057 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:44,057 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:44,057 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:44,057 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:44,057 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:44,057 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:44,057 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:44,057 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x5x_1ilj
2024-03-21 05:50:44,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-443m5wjn
2024-03-21 05:50:44,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fce5626e-149c-40d5-bc9b-641b0e9864ad
2024-03-21 05:50:44,058 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6caffcf8-2fc9-429c-9e1c-943b4b3b324c
2024-03-21 05:50:47,982 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35ab9ca0-673c-40de-a562-9dcf13840b7f
2024-03-21 05:50:47,983 - distributed.worker - INFO - Starting Worker plugin PreImport-4278a6a5-543a-4ee6-8fcc-e78dc92d995f
2024-03-21 05:50:47,983 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,004 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39503', status: init, memory: 0, processing: 0>
2024-03-21 05:50:48,005 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39503
2024-03-21 05:50:48,005 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38072
2024-03-21 05:50:48,006 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:48,007 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:48,007 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:48,286 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9dc7d45-7119-4d95-a632-3136799f14c5
2024-03-21 05:50:48,287 - distributed.worker - INFO - Starting Worker plugin PreImport-e67f1498-ba3d-4588-9e7b-4b0281d75fb8
2024-03-21 05:50:48,288 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,325 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37983', status: init, memory: 0, processing: 0>
2024-03-21 05:50:48,326 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37983
2024-03-21 05:50:48,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38086
2024-03-21 05:50:48,328 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:48,329 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:48,329 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,331 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:48,346 - distributed.worker - INFO - Starting Worker plugin PreImport-dfb3f16e-378a-4f4f-bee9-7e2b3f2122de
2024-03-21 05:50:48,348 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,350 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a6d7ea83-f425-415f-a1da-52f018fcd024
2024-03-21 05:50:48,350 - distributed.worker - INFO - Starting Worker plugin PreImport-a1c83980-8e4a-4e7f-9fc6-c2b5c66aca49
2024-03-21 05:50:48,351 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,372 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35987', status: init, memory: 0, processing: 0>
2024-03-21 05:50:48,373 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35987
2024-03-21 05:50:48,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38102
2024-03-21 05:50:48,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:48,375 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:48,375 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:48,378 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41721', status: init, memory: 0, processing: 0>
2024-03-21 05:50:48,378 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41721
2024-03-21 05:50:48,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38098
2024-03-21 05:50:48,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:48,381 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:48,381 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:48,443 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,463 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45311', status: init, memory: 0, processing: 0>
2024-03-21 05:50:48,464 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45311
2024-03-21 05:50:48,464 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38110
2024-03-21 05:50:48,464 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:48,465 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:48,465 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:48,615 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-895e21ab-7e5b-4113-8215-c27dc99ead92
2024-03-21 05:50:48,616 - distributed.worker - INFO - Starting Worker plugin PreImport-e6dd1d72-78f1-48dc-ac5e-4cd5bd4a9d7e
2024-03-21 05:50:48,616 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,626 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea4b5bcb-cb56-4d86-892c-4081a9f17ba4
2024-03-21 05:50:48,628 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,636 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41297', status: init, memory: 0, processing: 0>
2024-03-21 05:50:48,636 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41297
2024-03-21 05:50:48,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38114
2024-03-21 05:50:48,637 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:48,638 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:48,638 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:48,659 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44607', status: init, memory: 0, processing: 0>
2024-03-21 05:50:48,660 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44607
2024-03-21 05:50:48,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38116
2024-03-21 05:50:48,661 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:48,662 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:48,662 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:48,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:49,103 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:49,134 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42187', status: init, memory: 0, processing: 0>
2024-03-21 05:50:49,135 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42187
2024-03-21 05:50:49,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38124
2024-03-21 05:50:49,136 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:49,138 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:49,138 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:49,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:49,221 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:50:49,222 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:50:49,222 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:50:49,222 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:50:49,222 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:50:49,223 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:50:49,223 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:50:49,223 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:50:49,227 - distributed.scheduler - INFO - Remove client Client-f29fce6c-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:49,227 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37982; closing.
2024-03-21 05:50:49,227 - distributed.scheduler - INFO - Remove client Client-f29fce6c-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:49,228 - distributed.scheduler - INFO - Close client connection: Client-f29fce6c-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:49,229 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44679'. Reason: nanny-close
2024-03-21 05:50:49,230 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:49,230 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45917'. Reason: nanny-close
2024-03-21 05:50:49,231 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:49,232 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42079'. Reason: nanny-close
2024-03-21 05:50:49,232 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42187. Reason: nanny-close
2024-03-21 05:50:49,232 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:49,232 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38007'. Reason: nanny-close
2024-03-21 05:50:49,232 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37983. Reason: nanny-close
2024-03-21 05:50:49,232 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:49,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42597'. Reason: nanny-close
2024-03-21 05:50:49,233 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45311. Reason: nanny-close
2024-03-21 05:50:49,233 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:49,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42731'. Reason: nanny-close
2024-03-21 05:50:49,233 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41297. Reason: nanny-close
2024-03-21 05:50:49,233 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:49,234 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45779'. Reason: nanny-close
2024-03-21 05:50:49,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44607. Reason: nanny-close
2024-03-21 05:50:49,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:49,234 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33293'. Reason: nanny-close
2024-03-21 05:50:49,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:50:49,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41721. Reason: nanny-close
2024-03-21 05:50:49,234 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:50:49,234 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38124; closing.
2024-03-21 05:50:49,234 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:50:49,235 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39503. Reason: nanny-close
2024-03-21 05:50:49,235 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:50:49,235 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42187', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000249.235203')
2024-03-21 05:50:49,235 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:50:49,235 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38110; closing.
2024-03-21 05:50:49,235 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35987. Reason: nanny-close
2024-03-21 05:50:49,236 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:49,236 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45311', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000249.236618')
2024-03-21 05:50:49,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:50:49,236 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:49,236 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:49,237 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:50:49,237 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38086; closing.
2024-03-21 05:50:49,237 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:49,237 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:50:49,237 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:50:49,238 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000249.238085')
2024-03-21 05:50:49,238 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:49,238 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38114; closing.
2024-03-21 05:50:49,238 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:49,239 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:49,239 - distributed.nanny - INFO - Worker closed
2024-03-21 05:50:49,239 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38110>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-21 05:50:49,241 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38116; closing.
2024-03-21 05:50:49,241 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41297', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000249.24152')
2024-03-21 05:50:49,241 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38072; closing.
2024-03-21 05:50:49,242 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38098; closing.
2024-03-21 05:50:49,242 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44607', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000249.2425683')
2024-03-21 05:50:49,243 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000249.2429767')
2024-03-21 05:50:49,243 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000249.2434187')
2024-03-21 05:50:49,243 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38102; closing.
2024-03-21 05:50:49,244 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35987', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000249.2442505')
2024-03-21 05:50:49,244 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:50:50,195 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:50:50,195 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:50:50,196 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:50:50,197 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:50:50,197 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-21 05:50:52,470 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:52,474 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:50:52,478 - distributed.scheduler - INFO - State start
2024-03-21 05:50:52,500 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:50:52,500 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:50:52,501 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:50:52,501 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:50:52,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38959'
2024-03-21 05:50:52,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38439'
2024-03-21 05:50:52,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45305'
2024-03-21 05:50:52,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43151'
2024-03-21 05:50:52,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38749'
2024-03-21 05:50:52,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35781'
2024-03-21 05:50:52,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36607'
2024-03-21 05:50:52,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33427'
2024-03-21 05:50:53,571 - distributed.scheduler - INFO - Receive client connection: Client-f900e8ed-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:50:53,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41508
2024-03-21 05:50:54,579 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:54,579 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:54,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:54,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:54,583 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:54,583 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:54,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:54,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34505
2024-03-21 05:50:54,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34505
2024-03-21 05:50:54,585 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40595
2024-03-21 05:50:54,585 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:54,585 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:54,585 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:54,585 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:54,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7zlokri6
2024-03-21 05:50:54,585 - distributed.worker - INFO - Starting Worker plugin PreImport-059b8268-e124-44c9-bf37-21e9f4830eb7
2024-03-21 05:50:54,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d29756e-d035-4ec4-87f9-53a9e7c79ace
2024-03-21 05:50:54,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:54,587 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32931
2024-03-21 05:50:54,587 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32931
2024-03-21 05:50:54,587 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40615
2024-03-21 05:50:54,587 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:54,587 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:54,588 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:54,588 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:54,588 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gb4vj1om
2024-03-21 05:50:54,588 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f70c8c45-09d5-451b-96c3-ba5951297480
2024-03-21 05:50:54,588 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:54,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33281
2024-03-21 05:50:54,589 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33281
2024-03-21 05:50:54,589 - distributed.worker - INFO - Starting Worker plugin RMMSetup-738f2022-83d3-4256-aaf1-21cb969968af
2024-03-21 05:50:54,589 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36395
2024-03-21 05:50:54,589 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:54,589 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:54,589 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:54,589 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:54,589 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vf3k8eaq
2024-03-21 05:50:54,589 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4086b55-ce25-4575-bd65-492950b12307
2024-03-21 05:50:54,596 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:54,596 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:54,600 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:54,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40817
2024-03-21 05:50:54,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40817
2024-03-21 05:50:54,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32937
2024-03-21 05:50:54,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:54,602 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:54,602 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:54,602 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:54,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uv76mc37
2024-03-21 05:50:54,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d2d6867-8d36-436a-b65f-e9b875491119
2024-03-21 05:50:54,602 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e447c7cd-ee86-4b79-9588-e5ab257e2f13
2024-03-21 05:50:54,606 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:54,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:54,611 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:54,612 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44127
2024-03-21 05:50:54,612 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44127
2024-03-21 05:50:54,612 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44071
2024-03-21 05:50:54,612 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:54,612 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:54,612 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:54,612 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:54,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b1e4gcsp
2024-03-21 05:50:54,612 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4675d1a3-60ea-42ef-ba0c-34e552c10d86
2024-03-21 05:50:54,613 - distributed.worker - INFO - Starting Worker plugin PreImport-a065a4d1-3858-4723-b3fd-ec307bffbf9b
2024-03-21 05:50:54,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8f1c6d5-601c-463e-9c7e-3d5f7a892771
2024-03-21 05:50:54,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:54,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:54,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:54,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:54,666 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:54,667 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42575
2024-03-21 05:50:54,667 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42575
2024-03-21 05:50:54,667 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37993
2024-03-21 05:50:54,667 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:54,667 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:54,667 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:54,667 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:54,668 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-issem4n7
2024-03-21 05:50:54,668 - distributed.worker - INFO - Starting Worker plugin PreImport-774e1581-d181-432d-86e3-abf509a22a25
2024-03-21 05:50:54,668 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af9c7b42-d032-486c-b4d2-b9278d5c3629
2024-03-21 05:50:54,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1296c35-b51c-495d-945b-09dcf277ae70
2024-03-21 05:50:54,669 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:50:54,669 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:50:54,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:54,670 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37147
2024-03-21 05:50:54,670 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37147
2024-03-21 05:50:54,670 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41593
2024-03-21 05:50:54,670 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:54,670 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:54,670 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:54,670 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:54,670 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g3rx21z5
2024-03-21 05:50:54,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b17b8d8-50ae-42a9-aae5-7bf0bd36d16d
2024-03-21 05:50:54,673 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:50:54,674 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34983
2024-03-21 05:50:54,674 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34983
2024-03-21 05:50:54,674 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33495
2024-03-21 05:50:54,675 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:50:54,675 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:54,675 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:50:54,675 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:50:54,675 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vbnlhrdw
2024-03-21 05:50:54,675 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f76c2e7-b63d-4dd8-85d8-5e9e28efe6df
2024-03-21 05:50:54,675 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f8a264a-e0f5-4bb6-b46a-78e4e408b230
2024-03-21 05:50:57,061 - distributed.worker - INFO - Starting Worker plugin PreImport-b40eded9-49b3-458f-a10c-26cbdde28cbd
2024-03-21 05:50:57,063 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,064 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3528fadb-b5da-4fc2-9810-db23c17c5d63
2024-03-21 05:50:57,064 - distributed.worker - INFO - Starting Worker plugin PreImport-c64d7eda-e4fe-4a3e-902c-875cc71940d1
2024-03-21 05:50:57,064 - distributed.worker - INFO - Starting Worker plugin PreImport-5956d070-ba81-4deb-8cb8-8e4c91b0b6e2
2024-03-21 05:50:57,065 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,065 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,076 - distributed.worker - INFO - Starting Worker plugin PreImport-a9aee35e-fc04-4101-9dd6-815ab4d1dc82
2024-03-21 05:50:57,076 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,076 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,079 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
2024-03-21 05:50:57,082 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-38fd346f-f5c7-4d34-a91e-b65b0626b92a
2024-03-21 05:50:57,083 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34505. Reason: failure-to-start-<class 'RuntimeError'>
2024-03-21 05:50:57,084 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1405d0ce-8f60-455c-876a-5b46afc67287
2024-03-21 05:50:57,083 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:50:57,085 - distributed.worker - INFO - Starting Worker plugin PreImport-1d0d7a1a-fa31-4cba-9bd1-9b576f67b9b5
2024-03-21 05:50:57,086 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,086 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,088 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33281', status: init, memory: 0, processing: 0>
2024-03-21 05:50:57,089 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33281
2024-03-21 05:50:57,089 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41538
2024-03-21 05:50:57,090 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:57,091 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:57,091 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,090 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:57,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:57,094 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40817', status: init, memory: 0, processing: 0>
2024-03-21 05:50:57,095 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40817
2024-03-21 05:50:57,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41542
2024-03-21 05:50:57,096 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:57,097 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:57,097 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,097 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32931', status: init, memory: 0, processing: 0>
2024-03-21 05:50:57,097 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32931
2024-03-21 05:50:57,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41530
2024-03-21 05:50:57,098 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:57,099 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42575', status: init, memory: 0, processing: 0>
2024-03-21 05:50:57,099 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:57,099 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42575
2024-03-21 05:50:57,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41546
2024-03-21 05:50:57,100 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:57,100 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,100 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34983', status: init, memory: 0, processing: 0>
2024-03-21 05:50:57,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:57,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34983
2024-03-21 05:50:57,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41558
2024-03-21 05:50:57,101 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:57,101 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:57,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:57,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:57,102 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:57,102 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:57,118 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44127', status: init, memory: 0, processing: 0>
2024-03-21 05:50:57,119 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44127
2024-03-21 05:50:57,119 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41562
2024-03-21 05:50:57,120 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:57,121 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:57,121 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:57,125 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37147', status: init, memory: 0, processing: 0>
2024-03-21 05:50:57,125 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37147
2024-03-21 05:50:57,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41576
2024-03-21 05:50:57,127 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:50:57,128 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:50:57,128 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:50:57,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:50:57,129 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:57,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38959'. Reason: nanny-instantiate-failed
2024-03-21 05:50:57,134 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:50:57,155 - distributed.nanny - INFO - Worker process 44008 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:50:57,159 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44039 parent=43840 started daemon>
2024-03-21 05:50:57,156 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:41430'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41430>: Stream is closed
2024-03-21 05:50:57,159 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44035 parent=43840 started daemon>
2024-03-21 05:50:57,160 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44031 parent=43840 started daemon>
2024-03-21 05:50:57,160 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44027 parent=43840 started daemon>
2024-03-21 05:50:57,160 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44022 parent=43840 started daemon>
2024-03-21 05:50:57,160 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44017 parent=43840 started daemon>
2024-03-21 05:50:57,160 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44012 parent=43840 started daemon>
2024-03-21 05:50:57,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:41546 has been closed.
2024-03-21 05:50:57,185 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42575', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000257.1858366')
2024-03-21 05:50:57,187 - distributed.core - INFO - Connection to tcp://127.0.0.1:41558 has been closed.
2024-03-21 05:50:57,187 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34983', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000257.187582')
2024-03-21 05:50:57,188 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41558>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41558>: Stream is closed
2024-03-21 05:50:57,189 - distributed.core - INFO - Connection to tcp://127.0.0.1:41538 has been closed.
2024-03-21 05:50:57,189 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33281', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000257.1895206')
2024-03-21 05:50:57,189 - distributed.core - INFO - Connection to tcp://127.0.0.1:41542 has been closed.
2024-03-21 05:50:57,190 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40817', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000257.189956')
2024-03-21 05:50:57,190 - distributed.core - INFO - Connection to tcp://127.0.0.1:41562 has been closed.
2024-03-21 05:50:57,190 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44127', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000257.1905541')
2024-03-21 05:50:57,191 - distributed.core - INFO - Connection to tcp://127.0.0.1:41576 has been closed.
2024-03-21 05:50:57,191 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37147', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000257.1913817')
2024-03-21 05:50:57,191 - distributed.core - INFO - Connection to tcp://127.0.0.1:41530 has been closed.
2024-03-21 05:50:57,191 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32931', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000257.1919188')
2024-03-21 05:50:57,192 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:50:57,451 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44039 exit status was already read will report exitcode 255
2024-03-21 05:50:57,487 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44031 exit status was already read will report exitcode 255
2024-03-21 05:50:57,769 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:41488'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41488>: Stream is closed
2024-03-21 05:50:57,770 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:41472'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41472>: Stream is closed
2024-03-21 05:51:09,649 - distributed.scheduler - INFO - Remove client Client-f900e8ed-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:51:09,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41508; closing.
2024-03-21 05:51:09,650 - distributed.scheduler - INFO - Remove client Client-f900e8ed-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:51:09,651 - distributed.scheduler - INFO - Close client connection: Client-f900e8ed-e746-11ee-a765-d8c49764f6bb
2024-03-21 05:51:09,651 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:51:09,652 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:51:09,652 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:51:09,654 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:51:09,654 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-21 05:51:11,875 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:51:11,879 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37989 instead
  warnings.warn(
2024-03-21 05:51:11,883 - distributed.scheduler - INFO - State start
2024-03-21 05:51:11,885 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-issem4n7', purging
2024-03-21 05:51:11,885 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-b1e4gcsp', purging
2024-03-21 05:51:11,885 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vf3k8eaq', purging
2024-03-21 05:51:11,886 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vbnlhrdw', purging
2024-03-21 05:51:11,886 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-g3rx21z5', purging
2024-03-21 05:51:11,886 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-uv76mc37', purging
2024-03-21 05:51:11,886 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gb4vj1om', purging
2024-03-21 05:51:12,015 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:51:12,016 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:51:12,016 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37989/status
2024-03-21 05:51:12,016 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:51:12,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37439'
2024-03-21 05:51:12,429 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34151'
2024-03-21 05:51:12,438 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37635'
2024-03-21 05:51:12,452 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35579'
2024-03-21 05:51:12,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43797'
2024-03-21 05:51:12,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35089'
2024-03-21 05:51:12,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42633'
2024-03-21 05:51:12,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37725'
2024-03-21 05:51:12,815 - distributed.scheduler - INFO - Receive client connection: Client-049e637f-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:12,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58824
2024-03-21 05:51:14,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:14,448 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:14,451 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:14,451 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:14,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:14,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:14,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:14,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:14,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:14,453 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44605
2024-03-21 05:51:14,453 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44605
2024-03-21 05:51:14,453 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37385
2024-03-21 05:51:14,453 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:14,453 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:14,453 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:14,453 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:14,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lrnzudov
2024-03-21 05:51:14,454 - distributed.worker - INFO - Starting Worker plugin PreImport-0aec9afa-1d9f-43b8-93d1-0b046b5b8ac5
2024-03-21 05:51:14,454 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61453368-82a0-45ce-9e83-cc087f9d9f96
2024-03-21 05:51:14,456 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:14,456 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:14,456 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:14,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:14,456 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:14,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45307
2024-03-21 05:51:14,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45307
2024-03-21 05:51:14,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41487
2024-03-21 05:51:14,457 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:14,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35271
2024-03-21 05:51:14,457 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:14,457 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:14,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35271
2024-03-21 05:51:14,457 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:14,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41883
2024-03-21 05:51:14,457 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rmfu72t7
2024-03-21 05:51:14,457 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:14,457 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:14,457 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:14,457 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:14,457 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lkelpriy
2024-03-21 05:51:14,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39827
2024-03-21 05:51:14,457 - distributed.worker - INFO - Starting Worker plugin PreImport-51310d05-cc65-4acb-8066-88e442e80800
2024-03-21 05:51:14,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39827
2024-03-21 05:51:14,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33891
2024-03-21 05:51:14,457 - distributed.worker - INFO - Starting Worker plugin RMMSetup-904c214b-4544-4b89-9141-5fe8654481ee
2024-03-21 05:51:14,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:14,458 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:14,458 - distributed.worker - INFO - Starting Worker plugin PreImport-e665a9a4-1b2f-45c3-af7b-133f770160fc
2024-03-21 05:51:14,458 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:14,458 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:14,458 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-472b24af-77ab-4be0-a401-6c609b5b7036
2024-03-21 05:51:14,458 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8veg_se6
2024-03-21 05:51:14,458 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:14,458 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:14,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e1eaca7-a321-4c23-a08d-2ae285dc1635
2024-03-21 05:51:14,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e491f53d-44f9-45e9-93b5-312c03e129e9
2024-03-21 05:51:14,460 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:14,461 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43743
2024-03-21 05:51:14,461 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43743
2024-03-21 05:51:14,461 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33079
2024-03-21 05:51:14,462 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:14,462 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:14,462 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:14,462 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:14,462 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e6ir58xa
2024-03-21 05:51:14,462 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0eba813-4b0b-4b00-91b0-fc762dcd2d9c
2024-03-21 05:51:14,462 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:14,462 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f549f3d4-163b-4502-b835-e8d9e7b98b23
2024-03-21 05:51:14,463 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46471
2024-03-21 05:51:14,463 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46471
2024-03-21 05:51:14,463 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41087
2024-03-21 05:51:14,463 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:14,463 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:14,463 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:14,463 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:14,463 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:14,463 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:14,463 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fan05x2i
2024-03-21 05:51:14,464 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-abe85f50-0a72-4e61-8cec-f56145d0b061
2024-03-21 05:51:14,464 - distributed.worker - INFO - Starting Worker plugin PreImport-8601c8b6-2d5f-4d8a-910e-3c60d5077237
2024-03-21 05:51:14,464 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c0fa5fe-abfd-4218-88db-bfd80b1eef6d
2024-03-21 05:51:14,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:14,468 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:14,468 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:14,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37073
2024-03-21 05:51:14,469 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37073
2024-03-21 05:51:14,469 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44783
2024-03-21 05:51:14,469 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:14,469 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:14,469 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:14,469 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:14,469 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bubo8t93
2024-03-21 05:51:14,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c480c4dd-2336-4326-a937-d406fecb845f
2024-03-21 05:51:14,469 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4776398c-7bff-4576-8275-d5ce6ff3ef51
2024-03-21 05:51:14,472 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:14,473 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37399
2024-03-21 05:51:14,473 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37399
2024-03-21 05:51:14,473 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38117
2024-03-21 05:51:14,473 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:14,474 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:14,474 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:14,474 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:14,474 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p0m0g9l5
2024-03-21 05:51:14,474 - distributed.worker - INFO - Starting Worker plugin PreImport-da7ac263-b968-4ece-9690-e658185eda44
2024-03-21 05:51:14,474 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f5f11e6-11e6-45e8-967a-ee1053c0ef60
2024-03-21 05:51:14,474 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8391b1a0-a361-4e9f-85db-e80f25680ce5
2024-03-21 05:51:16,842 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ac22d9b-971c-4583-8070-14af9cc1b4b2
2024-03-21 05:51:16,848 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,881 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44605', status: init, memory: 0, processing: 0>
2024-03-21 05:51:16,882 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44605
2024-03-21 05:51:16,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58844
2024-03-21 05:51:16,884 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:16,885 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:16,885 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:16,909 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,911 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb917ac6-913c-4c2a-a552-315ba43d28de
2024-03-21 05:51:16,912 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,925 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-21 05:51:16,928 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35271. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:51:16,929 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef7fbd50-0886-4efc-8a57-ad7542987fce
2024-03-21 05:51:16,929 - distributed.worker - INFO - Starting Worker plugin PreImport-bd48581f-735d-4b40-8a27-b9ad0e57934b
2024-03-21 05:51:16,928 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:51:16,930 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,933 - distributed.worker - INFO - Starting Worker plugin PreImport-19a61d32-b452-4974-8b97-ca93376c18d6
2024-03-21 05:51:16,934 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,937 - distributed.worker - INFO - Starting Worker plugin PreImport-04238f85-17fc-46ec-933c-53e0757de0a2
2024-03-21 05:51:16,936 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:16,938 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,939 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,946 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46471', status: init, memory: 0, processing: 0>
2024-03-21 05:51:16,947 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46471
2024-03-21 05:51:16,947 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58858
2024-03-21 05:51:16,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:16,949 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:16,950 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,950 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45307', status: init, memory: 0, processing: 0>
2024-03-21 05:51:16,951 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45307
2024-03-21 05:51:16,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58870
2024-03-21 05:51:16,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:16,952 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39827', status: init, memory: 0, processing: 0>
2024-03-21 05:51:16,952 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39827
2024-03-21 05:51:16,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58882
2024-03-21 05:51:16,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:16,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:16,954 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:16,954 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,954 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:16,954 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:16,956 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37073', status: init, memory: 0, processing: 0>
2024-03-21 05:51:16,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:16,956 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37073
2024-03-21 05:51:16,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58886
2024-03-21 05:51:16,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:16,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:16,958 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:16,960 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43743', status: init, memory: 0, processing: 0>
2024-03-21 05:51:16,960 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43743
2024-03-21 05:51:16,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58892
2024-03-21 05:51:16,961 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37399', status: init, memory: 0, processing: 0>
2024-03-21 05:51:16,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:16,962 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37399
2024-03-21 05:51:16,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58906
2024-03-21 05:51:16,962 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:16,962 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,963 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:16,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:16,964 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:16,964 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:16,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:16,984 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:16,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37439'. Reason: nanny-instantiate-failed
2024-03-21 05:51:16,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:51:17,043 - distributed.nanny - INFO - Worker process 44292 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:51:17,047 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44323 parent=44124 started daemon>
2024-03-21 05:51:17,045 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:58760'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58760>: Stream is closed
2024-03-21 05:51:17,048 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44319 parent=44124 started daemon>
2024-03-21 05:51:17,048 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44315 parent=44124 started daemon>
2024-03-21 05:51:17,048 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44311 parent=44124 started daemon>
2024-03-21 05:51:17,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44306 parent=44124 started daemon>
2024-03-21 05:51:17,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44301 parent=44124 started daemon>
2024-03-21 05:51:17,049 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44296 parent=44124 started daemon>
2024-03-21 05:51:17,076 - distributed.core - INFO - Connection to tcp://127.0.0.1:58892 has been closed.
2024-03-21 05:51:17,076 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43743', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000277.0767212')
2024-03-21 05:51:17,078 - distributed.core - INFO - Connection to tcp://127.0.0.1:58886 has been closed.
2024-03-21 05:51:17,079 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37073', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000277.0789113')
2024-03-21 05:51:17,079 - distributed.core - INFO - Connection to tcp://127.0.0.1:58882 has been closed.
2024-03-21 05:51:17,079 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39827', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000277.0795643')
2024-03-21 05:51:17,080 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58886>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58886>: Stream is closed
2024-03-21 05:51:17,081 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58882>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58882>: Stream is closed
2024-03-21 05:51:17,082 - distributed.core - INFO - Connection to tcp://127.0.0.1:58906 has been closed.
2024-03-21 05:51:17,082 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37399', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000277.0827725')
2024-03-21 05:51:17,083 - distributed.core - INFO - Connection to tcp://127.0.0.1:58844 has been closed.
2024-03-21 05:51:17,083 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44605', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000277.083551')
2024-03-21 05:51:17,085 - distributed.core - INFO - Connection to tcp://127.0.0.1:58870 has been closed.
2024-03-21 05:51:17,085 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45307', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000277.0855198')
2024-03-21 05:51:17,085 - distributed.core - INFO - Connection to tcp://127.0.0.1:58858 has been closed.
2024-03-21 05:51:17,086 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46471', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000277.0860822')
2024-03-21 05:51:17,086 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:51:17,086 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58870>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58870>: Stream is closed
2024-03-21 05:51:17,087 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58858>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58858>: Stream is closed
2024-03-21 05:51:17,379 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44319 exit status was already read will report exitcode 255
2024-03-21 05:51:28,863 - distributed.scheduler - INFO - Remove client Client-049e637f-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:28,863 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58824; closing.
2024-03-21 05:51:28,864 - distributed.scheduler - INFO - Remove client Client-049e637f-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:28,864 - distributed.scheduler - INFO - Close client connection: Client-049e637f-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:28,865 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:51:28,865 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:51:28,866 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:51:28,867 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:51:28,867 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-21 05:51:31,098 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:51:31,102 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36637 instead
  warnings.warn(
2024-03-21 05:51:31,106 - distributed.scheduler - INFO - State start
2024-03-21 05:51:31,108 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fan05x2i', purging
2024-03-21 05:51:31,108 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-e6ir58xa', purging
2024-03-21 05:51:31,109 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rmfu72t7', purging
2024-03-21 05:51:31,109 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-lrnzudov', purging
2024-03-21 05:51:31,110 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-bubo8t93', purging
2024-03-21 05:51:31,110 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8veg_se6', purging
2024-03-21 05:51:31,110 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-p0m0g9l5', purging
2024-03-21 05:51:31,131 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:51:31,132 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:51:31,132 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36637/status
2024-03-21 05:51:31,133 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:51:31,245 - distributed.scheduler - INFO - Receive client connection: Client-1019fc68-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:31,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36594
2024-03-21 05:51:31,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34849'
2024-03-21 05:51:31,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38821'
2024-03-21 05:51:31,514 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41317'
2024-03-21 05:51:31,525 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41209'
2024-03-21 05:51:31,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35057'
2024-03-21 05:51:31,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43631'
2024-03-21 05:51:31,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43183'
2024-03-21 05:51:31,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45817'
2024-03-21 05:51:33,886 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:33,887 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:33,894 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:33,896 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36397
2024-03-21 05:51:33,896 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36397
2024-03-21 05:51:33,896 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44971
2024-03-21 05:51:33,896 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:33,896 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:33,897 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:33,897 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:33,897 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ll2ztotn
2024-03-21 05:51:33,897 - distributed.worker - INFO - Starting Worker plugin PreImport-68d6d6ea-0e51-40a4-bc52-f449247a8394
2024-03-21 05:51:33,898 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57a96815-7573-492e-8e7e-98c77b8cace0
2024-03-21 05:51:33,905 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:33,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:33,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:33,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:33,911 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:33,913 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40557
2024-03-21 05:51:33,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40557
2024-03-21 05:51:33,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39381
2024-03-21 05:51:33,913 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:33,913 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:33,913 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:33,913 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:33,913 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1odz2gkt
2024-03-21 05:51:33,913 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfd9a667-c4a8-4249-9559-eea848be84c0
2024-03-21 05:51:33,913 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad274695-fc82-4fef-b17e-4f256cabf4fc
2024-03-21 05:51:33,916 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:33,918 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41467
2024-03-21 05:51:33,918 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41467
2024-03-21 05:51:33,918 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33789
2024-03-21 05:51:33,918 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:33,918 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:33,918 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:33,918 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:33,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s97a1n2v
2024-03-21 05:51:33,919 - distributed.worker - INFO - Starting Worker plugin PreImport-483f0cc2-257a-496c-8a69-27f171bbb483
2024-03-21 05:51:33,919 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49ebd60a-129e-498b-a8a7-9f8302a16fe3
2024-03-21 05:51:33,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:33,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:33,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:33,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:33,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:33,925 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:33,927 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:33,928 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:33,928 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36297
2024-03-21 05:51:33,928 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36297
2024-03-21 05:51:33,928 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39875
2024-03-21 05:51:33,928 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:33,928 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:33,929 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:33,929 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:33,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k5ugqjwo
2024-03-21 05:51:33,929 - distributed.worker - INFO - Starting Worker plugin PreImport-69d0f5f4-740c-4732-8e12-52bdb2e7a7bd
2024-03-21 05:51:33,929 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f16ac563-8836-4e04-9481-b7ff926d9322
2024-03-21 05:51:33,929 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38549
2024-03-21 05:51:33,929 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38549
2024-03-21 05:51:33,930 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38489
2024-03-21 05:51:33,930 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:33,930 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:33,930 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a829d04f-1d8f-4140-a87c-7bcd49156dd9
2024-03-21 05:51:33,930 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:33,930 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:33,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y50dywqd
2024-03-21 05:51:33,930 - distributed.worker - INFO - Starting Worker plugin PreImport-a800497f-2cec-4d42-bff2-94c7666d07e2
2024-03-21 05:51:33,930 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a455480e-b3e1-4b05-b49b-4d217afd1fea
2024-03-21 05:51:33,931 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:33,931 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bee2726a-e471-4b12-bb88-ef9c0498e74a
2024-03-21 05:51:33,932 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42767
2024-03-21 05:51:33,932 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42767
2024-03-21 05:51:33,932 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38661
2024-03-21 05:51:33,932 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:33,932 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:33,932 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:33,932 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:33,933 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yfrfys0p
2024-03-21 05:51:33,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9a83709-ce8d-4c62-ba1b-8b874e900d44
2024-03-21 05:51:33,933 - distributed.worker - INFO - Starting Worker plugin PreImport-7729b11a-9bc0-40b9-8583-ccc7b553ea43
2024-03-21 05:51:33,933 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05eb28dc-8b83-4e85-b4dd-ab9f9421ddb3
2024-03-21 05:51:33,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:33,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:33,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:33,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:33,941 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:33,941 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:33,942 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35591
2024-03-21 05:51:33,942 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46481
2024-03-21 05:51:33,942 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35591
2024-03-21 05:51:33,942 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46481
2024-03-21 05:51:33,942 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45295
2024-03-21 05:51:33,942 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:33,942 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44793
2024-03-21 05:51:33,942 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:33,942 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:33,942 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:33,942 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:33,943 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:33,943 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:33,943 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v5518qfb
2024-03-21 05:51:33,943 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:51:33,943 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2408hqdr
2024-03-21 05:51:33,943 - distributed.worker - INFO - Starting Worker plugin PreImport-b9e83114-7c1c-4575-bd26-77989b5a05b1
2024-03-21 05:51:33,943 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1670c74f-7ba4-459e-ad09-9cce7882e04e
2024-03-21 05:51:33,943 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b1bca6e-c3ca-4611-b31d-d3c266b10683
2024-03-21 05:51:33,943 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0a0f3ce-d18d-47a8-9062-2fc86555526b
2024-03-21 05:51:37,524 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8dd1ffd1-5a44-499c-b0a6-c6fa67e0fcdd
2024-03-21 05:51:37,527 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,532 - distributed.worker - INFO - Starting Worker plugin PreImport-91c0ed2b-07d2-408b-b233-18d69fbfcf68
2024-03-21 05:51:37,532 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,554 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40557', status: init, memory: 0, processing: 0>
2024-03-21 05:51:37,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40557
2024-03-21 05:51:37,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36682
2024-03-21 05:51:37,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:37,557 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:37,557 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,559 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:37,561 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0bdd3c16-8f91-42bd-855e-0ea6e11a4eef
2024-03-21 05:51:37,561 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,562 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41467', status: init, memory: 0, processing: 0>
2024-03-21 05:51:37,562 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41467
2024-03-21 05:51:37,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36686
2024-03-21 05:51:37,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:37,565 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:37,565 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:37,576 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,582 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35591', status: init, memory: 0, processing: 0>
2024-03-21 05:51:37,583 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35591
2024-03-21 05:51:37,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36696
2024-03-21 05:51:37,584 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:37,584 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:37,584 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,586 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:37,593 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,605 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36297', status: init, memory: 0, processing: 0>
2024-03-21 05:51:37,606 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36297
2024-03-21 05:51:37,606 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36706
2024-03-21 05:51:37,608 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:37,609 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:37,609 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:37,622 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38549', status: init, memory: 0, processing: 0>
2024-03-21 05:51:37,623 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38549
2024-03-21 05:51:37,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36708
2024-03-21 05:51:37,624 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:37,626 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:37,626 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,626 - distributed.worker - INFO - Starting Worker plugin PreImport-4beb96b9-7019-404d-963b-91b6a703a4f7
2024-03-21 05:51:37,628 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:37,632 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,654 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42767', status: init, memory: 0, processing: 0>
2024-03-21 05:51:37,655 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42767
2024-03-21 05:51:37,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36714
2024-03-21 05:51:37,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46481', status: init, memory: 0, processing: 0>
2024-03-21 05:51:37,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:37,656 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46481
2024-03-21 05:51:37,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36710
2024-03-21 05:51:37,657 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:37,657 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:37,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:37,659 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:37,659 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:37,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:38,065 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:51:38,067 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b45c764-148c-48b6-bd13-8543e34284b7
2024-03-21 05:51:38,067 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36397. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:51:38,067 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:51:38,071 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:38,098 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:51:38,101 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34849'. Reason: nanny-instantiate-failed
2024-03-21 05:51:38,101 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:51:38,117 - distributed.nanny - INFO - Worker process 44577 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:51:38,121 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44608 parent=44408 started daemon>
2024-03-21 05:51:38,118 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:36616'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36616>: Stream is closed
2024-03-21 05:51:38,121 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44604 parent=44408 started daemon>
2024-03-21 05:51:38,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44600 parent=44408 started daemon>
2024-03-21 05:51:38,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44596 parent=44408 started daemon>
2024-03-21 05:51:38,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44591 parent=44408 started daemon>
2024-03-21 05:51:38,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44586 parent=44408 started daemon>
2024-03-21 05:51:38,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44581 parent=44408 started daemon>
2024-03-21 05:51:38,147 - distributed.core - INFO - Connection to tcp://127.0.0.1:36696 has been closed.
2024-03-21 05:51:38,147 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35591', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000298.147748')
2024-03-21 05:51:38,148 - distributed.core - INFO - Connection to tcp://127.0.0.1:36714 has been closed.
2024-03-21 05:51:38,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42767', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000298.1488056')
2024-03-21 05:51:38,150 - distributed.core - INFO - Connection to tcp://127.0.0.1:36682 has been closed.
2024-03-21 05:51:38,150 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40557', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000298.1503515')
2024-03-21 05:51:38,151 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36682>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36682>: Stream is closed
2024-03-21 05:51:38,153 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36714>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-21 05:51:38,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:36686 has been closed.
2024-03-21 05:51:38,153 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41467', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000298.153673')
2024-03-21 05:51:38,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:36706 has been closed.
2024-03-21 05:51:38,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36297', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000298.1547024')
2024-03-21 05:51:38,155 - distributed.core - INFO - Connection to tcp://127.0.0.1:36708 has been closed.
2024-03-21 05:51:38,155 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38549', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000298.1553814')
2024-03-21 05:51:38,155 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36708>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36708>: Stream is closed
2024-03-21 05:51:38,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:36710 has been closed.
2024-03-21 05:51:38,156 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46481', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000298.1561615')
2024-03-21 05:51:38,156 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:51:38,228 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44600 exit status was already read will report exitcode 255
2024-03-21 05:51:38,255 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44591 exit status was already read will report exitcode 255
2024-03-21 05:51:38,399 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44608 exit status was already read will report exitcode 255
2024-03-21 05:51:47,270 - distributed.scheduler - INFO - Remove client Client-1019fc68-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:47,270 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36594; closing.
2024-03-21 05:51:47,271 - distributed.scheduler - INFO - Remove client Client-1019fc68-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:47,271 - distributed.scheduler - INFO - Close client connection: Client-1019fc68-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:47,272 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:51:47,272 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:51:47,273 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:51:47,274 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:51:47,274 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-21 05:51:49,385 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:51:49,389 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:51:49,392 - distributed.scheduler - INFO - State start
2024-03-21 05:51:49,394 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-v5518qfb', purging
2024-03-21 05:51:49,395 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-k5ugqjwo', purging
2024-03-21 05:51:49,395 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-s97a1n2v', purging
2024-03-21 05:51:49,396 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1odz2gkt', purging
2024-03-21 05:51:49,396 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-y50dywqd', purging
2024-03-21 05:51:49,396 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yfrfys0p', purging
2024-03-21 05:51:49,397 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2408hqdr', purging
2024-03-21 05:51:49,418 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:51:49,419 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:51:49,419 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:51:49,420 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:51:49,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40371'
2024-03-21 05:51:50,752 - distributed.scheduler - INFO - Receive client connection: Client-1af7c4b9-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:50,765 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60340
2024-03-21 05:51:51,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:51,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:51,890 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:51,891 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44743
2024-03-21 05:51:51,891 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44743
2024-03-21 05:51:51,891 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-21 05:51:51,891 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:51,891 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:51,891 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:51,891 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:51:51,891 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a9dz4ebl
2024-03-21 05:51:51,892 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c59e224f-601b-42af-b02c-7371a3ac865d
2024-03-21 05:51:51,892 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4033999-5619-42a7-bdcd-3ced01445b08
2024-03-21 05:51:51,892 - distributed.worker - INFO - Starting Worker plugin PreImport-87a51f4f-7005-4ec3-98d4-25ac742a5dd7
2024-03-21 05:51:51,892 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:51,945 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44743', status: init, memory: 0, processing: 0>
2024-03-21 05:51:51,947 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44743
2024-03-21 05:51:51,947 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60360
2024-03-21 05:51:51,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:51,949 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:51,949 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:51,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:51,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:51:51,995 - distributed.scheduler - INFO - Remove client Client-1af7c4b9-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:51,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60340; closing.
2024-03-21 05:51:51,995 - distributed.scheduler - INFO - Remove client Client-1af7c4b9-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:51,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40371'. Reason: nanny-close
2024-03-21 05:51:51,997 - distributed.scheduler - INFO - Close client connection: Client-1af7c4b9-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:51,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:51:51,998 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44743. Reason: nanny-close
2024-03-21 05:51:52,000 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:51:52,000 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60360; closing.
2024-03-21 05:51:52,001 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44743', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000312.0009055')
2024-03-21 05:51:52,001 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:51:52,001 - distributed.nanny - INFO - Worker closed
2024-03-21 05:51:52,562 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:51:52,562 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:51:52,562 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:51:52,563 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:51:52,564 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-21 05:51:56,835 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:51:56,839 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:51:56,843 - distributed.scheduler - INFO - State start
2024-03-21 05:51:56,864 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:51:56,865 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:51:56,866 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:51:56,866 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:51:57,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46071'
2024-03-21 05:51:57,294 - distributed.scheduler - INFO - Receive client connection: Client-1f6833d6-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:57,304 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60472
2024-03-21 05:51:58,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:51:58,905 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:51:59,492 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:51:59,493 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43637
2024-03-21 05:51:59,493 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43637
2024-03-21 05:51:59,493 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41651
2024-03-21 05:51:59,494 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:51:59,494 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:59,494 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:51:59,494 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:51:59,494 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yw58d6mt
2024-03-21 05:51:59,494 - distributed.worker - INFO - Starting Worker plugin PreImport-2b556f06-13e7-4170-aa8c-5ef84edbf0e9
2024-03-21 05:51:59,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3aa54a1-a0a0-4593-a164-fb204feecb16
2024-03-21 05:51:59,495 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec7b6977-f738-4cf4-a9bb-b4a0a3cfd353
2024-03-21 05:51:59,496 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:59,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43637', status: init, memory: 0, processing: 0>
2024-03-21 05:51:59,547 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43637
2024-03-21 05:51:59,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60490
2024-03-21 05:51:59,548 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:51:59,549 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:51:59,549 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:51:59,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:51:59,645 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:51:59,647 - distributed.scheduler - INFO - Remove client Client-1f6833d6-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:59,648 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60472; closing.
2024-03-21 05:51:59,648 - distributed.scheduler - INFO - Remove client Client-1f6833d6-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:59,648 - distributed.scheduler - INFO - Close client connection: Client-1f6833d6-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:51:59,649 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46071'. Reason: nanny-close
2024-03-21 05:51:59,650 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:51:59,651 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43637. Reason: nanny-close
2024-03-21 05:51:59,653 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60490; closing.
2024-03-21 05:51:59,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:51:59,653 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43637', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000319.6537778')
2024-03-21 05:51:59,654 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:51:59,654 - distributed.nanny - INFO - Worker closed
2024-03-21 05:52:00,314 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:52:00,315 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:52:00,315 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:52:00,316 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:52:00,316 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-21 05:52:02,527 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:02,532 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41029 instead
  warnings.warn(
2024-03-21 05:52:02,536 - distributed.scheduler - INFO - State start
2024-03-21 05:52:02,559 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:02,560 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:52:02,560 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41029/status
2024-03-21 05:52:02,561 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:52:05,042 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:35486'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35486>: Stream is closed
2024-03-21 05:52:05,358 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:52:05,358 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:52:05,359 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:52:05,359 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:52:05,360 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-21 05:52:07,597 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:07,602 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43179 instead
  warnings.warn(
2024-03-21 05:52:07,606 - distributed.scheduler - INFO - State start
2024-03-21 05:52:07,628 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:07,629 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-21 05:52:07,629 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43179/status
2024-03-21 05:52:07,630 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:52:07,773 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37847'
2024-03-21 05:52:08,100 - distributed.scheduler - INFO - Receive client connection: Client-25cd9ba4-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:08,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39002
2024-03-21 05:52:09,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:09,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:09,486 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:09,487 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40949
2024-03-21 05:52:09,487 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40949
2024-03-21 05:52:09,487 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36265
2024-03-21 05:52:09,487 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:52:09,487 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:09,487 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:09,487 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:52:09,487 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-z9d487je
2024-03-21 05:52:09,488 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4075cee9-9d70-4bab-8752-47a21c119e9d
2024-03-21 05:52:09,488 - distributed.worker - INFO - Starting Worker plugin PreImport-16c76a5c-ae88-4de8-b0de-2bdda047da96
2024-03-21 05:52:09,488 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af8545df-cc0c-4aae-bc4e-95e3a69e648c
2024-03-21 05:52:09,488 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:09,537 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40949', status: init, memory: 0, processing: 0>
2024-03-21 05:52:09,538 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40949
2024-03-21 05:52:09,538 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39020
2024-03-21 05:52:09,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:52:09,539 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:52:09,540 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:09,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:52:09,638 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:52:09,641 - distributed.scheduler - INFO - Remove client Client-25cd9ba4-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:09,641 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39002; closing.
2024-03-21 05:52:09,641 - distributed.scheduler - INFO - Remove client Client-25cd9ba4-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:09,642 - distributed.scheduler - INFO - Close client connection: Client-25cd9ba4-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:09,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37847'. Reason: nanny-close
2024-03-21 05:52:09,643 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:52:09,645 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40949. Reason: nanny-close
2024-03-21 05:52:09,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:52:09,646 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39020; closing.
2024-03-21 05:52:09,647 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40949', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000329.6470678')
2024-03-21 05:52:09,647 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:52:09,648 - distributed.nanny - INFO - Worker closed
2024-03-21 05:52:10,107 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:52:10,108 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:52:10,108 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:52:10,109 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-21 05:52:10,110 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-03-21 05:52:12,292 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:12,296 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38657 instead
  warnings.warn(
2024-03-21 05:52:12,300 - distributed.scheduler - INFO - State start
2024-03-21 05:52:12,321 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:12,322 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:52:12,323 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38657/status
2024-03-21 05:52:12,323 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:52:12,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46191'
2024-03-21 05:52:12,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36499'
2024-03-21 05:52:12,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42497'
2024-03-21 05:52:12,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33761'
2024-03-21 05:52:12,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45539'
2024-03-21 05:52:12,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45155'
2024-03-21 05:52:12,579 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40313'
2024-03-21 05:52:12,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43391'
2024-03-21 05:52:13,900 - distributed.scheduler - INFO - Receive client connection: Client-289a0246-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:13,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38808
2024-03-21 05:52:14,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:14,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:14,351 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:14,352 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35061
2024-03-21 05:52:14,352 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35061
2024-03-21 05:52:14,352 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39705
2024-03-21 05:52:14,352 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:14,352 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:14,352 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:14,352 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:52:14,352 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ybdnnebx
2024-03-21 05:52:14,353 - distributed.worker - INFO - Starting Worker plugin PreImport-62114e8b-0530-429c-ba3c-ca8269862092
2024-03-21 05:52:14,353 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7032b932-e9ae-4417-8308-b9e6a44d3db3
2024-03-21 05:52:14,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a92800b2-3bdb-46c6-ab49-6c96501126d3
2024-03-21 05:52:14,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:14,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:14,580 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:14,581 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:14,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:14,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33765
2024-03-21 05:52:14,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33765
2024-03-21 05:52:14,583 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38109
2024-03-21 05:52:14,583 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:14,583 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:14,583 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:14,583 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:52:14,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6627uq35
2024-03-21 05:52:14,584 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d0345c8-baae-47da-800c-704707db9daa
2024-03-21 05:52:14,584 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2fc5b091-ebb3-4971-ad6e-6b24149e59a5
2024-03-21 05:52:14,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:14,586 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35721
2024-03-21 05:52:14,586 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35721
2024-03-21 05:52:14,586 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43211
2024-03-21 05:52:14,586 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:14,586 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:14,587 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:14,587 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:52:14,587 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-duwrwnka
2024-03-21 05:52:14,587 - distributed.worker - INFO - Starting Worker plugin PreImport-ba6bec79-094f-47c6-af55-e17cf4da3232
2024-03-21 05:52:14,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d7f978a-3a37-4766-9090-45de0dbcc273
2024-03-21 05:52:14,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:14,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:14,595 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:14,596 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42877
2024-03-21 05:52:14,596 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42877
2024-03-21 05:52:14,596 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39887
2024-03-21 05:52:14,596 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:14,596 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:14,596 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:14,596 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:52:14,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gms5xr5u
2024-03-21 05:52:14,597 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e3aba83-55aa-4c2f-b223-8e673580031b
2024-03-21 05:52:14,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2050d58-bf42-4f85-afc5-10a4fa20b926
2024-03-21 05:52:14,599 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:14,599 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:14,602 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:14,602 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:14,602 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:14,602 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:14,602 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:14,603 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:14,604 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:14,604 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33303
2024-03-21 05:52:14,604 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33303
2024-03-21 05:52:14,605 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45177
2024-03-21 05:52:14,605 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:14,605 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:14,605 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:14,605 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:52:14,605 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r410_qcr
2024-03-21 05:52:14,605 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0bc2de88-9720-4221-9a7b-dd4361536796
2024-03-21 05:52:14,607 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:14,607 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:14,607 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:14,607 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36839
2024-03-21 05:52:14,608 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36839
2024-03-21 05:52:14,608 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41297
2024-03-21 05:52:14,608 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43265
2024-03-21 05:52:14,608 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:14,608 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:14,608 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43265
2024-03-21 05:52:14,608 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:14,608 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46831
2024-03-21 05:52:14,608 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39945
2024-03-21 05:52:14,608 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:52:14,608 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:14,608 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39945
2024-03-21 05:52:14,608 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qs199iqd
2024-03-21 05:52:14,608 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:14,608 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33811
2024-03-21 05:52:14,608 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:14,608 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:14,608 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:52:14,608 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:14,608 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-blzwxryl
2024-03-21 05:52:14,608 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:14,608 - distributed.worker - INFO - Starting Worker plugin RMMSetup-485ff3c5-566e-499b-847c-ae9f10b92202
2024-03-21 05:52:14,608 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:52:14,608 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iopeea07
2024-03-21 05:52:14,608 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db51a566-4df1-4003-83ee-8ba114e691ba
2024-03-21 05:52:14,608 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d44be04-9386-4373-8509-68e4fd5d1b9e
2024-03-21 05:52:14,610 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b41752e-3e03-4aef-9f6f-503bf26fb48d
2024-03-21 05:52:16,090 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:52:16,092 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35061. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:52:16,092 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:52:16,097 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:52:16,120 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:52:16,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46191'. Reason: nanny-instantiate-failed
2024-03-21 05:52:16,124 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:52:16,268 - distributed.nanny - INFO - Worker process 45675 was killed by signal 15
2024-03-21 05:52:16,270 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38742'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38742>: Stream is closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:52:16,273 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45706 parent=45507 started daemon>
2024-03-21 05:52:16,273 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45702 parent=45507 started daemon>
2024-03-21 05:52:16,273 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45698 parent=45507 started daemon>
2024-03-21 05:52:16,273 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45694 parent=45507 started daemon>
2024-03-21 05:52:16,273 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45690 parent=45507 started daemon>
2024-03-21 05:52:16,273 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45684 parent=45507 started daemon>
2024-03-21 05:52:16,274 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45679 parent=45507 started daemon>
2024-03-21 05:52:16,466 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45702 exit status was already read will report exitcode 255
2024-03-21 05:52:16,686 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38802'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38802>: Stream is closed
2024-03-21 05:52:16,687 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38788'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38788>: Stream is closed
2024-03-21 05:52:16,687 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38784'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38784>: Stream is closed
2024-03-21 05:52:16,688 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38776'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38776>: Stream is closed
2024-03-21 05:52:16,688 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38764'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38764>: Stream is closed
2024-03-21 05:52:16,688 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38762'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38762>: Stream is closed
2024-03-21 05:52:16,689 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38750'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38750>: Stream is closed
2024-03-21 05:52:29,938 - distributed.scheduler - INFO - Remove client Client-289a0246-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:29,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38808; closing.
2024-03-21 05:52:29,938 - distributed.scheduler - INFO - Remove client Client-289a0246-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:29,939 - distributed.scheduler - INFO - Close client connection: Client-289a0246-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:29,940 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:52:29,940 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:52:29,941 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:52:29,941 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:52:29,942 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-21 05:52:32,117 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:32,122 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:52:32,125 - distributed.scheduler - INFO - State start
2024-03-21 05:52:32,127 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-blzwxryl', purging
2024-03-21 05:52:32,128 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6627uq35', purging
2024-03-21 05:52:32,128 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qs199iqd', purging
2024-03-21 05:52:32,129 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-gms5xr5u', purging
2024-03-21 05:52:32,129 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-r410_qcr', purging
2024-03-21 05:52:32,129 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-duwrwnka', purging
2024-03-21 05:52:32,130 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-iopeea07', purging
2024-03-21 05:52:32,160 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:32,161 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:52:32,161 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:52:32,162 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:52:32,320 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39225'
2024-03-21 05:52:33,591 - distributed.scheduler - INFO - Receive client connection: Client-347a1ef1-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:33,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38674
2024-03-21 05:52:34,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:34,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:34,100 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:34,100 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40967
2024-03-21 05:52:34,100 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40967
2024-03-21 05:52:34,101 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42315
2024-03-21 05:52:34,101 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:34,101 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:34,101 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:34,101 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:52:34,101 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-giaje8cw
2024-03-21 05:52:34,101 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c94dd59-38d7-45f9-9918-10ff9d29ff24
2024-03-21 05:52:34,427 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:52:34,428 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98a602ed-4b48-4963-a624-88e78b150d08
2024-03-21 05:52:34,429 - distributed.worker - INFO - Starting Worker plugin PreImport-683a40fc-5270-4a3b-ac2e-88833f2ebf96
2024-03-21 05:52:34,430 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40967. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:52:34,430 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:52:34,436 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:52:34,451 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:52:34,454 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39225'. Reason: nanny-instantiate-failed
2024-03-21 05:52:34,455 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:52:34,512 - distributed.nanny - INFO - Worker process 45938 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:52:34,514 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:38658'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38658>: Stream is closed
2024-03-21 05:52:43,626 - distributed.scheduler - INFO - Remove client Client-347a1ef1-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:43,626 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38674; closing.
2024-03-21 05:52:43,626 - distributed.scheduler - INFO - Remove client Client-347a1ef1-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:43,627 - distributed.scheduler - INFO - Close client connection: Client-347a1ef1-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:43,628 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:52:43,628 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:52:43,629 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:52:43,630 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:52:43,630 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-21 05:52:45,841 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:45,845 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:52:45,848 - distributed.scheduler - INFO - State start
2024-03-21 05:52:45,868 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:52:45,869 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:52:45,870 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:52:45,870 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:52:46,011 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43973'
2024-03-21 05:52:47,109 - distributed.scheduler - INFO - Receive client connection: Client-3c990750-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:47,122 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44672
2024-03-21 05:52:47,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:52:47,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:52:47,688 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:52:47,689 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39491
2024-03-21 05:52:47,689 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39491
2024-03-21 05:52:47,689 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43031
2024-03-21 05:52:47,689 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:52:47,689 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:52:47,689 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:52:47,689 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:52:47,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-msxmc1f5
2024-03-21 05:52:47,689 - distributed.worker - INFO - Starting Worker plugin PreImport-a23f60a9-6dde-4a03-b4aa-91e84d6b49e6
2024-03-21 05:52:47,690 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ec65192-a3bc-481b-8780-a44e5516ee93
2024-03-21 05:52:48,004 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:52:48,004 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a096e09-916c-40ae-ba5b-c516f4a081d2
2024-03-21 05:52:48,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39491. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:52:48,005 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:52:48,007 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:52:48,037 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:52:48,041 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43973'. Reason: nanny-instantiate-failed
2024-03-21 05:52:48,041 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:52:48,093 - distributed.nanny - INFO - Worker process 46121 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:52:48,094 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44670'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44670>: Stream is closed
2024-03-21 05:52:57,171 - distributed.scheduler - INFO - Remove client Client-3c990750-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:57,171 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44672; closing.
2024-03-21 05:52:57,172 - distributed.scheduler - INFO - Remove client Client-3c990750-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:57,172 - distributed.scheduler - INFO - Close client connection: Client-3c990750-e747-11ee-a765-d8c49764f6bb
2024-03-21 05:52:57,173 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:52:57,173 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:52:57,174 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:52:57,175 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:52:57,175 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] 2024-03-21 05:53:16,797 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:53:16,802 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:53:16,814 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:53:17,389 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f1777d558b0>>, <Task finished name='Task-39' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-547' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-559' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] 2024-03-21 05:53:32,646 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:53:32,650 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:53:32,659 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:53:33,257 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f6f1e978340>>, <Task finished name='Task-35' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-666' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-677' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41203 instead
  warnings.warn(
2024-03-21 05:53:45,672 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:53:45,677 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:53:45,719 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:53:46,261 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7efc3a43f550>>, <Task finished name='Task-38' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-662' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-673' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44451 instead
  warnings.warn(
2024-03-21 05:53:57,691 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:53:57,696 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:53:57,713 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:53:58,140 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fbefb76d550>>, <Task finished name='Task-41' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-597' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-609' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [999] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_UNKNOWN
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43951 instead
  warnings.warn(
2024-03-21 05:54:04,250 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-7:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32951 instead
  warnings.warn(
[1711000447.953873] [dgx13:47731:0]            sock.c:481  UCX  ERROR bind(fd=180 addr=0.0.0.0:50112) failed: Address already in use
2024-03-21 05:54:12,571 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:54:12,576 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:54:12,607 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
[1711000452.765757] [dgx13:47830:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:52888) failed: Address already in use
[1711000452.997312] [dgx13:47844:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:58184) failed: Address already in use
2024-03-21 05:54:13,286 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f77f8611340>>, <Task finished name='Task-42' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-572' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-583' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41233 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] 2024-03-21 05:54:35,530 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-11:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 37, in _test_local_cluster
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37731 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35157 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35143 instead
  warnings.warn(
Process SpawnProcess-16:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] Process SpawnProcess-17:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] Process SpawnProcess-18:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] Process SpawnProcess-22:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44095 instead
  warnings.warn(
2024-03-21 05:56:48,411 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #012] ep: 0x7f5a713060c0, tag: 0x6d67ce1bef6f1c6b, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #012] ep: 0x7f5a713060c0, tag: 0x6d67ce1bef6f1c6b, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
Process SpawnProcess-23:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45727 instead
  warnings.warn(
Process SpawnProcess-24:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] 2024-03-21 05:57:02,488 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39825 instead
  warnings.warn(
2024-03-21 05:57:04,706 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] 2024-03-21 05:57:06,728 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35557 instead
  warnings.warn(
2024-03-21 05:57:09,691 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] 2024-03-21 05:57:12,672 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37967 instead
  warnings.warn(
2024-03-21 05:57:15,718 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] Process SpawnProcess-34:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] Process SpawnProcess-35:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] Process SpawnProcess-36:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36177 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34163 instead
  warnings.warn(
Process SpawnProcess-40:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37259 instead
  warnings.warn(
Process SpawnProcess-41:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40915 instead
  warnings.warn(
[1711000758.279764] [dgx13:55571:0]            sock.c:481  UCX  ERROR bind(fd=135 addr=0.0.0.0:38083) failed: Address already in use
Process SpawnProcess-42:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39067 instead
  warnings.warn(
2024-03-21 05:59:27,805 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-43:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34571 instead
  warnings.warn(
2024-03-21 05:59:29,944 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-44:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35187 instead
  warnings.warn(
2024-03-21 05:59:32,035 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-45:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37759 instead
  warnings.warn(
2024-03-21 05:59:36,580 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-46:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37695 instead
  warnings.warn(
2024-03-21 05:59:41,726 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-47:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32989 instead
  warnings.warn(
2024-03-21 05:59:46,591 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-48:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42301 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44553 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39107 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41079 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33691 instead
  warnings.warn(
Process SpawnProcess-54:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4671, in update_graph
    graph = deserialize(graph_header, graph_frames).data
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 439, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 101, in pickle_loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 281, in _test_dataframe_shuffle_merge
    got = ddf1.merge(ddf2, on="key").set_index("key").compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/sorting.py", line 46, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cudf/core.py", line 212, in set_index
    return super().set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 5551, in set_index
    return set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 259, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 262, in set_index
    divisions, mins, maxes, presorted = _calculate_divisions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 61, in _calculate_divisions
    divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2243, in _gather
    raise exception.with_traceback(traceback)
RuntimeError: Error during deserialization of the task graph. This frequently
occurs if the Scheduler and Client have different environments.
For more information, see
https://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments

FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33285 instead
  warnings.warn(
Process SpawnProcess-55:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 273, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37667 instead
  warnings.warn(
Process SpawnProcess-56:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 273, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41191 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32905 instead
  warnings.warn(
2024-03-21 06:02:22,584 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-03-21 06:02:22,585 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43789 instead
  warnings.warn(
[1711000948.001437] [dgx13:58119:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:47269) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33773 instead
  warnings.warn(
Process SpawnProcess-60:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 273, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41137 instead
  warnings.warn(
Process SpawnProcess-61:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 273, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34211 instead
  warnings.warn(
Process SpawnProcess-62:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 273, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46257 instead
  warnings.warn(
2024-03-21 06:02:59,236 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-63:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35457 instead
  warnings.warn(
2024-03-21 06:03:01,104 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-64:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45449 instead
  warnings.warn(
2024-03-21 06:03:02,968 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-65:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36793 instead
  warnings.warn(
2024-03-21 06:03:05,840 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-66:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43479 instead
  warnings.warn(
2024-03-21 06:03:08,685 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-67:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36273 instead
  warnings.warn(
2024-03-21 06:03:11,512 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-68:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42033 instead
  warnings.warn(
Process SpawnProcess-69:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 312, in _test_jit_unspill
    df = cudf.DataFrame.from_pandas(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35855 instead
  warnings.warn(
2024-03-21 06:03:20,516 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-70:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 302, in _test_jit_unspill
    with dask_cuda.LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39315 instead
  warnings.warn(
2024-03-21 06:03:23,373 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-71:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 302, in _test_jit_unspill
    with dask_cuda.LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] 2024-03-21 06:03:42,441 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 06:03:42,446 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] SKIPPED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] 2024-03-21 06:03:48,403 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-03-21 06:03:48,407 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:33775', name: 4, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 939, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32961 instead
  warnings.warn(
2024-03-21 06:03:51,372 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-107:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 31, in _test_initialize_ucx_tcp
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40645 instead
  warnings.warn(
2024-03-21 06:03:53,819 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-108:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 78, in _test_initialize_ucx_nvlink
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43997 instead
  warnings.warn(
2024-03-21 06:03:56,243 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-109:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 126, in _test_initialize_ucx_infiniband
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34907 instead
  warnings.warn(
2024-03-21 06:03:58,765 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-110:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 176, in _test_initialize_ucx_all
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] 2024-03-21 06:04:08,799 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 06:04:08,812 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] 2024-03-21 06:04:15,216 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 06:04:15,224 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-03-21 06:04:27,705 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:27,706 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:27,880 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:27,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:27,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:27,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:27,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:27,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:27,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:27,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:27,972 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:27,972 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:28,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:28,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:28,022 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:28,022 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:28,292 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:28,294 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38205
2024-03-21 06:04:28,294 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38205
2024-03-21 06:04:28,295 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39887
2024-03-21 06:04:28,295 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,295 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,295 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:28,295 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3cft6l77
2024-03-21 06:04:28,296 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df331d55-8e8c-4633-838a-43239c11253a
2024-03-21 06:04:28,296 - distributed.worker - INFO - Starting Worker plugin PreImport-686335e7-c34d-418e-8925-c7afd8952fc2
2024-03-21 06:04:28,296 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f076fb2b-e77c-446b-96be-a330eb5d8862
2024-03-21 06:04:28,297 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:04:28,361 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,361 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,363 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46501
2024-03-21 06:04:28,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:28,483 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41055
2024-03-21 06:04:28,483 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41055
2024-03-21 06:04:28,483 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35645
2024-03-21 06:04:28,483 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,483 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,483 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:28,484 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ubnktnsx
2024-03-21 06:04:28,484 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fec1b73e-fd8b-4042-9b3c-14e4c09389d6
2024-03-21 06:04:28,485 - distributed.worker - INFO - Starting Worker plugin PreImport-a75e6d3d-f981-4213-8fe1-211f1cc49eca
2024-03-21 06:04:28,485 - distributed.worker - INFO - Starting Worker plugin RMMSetup-90477096-12c8-45ae-86a3-452c62be996e
2024-03-21 06:04:28,485 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:28,494 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38369
2024-03-21 06:04:28,494 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38369
2024-03-21 06:04:28,494 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39297
2024-03-21 06:04:28,494 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,494 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,494 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:28,494 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zpchkm6q
2024-03-21 06:04:28,494 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4128bec-d458-40e7-bff9-f2f6c1b17c71
2024-03-21 06:04:28,495 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-242a1a06-2eb7-4cf1-ab12-69fbe0cc808b
2024-03-21 06:04:28,495 - distributed.worker - INFO - Starting Worker plugin PreImport-27f4ca93-c42e-4b05-ab23-f1551f4b6673
2024-03-21 06:04:28,496 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:28,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45223
2024-03-21 06:04:28,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45223
2024-03-21 06:04:28,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37169
2024-03-21 06:04:28,525 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,525 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,525 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:28,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cjb085f5
2024-03-21 06:04:28,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb18228d-f9df-4f62-8b40-205f67573bfb
2024-03-21 06:04:28,525 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdde01a3-9d54-47c9-9fd0-b098b43d0edc
2024-03-21 06:04:28,525 - distributed.worker - INFO - Starting Worker plugin PreImport-1da942d8-8eb3-4d4c-97eb-c42d973c4616
2024-03-21 06:04:28,525 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,573 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:04:28,574 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,574 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46501
2024-03-21 06:04:28,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:04:28,586 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,586 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46501
2024-03-21 06:04:28,594 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:28,595 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38751
2024-03-21 06:04:28,595 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38751
2024-03-21 06:04:28,595 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46203
2024-03-21 06:04:28,595 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,595 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,595 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:28,595 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lp2an1ug
2024-03-21 06:04:28,596 - distributed.worker - INFO - Starting Worker plugin RMMSetup-422fada4-f5ce-4d27-a4d9-ae8d35a4f747
2024-03-21 06:04:28,596 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a2a6b08-d52c-47ab-9c1e-3f908a4a0dbd
2024-03-21 06:04:28,596 - distributed.worker - INFO - Starting Worker plugin PreImport-2f26803d-3692-475e-bc6d-34eec40bb0ae
2024-03-21 06:04:28,596 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,604 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:28,605 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44897
2024-03-21 06:04:28,605 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44897
2024-03-21 06:04:28,605 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40435
2024-03-21 06:04:28,605 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,605 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,605 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:28,605 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:04:28,605 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w9eyq9de
2024-03-21 06:04:28,606 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44effb13-5bb9-43df-8eb8-5887b38b2202
2024-03-21 06:04:28,606 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5a2d32f-b247-4990-8a8c-ea772968a238
2024-03-21 06:04:28,606 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,606 - distributed.worker - INFO - Starting Worker plugin PreImport-dbe63bf1-6681-45da-93e9-a55c6cbb51f2
2024-03-21 06:04:28,606 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,606 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46501
2024-03-21 06:04:28,673 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:04:28,674 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,674 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,675 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46501
2024-03-21 06:04:28,680 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:04:28,681 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,681 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46501
2024-03-21 06:04:28,694 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:28,695 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42457
2024-03-21 06:04:28,695 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42457
2024-03-21 06:04:28,695 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39853
2024-03-21 06:04:28,695 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,695 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,695 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:28,695 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oreq6554
2024-03-21 06:04:28,695 - distributed.worker - INFO - Starting Worker plugin PreImport-b771a100-6163-4d39-be9a-b84c3d9a083e
2024-03-21 06:04:28,695 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85e797be-73c6-45b3-8bb1-8bf54f70744b
2024-03-21 06:04:28,696 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f722066-80f1-4106-8e6c-5368aa2ecf81
2024-03-21 06:04:28,696 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:28,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42077
2024-03-21 06:04:28,707 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42077
2024-03-21 06:04:28,707 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38865
2024-03-21 06:04:28,707 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,707 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,707 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:28,707 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hv2ge7v0
2024-03-21 06:04:28,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1dd9e555-7ef2-4e98-9ffd-77e1e57f47c8
2024-03-21 06:04:28,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe4d5092-d42c-4690-ba00-66f0caff3499
2024-03-21 06:04:28,710 - distributed.worker - INFO - Starting Worker plugin PreImport-47bebb13-3e94-479b-a342-f1b97123041c
2024-03-21 06:04:28,710 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,765 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:04:28,766 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,766 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,767 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46501
2024-03-21 06:04:28,782 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:04:28,783 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46501
2024-03-21 06:04:28,783 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:28,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46501
2024-03-21 06:04:28,828 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 06:04:28,828 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 06:04:28,828 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 06:04:28,828 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 06:04:28,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 06:04:28,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 06:04:28,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 06:04:28,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 06:04:28,834 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38205. Reason: nanny-close
2024-03-21 06:04:28,834 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45223. Reason: nanny-close
2024-03-21 06:04:28,836 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41055. Reason: nanny-close
2024-03-21 06:04:28,836 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38369. Reason: nanny-close
2024-03-21 06:04:28,836 - distributed.core - INFO - Connection to tcp://127.0.0.1:46501 has been closed.
2024-03-21 06:04:28,837 - distributed.core - INFO - Connection to tcp://127.0.0.1:46501 has been closed.
2024-03-21 06:04:28,837 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44897. Reason: nanny-close
2024-03-21 06:04:28,838 - distributed.core - INFO - Connection to tcp://127.0.0.1:46501 has been closed.
2024-03-21 06:04:28,838 - distributed.core - INFO - Connection to tcp://127.0.0.1:46501 has been closed.
2024-03-21 06:04:28,838 - distributed.nanny - INFO - Worker closed
2024-03-21 06:04:28,838 - distributed.nanny - INFO - Worker closed
2024-03-21 06:04:28,839 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42457. Reason: nanny-close
2024-03-21 06:04:28,839 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42077. Reason: nanny-close
2024-03-21 06:04:28,839 - distributed.core - INFO - Connection to tcp://127.0.0.1:46501 has been closed.
2024-03-21 06:04:28,839 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38751. Reason: nanny-close
2024-03-21 06:04:28,839 - distributed.nanny - INFO - Worker closed
2024-03-21 06:04:28,839 - distributed.nanny - INFO - Worker closed
2024-03-21 06:04:28,840 - distributed.nanny - INFO - Worker closed
2024-03-21 06:04:28,840 - distributed.core - INFO - Connection to tcp://127.0.0.1:46501 has been closed.
2024-03-21 06:04:28,841 - distributed.core - INFO - Connection to tcp://127.0.0.1:46501 has been closed.
2024-03-21 06:04:28,841 - distributed.core - INFO - Connection to tcp://127.0.0.1:46501 has been closed.
2024-03-21 06:04:28,842 - distributed.nanny - INFO - Worker closed
2024-03-21 06:04:28,842 - distributed.nanny - INFO - Worker closed
2024-03-21 06:04:28,842 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool 2024-03-21 06:04:35,887 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:04:35,891 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async 2024-03-21 06:04:45,180 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
2024-03-21 06:04:45,185 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size 2024-03-21 06:04:50,072 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
2024-03-21 06:04:50,076 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging 2024-03-21 06:04:55,141 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:04:55,147 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-03-21 06:04:59,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:04:59,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:04:59,274 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:04:59,274 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36245
2024-03-21 06:04:59,275 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36245
2024-03-21 06:04:59,275 - distributed.worker - INFO -           Worker name:                          0
2024-03-21 06:04:59,275 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34803
2024-03-21 06:04:59,275 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38989
2024-03-21 06:04:59,275 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:04:59,275 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:04:59,275 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 06:04:59,275 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v2qclag1
2024-03-21 06:04:59,275 - distributed.worker - INFO - Starting Worker plugin PreImport-b5b39be9-5e64-4b62-b77e-581794c70994
2024-03-21 06:04:59,278 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-03-21 06:04:59,278 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78e35126-3f30-451b-9d8f-61c5ce7aa3ab
2024-03-21 06:04:59,279 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a1a39f8-6867-4c47-987b-14fd1dcdb0cc
2024-03-21 06:04:59,279 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36245. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-03-21 06:04:59,279 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 06:04:59,280 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-03-21 06:05:03,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:05:03,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:05:03,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:05:03,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:05:03,132 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:05:03,132 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:05:03,138 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:05:03,138 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:05:03,157 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:05:03,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:05:03,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:05:03,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:05:03,201 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:05:03,201 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:05:03,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:05:03,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:05:03,711 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:05:03,712 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40243
2024-03-21 06:05:03,712 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40243
2024-03-21 06:05:03,712 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42409
2024-03-21 06:05:03,712 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,712 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,712 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:05:03,713 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:05:03,713 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1phzzt9p
2024-03-21 06:05:03,713 - distributed.worker - INFO - Starting Worker plugin RMMSetup-927f7b55-21cf-4f64-b1b5-d9f06b53be8a
2024-03-21 06:05:03,713 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7879cc18-aec3-4f14-9ea5-315455eba04a
2024-03-21 06:05:03,713 - distributed.worker - INFO - Starting Worker plugin PreImport-d31eebd9-d8de-4ad0-8e69-98ed96cb10ef
2024-03-21 06:05:03,713 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:05:03,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41935
2024-03-21 06:05:03,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41935
2024-03-21 06:05:03,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46349
2024-03-21 06:05:03,764 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,764 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,764 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:05:03,764 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:05:03,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l6ymhkcg
2024-03-21 06:05:03,764 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86e767ba-b418-45c5-a452-19afd03083b6
2024-03-21 06:05:03,764 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f0e0d2e-fbf1-4223-879a-d4c541c3c251
2024-03-21 06:05:03,764 - distributed.worker - INFO - Starting Worker plugin PreImport-ca19839a-cdb4-4a32-a505-73f0aed8ebcd
2024-03-21 06:05:03,764 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,768 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:05:03,769 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,769 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,770 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32943
2024-03-21 06:05:03,780 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:05:03,781 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36007
2024-03-21 06:05:03,781 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36007
2024-03-21 06:05:03,781 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43927
2024-03-21 06:05:03,781 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,781 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,781 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:05:03,781 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:05:03,781 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bu76kzfg
2024-03-21 06:05:03,781 - distributed.worker - INFO - Starting Worker plugin PreImport-d3956316-5ed1-49cc-9549-839f486de803
2024-03-21 06:05:03,781 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63c4c41d-7565-4aa4-9034-22266440d8ab
2024-03-21 06:05:03,782 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efcc10a3-5cdb-4ff1-b5d9-dd5584e49529
2024-03-21 06:05:03,782 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,794 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:05:03,795 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34669
2024-03-21 06:05:03,795 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34669
2024-03-21 06:05:03,795 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42625
2024-03-21 06:05:03,795 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,795 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,795 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:05:03,795 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:05:03,795 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0bza3dq6
2024-03-21 06:05:03,796 - distributed.worker - INFO - Starting Worker plugin PreImport-c9f8784f-17cd-4ce9-a7ec-978956a6ec38
2024-03-21 06:05:03,796 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1967821-68e8-4c17-8317-c0fa975ee52a
2024-03-21 06:05:03,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-932a632d-f16c-42fe-b040-879f54ce6dbc
2024-03-21 06:05:03,796 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:05:03,811 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44727
2024-03-21 06:05:03,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44727
2024-03-21 06:05:03,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41381
2024-03-21 06:05:03,811 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,811 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,811 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:05:03,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:05:03,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-85p2hnwx
2024-03-21 06:05:03,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cdb6572c-1ce7-4fda-9696-3092254a9d16
2024-03-21 06:05:03,812 - distributed.worker - INFO - Starting Worker plugin PreImport-6faef5bf-f759-416d-b641-3cbf0aa7337f
2024-03-21 06:05:03,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b340a4b9-169f-4251-9f5a-c77ce85cafd7
2024-03-21 06:05:03,812 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,822 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:05:03,823 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43723
2024-03-21 06:05:03,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43723
2024-03-21 06:05:03,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44865
2024-03-21 06:05:03,823 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,823 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,823 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:05:03,823 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:05:03,823 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9k4fnmbx
2024-03-21 06:05:03,823 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:05:03,824 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e3f8b5a-f330-4126-8ee3-2ece8a5fefc8
2024-03-21 06:05:03,824 - distributed.worker - INFO - Starting Worker plugin PreImport-1807d1ec-8aae-4868-a2e2-5ed6e652cb40
2024-03-21 06:05:03,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c30780d6-130a-4bf1-b656-d7a95b51fc0d
2024-03-21 06:05:03,824 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35329
2024-03-21 06:05:03,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35329
2024-03-21 06:05:03,825 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35925
2024-03-21 06:05:03,825 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,825 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,825 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:05:03,825 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:05:03,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ggunzop
2024-03-21 06:05:03,826 - distributed.worker - INFO - Starting Worker plugin PreImport-1b99495d-8750-4b20-9b3f-4093c569aaa4
2024-03-21 06:05:03,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-882baa12-5465-4e7b-bd94-c3b37ecc27cf
2024-03-21 06:05:03,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-416eb6b2-6647-479f-a81c-717b285aa920
2024-03-21 06:05:03,827 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:05:03,841 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,841 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32943
2024-03-21 06:05:03,900 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:05:03,901 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45261
2024-03-21 06:05:03,901 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45261
2024-03-21 06:05:03,901 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35809
2024-03-21 06:05:03,901 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,901 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,901 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:05:03,901 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:05:03,901 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nt6uoeza
2024-03-21 06:05:03,901 - distributed.worker - INFO - Starting Worker plugin RMMSetup-228c1a44-24fa-4e3d-9b9a-b83f66b9ca90
2024-03-21 06:05:03,902 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79b5b700-c264-4b7c-9ea1-43801a618912
2024-03-21 06:05:03,902 - distributed.worker - INFO - Starting Worker plugin PreImport-00d0a7fa-599b-4df6-8e12-55410c3704d4
2024-03-21 06:05:03,902 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:05:03,936 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,936 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32943
2024-03-21 06:05:03,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:05:03,962 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,962 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32943
2024-03-21 06:05:03,987 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:05:03,988 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32943
2024-03-21 06:05:03,988 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:03,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32943
2024-03-21 06:05:03,999 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:05:04,000 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32943
2024-03-21 06:05:04,000 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:04,001 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32943
2024-03-21 06:05:04,004 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:05:04,004 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32943
2024-03-21 06:05:04,005 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:04,006 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32943
2024-03-21 06:05:04,018 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:05:04,019 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32943
2024-03-21 06:05:04,019 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:05:04,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32943
2024-03-21 06:05:04,050 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41935. Reason: nanny-close
2024-03-21 06:05:04,051 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40243. Reason: nanny-close
2024-03-21 06:05:04,051 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36007. Reason: nanny-close
2024-03-21 06:05:04,052 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44727. Reason: nanny-close
2024-03-21 06:05:04,052 - distributed.core - INFO - Connection to tcp://127.0.0.1:32943 has been closed.
2024-03-21 06:05:04,053 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34669. Reason: nanny-close
2024-03-21 06:05:04,053 - distributed.core - INFO - Connection to tcp://127.0.0.1:32943 has been closed.
2024-03-21 06:05:04,053 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43723. Reason: nanny-close
2024-03-21 06:05:04,054 - distributed.core - INFO - Connection to tcp://127.0.0.1:32943 has been closed.
2024-03-21 06:05:04,054 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45261. Reason: nanny-close
2024-03-21 06:05:04,054 - distributed.nanny - INFO - Worker closed
2024-03-21 06:05:04,054 - distributed.core - INFO - Connection to tcp://127.0.0.1:32943 has been closed.
2024-03-21 06:05:04,054 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35329. Reason: nanny-close
2024-03-21 06:05:04,055 - distributed.nanny - INFO - Worker closed
2024-03-21 06:05:04,055 - distributed.core - INFO - Connection to tcp://127.0.0.1:32943 has been closed.
2024-03-21 06:05:04,055 - distributed.core - INFO - Connection to tcp://127.0.0.1:32943 has been closed.
2024-03-21 06:05:04,055 - distributed.nanny - INFO - Worker closed
2024-03-21 06:05:04,056 - distributed.nanny - INFO - Worker closed
2024-03-21 06:05:04,056 - distributed.core - INFO - Connection to tcp://127.0.0.1:32943 has been closed.
2024-03-21 06:05:04,056 - distributed.nanny - INFO - Worker closed
2024-03-21 06:05:04,056 - distributed.core - INFO - Connection to tcp://127.0.0.1:32943 has been closed.
2024-03-21 06:05:04,056 - distributed.nanny - INFO - Worker closed
2024-03-21 06:05:04,058 - distributed.nanny - INFO - Worker closed
2024-03-21 06:05:04,058 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-03-21 06:05:10,938 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:05:10,941 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration 2024-03-21 06:05:13,781 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:05:13,785 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-03-21 06:05:16,085 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:05:16,088 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] 2024-03-21 06:05:19,148 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 06:05:19,151 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] 2024-03-21 06:05:23,410 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:05:23,418 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f446365e2b0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:05:25,422 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-03-21 06:05:43,543 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:05:43,551 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:37968 remote=tcp://127.0.0.1:37165>: Stream is closed
2024-03-21 06:05:43,554 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7eb4e65280>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:05:45,557 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-03-21 06:06:04,080 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:06:04,087 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fc680ead2b0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:06:06,091 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-03-21 06:06:34,540 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:06:34,547 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f09943d02e0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:06:36,551 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-03-21 06:07:05,026 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:07:05,033 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f5c0b16b2e0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:07:07,037 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-03-21 06:07:35,533 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:07:35,540 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f4ff78bc2e0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:07:37,543 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-03-21 06:08:06,060 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:08:06,067 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f14c4d5e2e0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:08:08,071 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-03-21 06:08:36,392 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:08:36,399 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fb5565ac2b0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:08:38,403 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-03-21 06:09:10,690 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:09:10,694 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 12 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
