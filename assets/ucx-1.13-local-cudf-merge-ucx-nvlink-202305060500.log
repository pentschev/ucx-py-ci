2023-05-06 06:57:17,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:17,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:17,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:17,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:17,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:17,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:17,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:17,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:17,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:17,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:17,229 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:17,229 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:17,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:17,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:17,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:17,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:71442:0:71442] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71442) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f9a80f4311c]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x302ff) [0x7f9a80f432ff]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x30634) [0x7f9a80f43634]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f9b121e7420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x1b) [0x7f9a80fc428b]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f9a80fee0b8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x216a7) [0x7f9a80ef56a7]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21c28) [0x7f9a80ef5c28]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x240fc) [0x7f9a80ef80fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7f9a80f4d639]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f9a80ef81ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f9a80fc0f1a]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7f9a810706e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55e8effdfb08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55e8effd0112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55e8effc927a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55e8effdac05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e8effca81b]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55e8effef70e]
19  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f9a91c6f2fe]
20  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55e8effd32bc]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55e8eff86817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55e8effd1f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55e8effcfd36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55e8effdaef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e8effca81b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55e8effdaef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e8effca81b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55e8effdaef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e8effca81b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55e8effdaef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e8effca81b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55e8effc927a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55e8effdac05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55e8effcefa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55e8effc927a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55e8effe8935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55e8effe9104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55e8f00affc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55e8effd32bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55e8effce1bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55e8effdaef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55e8effe8c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55e8effce1bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55e8effdaef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e8effca81b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55e8effc927a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55e8effdac05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55e8effca81b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55e8effdaef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55e8effca568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55e8effc927a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55e8effdac05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55e8effcb3cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55e8effc927a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55e8effc8f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55e8effc8eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55e8f00798bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55e8f00a7adc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55e8f00a3c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55e8f009b7ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55e8f009b6bd]
=================================
2023-05-06 06:57:24,767 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:55163 -> ucx://127.0.0.1:53529
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f94a25b9100, tag: 0xfcd93b192b9b1614, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-06 06:57:24,842 - distributed.nanny - WARNING - Restarting worker
[dgx13:71449:0:71449] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71449) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7fcbf3e4b11c]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x302ff) [0x7fcbf3e4b2ff]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x30634) [0x7fcbf3e4b634]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fccbf110420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x1b) [0x7fcbf3ecc28b]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7fcbf3ef60b8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x216a7) [0x7fcbf3dfd6a7]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21c28) [0x7fcbf3dfdc28]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x240fc) [0x7fcbf3e000fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7fcbf3e55639]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7fcbf3e001ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7fcbf3ec8f1a]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7fcbf3f786e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x561be3890b08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x561be3881112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561be387a27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561be388bc05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561be387b81b]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x561be38a070e]
19  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fcc3eb972fe]
20  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x561be38842bc]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x561be3837817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x561be3882f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x561be3880d36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561be388bef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561be387b81b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561be388bef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561be387b81b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561be388bef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561be387b81b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561be388bef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561be387b81b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561be387a27a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561be388bc05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x561be387ffa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561be387a27a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x561be3899935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x561be389a104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x561be3960fc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x561be38842bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x561be387f1bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561be388bef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x561be3899c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x561be387f1bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561be388bef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561be387b81b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561be387a27a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561be388bc05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x561be387b81b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x561be388bef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x561be387b568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561be387a27a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x561be388bc05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x561be387c3cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x561be387a27a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x561be3879f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x561be3879eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x561be392a8bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x561be3958adc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x561be3954c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x561be394c7ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x561be394c6bd]
=================================
[dgx13:71446:0:71446] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71446) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7ff539efb11c]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x302ff) [0x7ff539efb2ff]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x30634) [0x7ff539efb634]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7ff5dd1d7420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x1b) [0x7ff539f7c28b]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7ff539fa60b8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x216a7) [0x7ff54c0386a7]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21c28) [0x7ff54c038c28]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x240fc) [0x7ff54c03b0fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7ff539f05639]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7ff54c03b1ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7ff539f78f1a]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7ff54c07e6e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5560c48d4b08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5560c48c5112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5560c48be27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5560c48cfc05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5560c48bf81b]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5560c48e470e]
19  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7ff5d000c2fe]
20  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5560c48c82bc]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5560c487b817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5560c48c6f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5560c48c4d36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5560c48cfef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5560c48bf81b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5560c48cfef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5560c48bf81b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5560c48cfef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5560c48bf81b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5560c48cfef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5560c48bf81b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5560c48be27a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5560c48cfc05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5560c48c3fa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5560c48be27a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5560c48dd935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5560c48de104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5560c49a4fc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5560c48c82bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5560c48c31bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5560c48cfef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5560c48ddc72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5560c48c31bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5560c48cfef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5560c48bf81b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5560c48be27a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5560c48cfc05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5560c48bf81b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5560c48cfef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5560c48bf568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5560c48be27a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5560c48cfc05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5560c48c03cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5560c48be27a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5560c48bdf07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5560c48bdeb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5560c496e8bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5560c499cadc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5560c4998c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5560c49907ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5560c49906bd]
=================================
[dgx13:71441:0:71441] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71441) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f94a311211c]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x302ff) [0x7f94a31122ff]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x30634) [0x7f94a3112634]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f95423bf420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x1b) [0x7f94a319328b]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f94a31bd0b8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x216a7) [0x7f94a30c46a7]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21c28) [0x7f94a30c4c28]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x240fc) [0x7f94a30c70fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7f94a311c639]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f94a30c71ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f94a318ff1a]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7f94a323f6e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5631558ecb08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5631558dd112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631558d627a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5631558e7c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631558d781b]
18  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631558e7ef3]
19  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x5631558f5a16]
20  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x563155a059b1]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x563155893817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5631558def83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5631558dcd36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631558e7ef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631558d781b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631558e7ef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631558d781b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631558e7ef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631558d781b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631558e7ef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631558d781b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631558d627a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5631558e7c05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5631558dbfa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631558d627a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5631558f5935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5631558f6104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5631559bcfc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5631558e02bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5631558db1bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631558e7ef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5631558f5c72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5631558db1bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631558e7ef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631558d781b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631558d627a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5631558e7c05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631558d781b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631558e7ef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5631558d7568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631558d627a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5631558e7c05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5631558d83cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631558d627a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5631558d5f07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5631558d5eb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5631559868bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5631559b4adc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5631559b0c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5631559a87ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5631559a86bd]
=================================
2023-05-06 06:57:25,767 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44397
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fb6e818d140, tag: 0x5d5f7b86c5521728, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fb6e818d140, tag: 0x5d5f7b86c5521728, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-05-06 06:57:25,768 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:36575 -> ucx://127.0.0.1:44397
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f896144b140, tag: 0xa40fd213872f9ba3, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-06 06:57:25,769 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52725 -> ucx://127.0.0.1:44397
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fb6e818d100, tag: 0xd01232669111622a, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-06 06:57:25,768 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44397
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fc5864751c0, tag: 0x5320d4cef4bb6ee9, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fc5864751c0, tag: 0x5320d4cef4bb6ee9, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-06 06:57:25,773 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44397
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-05-06 06:57:25,819 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52531
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fc586475300, tag: 0x828ae62f4fc345c5, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fc586475300, tag: 0x828ae62f4fc345c5, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-05-06 06:57:25,820 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58973 -> ucx://127.0.0.1:52531
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fc586475140, tag: 0x3f0e4ccb73a64025, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-06 06:57:25,824 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52531
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-05-06 06:57:25,845 - distributed.nanny - WARNING - Restarting worker
Task exception was never retrieved
future: <Task finished name='Task-1027' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Endpoint timeout')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Endpoint timeout
2023-05-06 06:57:25,850 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58641 -> ucx://127.0.0.1:52531
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 315, in write
    await self.ep.send(struct.pack("?Q", False, nframes))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 629, in send
    self._ep.raise_on_error()
  File "ucp/_libs/ucx_endpoint.pyx", line 355, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXError: Endpoint 0x7ff1e4f27180 error: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-06 06:57:25,865 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55163
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f896144b2c0, tag: 0xb50228471b9d5b1, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f896144b2c0, tag: 0xb50228471b9d5b1, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-06 06:57:25,890 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55163
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7fb6e818d180, tag: 0xcaad0d4f0482794, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7fb6e818d180, tag: 0xcaad0d4f0482794, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-06 06:57:25,920 - distributed.nanny - WARNING - Restarting worker
2023-05-06 06:57:25,940 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55163
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 469, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7fc586475200, tag: 0x9799aad3987e9e44, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:55163 after 30 s
2023-05-06 06:57:25,962 - distributed.nanny - WARNING - Restarting worker
2023-05-06 06:57:26,365 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:26,365 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:26,434 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:52531
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1022, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 289, in write
    raise CommClosedError("Endpoint is closed -- unable to send message")
distributed.comm.core.CommClosedError: Endpoint is closed -- unable to send message
2023-05-06 06:57:26,435 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:58641 -> ucx://127.0.0.1:55163
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 289, in write
    raise CommClosedError("Endpoint is closed -- unable to send message")
distributed.comm.core.CommClosedError: Endpoint is closed -- unable to send message
2023-05-06 06:57:26,443 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:55163
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/comm.py", line 79, in stream_send
    return _call_ucx_api(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/comm.py", line 31, in _call_ucx_api
    req = func(*args, **kwargs)
  File "ucp/_libs/transfer_stream.pyx", line 64, in ucp._libs.ucx_api.stream_send_nb
  File "ucp/_libs/ucx_endpoint.pyx", line 355, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXError: Endpoint 0x7ff1e4f27280 error: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 468, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
[dgx13:71452:0:71452] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71452) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7ff1e5a9e11c]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x302ff) [0x7ff1e5a9e2ff]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x30634) [0x7ff1e5a9e634]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7ff29ed2d420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x1b) [0x7ff1e5b1f28b]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7ff1e5b490b8]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x216a7) [0x7ff1e5a506a7]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21c28) [0x7ff1e5a50c28]
 8  /opt/conda/envs/gdf/lib/libuct.so.0(+0x240fc) [0x7ff1e5a530fc]
 9  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xf9) [0x7ff1e5aa8639]
10  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7ff1e5a531ab]
11  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7ff1e5b1bf1a]
12  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x286e1) [0x7ff1e5bcb6e1]
13  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x562be3604b08]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x562be35f5112]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562be35ee27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x562be35ffc05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562be35ef81b]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x562be361470e]
19  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7ff21e7bd2fe]
20  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x562be35f82bc]
21  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x562be35ab817]
22  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x562be35f6f83]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x562be35f4d36]
24  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562be35ffef3]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562be35ef81b]
26  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562be35ffef3]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562be35ef81b]
28  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562be35ffef3]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562be35ef81b]
30  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562be35ffef3]
31  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562be35ef81b]
32  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562be35ee27a]
33  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x562be35ffc05]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x562be35f3fa7]
35  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562be35ee27a]
36  /opt/conda/envs/gdf/bin/python(+0x147935) [0x562be360d935]
37  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x562be360e104]
38  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x562be36d4fc8]
39  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x562be35f82bc]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x562be35f31bb]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562be35ffef3]
42  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x562be360dc72]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x562be35f31bb]
44  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562be35ffef3]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562be35ef81b]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562be35ee27a]
47  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x562be35ffc05]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x562be35ef81b]
49  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x562be35ffef3]
50  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x562be35ef568]
51  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562be35ee27a]
52  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x562be35ffc05]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x562be35f03cb]
54  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x562be35ee27a]
55  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x562be35edf07]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x562be35edeb9]
57  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x562be369e8bb]
58  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x562be36ccadc]
59  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x562be36c8c24]
60  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x562be36c07ed]
61  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x562be36c06bd]
=================================
2023-05-06 06:57:26,696 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:52725 -> ucx://127.0.0.1:58641
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fb6e818d380, tag: 0x6ad99e989acf130e, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-05-06 06:57:26,696 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58641
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fc5864752c0, tag: 0xe3503106859721d3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fc5864752c0, tag: 0xe3503106859721d3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-05-06 06:57:26,696 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58641
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f896144b240, tag: 0x957357be2e9578df, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f896144b240, tag: 0x957357be2e9578df, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-05-06 06:57:26,697 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58641
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fb6e818d200, tag: 0xd605242138972c6f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fb6e818d200, tag: 0xd605242138972c6f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-05-06 06:57:26,782 - distributed.nanny - WARNING - Restarting worker
terminate called after throwing an instance of 'rmm::out_of_memory'
  what():  std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-06 06:57:27,355 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:27,355 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:27,437 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:27,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:27,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:27,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:27,599 - distributed.nanny - WARNING - Restarting worker
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 742, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Exception ignored in: 'cupy.cuda.thrust.cupy_malloc'
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 742, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-06 06:57:28,257 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-5366301a596b755bb8c33997d3ce7726', 3)
Function:  generate_chunk
args:      (3, 100000000, 8, 'build', 0.3, True)
kwargs:    {}
Exception: "RuntimeError('transform: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 742, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Exception ignored in: 'cupy.cuda.thrust.cupy_malloc'
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 742, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-05-06 06:57:28,325 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-3393696d3757f92da87d0dddd0c6ae01', 1)
Function:  generate_chunk
args:      (1, 100000000, 8, 'other', 0.3, True)
kwargs:    {}
Exception: "RuntimeError('transform: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-05-06 06:57:28,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:28,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:29,161 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-06 06:57:29,161 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-06 06:57:31,220 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 330, in write
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,221 - distributed.core - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,221 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 830, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 987, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 330, in write
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,221 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,230 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1049, in send_recv
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 830, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 987, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 330, in write
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,236 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 330, in write
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,236 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 830, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 987, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 330, in write
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,242 - distributed.core - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,242 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,246 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 330, in write
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,246 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 830, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 987, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 330, in write
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,255 - distributed.core - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
2023-05-06 06:57:31,255 - distributed.worker - ERROR - [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 387, in read
    synchronize_stream(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 94, in synchronize_stream
    stream.synchronize()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 2230, in synchronize
    driver.cuStreamSynchronize(self.handle)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [700] Call to cuStreamSynchronize results in UNKNOWN_CUDA_ERROR
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
